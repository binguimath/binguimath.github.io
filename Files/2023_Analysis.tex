% !TeX spellcheck = en_US
% !TEX program = pdflatex
\documentclass[12pt,b5paper,notitlepage]{article}
\usepackage[b5paper, margin={0.5in,0.65in}]{geometry}
%\usepackage{fullpage}
\usepackage{amsmath,amscd,amssymb,amsthm,mathrsfs,amsfonts,layout,indentfirst,graphicx,caption,mathabx, stmaryrd,appendix,calc,imakeidx,upgreek} % mathabx for \wtidecheck
%\usepackage{ulem} %wave underline
\usepackage[dvipsnames]{xcolor}
\usepackage{palatino}  %template
\usepackage{slashed} % Dirac operator
\usepackage{mathrsfs} % Enable using \mathscr
%\usepackage{eufrak}  another template/font
\usepackage{extarrows} % long equal sign, \xlongequal{blablabla}
\usepackage{enumitem} % enumerate label change e.g. [label=(\alph*)]  shows (a) (b) 




\usepackage{CJK}   % Chinese package





\usepackage{csquotes} % \begin{displayquote}   \begin{displaycquote}  for quotation
\usepackage{epigraph}   %\epigraph{}{}  for quotation
%\pmb  mandatory math bold 

\usepackage{fancyhdr} % date in footer

%\usepackage{soul}  %\ul underline break line automatically

\usepackage{ulem}  % \uline  underline break line   also    \uwave

\usepackage{relsize} % use \mathlarger \larger \text{\larger[2]$...$} to enlarge the size of math symbols

\usepackage{verbatim}  % comment environment


\usepackage{halloweenmath} % Interesting halloween math symbols

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{tcolorbox}
\tcbuselibrary{theorems}
% box around equations   \tcboxmath
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% circled colon and thick colon \hcolondel and \colondel

\usepackage{pdfrender}

\newcommand*{\hollowcolon}{%
	\textpdfrender{
		TextRenderingMode=Stroke,
		LineWidth=.1bp,
	}{:}%
}

\newcommand{\hcolondel}[1]{%
	\mathopen{\hollowcolon}#1\mathclose{\hollowcolon}%
}
\newcommand{\colondel}[1]{%
	\mathopen{:}#1\mathclose{:}%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\usepackage{tikz}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

\usepackage{tikz-cd}
\usepackage[nottoc]{tocbibind}   % Add  reference to ToC


\makeindex


% The following set up the line spaces between items in \thebibliography
\usepackage{lipsum}  
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
	\OLDthebibliography{#1}
	\setlength{\parskip}{0pt}
	\setlength{\itemsep}{2pt} 
}


%\hyperref{page.10}{...}

\allowdisplaybreaks  %allow aligns to break between pages
\usepackage{latexsym}
\usepackage{chngcntr}
\usepackage[colorlinks,linkcolor=blue,anchorcolor=blue, linktocpage,
%pagebackref
]{hyperref}
\hypersetup{ urlcolor=cyan,
	citecolor=[rgb]{0,0.5,0}}


\setcounter{tocdepth}{2}	 %hide subsections in the content


\counterwithin{figure}{section}

\counterwithin*{footnote}{section}   % Footnote numbering is recounted from the beginning of each subsection

\pagestyle{plain}

\captionsetup[figure]
{
	labelsep=none	
}













\theoremstyle{definition}
\newtheorem{df}{Definition}[section]
\newtheorem{eg}[df]{Example}
\newtheorem{exe}[df]{Exercise}
\newtheorem{rem}[df]{Remark}
\newtheorem{obs}[df]{Observation}
\newtheorem{ass}[df]{Assumption}
\newtheorem{cv}[df]{Convention}
\newtheorem{prin}[df]{Principle}
\newtheorem{nota}[df]{Notation}
\newtheorem*{axiom}{Axiom}
\newtheorem{coa}[df]{Theorem}
\newtheorem{srem}[df]{$\star$ Remark}
\newtheorem{seg}[df]{$\star$ Example}
\newtheorem{sexe}[df]{$\star$ Exercise}
\newtheorem{sdf}[df]{$\star$ Definition}




\newtheorem{prob}{\color{red}Problem}[section]
%\renewcommand*{\theprob}{{\color{red}\arabic{section}.\arabic{prob}}}
\newtheorem{sprob}[prob]{\color{red}$\star$ Problem}
%\renewcommand*{\thesprob}{{\color{red}\arabic{section}.\arabic{sprob}}}
% \newtheorem{ssprob}[prob]{$\star\star$ Problem}



\theoremstyle{plain}
\newtheorem{thm}[df]{Theorem}
\newtheorem{ccl}[df]{Conclusion}
\newtheorem{thd}[df]{Theorem-Definition}
\newtheorem{pp}[df]{Proposition}
\newtheorem{co}[df]{Corollary}
\newtheorem{lm}[df]{Lemma}
\newtheorem{sthm}[df]{$\star$ Theorem}
\newtheorem{slm}[df]{$\star$ Lemma}
\newtheorem{claim}[df]{Claim}
\newtheorem{spp}[df]{$\star$ Proposition}
\newtheorem{scorollary}[df]{$\star$ Corollary}


\newtheorem{cond}{Condition}
\newtheorem{Mthm}{Main Theorem}
\renewcommand{\thecond}{\Alph{cond}} % "letter-numbered" theorems
\renewcommand{\theMthm}{\Alph{Mthm}} % "letter-numbered" theorems


%\substack   multiple lines under sum
%\underset{b}{a}   b is under a


% Remind: \overline{L_0}



\usepackage{calligra}
\DeclareMathOperator{\shom}{\mathscr{H}\text{\kern -3pt {\calligra\large om}}\,}
\DeclareMathOperator{\sext}{\mathscr{E}\text{\kern -3pt {\calligra\large xt}}\,}
\DeclareMathOperator{\Rel}{\mathscr{R}\text{\kern -3pt {\calligra\large el}~}\,}
\DeclareMathOperator{\sann}{\mathscr{A}\text{\kern -3pt {\calligra\large nn}}\,}
\DeclareMathOperator{\send}{\mathscr{E}\text{\kern -3pt {\calligra\large nd}}\,}
\DeclareMathOperator{\stor}{\mathscr{T}\text{\kern -3pt {\calligra\large or}}\,}

\usepackage{aurical}
\DeclareMathOperator{\VVir}{\text{\Fontlukas V}\text{\kern -0pt {\Fontlukas\large ir}}\,}






\newcommand{\fk}{\mathfrak}
\newcommand{\mc}{\mathcal}
\newcommand{\wtd}{\widetilde}
\newcommand{\wht}{\widehat}
\newcommand{\wch}{\widecheck}
\newcommand{\ovl}{\overline}
\newcommand{\udl}{\underline}
\newcommand{\tr}{\mathrm{t}} %transpose
\newcommand{\Tr}{\mathrm{Tr}}
\newcommand{\End}{\mathrm{End}} %endomorphism
\newcommand{\idt}{\mathbf{1}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\Conf}{\mathrm{Conf}}
\newcommand{\Res}{\mathrm{Res}}
\newcommand{\res}{\mathrm{res}}
\newcommand{\KZ}{\mathrm{KZ}}
\newcommand{\ev}{\mathrm{ev}}
\newcommand{\coev}{\mathrm{coev}}
\newcommand{\opp}{\mathrm{opp}}
\newcommand{\Rep}{\mathrm{Rep}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Dom}{\mathrm{Dom}}
\newcommand{\loc}{\mathrm{loc}}
\newcommand{\con}{\mathrm{c}}
\newcommand{\uni}{\mathrm{u}}
\newcommand{\ssp}{\mathrm{ss}}
\newcommand{\di}{\slashed d}
\newcommand{\Diffp}{\mathrm{Diff}^+}
\newcommand{\Diff}{\mathrm{Diff}}
\newcommand{\PSU}{\mathrm{PSU}(1,1)}
\newcommand{\Vir}{\mathrm{Vir}}
\newcommand{\Witt}{\mathscr W}
\newcommand{\Span}{\mathrm{Span}}
\newcommand{\pri}{\mathrm{p}}
\newcommand{\ER}{E^1(V)_{\mathbb R}}
\newcommand{\prth}[1]{( {#1})}
\newcommand{\bk}[1]{\langle {#1}\rangle}
\newcommand{\bigbk}[1]{\big\langle {#1}\big\rangle}
\newcommand{\Bigbk}[1]{\Big\langle {#1}\Big\rangle}
\newcommand{\biggbk}[1]{\bigg\langle {#1}\bigg\rangle}
\newcommand{\Biggbk}[1]{\Bigg\langle {#1}\Bigg\rangle}
\newcommand{\GA}{\mathscr G_{\mathcal A}}
\newcommand{\vs}{\varsigma}
\newcommand{\Vect}{\mathrm{Vec}}
\newcommand{\Vectc}{\mathrm{Vec}^{\mathbb C}}
\newcommand{\scr}{\mathscr}
\newcommand{\sjs}{\subset\joinrel\subset}
\newcommand{\Jtd}{\widetilde{\mathcal J}}
\newcommand{\gk}{\mathfrak g}
\newcommand{\hk}{\mathfrak h}
\newcommand{\xk}{\mathfrak x}
\newcommand{\yk}{\mathfrak y}
\newcommand{\zk}{\mathfrak z}
\newcommand{\pk}{\mathfrak p}
\newcommand{\hr}{\mathfrak h_{\mathbb R}}
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\DHR}{\mathrm{DHR}_{I_0}}
\newcommand{\Repi}{\mathrm{Rep}_{\wtd I_0}}
\newcommand{\im}{\mathbf{i}}
\newcommand{\Co}{\complement}
%\newcommand{\Cu}{\mathcal C^{\mathrm u}}
\newcommand{\RepV}{\mathrm{Rep}^\uni(V)}
\newcommand{\RepA}{\mathrm{Rep}(\mathcal A)}
\newcommand{\RepN}{\mathrm{Rep}(\mathcal N)}
\newcommand{\RepfA}{\mathrm{Rep}^{\mathrm f}(\mathcal A)}
\newcommand{\RepAU}{\mathrm{Rep}^\uni(A_U)}
\newcommand{\RepU}{\mathrm{Rep}^\uni(U)}
\newcommand{\RepL}{\mathrm{Rep}^{\mathrm{L}}}
\newcommand{\HomL}{\mathrm{Hom}^{\mathrm{L}}}
\newcommand{\EndL}{\mathrm{End}^{\mathrm{L}}}
\newcommand{\Bim}{\mathrm{Bim}}
\newcommand{\BimA}{\mathrm{Bim}^\uni(A)}
%\newcommand{\shom}{\scr Hom}
\newcommand{\divi}{\mathrm{div}}
\newcommand{\sgm}{\varsigma}
\newcommand{\SX}{{S_{\fk X}}}
\newcommand{\DX}{D_{\fk X}}
\newcommand{\mbb}{\mathbb}
\newcommand{\mbf}{\mathbf}
\newcommand{\bsb}{\boldsymbol}
\newcommand{\blt}{\bullet}
\newcommand{\Vbb}{\mathbb V}
\newcommand{\Ubb}{\mathbb U}
\newcommand{\Xbb}{\mathbb X}
\newcommand{\Kbb}{\mathbb K}
\newcommand{\Abb}{\mathbb A}
\newcommand{\Wbb}{\mathbb W}
\newcommand{\Mbb}{\mathbb M}
\newcommand{\Gbb}{\mathbb G}
\newcommand{\Cbb}{\mathbb C}
\newcommand{\Nbb}{\mathbb N}
\newcommand{\Zbb}{\mathbb Z}
\newcommand{\Qbb}{\mathbb Q}
\newcommand{\Pbb}{\mathbb P}
\newcommand{\Rbb}{\mathbb R}
\newcommand{\Ebb}{\mathbb E}
\newcommand{\Dbb}{\mathbb D}
\newcommand{\Hbb}{\mathbb H}
\newcommand{\cbf}{\mathbf c}
\newcommand{\Rbf}{\mathbf R}
\newcommand{\wt}{\mathrm{wt}}
\newcommand{\Lie}{\mathrm{Lie}}
\newcommand{\btl}{\blacktriangleleft}
\newcommand{\btr}{\blacktriangleright}
\newcommand{\svir}{\mathcal V\!\mathit{ir}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cok}{\mathrm{Coker}}
\newcommand{\Sbf}{\mathbf{S}}
\newcommand{\low}{\mathrm{low}}
\newcommand{\Sp}{\mathrm{Sp}}
\newcommand{\Rng}{\mathrm{Rng}}
\newcommand{\vN}{\mathrm{vN}}
\newcommand{\Ebf}{\mathbf E}
\newcommand{\Nbf}{\mathbf N}
\newcommand{\Stb}{\mathrm {Stb}}
\newcommand{\SXb}{{S_{\fk X_b}}}
\newcommand{\pr}{\mathrm {pr}}
\newcommand{\SXtd}{S_{\wtd{\fk X}}}
\newcommand{\univ}{\mathrm {univ}}
\newcommand{\vbf}{\mathbf v}
\newcommand{\ubf}{\mathbf u}
\newcommand{\wbf}{\mathbf w}
\newcommand{\CB}{\mathrm{CB}}
\newcommand{\Perm}{\mathrm{Perm}}
\newcommand{\Orb}{\mathrm{Orb}}
\newcommand{\Lss}{{L_{0,\mathrm{s}}}}
\newcommand{\Lni}{{L_{0,\mathrm{n}}}}
\newcommand{\UPSU}{\widetilde{\mathrm{PSU}}(1,1)}
\newcommand{\Sbb}{{\mathbb S}}
\newcommand{\Gc}{\mathscr G_c}
\newcommand{\Obj}{\mathrm{Obj}}
\newcommand{\bpr}{{}^\backprime}
\newcommand{\fin}{\mathrm{fin}}
\newcommand{\Ann}{\mathrm{Ann}}
\newcommand{\Real}{\mathrm{Re}}
\newcommand{\Imag}{\mathrm{Im}}
%\newcommand{\cl}{\mathrm{cl}}
\newcommand{\Ind}{\mathrm{Ind}}
\newcommand{\Supp}{\mathrm{Supp}}
\newcommand{\Specan}{\mathrm{Specan}}
\newcommand{\red}{\mathrm{red}}
\newcommand{\uph}{\upharpoonright}
\newcommand{\Mor}{\mathrm{Mor}}
\newcommand{\pre}{\mathrm{pre}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Jac}{\mathrm{Jac}}
\newcommand{\emb}{\mathrm{emb}}
\newcommand{\Sg}{\mathrm{Sg}}
\newcommand{\Nzd}{\mathrm{Nzd}}
\newcommand{\Owht}{\widehat{\scr O}}
\newcommand{\Ext}{\mathrm{Ext}}
\newcommand{\Tor}{\mathrm{Tor}}
\newcommand{\Com}{\mathrm{Com}}
\newcommand{\Mod}{\mathrm{Mod}}
\newcommand{\nk}{\mathfrak n}
\newcommand{\mk}{\mathfrak m}
\newcommand{\Ass}{\mathrm{Ass}}
\newcommand{\depth}{\mathrm{depth}}
\newcommand{\Coh}{\mathrm{Coh}}
\newcommand{\Gode}{\mathrm{Gode}}
\newcommand{\Fbb}{\mathbb F}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Modf}{\mathrm{Mod}^{\mathrm f}}
\newcommand{\codim}{\mathrm{codim}}
\newcommand{\card}{\mathrm{card}}
\newcommand{\dps}{\displaystyle}
\newcommand{\Int}{\mathrm{Int}}
\newcommand{\Nbh}{\mathrm{Nbh}}
\newcommand{\Pnbh}{\mathrm{PNbh}}
\newcommand{\Cl}{\mathrm{Cl}}
\newcommand{\diam}{\mathrm{diam}}
\newcommand{\eps}{\varepsilon}
\newcommand{\Vol}{\mathrm{Vol}}






\usepackage{tipa} % wierd symboles e.g. \textturnh
\newcommand{\tipar}{\text{\textrtailr}}
\newcommand{\tipaz}{\text{\textctyogh}}
\newcommand{\tipaomega}{\text{\textcloseomega}}
\newcommand{\tipae}{\text{\textrhookschwa}}
\newcommand{\tipaee}{\text{\textreve}}
\newcommand{\tipak}{\text{\texthtk}}





\usepackage{tipx}
\newcommand{\tipxgamma}{\text{\textfrtailgamma}}
\newcommand{\tipxcc}{\text{\textctstretchc}}
\newcommand{\tipxphi}{\text{\textqplig}}















\numberwithin{equation}{section}




\title{Qiuzhen Lectures on Analysis}
\author{{\sc Bin Gui}
	%\\
	%{\small Department of Mathematics, Rutgers university}\\
	%{\small bin.gui@rutgers.edu}
}
%\date{}

%\definecolor{mycolor}{RGB}{227,237,205} \pagecolor{mycolor}

\begin{document}\sloppy % avoid stretch into margins
	\pagenumbering{arabic}
	%\pagenumbering{gobble}
	\setcounter{page}{1}
	\setcounter{section}{-1}
	%\setcounter{equation}{6}



	






	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



	
	\maketitle
\small  \hyperlink{beforeindex}{Last page before index}~~~~~~   \hyperlink{page.5}{Last page of TOC}
%\hyperlink{endoftoc}{Last page of TOC}

\noindent Sections on history include but are not limited to: 
\ref{lb55} (point-set topology), \ref{lb549} (Hilbert spaces, Riesz-Fischer theorem), \ref{lb550} (integral theory, Fourier series), \ref{lb543} (Banach-Alaoglu, Hahn-Banach), \ref{lb551} (functional calculus, spectral theory), \ref{lb548} (quotient Banach spaces, Hahn-Banach), \ref{lb671} and most part of Ch. \ref{lb672} (Hilbert spaces), \ref{lb733} (measurable sets)
\normalsize
%\thispagestyle{empty}	 %remove page number of this page


%Contents hyperlinks: \hyperlink{page.2}{Page 2}, \hyperlink{page.3}{Page 3}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-0.5cm}
\makeatletter
\newcommand*{\toccontents}{\@starttoc{toc}}
\makeatother
\toccontents

\hypertarget{endoftoc}{}

	
% title and table of contents same page, no content title

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\section{Preface}

\subsection{Notations}
Note: Topics marked with $\star\star$ are technical and/or their methods are rarely used in later studies. Topics marked with $\star$ are interesting, but not necessarily technical or difficult. They are not essential for understanding the rest of the notes. You can skim or skip the starred topics on first reading. When a chapter/section/subsection is starred, it means that all of the material in that chapter/section/subsection is starred.

We use frequently the abbreviations:
\begin{gather*}
\text{iff=if and only if}\\
\text{LHS=left hand side}\qquad
\text{RHS=reft hand side}\\
\text{$\exists$=there exists}\qquad \text{$\forall$=for all}\\
\text{i.e.=id est=that is=namely}\qquad\text{e.g.=for example}\\
\text{cf.=compare/check/see/you are referred to}\\
\text{resp.=respectively}\qquad 
\text{WLOG=without loss of generality}\\
\end{gather*}
If $P,Q$ are properties, then
\begin{align*}
P\land Q=P\text{ and }Q\qquad P\lor Q=P\text{ or }Q\qquad \neg P=\text{ Not }P
\end{align*}
When we write $A:=B$ or $A\xlongequal{\mathrm{def}}B$, we mean that $A$ is defined by the expression $B$. When we write $A\equiv B$, we mean that $A$ are $B$ are different symbols of the same object.

If $\Fbb$ is any field (e.g. $\Qbb,\Rbb,\Cbb$), we let $\Fbb^\times=\Fbb\setminus\{0\}$. \index{F@$\Fbb^\times=\Fbb\setminus\{0\}$} If $\alpha$ is a complex number and $n\in\Nbb$, we define the \textbf{binomial coefficient}\index{zz@$\alpha\choose n$}  \index{00@Binomial coefficient}
\begin{align}
{\alpha\choose n}=\left\{
\begin{array}{ll}
\dps\frac{\alpha\cdot(\alpha-1)\cdots (\alpha-n+1)}{n!} &\text{ if }n\geq 1\\[1ex]
1&\text{ if }n=0
\end{array}
\right.
\end{align}
where $n!=n(n-1)(n-2)\cdots 2\cdot 1$ and $0!=1$. The bold letter $\im$ means \index{i@$\im=\sqrt{-1}$}
\begin{align*}
\im=\sqrt{-1}
\end{align*}
If $z=a+b\im$ where $a,b\in\Rbb$, we let
\begin{align*}
\Real(z)=a\qquad \Imag(z)=b
\end{align*}
If $x,y$ are two elements, we let \index{zz@$\delta_{x,y}$}
\begin{align}\label{eq154}
\delta_{x,y}=\left\{
\begin{array}{ll}
1&\text{if }x=y\\
0&\text{if }x\neq y
\end{array}
\right.
\end{align}

Additions and multiplications in $[-\infty,+\infty]$ and $[0,+\infty]$ are described in Def. \ref{lb114}.


%Topics in the ``Problems" section (or the whole ``Problems" sections) marked with $\heartsuit$ are important problems that will be used later in this course.


\section{Basic set theory and numbers}





In this chapter, we discuss informally some of the basic notions in set theory and basic properties about numbers. A more thorough treatment can be found in \cite[Ch. 1]{Mun} (for set theory) and \cite[Ch. 1]{Rud-P} (for numbers). 





\subsection{Basic operations and axioms}
Intuitively, a set denotes a collection of elements. For instance:\index{N@$\Nbb=\{0,1,2,\dots\}$} \index{Z@$\Zbb_+=\{1,2,\dots\}$}
\begin{gather*}
\Zbb=\{\text{all integers}\}\qquad \Nbb=\Zbb_{\geq0}=\{n\in\Zbb:n\geq0\}\qquad \Zbb_+=\{n\in\Zbb:n>0\}
\end{gather*}
have infinitely many elements. (In this course, we will not be concerned with the rigorous construction of natural numbers and integers from Peano axioms.) We also let
\begin{align*}
\Qbb=\{\text{all rational numbers}\}\qquad\Rbb=\{\text{all real numbers}\}
\end{align*}
if we that rational and real numbers exist and satisfy the properties we are familiar with in high school mathematics. (We will construct $\Qbb$ and $\Rbb$ rigorously, by the way.)


Set theory is the foundation of modern mathematics. It consists of several Axioms telling us what we can do about the sets. For example, the following way of describing sets
\begin{align}
\{x: x\text{ satisfies property...}\}  \label{eq1}
\end{align}
is illegal, since it gives \textbf{Russell's paradox}: Consider
\begin{align}
S=\{A: A\text{ is a set and }A\notin A\}\label{eq12}
\end{align}
If $S$ were a set, then $S\in S\Rightarrow S\notin S$ and $S\notin S\Rightarrow S\in S$. This is something every mathematician doesn't want to happen.

Instead, the following way of defining sets is legitimate:
\begin{align}
\{x\in X:x\text{ satisfies property}\dots\}  \label{eq2}
\end{align}
where \textit{$X$ is a given set}.  For instance, we can define the \textbf{difference} of two sets:\index{AB@$A\backslash B$}
\begin{align*}
A\setminus B=A-B=\{x\in A:x\notin B\}
\end{align*}




So let us figure out the legal way of defining unions and intersections of sets. The crucial point is that we assume the following axiom:
\begin{axiom}
If $\scr A$ is a set of sets, then there exists a set $X$ such that $A\subset X$ for all $A\in\scr A$.
\end{axiom}

Thus, if $\scr A$ is a set of sets, let $X$ satisfy $A\subset X$ for all $A\in\scr A$, then we can define the \textbf{union} and the \textbf{intersection} 
\begin{subequations}\label{eq3}
\begin{gather}
\bigcup_{A\in\scr A}A=\{x\in X:\text{there exists $A\in\scr A$ such that $x\in A$}\}\\
\bigcap_{A\in\scr A}A=\{x\in X:\text{for all $A\in\scr A$ we have $x\in A$}\}
\end{gather}
\end{subequations}
It is clear that this definition does not rely on the particular choice of $X$.

\begin{rem}
In many textbooks, it is not uncommon that sets are defined as in \eqref{eq1}. You should interpret such definition as \eqref{eq2}, where the set $X$ is omitted because it is clear from the context. For instance, if the context is clear, the set $\{x\in\Rbb:x\geq 0\}$ could be simply written as $\{x:x\geq0\}$ or even $\{x\geq0\}$. By the same token, the phrase ``$\in X$" in \eqref{eq3} could be omitted. So we can also write
\begin{gather*}
A\cup B=\{x: x\in A\text{ or }x\in B\} \qquad  A\cap B=\{x: x\in A\text{ and }x\in B\}
\end{gather*}
which are special cases of \eqref{eq3}.
\end{rem}


\begin{rem}
In the same spirit, when discussing subsets of a given ``large" set $X$, and if $X$ is clear from the context, we shall write $X\setminus A$ (where $A\subset X$) as $A^c$ \index{Ax@$A^c$, the complement of $A$} and call it the \textbf{complement} of $A$.
\end{rem}


\begin{eg}
We have
\begin{gather*}
\bigcup_{x\in(1,+\infty)}[0,x)=[0,+\infty)\qquad\bigcap_{n\in\Zbb_+}(0,1+1/n)=(0,1]\qquad \bigcup_{n\in\Nbb}(0,1-1/n]=(0,1)
\end{gather*}
The readers may notice that these examples are not exactly in the form \eqref{eq3}. They are actually unions and intersections of indexed families of sets. (See Def. \ref{lb1}.) We need some preparation before discussing this notion.
\end{eg}




\begin{axiom}
If $A_1,\dots,A_n$ are sets, their \textbf{Cartesian product} exists:
\begin{align*}
A_1\times\cdots\times A_n=\{(a_1,\dots,a_n): a_i\in A_i\text{ for all }1\leq i\leq n\}
\end{align*}
where two elements $(a_1,\dots,a_n)$ and $(b_1,\dots,b_n)$ of the Cartesian product are regarded equal iff $a_1=b_1,\dots,a_n=b_n$. We also write
\begin{align*}
(a_1,\dots,a_n)=a_1\times\cdots\times a_n
\end{align*}
especially when $a,b$ are real numbers and $(a,b)$ can mean an open interval. We understand $A_1\times\cdots\times A_n$ as $\emptyset$ if some $A_i$ is $\emptyset$.

If $A_1=\cdots=A_n=A$, we write the Cartesian product as $A^n$. \hfill\qedsymbol
\end{axiom}

\begin{eg}
Assume that the set of real numbers $\Rbb$ exists. Then the set of complex numbers $\Cbb$ \index{C@$\Cbb$, the set of complex numbers} is defined to be $\Rbb^2=\Rbb\times\Rbb$ as a set. We write $(a,b)\in\Cbb$ as $a+b\im$ where $a,b\in\Rbb$. Define
\begin{gather*}
(a+b\im)+(c+d\im)=(a+c)+(b+d)\im\\
(a+b\im)\cdot (c+d\im)=(ac-bd)+(ad+bc)\im
\end{gather*}
Define the zero element $0$ of $\Cbb$ to be $0+0\im$. More generally, we consider $\Rbb$ as a subset of $\Cbb$ by viewing $a\in\Rbb$ as $a+0\im\in\Cbb$. This defines the usual arithmetic of complex numbers. 

If $z=a+b\im$, we define its \textbf{absolute value} $|z|=\sqrt{a^2+b^2}$. Then $z=0$ iff $|z|=0$. We define the \textbf{(complex) conjugate} of $z$ to be $\ovl z=a-b\im$. Then $|z|^2=z\ovl z$.

If $z\neq 0$, then there clearly exists a unique $z^{-1}\in\Cbb$ such that $zz^{-1}=z^{-1}z=1$:  $z^{-1}=|z|^{-2}\cdot \ovl z$. Thus, using the language of modern algebra, $\Cbb$ is a  \textbf{field}.\footnote{See Rem. \ref{lb166} or  \cite[Def. 1.12]{Rud-P} for the definition of fields. Rather than memorizing the full definition of fields, it is more important to keep in mind some typical (counter)examples: $\Qbb,\Rbb,\Cbb$ are fields. $\Zbb$ is not a field, because not every non-zero element of $\Zbb$ has an inverse. The set of quaternions $\{a+b\im+c\mathbf{j}+d\mathbf{k}: a,b,c,d\in\Rbb\}$ is not a field because it is not commutative ($\im\mathbf{j}=-\mathbf{j}\im=\mathbf{k}$). The set of rational functions $P(x)/Q(x)$, where $P,Q$ are polynomials with coefficients in $\Rbb$ and $Q\neq 0$, is a field.}  \hfill\qedsymbol
\end{eg}


The axiom of Cartesian product allows us to define relations and functions:

\begin{df}
If $A,B$ are sets, a subset $R$ of $A\times B$ is called a \textbf{relation}. For $(a,b)\in A\times B$, we write $aRb$ iff $(x,y)\in R$. We understand ``$aRb$" as ``$a$ is related to $b$ through the relation $R$".
\end{df}


\begin{df}\label{lb39}
A relation $f$ of $A,B$  is called a \textbf{function} or a \textbf{map} (or a \textbf{mapping}), if for every $a\in A$ there is a unique $b\in B$ such that $afb$. In this case, we write $b=f(a)$. 

When we write $f:A\rightarrow B$, we always mean that $A,B$ are sets and $f$ is a function from $A$ to $B$. $A$ and $B$ are called respectively the \textbf{domain} and the \textbf{codomain} of $f$. (Sometimes people also use the words ``source" and ``target" to denote $A$ and $B$.) 

If $E\subset A$ and $F\subset B$, we define the \textbf{image under $f$} of $E$  and the \textbf{preimage under $f$} of $F$ to be
\begin{gather*}
f(E)=\{b\in B:\exists a\in E\text{ such that }b=f(a)\}\\
f^{-1}(F)=\{a\in A: f(a)\in F\}.
\end{gather*}
$f(A)$ is simply called the \textbf{image} of $f$, or the \textbf{range} of $f$. If $b\in B$, $f^{-1}(\{b\})$ is often abbreviated to $f^{-1}(b)$. The function \index{f@$f\lvert_E$, the restriction of $f$ to $E$}
\begin{align*}
f|_E:E\rightarrow B\qquad x\mapsto f(x)
\end{align*}
is called the \textbf{restriction}  of $f$ to $E$. \hfill\qedsymbol
\end{df}


The intuition behind the definition of functions is clear: we understand functions as the same as their graphs. So a subset $f$ of the ``coordinate plane" $A\times B$ is the graph of a function iff it ``intersects every vertical line precisely once".


\begin{rem}\label{lb40}
According to our definition, $\emptyset$ (as a subset of $\emptyset\times B$) is the only function from $\emptyset$ to $B$. (A false assumption implies any statement.) If $A\neq\emptyset$, there are no functions $A\rightarrow\emptyset$.
\end{rem}


\begin{df}\label{lb13}
A function $x:\Zbb_+\rightarrow A$ is called a \textbf{sequence in $A$}. We write $x(n)$ as $x_n$, and write this sequence as $(x_n)_{n\in\Zbb_+}$ (or simply $(x_n)_n$ or $(x_n)$).
\end{df}

Many people write such a sequence as $\{x_n\}_{n\in\Zbb_+}$. We do not use this notation, since it can be confused with $\{x_n: n\in\Zbb_+\}$ (the range of the function $x$).



\begin{axiom}
If $X$ is a set, then the \textbf{power set} \index{00@Power set $2^X$} $2^X$ exists, where
\begin{align*}
2^X=\{\text{Subsets of }X\}
\end{align*}
\end{axiom}

\begin{eg}
The set $2^{\{1,2,3\}}$ has $8$ elements: $\emptyset$, $\{1\}$, $\{2\}$, $\{3\}$, $\{1,2\}$, $\{1,3\}$, $\{2,3\}$, $\{1,2,3\}$. Surprisingly, $8=2^3$. As we shall see in Exp. \ref{lb11} and Cor. \ref{lb12}, this relationship holds more generally, which explains the terminology $2^X$.  
\end{eg}

Now we are ready to define indexed families of sets.
\begin{df}\label{lb1}
An \textbf{indexed family of sets} \index{00@Indexed family of sets}  $(S_i)_{i\in I}$ is defined to be a function $S:I\rightarrow 2^X$ for some sets $I,X$. We write $S(i)$ as $S_i$. (So $S_i$ is a subset of $X$.) $I$ is called the \textbf{index set}. Define
\begin{align*}
\bigcup_{i\in I}S_i= \bigcup_{T\in S(I)}T\qquad \bigcap_{i\in I}S_i= \bigcap_{T\in S(I)}T
\end{align*}
Note that $S(I)$ is the image of the function $S$.
\end{df}


\begin{eg}
In the union $\bigcup_{x\in(1,+\infty)}[0,x)$, the index set is $I=(1,+\infty)$, and $X$ can be the set of real numbers $\Rbb$. Then $S:I\rightarrow 2^X$ is defined to be $S_i=S(i)=[0,i)$.
\end{eg}




\begin{exe}\label{lb5}
Let $f:A\rightarrow B$ be a function. We say that $f$ is \textbf{injective} if for all $a_1,a_2\in A$ satisfying $a_1\neq a_2$ we have $f(a_1)\neq f(a_2)$. We say that $f$ is \textbf{surjective} if for each $b\in B$ we have $f^{-1}(b)\neq\emptyset$. $f$ is called \textbf{bijective} if it is both surjective and bijective. Define the \textbf{identity maps} $\id_A:A\rightarrow A,a\mapsto a$ \index{id@$\id_A$} and $\id_B$ in a similar way. Prove that
\begin{subequations}
\begin{gather}
\text{$f$ is injective $\Longleftrightarrow$ there is $g:B\rightarrow A$ such that $g\circ f=\id_A$}\label{eq4}\\
\text{$f$ is surjective $\Longleftrightarrow$ there is $g:B\rightarrow A$ such that $f\circ g=\id_B$}\label{eq5}\\
\text{$f$ is bijective $\Longleftrightarrow$ there is $g:B\rightarrow A$ such that $g\circ f=\id_A$ and $f\circ g=\id_B$}\label{eq6}
\end{gather}
\end{subequations}
Show that the $g$ in \eqref{eq4} (resp. \eqref{eq5}, \eqref{eq6}) is surjective (resp. injective, bijective).
\end{exe}


The equivalence \eqref{eq5} is subtler, since its proof requires Axiom of Choice.


\begin{axiom}
Let $(S_i)_{i\in I}$ be an indexed family of sets. The \textbf{Axiom of Choice} asserts that if $S_i\neq\emptyset$ for all $i\in I$, then there exists a function (the \textbf{choice function})
\begin{align*}
f:I\rightarrow \bigcup_{i\in I}S_i
\end{align*}
such that $f(i)\in S_i$ for each $i\in I$.
\end{axiom}


Intuitively, the axiom of choice says that for each $i\in I$ we can choose an element $f(i)$ of $S_i$. And such choice gives a function $f$.


\begin{eg}
Let $f:A\rightarrow B$ be surjective. Then each member of the family $(f^{-1}(b))_{b\in B}$ is nonempty. Thus, by axiom of choice, there is a choice function $g$ defined on the index set $B$ such that $g(b)\in f^{-1}(b)$ for each $b$. Clearly $f\circ g=\id_B$.
\end{eg}



\begin{rem}
Suppose that each member $S_i$ of the family $(S_i)_{i\in I}$ has exactly one element. Then the existence of a choice function does not require Axiom of Choice: Let $X=\bigcup_{i\in I}S_i$ and define relation
\begin{align*}
f=\{(i,x)\in I\times X: x\in S_i\}
\end{align*}
Then one checks easily that this relation between $I$ and $X$ is a function, and that it is the (necessarily unique) choice function of $(S_i)_{i\in I}$.
\end{rem}

According to the above remark, one does not need Axiom of Choice to prove \eqref{eq4} and \eqref{eq6}. Can you see why?


\subsection{Partial and total orders, equivalence relations}\label{lb299}

\begin{df}
Let $A$ be a set. A \textbf{partial order} (or simply an \textbf{order}) $\leq$ on $A$ is a relation on $A\times A$ satisfying for all $a,b,c\in A$ that:
\begin{itemize}
\item (Reflexivity) $a\leq a$.
\item (Antisymmetry) If $a\leq b$ and $b\leq a$ then $a=b$.
\item (Transitivity) If $a\leq b$ and $b\leq c$ then $a\leq c$.
\end{itemize}
We write $b\geq a$ iff $a\leq b$. Write $a>b$ iff $a\geq b$ and $a\neq b$. Write $a<b$ iff $b>a$. So $\geq$ is also an order on $A$. The pair $(A,\leq)$ is called a \textbf{partially ordered set}, or simply a \textbf{poset}. \index{00@Poset=partially ordered set} A partial order $\leq$ on $A$ is called a \textbf{total order}, if for every $a,b\in A$ we have either $a\leq b$ or $b\leq a$.
\end{df}


\begin{eg}
The following are examples of orders.
\begin{itemize}
\item Assume that $\Rbb$ exists. Then $\Rbb$ has the canonical total order, which restricts to the total order of $\Zbb$. This is the total order that everyone is familiar with.
\item Let $X$ be a set. Then $(2^X,\subset)$ is a poset.
\item $\Rbb^2$ is a poset, if we define $(a,b)\leq (c,d)$ to be $a\leq c$ and $b\leq d$. 
\end{itemize}
\end{eg}


\begin{df}\label{lb156}
A relation $\sim$ on a set $A$ is called an \textbf{equivalence relation}, \index{00@Equivalence relation} if for all $a,b,c\in A$ we have
\begin{itemize}
\item (Reflexivity) $a\sim a$.
\item (Symmetry) $a\sim b$ iff $b\sim a$.
\item (Transitivity) If $a\sim b$ and $b\sim c$ then $a\sim c$.
\end{itemize}
\end{df}

Later, we will use the notions of partial orders and equivalence relation not just for a set, but for a collection of objects ``larger" than a set. See Sec. \ref{lb4}.

\begin{df}\label{lb157}
Let $A$ be a set, together with an equivalence relation $\sim$. Define a new set
\begin{align*}
{A/\sim}=\{[a]: a\in A\}
\end{align*}
where the notion $[a]$ can be understood in the following two equivalent ways (we leave it to the readers to check the equivalence):
\begin{itemize}
\item[(1)] $[a]$ is a new symbol. We understand $[a]$ and $[b]$ as equal iff $a\sim b$.
\item[(2)] $[a]=\{x\in A: x\sim a \}$
\end{itemize}
We call $[a]$ the \textbf{equivalence class} (or the \textbf{residue class}) of $a$, and call $A/\sim$ the \textbf{quotient set} \index{00@Quotient sets} of $A$ under $\sim$. The surjective map $\pi:a\in A\mapsto [a]\in {A/\sim}$ is called the \textbf{quotient map}.
\end{df}


\begin{exe}
Prove that every surjective map  is equivalent to a quotient map. More precisely, prove that for every surjection $f:A\rightarrow B$, there is an equivalence relation $\sim$ on $A$ and a bijective map $\Phi:{A/\sim}\rightarrow B$ such that the following diagram commutes (i.e. $f=\Phi\circ\pi$):
\begin{equation}\label{eq7}
\begin{tikzcd}[column sep=small]
                          & A \arrow[rd, "f"] \arrow[ld, "\pi"'] &   \\
{A/\sim} \arrow[rr, "\Phi"] &                                      & B
\end{tikzcd}
\end{equation}
\end{exe}


This is the first time we see commutative diagrams. Commutative diagrams are very useful for indicating that certain maps between sets are ``equivalent" or are satisfying some more general relations. For example, \eqref{eq7} shows that the maps $f$ and $\pi$ are equivalent, and that this equivalence is implemented by the bijection $\Phi$. The formal definition of commutative diagrams is the following:


\begin{df}
A diagram (i.e. some sets denoted by symbols, and some maps denoted by arrows) is called a \textbf{commutative diagram}, \index{00@Commutative diagram} if all directed paths in the diagram with the same start and endpoints lead to the same result.
\end{df}


Here is an example of commutative diagram in linear algebra. This example assumes some familiarity with the basic properties of vector spaces \index{00@Vector spaces} and linear maps.\footnote{Again, we refer the readers to Internet or any Linear Algebra textbook (e.g. \cite{Axl}) for the definition of vector spaces and linear maps.}


\begin{eg}\label{lb67}
Let $V,W$ be vector spaces over a field $\Fbb$ with finite dimensions $m,n$ respectively. Let $e_1,\dots,e_m$ be a basis of $V$, and let $\eps_1,\dots,\eps_n$ be a basis of $W$. We know that there are unique linear isomorphisms $\Phi:\Fbb^m\xrightarrow{\simeq} V$ and $\Psi:\Fbb^n\xrightarrow{\simeq} W$ such that
\begin{align*}
\Phi(a_1,\dots,a_m)=a_1e_1+\cdots+a_me_m\qquad \Psi(b_1,\dots,b_n)=b_1\eps_1+\cdots+b_n\eps_n
\end{align*}
Let $T:V\rightarrow W$ be a \index{00@Linear maps} \textbf{linear map}, i.e., a map satisfying $T(a\xi+b\eta)=aT\xi+bT\eta$ for all $a,b\in\Fbb,\xi,\eta\in V$. Then there is a unique $n\times m$ matrix $A\in\Fbb^{n\times m}$ \index{Fnm@$\Fbb^{n\times m}$, the set of $n\times m$ matrices} (viewed as a linear map $\Fbb^m\rightarrow\Fbb^n$ defined by matrix multiplication) such that the following diagram commutes:
\begin{equation}
\begin{tikzcd}
\Fbb^m \arrow[r,"\Phi","\simeq"'] \arrow[d,"A"'] & V \arrow[d,"T"] \\
\Fbb^n \arrow[r,"\Psi","\simeq"']           & W          
\end{tikzcd}
\end{equation} 
namely, $T\Phi=\Psi A$. This commutative diagram tells us that $T$ is equivalent to its \textbf{matrix representation} \index{00@Matrix representation} $A$ under the bases $e_\blt,\eps_\star$, and that this equivalence is implemented by the linear isomorphisms $\Phi$ (on the sources) and $\Psi$ (on the targets). 
\end{eg}

Commutative diagrams are ubiquitous in mathematics. You should learn how to read commutative diagrams and understand their intuitive meanings. We will see more examples in the future of this course.



\subsection{$\Qbb$, $\Rbb$, and $\overline{\mathbb R}=[-\infty,+\infty]$}



Using equivalence classes, one can construct rational numbers from integers, and real numbers from rationals. We leave the latter construction to the future, and discuss the construction of rationals here.

\begin{eg}[Construction of $\Qbb$ from $\Zbb$]\label{lb17}
Define a relation on $\Zbb\times\Zbb^\times$ (where $\Zbb^\times=\Zbb\setminus\{0\}$) as follows. If $(a,b),(a',b')\in\Zbb\times\Zbb^\times$, we say $(a,b)\sim(a',b')$ iff $ab'=a'b$. It is a routine check that $\sim$ is an equivalence relation. Let \index{Q@$\Qbb$, the field of rational numbers}  $\Qbb=(\Zbb\times\Zbb^\times)/\sim$, and write the equivalence class of $(a,b)$ as $a/b$ or $\frac ab$. Define additions and multiplications in $\Qbb$ to be
\begin{align*}
\frac ab+\frac cd=\frac{ad+bc}{bd}\qquad \frac ab\cdot\frac cd=\frac{ac}{bd}
\end{align*}
We leave it to the readers to check that this definition is \index{00@Well defined} \textbf{well-defined}: If $(a,b)\sim(a',b')$ and $(c,d)\sim(c',d')$ then $(ad+bc,bd)\sim(a'd'+b'c',b'd')$ and $(ac,bd)\sim(a'c',b'd')$.

We regard $\Zbb$ as a subset of $\Qbb$ by identifying $n\in\Zbb$ with $\frac n1$. (This is possible since the map $n\in\Zbb\mapsto \frac n1\in\Qbb$ is injective.) Each $a/b\in\Qbb$ has additive inverse $\frac{-a}b$. If $a/b\in\Qbb$ is not zero (i.e. $(a,b)\nsim (0,1)$), then $a/b$ has multiplicative inverse $b/a$. This makes $\Qbb$ a field: the field of \textbf{rational numbers}.

If $a/b\in\Qbb$, we say $a/b\geq 0$ if $ab\geq0$. Check that this is well-defined (i.e., if $(a,b)\sim(a',b')$, then $ab\geq0$ iff $a'b'\geq0$). More generally, if $a/b,c/d\in\Qbb$, we say $\frac ab\geq \frac cd$ if $\frac ab-\frac cd\geq0$. Check that $\geq$ is a total order on $\Qbb$.  Check that $\Qbb$ is an Archimedean ordered field, defined below.\hfill\qedsymbol
\end{eg}


\begin{df}\label{lb161}
A field $\Fbb$, together with a total order $\leq$, is called an \index{00@Ordered field} \textbf{ordered field}, if for every $a,b,c\in\Fbb$ we have
\begin{itemize}
\item (Addition preserves $\leq$) If $a\leq b$ then $a+c\leq b+c$.
\item (Multiplication by $\Fbb_{\geq0}$ preserves $\geq0$) If $a,b\geq 0$ then $ab\geq0$.
\end{itemize}
These two properties relate $\leq$ to $+$ and $\cdot$ respectively.
\end{df}

\begin{rem}
Many familiar properties about inequalities in $\Qbb$ hold for an ordered field. For instance: 
\begin{gather*}
a\geq b~\wedge~ c\geq d \qquad\Longrightarrow\qquad a+c\geq b+d\\
a\geq0\qquad\Longleftrightarrow\qquad -a\leq0\\
a\geq0~\wedge~b\geq c\qquad\Longleftrightarrow\qquad ab\geq ac\\
a\leq0~\wedge~b\geq c\qquad\Longleftrightarrow\qquad ab\leq a\\
a^2\geq0\\
0<a\leq b\qquad\Longrightarrow\qquad 0< b^{-1}\leq a^{-1}
\end{gather*}
Check them yourself, or see \cite[Prop. 1.18]{Rud-P}.
\end{rem}


\begin{df}
We say that an ordered field $\Fbb$ satisfies \index{00@Archimedean property} \textbf{Archimedean property} if for each $a,b\in\Fbb$ we have
\begin{align*}
a> 0\qquad\Longrightarrow \qquad\exists n\in\Nbb\text{ such that }na>b
\end{align*}
where $na$ means $\underbrace{a+\cdots+a}_{n}$.
\end{df}

\begin{eg}
$\Qbb$ satisfies Archimedean property. Indeed, let $a,b\in\Qbb$ and $a>0$. Then $a=p/q$ and $b=r/s$ where $p,q,s\in\Zbb_+$ and $r\in\Zbb$. So $na>b$ where $n=q|r|+q$.
\end{eg}



Prop. \ref{lb2} gives an important application of Archimedian property. We will use this in the construction of $\Rbb$ from $\Qbb$, and in the proof that $\Qbb$ is dense in $\Rbb$. 

\begin{df}
Let $\Fbb$ be a field. A subset $\Ebb\subset\Fbb$ is called a  \textbf{subfield} \index{00@Subfield} of $\Fbb$, if $\Ebb$ contains the $1$ of $\Fbb$, and if $\Ebb$ is closed under the operations of addition, multiplication, taking negative, and taking inverse in $\Fbb$ (i.e. if $a,b\in\Ebb$ then $a+b,ab,-a\in\Ebb$, and $a^{-1}\in\Ebb$ whenever $a\neq 0$). We also call $\Fbb$ a \index{00@Field extension} \textbf{field extension} of $\Ebb$, since $\Ebb$ is clearly a field.
\end{df}

Note that if $\Ebb$ is a subfield of $\Fbb$, the $0$ of $\Fbb$ is in $\Ebb$ since $0=1+(-1)\in\Ebb$.

\begin{df}
Let $\Ebb$ be an ordered field. A field extension $\Fbb$ of $\Ebb$ is called an \index{00@Ordered field extension=ordered subfield} \textbf{ordered field extension}, if $\Fbb$ is equipped with a total order $\leq$ such that $\Fbb$ is an ordered field, and if the order $\leq$ of $\Fbb$ restricts to that of $\Ebb$. We also call $\Ebb$ an \textbf{ordered subfield} of $\Fbb$.
\end{df}

Our typical example of ordered field extension will be $\Qbb\subset\Rbb$.

\begin{pp}\label{lb2}
Let $\Fbb$ be an ordered field extension  of $\Qbb$. Assume that $\Fbb$ is Archimedean. Then for every $x,y\in\Fbb$ satisfying $x<y$, there exists $p\in\Qbb$ such that $x<p<y$.
\end{pp}

\begin{proof}
Assume $x,y\in\Fbb$ and $x<y$. Then $y-x>0$ (since $y-x\neq 0$ and $-x+x\leq -x+y$). By Archimedean property, there exists $n\in\Zbb_+$ such that $n(y-x)>1$. So $\displaystyle y-x>\frac 1n$ and hence $\displaystyle x+\frac 1n<y$.

Let us prove that the subset
\begin{equation*}
A=\big\{k\in\Zbb: \frac {~k~}{n}\leq x\big\}
\end{equation*}
is nonempty and bounded from above in $\Zbb$. By Archimedean property, there is $m\in\Zbb_+$ such that $m>nx$, i.e. $\displaystyle \frac mn>x$. So for each $k\in\Zbb_+$ satisfying $k\geq m$, we have $\displaystyle \frac kn=\frac mn+\frac{k-m}n>x$. Therefore, for each $k\in A$ we have $k<m$. So $A$ is bounded above. By Archimedean property again, there is $l\in\Zbb_+$ such that $\displaystyle \frac ln>-x$. So $\displaystyle -\frac ln<x$, and hence $A$ is nonempty.

We now use the fact that \textit{every nonempty subset of $\Zbb$ bounded above has a maximal element}. Let $k=\max A$. Since $k+1\notin A$, we have $\displaystyle x<\frac{k+1}n$. Since $\displaystyle \frac kn\leq x$, we have
\begin{align*}
\frac{k+1}n=\frac kn+\frac 1n\leq x+\frac 1n<y
\end{align*}
This proves $x<p<y$ with $\displaystyle p=\frac{k+1}n$.
\end{proof}

To introduce $\Rbb$ formally, we need more definitions:

\begin{df}
Let $(X,\leq)$ be a poset and $E\subset X$. An \textbf{upper bound of $E$ in $X$} \index{00@Upper bound} is an element $x\in X$ satisfying $e\leq x$ for all $e\in E$. An upper bound $x\in X$ of $E$ is called a \textbf{least upper bound} or a \textbf{supremum} \index{00@Supremum}  if $x\leq y$ for every upper bound $y\in Y$ of $E$. In this case, we write the supremum as \index{sup@$\sup E$} $\sup E$. It is not hard to check that supremums are unique if they exist.

We leave it to the readers to define \textbf{lower bounds} and the \textbf{greatest lower bound} (if exists) of $E$, also called the \textbf{infinimum} \index{00@Infinimum} and is denoted by \index{inf@$\inf E$} $\inf E$. \hfill\qedsymbol
\end{df}


\begin{df}
Let $(X,\leq)$ be a poset. We say that $X$ satisfies the \textbf{least-upper-bound property}, if every every nonempty subset $E\subset X$ which is bounded above (i.e. $E$ has an upper bound) has a supremum in $X$. The \textbf{greatest-lower-bound property} is defined in the opposite way.
\end{df}

\begin{eg}
$\Zbb$ satisfies the least-upper-bound and the greatest-lower-bound property: Let $A\subset \Zbb$. If $A$ is bounded above (resp. below), then the maximum $\max A$ (resp. minimum $\min A$) exists and is the supremum (resp. infinimum) of $A$.
\end{eg}

\begin{eg}
Let $X$ be a set. Then $(2^X,\subset)$ satisfies the least-upper-bound and the greatest-lower-bound property: Let $\scr A\subset 2^X$, i.e., $\scr A$ is a set of subsets of $X$. Then $\scr A$ is bounded from above by $X$, and is bounded from below by $\emptyset$. Moreover,
\begin{align*}
\sup\scr A=\bigcup_{A\in\scr A}A\qquad \inf\scr A=\bigcap_{A\in\scr A}A
\end{align*}
\end{eg}





\begin{thm}\label{lb3}
There is an ordered field extension of $\Qbb$ which is Archimedian and satisfies the least-upper-bound property. This field is denoted by  $\Rbb$. Its elements are called \index{00@Real number} \textbf{real numbers}.
\end{thm}

Thm. \ref{lb3} will be proved in Ch. \ref{lb167}. Note that by taking negative, we see that $\Rbb$ also satisfies the greatest-lower-bound property.


\begin{rem}
The ordered field extensions satisfying the conditions in Thm. \ref{lb3} are unique ``up to isomorphisms". (The words ``\textbf{isomorphism}"\index{00@Isomorphism}  and ``equivalence" are often interchangeable, though ``isomorphism" is more often used in the algebraic setting, whereas ``equivalence" can be used in almost every context. For example, in point-set topology, ``equivalence" means ``homeomorphism".) We leave it to the readers to give the precise statement. We will not use this uniqueness in this course. 

Note that to compare two extensions $\Fbb,\Rbb$ of $\Qbb$, it is very confusing to regard $\Qbb$ as a subset of both $\Fbb$ and $\Rbb$. You'd better consider two different injective maps $\tau:\Qbb\rightarrow \Fbb$ and $\iota:\Qbb\rightarrow\Rbb$ preserving the algebraic operations and the order of $\Qbb$, and use a commutative diagram to indicate that $\tau$ and $\iota$ are equivalent. (Thus, what's happening here is that we have an equivalence of maps, not just an equivalence of the fields $\Fbb$ and $\Rbb$.) \hfill\qedsymbol
\end{rem}


\begin{df}\label{lb114}
Let $-\infty,+\infty$ be two different symbols, and extend the total order $\leq$ of $\Rbb$ to the \textbf{extended real line}\index{R@$\ovl\Rbb=[-\infty,+\infty]=\Rbb\cup\{-\infty,+\infty\}$}
\begin{align*}
\ovl\Rbb=\Rbb\cup\{-\infty,+\infty\}
\end{align*}
by letting $-\infty<x<+\infty$ for all $x\in\Rbb$. Define for each $x\in\Rbb$ that
\begin{gather*}
x\pm\infty=\pm\infty+x=\pm\infty\qquad +\infty-(-\infty)=+\infty\\
x\cdot(\pm\infty)=\pm\infty\cdot x=\left\{
\begin{array}{cc}
\pm\infty&\text{if }x>0\\
0&\text{if }x=0\\
\mp\infty&\text{if }x<0
\end{array}
\right.\\
\frac x{\pm\infty}=0\\
\frac{\pm\infty}{x}=x^{-1}\cdot(\pm\infty)\qquad \text{if }x\neq0
\end{gather*}
If $a,b\in\ovl\Rbb$ and $a\leq b$, we define \textbf{intervals} \index{00@Interval} with endpoints \index{00@Endpoints of an interval} $a,b$:
\begin{gather}\label{eq8}
\begin{gathered}
[a,b]=\{x\in\ovl\Rbb:a\leq x\leq b\}\qquad (a,b)=\{x\in\ovl\Rbb:a< x< b\}\\
(a,b]=\{x\in\ovl\Rbb:a< x\leq b\}\qquad [a,b)=\{x\in\ovl\Rbb:a\leq x< b\}
\end{gathered}
\end{gather}
So $\Rbb=(-\infty,+\infty)$ and $\ovl\Rbb=[-\infty,+\infty]$. If $a,b$ are in $\Rbb$, we say that the corresponding interval is \textbf{bounded}. \index{00@Bounded interval}
\end{df}

In this course, unless otherwise stated, an interval always means one of the four sets in \eqref{eq8}. The first two intervals are called respectively a \textbf{closed interval} and an \textbf{open interval}.


\begin{rem}
Clearly, every subset $E$ of $\ovl\Rbb$ is bounded and has a supremum and an infinimum. We have that $\sup E=+\infty$ iff $E$ is not bounded above in $\Rbb$, and that $\inf E=-\infty$ iff $E$ is not bounded below in $\Rbb$. 
\end{rem}


\subsection{Cardinalities, countable sets, and product spaces $Y^X$}\label{lb4}


\begin{df}
Let $A$ and $B$ be sets. We say that $A$ and $B$ have the same \textbf{cardinality} \index{00@Cardinality $\card(A)$} and write $\card(A)=\card(B)$ (or simply $A\approx B$), if there is a bijection $f:A\rightarrow B$. We write $\card(A)\leq\card(B)$ (or simply $A\precsim B$) if $A$ and a subset of $B$ have the same cardinality. 
\end{df}



\begin{exe}\label{lb9}
Show that $\card(A)\leq\card(B)$ iff there is an injection $f:A\rightarrow B$, iff there is a surjection $g:B\rightarrow A$. (You need either Axiom of Choice or its consequence \eqref{eq5} to prove the last equivalence.)
\end{exe}

It is clear that $\approx$ is an equivalence relation on the collection of sets. It is also true that $\precsim$ is a partial order: Reflexivity and transitivity are easy to show. The proof of antisymmetry is more involved:



\begin{thm}[Schr\"oder-Bernstein]\label{lb8}\index{00@Schr\"oder-Bernstein theorem}
Let $A,B$ be two sets. If $A\precsim B$ and $B\precsim A$, then $A\approx B$.
\end{thm}

\begin{proof}[$\star\star$ Proof]
Assume WLOG that $A\subset B$. Let $f:B\rightarrow A$ be an injection. Let $A_n=f^n(A)$ defined inductively by $f^0(A)=A$, $f^n(A)=f(f^{n-1}(A))$. Let $B_n=f^n(B)$. Then
\begin{align*}
B_0\supset A_0\supset \cdots\supset B_n\supset A_n\supset B_{n+1}\supset\cdots
\end{align*}
In particular, $C=\bigcap_{n\in\Nbb}A_n$ equals $\bigcap_{n\in\Nbb}B_n$. Note that $f$ gives a bijection $B_n\setminus A_n\rightarrow B_{n+1}\setminus A_{n+1}$ (since $f$ gives bijections $B_n\rightarrow B_{n+1}$ and $A_n\rightarrow A_{n+1}$). Therefore, we have a bijection $g:B\rightarrow A$ defined by
\begin{gather*}
g(x)=\left\{
{\begin{array}{ll}
f(x)&\text{if $x\in B_n\setminus A_n$ for some $n\in\Nbb$}\\[0.5ex]
x&\text{otherwise}
\end{array}}
\right.
\end{gather*}
where ``otherwise" means either $x\in C$ or $x\in A_n\setminus B_{n+1}$ for some $n$.
\end{proof}

Intuition about the above proof: View $B$ as an onion. The layers of $B$ are $B_n\setminus A_n$ (the odd layers) and $A_n\setminus B_{n+1}$ (the even layers). The bijection $g$ maps each odder layer to the subsequent odd one, and fixes the even layers and the core $C$.


\begin{eg}\label{lb6}
If $-\infty<a<b<+\infty$, then $(0,1)\approx (a,b)$.
\end{eg}
\begin{proof}
$f:(0,1)\rightarrow (a,b)$ sending $x$ to $(b-a)x+a$ is a bijection.
\end{proof}

\begin{eg}\label{lb7}
If $-\infty<a<b<+\infty$, then $\Rbb\approx (a,b)$
\end{eg}

\begin{proof}
By the previous example, it suffices to prove $\Rbb\approx(-1,1)$. The function
\begin{gather}\label{eq20}
f:\Rbb\rightarrow(-1,1)\qquad f(x)=\left\{
\begin{array}{ll}
\frac x{1+x}&\text{ if $x\geq0$}\\[0.5ex]
-f(-x)&\text{ if $x<0$}
\end{array}
\right.
\end{gather}
is bijective.
\end{proof}


Alternatively, one may use the tangent function to give a bijection between $(-\pi/2,\pi/2)$ and $\Rbb$. I have avoided this method, since \eqref{eq20} is more elementary than trigonometric functions. The mathematically rigorous definition of trigonometric functions and the verification of their well-known properties are far from easy tasks. 



\begin{pp}
Let $I$ be an interval with endpoints $a<b$ in $\ovl\Rbb$. Then $I\approx\Rbb$.
\end{pp}

\begin{proof}
Let $A=(0,1)\cup\{-\infty,+\infty\}$. By Exp.  \ref{lb7}, we have
\begin{align*}
(a,b)\subset I\precsim \ovl\Rbb\approx A\approx[0,1]\subset (-2,2)\approx (a,b)
\end{align*}
So $I\approx\ovl\Rbb$ by Schr\"oder-Bernstein Thm. \ref{lb8}. In particular, $\Rbb=(-\infty,+\infty)\approx\ovl\Rbb$.
\end{proof}


\begin{df}
A set $A$ is called \textbf{finite} if $A\precsim\{1,\dots,n\}$ for some $n\in\Zbb_+$. $A$ is called  \textbf{countable} if $A\precsim\Nbb$. \index{00@Countable}
\end{df}

Clearly, a set $A$ is finite iff either $A\approx\emptyset$ or $A\approx\{1,\dots,n\}$ for some $n\in\Zbb_+$.

\begin{rem}
Let $A\subset\Nbb$. If $A$ is bounded above, then $A\subset\{0,\dots,n\}$ and hence $A$ is finite. If $A$ is not bounded above, then we can construct a strictly increasing sequence $(x_n)_{n\in\Nbb}$ in $A$. (Pick any $x_0\in A$. Suppose we have $x_n\in A$. Since $x_n$ is not an upper bounded of $A$, there is $x_{n+1}\in A$ larger than $x_n$. So $(x_n)_{n\in\Nbb}$ can be constructed inductively.) This gives an injection $\Nbb\rightarrow A$. Therefore $A\succsim \Nbb$, and hence $A\approx \Nbb$ by Schr\"oder-Bernstein.

It follows that if $B\precsim\Nbb$, then either $B$ is a finite set, or $B\approx\Nbb$. Therefore, ``a set $B$ is \textbf{countably infinite}" \index{00@Countably infinite} means the same as ``$B\approx\Nbb$".  \hfill\qedsymbol 
\end{rem}


\begin{thm}\label{lb15}
A countable union of countable sets is countable. In particular, $\Nbb\times\Nbb\approx\Nbb$.
\end{thm}

\begin{proof}
Recall Exe. \ref{lb9}. Let $A_1,A_2,\dots$ be countable sets. Since each $A_i$ is countable, there is a surjection $f_i:\Nbb\rightarrow A_i$. Thus, the map $f:\Nbb\times\Nbb\rightarrow \bigcup_i A_i$ defined by $f(i,j)=f_i(j)$ is surjective. Therefore, it suffices to show that there is a surjection $\Nbb\rightarrow\Nbb\times\Nbb$. This is true, since we have a bijection $g:\Nbb\rightarrow\Nbb\times\Nbb$ where $g(0),g(1),g(2),\dots$ are $(0,0)$, $(1,0)$, $(0,1)$, $(2,0)$, $(1,1)$, $(0,2)$, $(3,0)$, $(2,1)$, $(1,2)$, $(0,3)$, etc., as shown by the figure
\begin{align*}
\vcenter{\hbox{{
			\includegraphics[width=3.5cm]{fig1.png}}}}
\end{align*}
\end{proof}

As an application, we prove the extremely important fact that $\Qbb$ is countable.
\begin{co}
We have $\Nbb\approx\Zbb_+\approx\Zbb\approx \Qbb$.
\end{co}



\begin{proof}
Clearly $\Zbb_{<0}\approx\Nbb\approx \Zbb_+$. By Thm. \ref{lb15}, $\Zbb=\Zbb_{<0}\cup\Nbb$ is countably infinite, and hence $\Zbb\approx\Nbb$. It remains to prove $\Zbb\approx\Qbb$. By Schr\"oder-Bernstein, it suffices to prove $\Qbb\precsim\Zbb$.  By Thm. \ref{lb15} again, $\Zbb\times\Zbb\approx\Zbb$. By Exp. \ref{lb17}, there is a surjection from a subset of $\Zbb\times\Zbb$ to $\Qbb$. So $\Qbb\precsim\Zbb\times\Zbb\approx\Zbb$.
\end{proof}



Later, when we have learned Zorn's Lemma (an equivalent form of Axiom of Choice), we will be able to prove the following generalization of $\Nbb\times\Nbb\approx\Nbb$. So we defer the proof of the following theorem to a later section.

\begin{thm}\label{lb16}
Let $X$ be a infinite set. Then $X\times\Nbb\approx X$.
\end{thm}

\begin{proof}
See Thm. \ref{lb497}.
\end{proof}



Our next goal is to prove an exponential law $a^{b+c}=a^b\cdot a^c$ for cardinalities. For that purpose, we first need to define the set-theoretic operations that correspond to the summation $b+c$ and the exponential $a^b$.


\begin{df}
We write $X=\bigsqcup_{\alpha\in\scr A}A_\alpha$ \index{zz@$\bigsqcup_{\alpha\in\scr A}A_\alpha$, the disjoint union} and call $X$ the \textbf{disjoint union} \index{00@Disjoint union} of $(A_\alpha)_{\alpha\in\scr A}$,  if $X=\bigcup_{\alpha\in\scr A}A_\alpha$ and $(A_\alpha)_{\alpha\in\scr A}$ is a family of pairwise disjoint sets (i.e. $A_\alpha\cap A_\beta=\emptyset$ if $\alpha\neq\beta$). If moreover $\scr A=\{1,\dots,n\}$, we write $X=A_1\sqcup\cdots\sqcup A_n$.
\end{df}

\begin{df}
Let $X,Y$ be sets. Then \index{YX@$Y^X$, the set of functions $X\rightarrow Y$}
\begin{align}
Y^X=\{\text{functions }f:X\rightarrow Y\}
\end{align}
A more precise definition of $Y^X$ (in the spirit of \eqref{eq2}) is $\{f\in X\times Y \mid f:X\rightarrow Y\text{ is a function}\}$. Note that by Rem. \ref{lb40},
\begin{align}
Y^\emptyset=\{\emptyset\}  \label{eq10}
\end{align}
\end{df}

This new notation is compatible with the old one $Y^n=Y\times\cdots\times Y$:
\begin{eg}
Let $n\in\Zbb_+$. We have $Y^{\{1,\dots,n\}}\approx Y^n$ due to the bijection
\begin{align*}
Y^{\{1,\dots,n\}}\rightarrow Y^n\qquad f\mapsto (f(1),\dots,f(n))
\end{align*}
\end{eg}

\begin{rem}\label{lb18}
The above example suggests that in the general case that $X$ is not necessarily finite, we can view each function $f:X\rightarrow Y$ as $(f(x))_{x\in X}$, an \textbf{indexed family of elements} of $Y$ with index set $X$. Thus, intuitively and hence not quite rigorously, 
\begin{align}
Y^X=\underbrace{Y\times Y\times\cdots}_{\card(X)\text{ pieces}} \label{eq11}
\end{align}
This generalizes the intuition in Def. \ref{lb13} that a function $f:\Zbb_+\rightarrow Y$ is equivalently a sequence $(f(1),f(2),f(3),\dots)$.

The viewpoint that $Y^X$ is a \textbf{product space} with index set $X$ is very important and will be adopted frequently in this course. More generally, we can define:\hfill\qedsymbol
\end{rem}

\begin{df}
Let $(X_i)_{i\in I}$ be a family of sets with index set $I$. Their \textbf{product space} \index{00@Product space} \index{zz@$\prod_{i\in I}X_i$} is defined by
\begin{align*}
\prod_{i\in I}X_i =\{f\in \fk X^I:f(i)\in X_i\text{ for all }i\in I \}
\end{align*}
where $\fk X=\bigcup_{i\in I}X_i$. If each $X_i$ is nonempty, then $\prod_{i\in I}X_i$ is nonempty by Axiom of Choice. An element of $\prod_{i\in I}X_i$ is also written as $(f_i)_{i\in I}$ when the $i$-th component of it is $f_i\in X_i$.
\end{df}

In particular, if all $X_i$ are equal to $X$, then $X^I=\prod_{i\in I}X$.



\begin{eg}\label{lb11}
Let $X$ be a set. For each $A\subset X$, define the \textbf{characteristic function} \index{00@Characteristic function} \index{zz@$\chi_A$, the characteristic function of $A$} $\chi_A:X\rightarrow\{0,1\}$ to be
\begin{align*}
\chi_A(x)=\left\{
\begin{array}{ll}
1&\text{if }x\in A\\
0&\text{if }x\notin A
\end{array}
\right.
\end{align*}
Then we have
\begin{align*}
2^X\approx \{0,1\}^X
\end{align*}
since the following map is bijective:
\begin{gather*}
2^X\rightarrow\{0,1\}^X\qquad A\mapsto\chi_A
\end{gather*}
Its inverse is $f\in\{0,1\}^X\mapsto f^{-1}(1)\in 2^X$.
\end{eg}

\begin{pp}[Exponential Law]\label{lb10}
Suppose that $X=A_1\sqcup\cdots\sqcup A_n$. Then
\begin{align*}
Y^X\approx Y^{A_1}\times \cdots\times Y^{A_n}
\end{align*}
\end{pp}

\begin{proof}
We have a bijection
\begin{gather}\label{eq9}
\begin{gathered}
\Phi:Y^X\rightarrow Y^{A_1}\times \cdots\times Y^{A_n}\\
f\mapsto (f|_{A_1},\dots,f|_{A_n})
\end{gathered}
\end{gather}
where we recall that $f|_{A_i}$ is the restriction of $f$ to $A_i$. 
\end{proof}

\begin{exe}
Assume that $A_1,\dots,A_n$ are subsets of $X$. Define $\Phi$ by \eqref{eq9}. Prove that $\Phi$ is injective iff $X=A_1\cup\cdots\cup A_n$. Prove that $\Phi$ is surjective iff $A_1,\dots, A_n$ are pairwise disjoint. 
\end{exe}

\begin{co}\label{lb12}
Let $X,Y$ be finite sets with cardinalities $m,n\in\Nbb$ respectively. Assume that $Y\neq\emptyset$. Then $Y^X$ is a finite set with cardinality $n^m$.
\end{co}

\begin{proof}
The special case that $m=0$ (i.e. $X=\emptyset$, cf. \eqref{eq10}) and $m=1$ is clear. When $m>1$, assume WLOG that $X=\{1,\dots,m\}$. Then $X=\{1\}\sqcup\cdots\sqcup\{m\}$. Apply Prop. \ref{lb10} to this disjoint union. We see that $Y^X\simeq Y\times \cdots\times Y\simeq\{1,\dots,n\}^m$ has $n^m$ elements.
\end{proof}



We end this section with some (in)equalities about the cardinalities of product spaces. To begin with, we write $X\precnsim Y$ (or $\card(X)<\card(Y)$) if $X\precsim Y$ and $X\napprox Y$.

\begin{pp}\label{lb14}
Let $X,Y$ be sets with $\card(Y)\geq 2$ (i.e. $Y$ has at least two elements). Then $X\precnsim Y^X$. In particular, $X\precnsim 2^X$.
\end{pp}

\begin{proof}
The case $X=\emptyset$ is obvious since $0<1$. So we assume $Y\neq\emptyset$. Clearly $2^X\simeq\{0,1\}^X$ is $\precsim Y^X$. So it suffices to prove $X\precnsim 2^X$. Since the map $X\rightarrow 2^X$ sending $x$ to $\{x\}$ is injective, $X\precsim 2^X$. Let us prove $X\napprox 2^X$.

Assume that $X\approx 2^X$. So there is a bijection $\Phi:X\rightarrow 2^X$ sending each $x\in X$ to a subset $\Phi(x)$ of $X$. Motivated by Russell's Paradox \eqref{eq12}, we define
\begin{align*}
S=\{x\in X:x\notin \Phi(x)\}
\end{align*}
Since $\Phi$ is surjective, there exists $y\in X$ such that $S=\Phi(y)$. If $y\in\Phi(y)$, then $y\in S$, and hence $y\notin \Phi(y)$ by the definition of $S$. If $y\notin\Phi(y)$, then $y\notin S$, and hence $y\in\Phi(y)$ by the definition of $S$. This gives a contradiction.
\end{proof}


\begin{rem}
Write $\{1,\dots,n\}^X$ as $n^X$ for short. \index{nX@$n^X=\{1,\dots,n\}^X$} Assuming that real numbers have decimal, binary, or (more generally) base-$n$ representations where $n\in\Zbb_{\geq 2}$, then  $\Rbb\approx n^{\Nbb}$. So by Prop. \ref{lb14}, $\Nbb\precnsim\Rbb$, i.e. \textit{$\Rbb$ is uncountable}. The base-$n$ representations of real numbers suggest that $\card(n^\Nbb)$ is independent of $n$. This fact can be proved by elementary methods without  resorting to the analysis of real numbers:
\end{rem}

\begin{thm}
Let $X$ be an infinite set. Then
\begin{align*}
2^X\approx 3^X\approx 4^X\approx\cdots\approx \Nbb^X
\end{align*}
\end{thm}

\begin{proof}
First, we assume that $X=\Nbb$. Clearly, for each $n\in\Zbb_{\geq 2}$ we have $2^X\precsim n^X\precsim \Nbb^X$. Since elements of $\Nbb^X$ are subsets of $X\times\Nbb$ (i.e. elements of $2^{X\times\Nbb}$), we have
\begin{align*}
\Nbb^X\subset 2^{X\times\Nbb}\simeq 2^X
\end{align*}
since $X\times\Nbb\approx X$ by Thm. \ref{lb15}. So $2^X\approx n^X\approx \Nbb^X$ by Schr\"oder-Bernstein.

As pointed out earlier (cf. Thm. \ref{lb16}), it can be proved by Zorn's Lemma that $X\times\Nbb\approx X$ for every infinite set $X$. So the same conclusion holds for such $X$.
\end{proof}

\newpage

\section{Metric spaces}


We first give an informal introduction to metric spaces, hoping to motivate the readers from a (relatively) historical perspective. It is okay if you do not understand all of the concepts mentioned in the introduction on the first read. Simply return to this section when you feel unmotivated while formally studying these concepts in later sections. (The same suggestion applies to all the introductory sections and historical comments in our notes.)









\subsection{Introduction: what is point-set topology?}\label{lb55}


\begin{displayquote}
\small The method which has been used with success by Volterra and Hilbert consists in observing that a function (for instance a continuous one) can be replaced by a countable infinity of parameters. One treats the problem first as though one had only a finite number of parameters and then one goes to the limit... We believe that this method has played a useful role because it followed intuition, but that its time has passed... The most fruitful method in functional analysis seems to us to treat the element of which the functional depends directly as a variable and in the form in which it presents itself naturally.

\hfill ---- Fr\'echet, 1925 ~~(cf. \cite[Sec. 13.8]{Jah})
\end{displayquote}




In this chapter, we begin the study of point-set topology by learning one of its most important notions: metric spaces. Similar to \cite{Rud-P}, we prefer to introduce metric spaces and basic point-set topology at the early stage of our study. An obvious reason for doing so is that metric spaces provide a uniform language for the study of basic analysis problems in $\Rbb,\Rbb^n,\Cbb^n$, and more generally in function spaces such as the space of continuous functions $C([a,b])$ on the interval $[a,b]\subset\Rbb$. With the help of such a language, for example, many useful criteria for the convergence of series in $\Rbb$ and $\Cbb$ (e.g. root test, ratio test) are generalized straightforwardly to criteria for the \textit{uniform} convergence of series of functions in $C([a,b])$.

Point-set topology was born in 1906 when Fr\'echet defined metric spaces, motivated mainly by the study of function spaces in analysis (i.e. \textit{functional analysis}). Indeed, point-set topology and functional analysis are the two faces of the same coin: they both originated from the study of \textbf{functionals}, \index{00@Functionals} i.e., functions of functions. See for example \eqref{eq24}. The core ideas of point-set topology are as follows:
\begin{enumerate}[label=(\arabic*)]
\item Take $X$ to be a set of functions defined on a ``classical space" (e.g. the set of all continuous functions $f:[a,b]\rightarrow\Cbb$). Then a functional is a function  $S:X\rightarrow \Cbb$. This is a generalization of functions on $\Rbb,\Cbb,\Rbb^n,\Cbb^n$ or on their subsets.
\item Unlike $\Rbb^n$, a function space $X$ is usually ``infinite dimensional". Thus, one may think that a functional $S$ is a function with infinite variables. In point-set topology, this viewpoint is abandoned; the philosophy of point-set topology is diametrically opposed to that of multivariable calculus.\footnote{Very often, the formula of $S(f)$ involves an integral. See e.g. \eqref{eq24}. Mathematicians (e.g. Volterra, L\'evy, Fredholm, and early Hilbert) used to study $S(f)$ by discretizing $S(f)$, i.e., by approximating integrals by finite sums. Thus, $S$ is approximated by a sequence of functions with $n$ variables where $n\rightarrow\infty$. This viewpoint is abandoned in point-set topology.} Instead, \uline{one should view a functional $S$ as a function with one variable $x$}, where $x$ denotes a general point of the function space $X$.
\item Rather than looking at each variable/component and doing explicit muti-variable calculations, one uses geometric intuitions to study the analytic properties of functionals.\footnote{This is similar to linear algebra where one prefers vectors, linear subspaces, and linear operators to $n$-tuples, sets of solutions, and matrices.} \uline{These geometric intuitions (e.g. distances, open balls, convergence) are borrowed from  $\Rbb$ and $\Rbb^n$}  and are mostly irrelevant to dimensions or numbers of variables.
\end{enumerate}



(Sequential) \textbf{compactness}, \textbf{completeness}, and \textbf{separability} are prominent geometric properties that are useful in the study of the analytic properties of functionals. The importance of these three notions  was  already recognized by Fr\'echet by the time he defined metric spaces. The study of these three properties will be a main theme of our course.


Consider sequential compactness for example. The application of compactness to function spaces originated from the problems in calculus of variations. For instance, let $L(x,y,z)$ be a polynomial or (more generally) a continuous function in $3$ variables. We want to find a ``good" (e.g. differentiable) function $f:[0,1]\rightarrow \Rbb$ minimizing or maximizing the expression
\begin{subequations}\label{eq24}
\begin{align}
S(f)=\int_0^1 L(t,f(t),f'(t))dt
\end{align}
This is the general setting of Lagrangian mechanics. In the theory of integral equations, one considers the extreme values and points of the functional
\begin{align}
S(f)=\int_0^1\int_0^1 f(x)K(x,y)\ovl{f(y)}dxdy  \label{eq206}
\end{align}
\end{subequations}
where $K:[0,1]^2\rightarrow\Rbb$ is continuous and $f:[0,1]\rightarrow\Cbb$ is subject to the condition $\int_0^1 |f(x)|^2dx\leq 1$. Any $f$ maximizing (resp. minimizing) $S(f)$ is an eigenvector of the linear operator $g\mapsto \int_0^1 K(x,y)g(y)dy$ with maximal (resp. minimal) eigenvalue.


As we shall learn, (sequential) compactness is closely related to the problem of finding (or proving the existence of) maximal/minimal values of a continuous function and the points at which the function attains its maximum/minimum. So, in 19th century, when people were already familiar with sequential compactness in $\Rbb^n$ (e.g. Bolzano-Weierstrass theorem, Heine-Borel theorem), they applied compactness to function spaces and functionals. The idea is simple: Suppose we are given $X$, a set of functions (say continuous and differentiable) from $[a,b]$ to $\Rbb$. We want to find $f\in X$ maximizing $S(f)$. Here is an explicit process (see also the proof of Lem. \ref{lb56}):
\begin{itemize}
\item[(A)] Find a sequence $(f_n)_{n\in\Zbb_+}$ in $X$ such that $S(f_n)$ increases to $M=\sup S(X)$. 
\item[(B)] Define convergence in $X$ in a suitable way, and verify that $S:X\rightarrow\Rbb$ is continuous (i.e. if $f_n$ converges to $f$ in the way we define, then $S(f_n)\rightarrow S(f)$). 
\item[(C)] Suppose we can find a subsequence $(f_{n_k})_{k\in\Zbb_+}$ converging to some $f\in X$, then $S$ attains its maximum at $f$. In particular, $S(f)=M$ and hence $M<+\infty$. 
\end{itemize}


To carry out step (B), we need to define suitable geometric structures for a function space $X$ so that the convergence of sequences in $X$ and the continuity of functions $S:X\rightarrow\Rbb$ can be defined and studied in a similar pattern as that for $\Rbb^n$. \textbf{Metric} (of a metric space) and \textbf{topology} (of a topological space) are such geometric structures. As we shall learn, the topology of a metric space is uniquely determined by the convergence of sequences in this space. 

Step (C) can be carried out if every sequence in $X$ has a convergent subsequent, i.e., if $X$ is sequentially compact. Thus, we need a good criterion for sequential compactness for subsets of a function space.  Arzel\`a-Ascoli theorem, the  $C([a,b])$-version of Heine-Borel theorem, is such a criterion. This famous theorem was proved in late 19th century (and hence before the birth of point-set topology), and it gave an important motivation for Fr\'echet to consider  metric spaces in general. We will learn this theorem at the end of the first semester.


To summarize: Metric spaces are defined not just for fun. We introduce such geometric objects because we want to study the convergence of sequences and the analytic properties of continuous functions using geometric intuitions. And moreover, the examples we are interested in are not just subsets of $\Rbb^n$, but also subsets of function spaces. With this in mind, we now begin our journey into point-set topology.


\subsection{Basic definitions and examples}



\begin{df}
Let $X$ be a set. A function $d:X\times X\rightarrow\Rbb_{\geq0}$ is called a \textbf{metric} if for all $x,y,z\in X$ we have
\begin{enumerate}[label=(\arabic*)]
\item $d(x,y)=d(y,x)$.
\item $d(x,y)=0$ iff $x=y$.
\item (Triangle inequality) \index{00@Triangle inequality} $d(x,z)\leq d(x,y)+d(y,z)$.
\end{enumerate}
The pair $(X,d)$, or simply $X$, is called \index{00@Metric space} a \textbf{metric space}. If $x\in X$ and $r\in(0,+\infty]$,\footnote{We want open and closed balls to be nonempty. So we assume $r\neq0$ only for open balls.} the set \index{Br@$B_X(x,r)=B(x,r)$ and $\ovl B_X(x,r)=\ovl B(x,r)$}
\begin{align*}
B_X(x,r)=\{y\in X:d(x,y)<r\}
\end{align*}
often abbreviated to $B(x,r)$, is called the \textbf{open ball} with center $x$ and radius $r$. If $r\in[0,+\infty)$,
\begin{align*}
\ovl B_X(x,r)=\{y\in X:d(x,y)\leq r\}
\end{align*}
also abbreviated to $\ovl B(x,r)$, is called the \textbf{closed ball} with center $x$ and radius $r$.
\end{df}


We make some comments on this definition.


\begin{rem}
That ``$d(x,y)=0$ iff $x=y$" is very useful. Think about $X$ as a set of functions $[0,1]\rightarrow\Rbb$ and $d$ is a metric on $X$. To show that $f,g\in X$ are equal, instead of checking that infinitely many values are equal (i.e. $f(t)=g(t)$ for all $t\in\Rbb$), it suffices to check that one value (i.e. $d(f,g)$) is zero.
\end{rem}

\begin{rem}
Triangle inequality clearly implies ``polygon inequality":
\begin{align}
d(x_0,x_n)\leq\sum_{j=1}^n d(x_{j-1},x_j)
\end{align}
\end{rem}

\begin{rem}\label{lb20}
Choose distinct points $x,y\in X$. Then $x,y$ are separated by two open balls centered at them: there exists $r,\rho>0$ such that $B(x,r)\cap B(y,\rho)=\emptyset$. This is called the \textbf{Hausdorff property}. 

To see this fact, note that $d(x,y)>0$. Choose $r,\rho$ such that $r+\rho\leq d(x,y)$. If $z\in B(x,r)\cap B(y,\rho)$, then $d(x,z)+d(y,z)<r+\rho\leq d(x,y)$, contradicting triangle inequality.

We will see (cf. Prop. \ref{lb21}) that Hausdorff property guarantees that any sequence in a metric space cannot converge to two different points. Intuition: one cannot find a point which is very close to $x$ and $y$ at the same time.  \hfill\qedsymbol
\end{rem}



We give some examples, and leave it to the readers to check that they satisfy the definition of metric spaces. We assume that square roots of positive real numbers can be defined. (We will rigorously define square roots after we define $e^x$ using the series $\sum_{n\in\Nbb}x^n/n!$.)


\begin{eg}
$\Rbb$ is a metric space if we define $d(x,y)=|x-y|$
\end{eg}


\begin{eg}
On $\Rbb^n$, we can define \textbf{Euclidean metric} \index{00@Euclidean metric}
\begin{align*}
d(x,y)=\sqrt{(x_1-y_1)^2+\cdots+(x_n-y_n)^2}
\end{align*}
if $x_\blt,y_\blt$ are the components of $x,y$. The following are also metrics:
\begin{gather*}
d_1(x,y)=|x_1-y_1|+\cdot+|x_n-y_n|\\
d_\infty(x,y)=\max\{|x_1-y_1|,\dots,|x_n-y_n|\}
\end{gather*}
\end{eg}

\begin{eg}
The \textbf{Euclidean metric} on $\Cbb^n$ is
\begin{align*}
d(z,w)=\sqrt{|z_1-w_1|^2+\cdots+|z_n-w_n|^2}
\end{align*}
which agrees with the Euclidean metric on $\Rbb^{2n}$. The following are also metrics:
\begin{gather*}
d_1(z,w)=|z_1-w_1|+\cdot+|z_n-w_n|\\
d_\infty(z,w)=\max\{|z_1-w_1|,\dots,|z_n-w_n|\}
\end{gather*}
\end{eg}

\begin{cv}\label{lb33}
Unless otherwise stated, the metrics on $\Rbb^n$ and $\Cbb^n$ (and their subsets) are assumed to be the Euclidean metrics.
\end{cv}


\begin{rem}
One may wonder what the subscripts $1,\infty$ mean. This notation is actually due to the general fact that
\begin{equation*}
d_p(z,w)=\sqrt[p]{|z_1-w_1|^p+\cdots+|z_n-w_n|^p}
\end{equation*}
is a metric where $1\leq p< +\infty$, and $d_\infty=\lim_{q\rightarrow +\infty}d_q$. It is not easy to prove that $d_p$ satisfies triangle inequality: one needs Minkowski inequality. For now, we will not use such general $d_p$. And we will discuss Minkowski inequality in later sections.
\end{rem}


\begin{eg}\label{lb19}
Let $X=X_1\times\cdots\times X_N$ where each $X_i$ is a metric space with metric $d_i$. Write $x=(x_1,\dots,x_N)\in X$ and $y=(y_1,\dots,y_N)\in Y$. Then the following are metrics on $X$:
\begin{gather*}
d(x,y)=d_1(x_1,y_1)+\cdots+d_N(x_N,y_N)\\
\delta(x,y)=\max\{d_1(x_1,y_1),\dots,d_N(x_N,y_N)\}\\
\rho(x,y)=\sqrt{d_1(x_1,y_1)^2+\cdots+d_N(x_N,y_N)^2}
\end{gather*}
With respect to the metric $\delta$, the open balls of $X$ are ``polydisks"
\begin{align*}
B_X(x,r)=B_{X_1}(x_1,r)\times\cdots\times B_{X_N}(x_N,r)
\end{align*}
\end{eg}


There is no standard choice of metric on the product of metric spaces. $d,\delta,\rho$ are all good, and they are equivalent in the following sense:

\begin{df}
We say that two metrics $d_1,d_2$ on a set $X$ are \index{00@Equivalent metrics} \textbf{equivalent} and write $d_1\approx d_2$, if there exist $\alpha,\beta>0$ such that  for any $x,y\in X$ we have
\begin{gather*}
d_1(x,y)\leq\alpha d_2(x,y)\qquad d_2(x,y)\leq\beta d_1(x,y)
\end{gather*}  
This is an equivalence relation. More generally, we may write $d_1\precsim d_2$ if $d_1\leq \alpha d_2$ for some $\alpha>0$. Then $d_1\approx d_2$ iff $d_1\precsim d_2$ and $d_2\precsim d_1$.
\end{df}



\begin{eg}
In Exp. \ref{lb19}, we have $\delta\leq \rho\leq d\leq N\delta$. So $\delta\approx\rho\approx d$.
\end{eg}

\begin{cv}\label{lb32}
Given finitely many metric spaces $X_1,\dots,X_N$, the metric on their product space $X=X_1\times\cdots\times X_N$ is chosen to be any one that is equivalent to the ones defined in Exp. \ref{lb19}. In the case that each $X_i$ is a subset of $\Rbb$ or $\Cbb$, we follow Convention \ref{lb33} and choose the metric on $X$ to be the Euclidean metric (unless otherwise stated).
\end{cv}


\begin{df}\label{lb43}
Let $(X,d)$ be a metric space. Then a \textbf{metric subspace} \index{00@Metric subspace} denotes an object $(Y,d|_Y)$ where $Y\subset X$ and $d|_Y$ is the restriction of $d$ to $Y$, namely, for all $y_1,y_2\in Y$ we set
\begin{align*}
d|_Y(y_1,y_2)=d(y_1,y_2)
\end{align*}
\end{df}

\begin{cv}\label{lb76}
Suppose $Y$ is a subset of a given metric space $(X,d)$. Unless otherwise stated, the metric of $Y$ is chosen to be $d|_Y$ whenever $Y$ is viewed as a metric space. For example, the metric of any subset of $\Rbb^n$ is assumed to be the Euclidean metric, unless otherwise stated.
\end{cv}



\subsection{Convergence of sequences} \label{lb73}

\begin{df}
Let $(x_n)_{n\in\Zbb_+}$ be a sequence in a metric space $X$. Let $x\in X$. We say that $x$ is the \textbf{limit} of $x_n$ and write $\displaystyle\lim_{n\rightarrow\infty}x_n=x$ (or $x_n\rightarrow x$), if:
\begin{itemize}
\item For every real number $\varepsilon>0$ there exists $N\in\Zbb_+$ such that for every $n\geq N$ we have $d(x_n,x)<\varepsilon$. 
\end{itemize}
Equivalently, this means that every (nonempty) open ball centered at $x$ contains \textbf{all but finitely many} \index{00@All but finitely many $x_n$} $x_n$.\footnote{``All but finitely many $x_n$ satisfies..." means ``for all but finitely many $n$, $x_n$ satisfies...". It does NOT mean that ``all but finitely many elements of the set $\{x_n:n\in\Zbb_+\}$ satisfies...".} 
\end{df}

\begin{rem}\label{lb100}
The negation of $x_n\rightarrow x$ is clear: 
\begin{itemize}
\item There exists $\eps>0$ such that for all $N\in\Zbb_+$ there exists $n\geq N$ such that $d(x_n,x)\geq\eps$.
\end{itemize}
Namely, one changes each ``for all" to ``there exists", changes each ``there exists" to ``for all", and negate the last sentence.
\end{rem}


\begin{exe}
Show that in the above definition of limits,  it suffices to consider rational numbers $\varepsilon>0$. (Note: You need to use Prop. \ref{lb2}.) 
\end{exe}
This exercise implies that the definition of limits does not require the existence of real numbers, i.e., does not assume Thm. \ref{lb3}. Indeed, we will use limits of sequences (and ``double sequences") to prove Thm. \ref{lb3}.


In many textbooks and research papers, you will see phrases such as \index{00@Sufficiently large}
\begin{gather}
\text{$x_n$ satisfies property $P$ for } \textbf{sufficiently large} \text{ $n$}
\end{gather}
This means that ``there exists $N\in\Zbb_+$ such that $P$ holds for all $n\geq N$". (We also say that $x_n$ \textbf{eventually} satisfies $P$.) Then $\lim_{n\rightarrow\infty} x_n=x$ means that ``for every $\varepsilon>0$, we have $d(x_n,x)<\varepsilon$ for sufficiently large $n$". 

\begin{pp}\label{lb21}
Any sequence $(x_n)_{n\in\Zbb_+}$ in a metric space $X$ has at most one limit.
\end{pp}

\begin{proof}
Suppose $(x_n)_{n\in\Zbb_+}$ converges to $x,y\in X$ where $x\neq y$. By Hausdorff property (Rem. \ref{lb20}), there exist $r,\rho>0$ such that $B(x,r)\cap B(y,\rho)=\emptyset$. By the definition of $x_n\rightarrow x$, there exists $N_1\in\Zbb_+$ such that $x_n\in B(x,r)$ for all $n\geq N_1$. Similarly, $x_n\rightarrow y$ means that there is $N_2\in\Zbb_+$ such that $x_n\in B(y,\rho)$ for all $n\geq N_2$. Choose any $n\geq N_1,N_2$ (e.g. $n=\max\{N_1,N_2\}$). Then $x_n\in B(x,r)\cap B(y,\rho)=\emptyset$, impossible.
\end{proof}


\subsubsection{Methods for proving convergence and computing limits}



\begin{eg}
$\dps \lim_{n\rightarrow\infty}\frac 1n=0$.
\end{eg}


\begin{proof}
Choose any $\varepsilon\in\Qbb_{>0}$. By Archimedean property, there exists $N\in\Zbb_+$ such that $N\varepsilon>1$, i.e. $1/N<\varepsilon$. Thus, for all $n\geq N$ we have $1/n<\varepsilon$.
\end{proof}

\begin{pp}
Let $\Fbb\in\{\Qbb,\Rbb\}$ and $(x_n),(y_n)$ be sequences in $\Fbb$ converging to $x,y\in\Rbb$. If $x_n\leq y_n$ for all $n$, then $x\leq y$.
\end{pp}

\begin{proof}
If $y<x$, let $\varepsilon=x-y$. Then all but finitely many members of $(x_n)$ are in $(x-\varepsilon/2,x+\varepsilon/2)$, and all but finitely many members of $(y_n)$ are in $(y-\varepsilon/2,y+\varepsilon/2)$. Since $y+\eps/2<x-\eps/2$, there must exist $n$ such that $y_n<x_n$.
\end{proof}


\begin{df}
If $A$ and $B$ are posets (or more generally, preordered sets, see Def. \ref{lb116}), we say a function $f:A\rightarrow B$ is \textbf{increasing} \index{00@Increasing and decreasing} \index{00@Strictly increasing and strictly decreasing} (resp. \textbf{strictly increasing}), if for each $x,y\in A$ we have
\begin{gather*}
x\leq y\qquad\Longrightarrow\qquad f(x)\leq f(y)\\
\text{resp.}\\
x<y\qquad\Longrightarrow \qquad f(x)<f(y)
\end{gather*}
We leave the definitions of \textbf{decreasing} and \textbf{strictly decreasing} to the readers. We say that $f$ is \index{00@Monotonic} \index{00@Strictly monotonic} \textbf{monotonic} (resp. \textbf{strictly monotonic}), if $f$ is either increasing or decreasing (resp. either strictly increasing or strictly decreasing).
\end{df}


\begin{pp}\label{lb57}
Let $(x_n)_{n\in\Zbb}$ be a sequence in $[a,b]\subset\Rbb$. If $(x_n)$ is increasing (resp. decreasing), then $\dps\lim_{n\rightarrow \infty}x_n$ equals $\dps\sup\{x_n:n\in\Zbb_+\}$ (resp. $\dps\inf\{x_n:n\in\Zbb_+\}$).
\end{pp}

\begin{proof}
Assume $(x_n)$ increases. (The case of decreasing is similar and hence its proof is omitted.) Let $A=\sup\{x_n:n\in\Zbb_+\}<+\infty$. Then for each $\eps>0$ there is $N$ such that $x_N>A-\eps$ (since $A-\eps$ is not an upper bound). Since $(x_n)$ is increasing, for all $n\in\Nbb$ we have $A-\eps<x_n\leq A$ and so $|x_n-A|<\eps$.
\end{proof}



\begin{eg}\label{lb27}
Let $(x_n)_{n\in\Zbb_+}$ be a sequence in a metric space $X$, and let $x\in X$. It is easy to see that
\begin{align*}
\lim_{n\rightarrow\infty} x_n=x\qquad\Longleftrightarrow \qquad \lim_{n\rightarrow\infty} d(x_n,x)=0
\end{align*}
\end{eg}

\begin{eg}\label{lb28}
Suppose that $(a_n)$ and $(b_n)$ are sequences in $\Rbb_{\geq 0}$, that $a_n\leq b_n$ for all $n$, and that $b_n\rightarrow 0$. Then  $a_n\rightarrow 0$. 
\end{eg}
\begin{proof}
For each $\varepsilon>0$, $[0,\varepsilon)$ contains all but finitely many $b_n$, and hence all but finitely many $a_n$.
\end{proof}


More generally, we have:

\begin{pp}[\textbf{Squeeze theorem}]\label{lb29}\index{00@Squeeze theorem}
Suppose that $(x_n)$ is a sequence in a metric space $X$. Let $x\in X$. Suppose that there is a sequence $(a_n)$ in $\Rbb_{\geq 0}$ such that $\dps\lim_{n\rightarrow\infty}a_n=0$ and that $d(x_n,x)\leq a_n$ for all $n$. Then $\dps\lim_{n\rightarrow\infty} x_n=x$.
\end{pp}

\begin{proof}
This follows immediately from Exp. \ref{lb27} and \ref{lb28}.
\end{proof}

The above proposition explains why many people say that ``analysis is the art of inequalities": It transforms the problem of convergence to the problem of finding a sequence $(a_n)\in\Rbb_{\geq 0}$ converging to $0$ such that the inequality $d(x_n,x)\leq a_n$ holds. And very often, a good (hard) analyst is one who knows how to find such good sequences!






\begin{pp}\label{lb38}
Let $X=X_1\times\cdots\times X_N$ be a product of metric spaces $(X_i,d_i)$. Let $d$ be any of the three metrics of $X$ in Exp. \ref{lb19}. Let $\mbf x_n=(x_{1,n},\dots,x_{N,n})$ be a sequence in $X$. Let $\mbf y=(y_1,\dots,y_N)$. Then 
\begin{align*}
\lim_{n\rightarrow\infty} \mbf x_n=\mbf y\qquad\Longleftrightarrow \qquad \lim_{n\rightarrow\infty} x_{i,n}=y_i~~(\forall 1\leq i\leq N)
\end{align*}
\end{pp}

\begin{proof}
We let $d$ be the metric $\delta$ in Exp. \ref{lb19}, i.e. defined by $\max_j d_j(x_j,y_j)$. Now choose a sequence $(\mbf x_n)$ and an element $\mbf y$ in $X$. Then
\begin{align}
\mbf x_n\rightarrow \mbf y~~\Longleftrightarrow~~ d_X(\mbf x_n,\mbf y)\rightarrow 0 ~~\Longleftrightarrow~~ \max_{1\leq j\leq N} d_j(x_{j,n},y_j)\rightarrow 0  \label{eq13}
\end{align}


Suppose that the RHS of \eqref{eq13} is true. Fix any $1\leq i\leq N$. Then $d_i(x_{i,n},y_i)\leq \max_j d_j(x_{j,n},y_j)$. So $x_{i,n}\rightarrow y_i$ by Prop. \ref{lb29}.

Conversely, assume that for every $i$ we have $x_{i,n}\rightarrow y_i$. Then for every $\eps>0$ there is $K_i\in\Zbb_+$ such that $d_i(x_{i,n},y_i)<\varepsilon$ for every $n\geq K_i$. Let $K=\max\{K_1,\dots,K_N\}$. Then for all $n\geq K$ we have $\max_j d_j(x_{j,n},y_j)<\eps$. This proves the RHS of \eqref{eq13}.

If $d$ is one of the other two metrics in Exp. \ref{lb19}, one can either use a similar argument, or use the following important (but easy) fact.
\end{proof}





\begin{pp}
Let $d,\delta$ be two equivalent metrics on a set $X$. Let $(x_n)_{n\in\Zbb_+}$ and $x$ be in $X$. Then $(x_n)$ converges to $x$ under the metric $d$ iff  $(x_n)$ converges to $x$ under $\delta$. Namely, $d(x_n,x)\rightarrow 0$ iff $\delta(x_n,x)\rightarrow 0$.
\end{pp}

\begin{proof}
Prove it yourself. (Or see Prop. \ref{lb48}.)
\end{proof}

More useful formulas about limit will be given in Exp. \ref{lb111}.







\subsubsection{Criteria for divergence}\label{lb119}

\begin{df}
A subset $E$ of a metric space $(X,d)$ is called \index{00@Bounded subset} \textbf{bounded} if either $E=\emptyset$ or there exist $p\in X$ and $R\in\Rbb_{>0}$ such that $E\subset B_X(p,R)$. If $X$ is bounded, we also say that $d$ is a \textbf{bounded metric}. \index{00@Bounded metric}
\end{df}

\begin{rem}
Note that if $E$ is bounded, then  for \textit{any} $q\in X$ there exists $\wtd R\in\Rbb_{>0}$ such that $E\subset B_X(q,\wtd R)$. (Indeed, choose $\wtd R=R+d(p,q)$, then by triangle inequality, $B(p,R)\subset B(q,\wtd R)$.)
\end{rem}

Some easy examples are as follows.
\begin{eg}
In a metric space $X$, if $x\in X$ and $0<r<+\infty$, then $B(x,r)$ is bounded. Hence $\ovl B(x,r)$ is bounded (since it is inside $B(x,2r)$).
\end{eg}


%% Record #1 2023/09/18 two lectures  2

Also, it is easy to see:
\begin{eg}\label{lb22}
A finite union of bounded subsets is bounded.
\end{eg}

\begin{pp}\label{lb24}
Let $(x_n)_{n\in\Zbb_+}$ be a convergent sequence in a metric space $X$. Then $(x_n)_{n\in\Zbb_+}$ is bounded.
\end{pp}

By saying that a sequence \index{00@Bounded sequence} $(x_n)_{n\in\Zbb_+}$ in $X$ is \textbf{bounded}, we mean that its range in $X$ (namely $\{x_n:n\in\Zbb_+\}$) is bounded.

\begin{proof}
Suppose that $x_n\rightarrow x$. Then for each $\varepsilon>0$, say $\varepsilon=1$, all but finitely many elements of $x_n$ (say $x_1,\dots,x_N$) are in $B(x,1)$. So this whole sequence is in $A=\{x_1\}\cup\cdots\{x_N\}\cup B(x,1)$. Since each $\{x_i\}$ is bounded, and since $B(x,1)$ is bounded, $A$ is bounded by Exp. \ref{lb22}.
\end{proof}


\begin{rem}\label{lb26}
Prop. \ref{lb24} gives our first criterion on divergence: If a sequence is unbounded (e.g. $x_n=n^2$), then it does not converge. But there are many bounded and divergent sequences. (See Exp. \ref{lb25}.) In this case, we need the second criterion: If a sequence has two subsequences converging to two different points, then this sequence diverge. (See Prop. \ref{lb23})
\end{rem}




\begin{df}
Let $(x_n)_{n\in\Zbb_+}$ be a sequence in a set $X$. If $(n_k)_{k\in\Zbb_+}$ is a strictly increasing sequence in $\Zbb_+$, we say that $(x_{n_k})_{k\in\Zbb_+}$ is a \textbf{subsequence} of $(x_n)_{n\in\Zbb_+}$. \index{00@Subsequence}
\end{df}



Thus, a subsequence of $(x_n)_{n\in\Zbb_+}$ is equivalently the restriction of the function $x:\Zbb_+\rightarrow X$ to an infinite subset of $\Zbb_+$.

\begin{pp}\label{lb23}
Let $(x_n)_{n\in\Zbb_+}$ be a sequence in a metric space $X$ converging to $x\in X$. Then every subsequence $(x_{n_k})_{k\in\Zbb_+}$ converges to $x$.
\end{pp}

\begin{proof}
For every $\varepsilon>0$, $B(x,\varepsilon)$ contains all but finitely many $\{x_n:n\in\Zbb_+\}$, and hence all but finitely many $\{x_{n_k}:k\in\Zbb_+\}$.
\end{proof}

\begin{eg}\label{lb25}
The sequence $x_n=(-1)^n$ in $\Rbb$ is divergent, because the subsequence $(x_{2k})_{k\in\Zbb_+}$ converges to $1$, whereas $(x_{2k-1})_{k\in\Zbb_+}$ converges to $-1$. 
\end{eg}




One may wonder if the two criteria in Rem. \ref{lb26} are complete in order to determine whether a sequence diverges. This is true for sequences in $\Rbb^n$. We will discuss this topic later. (See Cor. \ref{lb75}.)



\subsection{Continuous maps of metric spaces}\label{lb185}

Continuous maps are a powerful tool for showing that a sequence converges.


\begin{df}\label{lb31}
Let $f:X\rightarrow Y$ be a map of metric spaces. Let $x\in X$. We say that $f$ is \textbf{continuous at $x$} if one of the following equivalent conditions hold:
\begin{itemize}[align=left]
\item [(1)] For every sequence $(x_n)_{n\in\Zbb_+}$ in $X$, we have
\begin{align*}
\lim_{n\rightarrow\infty} x_n=x\qquad\Longrightarrow\qquad \lim_{n\rightarrow\infty} f(x_n)=f(x)
\end{align*}
\item[(2)] For every $\varepsilon>0$, there exists $\delta>0$ such that for every $p\in X$ satisfying $d(p,x)<\delta$, we have $d(f(p),f(x))<\varepsilon$.
\item[(2')] For every $\varepsilon>0$, there exists $\delta>0$ such that $B_X(x,\delta)\subset f^{-1}(B_Y(f(x),\varepsilon)))$.
\end{itemize}
We say that $f$ is a \textbf{continuous map/function}, if $f$ is continuous at every point of $X$.
\end{df}

\begin{proof}[Proof of the equivalence]
(2)$\Leftrightarrow$(2'): Obvious.

(2)$\Rightarrow$(1): Assume (2). Assume $x_n\rightarrow x$. For every $\varepsilon>0$, let $\delta>0$ be as in (2). Then since $x_n\rightarrow x$, there is $N\in\Zbb_+$ such that for all $n\geq N$ we have $d(x_n,x)<\delta$. By (2), we have $d(f(x_n),f(x))<\varepsilon$ for all $n\geq N$. This proves $f(x_n)\rightarrow f(x)$.

$\neg$(2)$\Rightarrow$ $\neg$(1): Assume that (2) is not true. Then there exists $\varepsilon>0$ such that for every $\delta>0$, there exists $p\in X$ such that $d(p,x)<\delta$ and $d(f(p),f(x))\geq\varepsilon$. Thus, for every $n\in\Zbb_+$, by taking $\delta=1/n$, we see that there exists $x_n\in X$ such that $d(x_n,x)<1/n$ and $d(f(x_n),f(x))\geq\varepsilon$. By Squeeze theorem (Prop. \ref{lb29}), $x_n\rightarrow x$. But $f(x_n)\nrightarrow f(x)$ (i.e. $d(f(x_n),f(x))\nrightarrow 0$). So (1) is not true.
\end{proof}

\begin{rem}
One can compare Def. \ref{lb31}-(1) to the definition of linear maps. A map is continuous iff it \textit{preserves the convergence of sequences}, i.e., iff it maps convergent sequences to convergent ones. A map (between vector spaces) is linear iff it perserves the addition and the scalar multiplication of vectors. In general, a good map between two sets with ``structures" is a map which preserves the structures. (Thus, linear combinations encode the linear structures of vector spaces. Similarly, the convergence of sequences remembers the ``topological" structures of metric spaces.) As another example, we will define an isometry of metric spaces to be one that preserves the metrics (the structures of metric spaces), see Exe. \ref{lb46}.
\end{rem}

\begin{rem}\label{lb115}
In this section, we mainly use Def. \ref{lb31}-(1) to study continuity. But in later sections we will also use Def. \ref{lb31}-(2'). An advantage of (2') is that it is more geometric. Indeed, if $X$ is a metric space and $E\subset X$, we say that $x\in E$ is an \textbf{interior point of $E$ in $X$} \index{00@Interior point} if there exists $\delta>0$ such that $B_X(x,\delta)\subset E$. For example, a point  $z\in\Cbb$ is an interior point of the closed unit disk $\ovl B_\Cbb(0,1)=\{w\in\Cbb:|w|\leq 1\}$ iff $|z|<1$. 

Thus, Def. \ref{lb31}-(2') says that for any map of metric spaces $f:X\rightarrow Y$ and $x\in X$, the following are equivalent:
\begin{itemize}
\item[(a)] $f$ is continuous at $x$.
\item[(b)] For each $\eps>0$, every $x\in X$ is an interior point of $f^{-1}\big(B_Y(f(x),\varepsilon)\big)$.
\end{itemize}
We say that a subset $U\subset X$ is \textbf{open} \index{00@Open set} if each point of $U$ is an interior point. For example, by triangle inequality, every open ball in a metric space is an open set. Thus, we have:
\begin{itemize}
\item A map of metric spaces $f:X\rightarrow Y$ is continuous iff the preimage under $f$ of every open ball of $Y$ is an open subset of $X$.
\end{itemize}

In the study of point-set topology, we will see that many properties can be studied in two approaches: using sequences (or using nets, the natural generalizations of sequences) and their convergence, and using open sets. The first example of such property is continuity, as we have seen in Def. \ref{lb31}. Another prominent example is sequential compactness vs. compactness. These two approaches represent two (very) different intuitions: one is dynamic, while the other is static and more geometric. (So it is surprising that these two very things are actually equivalent!) Sometimes both approaches work for a problem, but sometimes only one of them works, or one of them is much simpler. If you are a beginner in analysis and point-set topology, I suggest that whenever you see one approach applied to a problem, try to think about whether the other approach also works and which one is better.   \hfill\qedsymbol
\end{rem}


\subsubsection{Methods for proving continuity}


\begin{lm}\label{lb30}
Let $f:X\rightarrow Y$ be a map of metric spaces.  Let $(B_i)_{i\in I}$ be a collection of open balls in $X$ such that $X=\bigcup_{i\in I}B_i$. Suppose that for each $i$, the restriction $f|_{B_i}:B_i\rightarrow Y$ is continuous. Then $f$ is continuous. 
\end{lm}

This lemma shows that if we can prove that $f$ is ``locally" continuous, then $f$ is globally continuous. 

\begin{proof}
Choose $(x_n)$ in $X$ converging to $x\in X$. We shall show $f(x_n)\rightarrow f(x)$. Choose $i$ such that $x\in B_i$. Then one can find $\delta>0$ such that $B(x,\delta)\subset B_i$. (In the language of point-set topology: $x$ is an interior point of $B_i$.) To see this, write $B_i=B(y,r)$. Since $x\in B(y,r)$, we have $r-d(x,y)>0$. Choose $0<\delta\leq r-d(x,y)$. Then triangle inequality implies $B(x,\delta)\subset B(y,r)$. 

Since $x_n\rightarrow x$, there is $N\in\Zbb_+$ such that $x_n\in B(x,\delta)$ for all $n\geq N$. Thus, $(x_{k+N})_{k\in\Zbb_+}$ converges in $B_i$ to $x$. Since $f|_{B_i}$ is continuous, $\lim_{k\rightarrow\infty} f(x_{k+N})=f(x)$. So $f(x_n)\rightarrow f(x)$.
\end{proof}







\begin{df}
A map of metric spaces $f:X\rightarrow Y$ is called \index{00@Lipschitz continuous} \textbf{Lipschitz continuous} if there exists $L\in\Rbb_{>0}$ (called \textbf{Lipschitz constant}) \index{00@Lipschitz constant} such that for all $x_1,x_2\in X$,
\begin{align}
d_Y\big(f(x_1),f(x_2) \big)\leq L\cdot d_X(x_1,x_2) \label{eq14}
\end{align}
\end{df}

\begin{lm}\label{lb34}
Lipschitz continuous maps are continuous. 
\end{lm}

\begin{proof}
Suppose that $f:X\rightarrow Y$ is Lipschitz continuous with Lipschitz constant $L$. Suppose $x_n\rightarrow x$ in $X$. Then $L\cdot d(x_n,x)\rightarrow 0$. By \eqref{eq14} and Squeeze theorem (Prop. \ref{lb29}), $f(x_n)\rightarrow f(x)$. (You can also use Def. \ref{lb31}-(2) to prove this lemma.)
\end{proof}

\begin{eg}\label{lb35}
Let $\Fbb\in\{\Qbb,\Rbb,\Cbb\}$. The map $z\in\Fbb\setminus \{0\}\mapsto z^{-1}\in\Fbb$ is continuous.
\end{eg}



\begin{proof}
Let this map be $f$. Since $\Fbb$ is covered by open balls $B(z,\delta)$ where $z\in\Fbb\setminus\{0\}$ and $0<\delta<|z|$, by Lem. \ref{lb30}, it suffices to prove that $f$ is continuous when restricted to every such $B(z,\delta)$. Let $\eps=|z|-\delta>0$. Choose $x,y\in B(z,\delta)$. Then $|x|=|x-z+z|\geq |z|-|z-x|>\eps$ by triangle inequality. Similarly, $|y|>\eps$. So
\begin{align*}
|f(x)-f(y)|=|x^{-1}-y^{-1}|=|x-y|/|xy|\leq \eps^{-2}|x-y|
\end{align*}
So $f|_{B(z,\delta)}$ has Lipschitz constant $\eps^{-2}$, and hence is continuous.
\end{proof}

(Question: in the above proof, is the map $f:\Fbb\setminus\{0\}\rightarrow\Fbb$ Lipschitz continuous?)

We have given a fancy way of proving that  if $(z_n)$ is a sequence in $\Fbb\setminus\{0\}$ converging to $z\in\Fbb\setminus\{0\}$, then $z_n^{-1}\rightarrow z^{-1}$. You should think about how to prove this fact directly using $\eps-N$ language, and compare your proof with the above proof to find the similarities!




\begin{pp}\label{lb41}
Let $\Fbb\in\{\Qbb,\Rbb,\Cbb\}$. Then the following maps are continuous:
\begin{gather*}
+:\Fbb\times\Fbb\rightarrow\Fbb\qquad (x,y)\mapsto x+y\\
-:\Fbb\times\Fbb\rightarrow\Fbb\qquad  (x,y)\mapsto x-y\\
\times: \Fbb\times\Fbb\rightarrow\Fbb\qquad  (x,y)\mapsto xy\\
\div:\Fbb\times\Fbb^\times\rightarrow\Fbb\qquad  (x,y)\mapsto x/y
\end{gather*}
\end{pp}


Recall our Convention \ref{lb32} on the metrics of finite product spaces. 


\begin{proof}
We only prove that the last two are continuous: the first two can be treated in a similar (and easier) way.

Denote the multiplication map by $\mu$. We choose the metric on $\Fbb^2$ to be $d(\mbf x,\mbf x')=\max\{|x_1-x_1'|,|x_2-x_2'|\}$. Since $\Fbb\times\Fbb$ is covered by open balls of the form $B(0,r)=\{(x,y)\in\Fbb^2:|x|<r,|y|<r\}$ where $0<r<+\infty$, by Lem. \ref{lb30} and \ref{lb34}, it suffices to show that $\mu|_{B(0,r)}$ is Lipschitz continuous. This is true, since for each $(x,y),(x',y')\in B(0,r)$, we have
\begin{align}\label{eq22}
\begin{aligned}
&|\mu(x,y)-\mu(x',y')|=|xy-x'y'|\leq |(x-x')y|+|x'(y-y')|\\
<&2r\cdot \max\{|x-x'|,|y-y'|\}=2r \cdot d((x,y),(x',y'))  
\end{aligned}
\end{align}

By Exp. \ref{lb35} and Prop. \ref{lb37}, the map $(x,y)\in\Fbb\times\Fbb^\times\mapsto (x,y^{-1})\in\Fbb\times\Fbb$ is continuous. So its composition with the continuous map $\mu$ is continuous, thanks to Prop. \ref{lb36}. So $\div$ is continuous. 
\end{proof}

\begin{pp}\label{lb36}
Suppose that $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ are continuous maps of metric spaces. Then $g\circ f:X\rightarrow Z$ is continuous.
\end{pp}

\begin{pp}\label{lb37}
Suppose that $f_i:X_i\rightarrow Y_i$ is a map of metric spaces, where $1\leq i\leq N$. Then the product map \index{00@Product map}
\begin{gather*}
f_1\times\cdots\times f_N:X_1\times\cdots\times X_N\rightarrow Y_1\times\cdots\times Y_N\\
(x_1,\dots,x_N)\mapsto (f_1(x_1),\dots,f_N(x_N))
\end{gather*}
is continuous if and only if $f_1,\dots,f_N$ are continuous.
\end{pp}

\begin{proof}[Proof of Prop. \ref{lb36} and \ref{lb37}]
Immediate from Def. \ref{lb31}-(1) and Prop. \ref{lb38}.
\end{proof}


\begin{co}[\textbf{Squeeze theorem}]\index{00@Squeeze theorem}\label{lb61}
Let $\Fbb\in\{\Qbb,\Rbb\}$ and $(x_n),(y_n),(z_n)$ be sequences in $\Rbb$. Assume that $x_n\leq y_n\leq z_n$ for all $n$. Assume that $x_n$ and $z_n$ both converge to $A\in\Rbb$. Then $\lim_{n\rightarrow\infty}y_n=A$. 
\end{co}

\begin{proof}
Let $a_n=y_n-x_n$ and $b_n=z_n-x_n$. Then $0\leq a_n\leq b_n$, and $\lim_n b_n=\lim_n z_n-\lim_n x_n= 0$ because the subtraction map is continuous (Prop. \ref{lb41}). By Exp. \ref{lb28}, $a_n\rightarrow 0$. Since $x_n\rightarrow A$, $y_n=x_n+a_n$ converges to $A$, since the addition map is continuous by Prop. \ref{lb41}.
\end{proof}

Again, this is a fancy way of proving Squeeze theorem. The readers should know how to prove it directly from the definition of limits of sequences.

We give some more examples of continuous maps.

\begin{eg}
By Prop. \ref{lb41}, $f(x,y,z)=x^2y+5y^4z^7-3xyz^2$ is a continuous function on $\Cbb^3$. Clearly $z\in\Cbb\mapsto \ovl z\in\Cbb$ is continuous. So $g(x,y,z)=f(\ovl x,\ovl y,z)+2\ovl{f(z,x^2,xy^{-9})}-5xy^{-2}\ovl{z^{-3}}$ is a continuous function on $\Cbb\times\Cbb^\times\times\Cbb^\times$.
\end{eg}


\begin{eg}\label{lb44}
Let $f,g:X\rightarrow \Fbb$ be continuous functions where $\Fbb\in\{\Qbb,\Rbb,\Cbb\}$. Then by Prop. \ref{lb41} and \ref{lb36}, $f\pm g$ and $fg$ are continuous, and $f/g$ is continuous when $0\notin g(X)$. Here
\begin{align}
(f\pm g)(x)=f(x)\pm g(x)\qquad (fg)(x)=f(x)g(x)\qquad (f/g)(x)=f(x)/g(x)
\end{align} 
\end{eg}


\begin{eg}\label{lb54}
Let $f:X\rightarrow Y$ be a map of metric spaces. Let $E,F$ be subsets of $X,Y$ respectively. (Recall that the metrics of subsets are chosen as in Def. \ref{lb43}.) 
\begin{itemize}
\item The inclusion map $\iota_E:E\rightarrow X,x\mapsto x$ is clearly continuous. Thus, if $f$ is continuous, then $f|_E:E\rightarrow Y$ is continuous, since $f|_E=f\circ\iota_E$.
\item If $f(X)\subset F$, then we can restrict the codomain of $f$ from $Y$ to $F$: let $\wtd f:X\rightarrow F$ be $\wtd f(x)=f(x)$. Then it is clear that $f$ is continuous iff $\wtd f$ is continuous.
\end{itemize}
\end{eg}




\begin{pp}
Let $X_1,\dots,X_N$ be metric spaces. The following  projection is clearly continuous:
\begin{gather*}
\pi_{X_i}:X_1\times\cdots\times X_N\rightarrow X_i\qquad(x_1,\dots,x_N)\mapsto x_i
\end{gather*}
\end{pp}

\begin{pp}
Let $f_i:X\rightarrow Y_i$ be maps where $X,Y_1,\dots,Y_N$ are continuous. Then 
\begin{gather*}
f_1\vee \cdots\vee f_N:X\rightarrow Y_1\times\cdots\times Y_N\qquad x\mapsto (f_1(x),\dots,f_N(x))
\end{gather*}
is continuous iff $f_1,\dots,f_N$ are continuous.
\end{pp}


\begin{eg}
Let $X$ be a metric space. Then $d:X\times X\rightarrow\Rbb,(x,y)\mapsto d(x,y)$ is Lipschiz continuous with Lipschitz constant $1$ (by triangle inequality). So $d$ is continuous.
\end{eg}

\begin{pp}\label{lb42}
Let $X$ be a metric space and $E\subset X$ is nonempty. Define \textbf{distance function} \index{00@Distance function $d(\cdot,E)$} \index{dE@$d(x,E)$} 
\begin{gather*}
d(\cdot,E):X\rightarrow\Rbb_{\geq0}\qquad
d(x,E)=\inf_{e\in E}d(x,e)
\end{gather*}
Then $d(\cdot,E)$ is has Lipschitz constant $1$. So $d(\cdot,E)$ is continuous.
\end{pp}


\begin{proof}
Choose any $x,y\in X$. By triangle inequality, for each $e\in E$ we have $d(x,e)\leq d(x,y)+d(y,e)$. Since $d(x,E)\leq d(x,e)$, we get  $d(x,E)\leq d(x,y)+d(y,e)$. Applying $\inf_{e\in E}$ to the RHS gives $d(x,E)\leq d(x,y)+d(y,E)$. Hence $d(x,E)-d(y,E)\leq d(x,y)$. Exchanging $x$ and $y$ gives
\begin{align}
\big|d(x,E)-d(y,E)\big|\leq d(x,y)
\end{align}
This proves that $d(\cdot,E)$ has Lipschitz constant $1$.
\end{proof}

\begin{df}
More generally, if $E,F$ are subsets of a metric space $E$, we can define \index{DEF@$d(E,F)$}
\begin{align}
d(E,F)=\inf_{e\in E,f\in F}d(e,f)
\end{align}
to be the \textbf{distance between $E$ and $F$}.
\end{df}

\begin{exe}
Let $E,F\subset X$. Prove that
\begin{align}
d(E,F)=\inf_{f\in F}d(E,f)
\end{align}
\end{exe}

\begin{eg}\label{lb45}
If $X$ is a metric space and $p\in X$, then by Prop. \ref{lb42} (or simply by triangle inequality), the function $d_p:x\in X\mapsto d(x,p)\in\Rbb$ has Lipschitz constant $1$ and hence is continuous. In particular, if $\Fbb\in\{\Rbb,\Cbb\}$,  the function $z\in\Fbb\mapsto |z|$ is continuous (since $|z|=d_\Fbb(z,0)$). Thus, if $f:X\rightarrow\Fbb$ is continuous, then $|f|:X\rightarrow\Rbb_{\geq0}$ is continuous where
\begin{align}
|f|(x)=|f(x)|
\end{align}
\end{eg}


\begin{eg}
Let $N\in\Zbb_+$. Then the following function is continuous:
\begin{align*}
\max:\Rbb^N\rightarrow \Rbb\qquad (x_1,\dots,x_N)\mapsto\max\{x_1,\dots,x_N\}\in\Rbb
\end{align*}
Similarly, the minimum function is continuous.
\end{eg}

\begin{proof}
To avoid confusion, we write $\max$ as $\max_N$. The case $N=1$ is obvious. When $N=2$, we have
\begin{align}
\max(x_1,x_2)=\frac{x_1+x_2+|x_1-x_2|}2
\end{align}
So $\max_2$ is continuous by Exp. \ref{lb44} and \ref{lb45}.

We use induction. Suppose we have proved that $\max_N$ is continuous. Then $\max_N\times\id_\Rbb:\Rbb^N\times \Rbb\rightarrow\Rbb\times\Rbb$ is continuous. So $\max_{N+1}=\max_2\circ(\max_N\times\id_\Rbb)$ is continuous.
\end{proof}


\subsection{Homeomorphisms and isometric isomorphisms; convergence in $\ovl{\Rbb}$}

\subsubsection{General theory}



\begin{df}\label{lb189}
A bijection of metric spaces $f:X\rightarrow Y$ is called a \textbf{homeomorphism} if one of the following equivalent (cf. Def. \ref{lb31}) statements holds:
\begin{itemize}
\item[(1)] $f:X\rightarrow Y$ and its inverse map $f^{-1}:Y\rightarrow X$ are continuous.
\item[(2)] For each sequence $(x_n)$ in $X$ and each $x\in X$, we have $\dps \lim_{n\rightarrow\infty}x_n=x$ iff $\dps\lim_{n\rightarrow\infty}f(x_n)=f(x)$.
\end{itemize}
If such $f$ exists, we say that $X,Y$ are \textbf{homeomorphic}.
\end{df}

A special case of the above definition is:
\begin{df}\label{lb144}
Let $X$ be a set with metrics $d,\delta$. We say that $d$ and $\delta$ induce the \textbf{same topology} on $X$ (or that $d,\delta$ are \index{00@Topologically equivalent metrics} \textbf{topologically equivalent}) if one of the following clearly equivalent statements holds:
\begin{itemize}
\item[(1)] The map $(X,d)\rightarrow (X,\delta),x\mapsto x$ is a homeomorphism.\footnote{We prefer not to call this map the identity map, because the metrics on the source and on the target are different.}
\item[(2)] For each sequence $(x_n)$ in $X$ and each $x\in X$, $(x_n)$ converges to $x$ under the metric $d$ iff $(x_n)$ converges to $x$ under $\delta$.
\end{itemize}
\end{df}


\begin{pp}\label{lb48}
Suppose that $d,\delta$ are equivalent metrics on a set $X$. Then $d,\delta$ are topologically equivalent.
\end{pp}

\begin{proof}
Suppose $\delta\leq\alpha d$ and $d\leq\beta\delta$ for some $\alpha,\beta>0$. Then the map $f:(X,d)\rightarrow (X,\delta),x\mapsto x$ and its inverse $f^{-1}$ have Lipschitz constants $\alpha$ and $\beta$ respectively. So $f,f^{-1}$ are continuous.
\end{proof}






\begin{exe}\label{lb46}
Let $f:X\rightarrow Y$ be a map of metric spaces. We say that $f:X\rightarrow Y$ is an \textbf{isometry} (or is \textbf{isometric}) \index{00@Isometry and isometric isomorphism} if for all $x_1,x_2\in X$ we have
\begin{align}
d_Y(f(x_1),f(x_2))=d_X(x_1,x_2) \label{eq15}
\end{align}
Show that an isometry is injective and continuous.

We say that $f$ is an \textbf{isometric isomorphism} if $f$ is a surjective isometry. If an isometric isomorphism between two metric spaces $X,Y$ exists, we say that $X$ and $Y$ are \textbf{isometric metric spaces}.\index{00@Isometric metric spaces} Show that an isometric isomorphism is a homeomorphism.   \hfill\qedsymbol
\end{exe}

\begin{rem}
Isometric isomorphisms are important examples of homeomorphisms. That $f:X\rightarrow Y$ is an isometric isomorphism means that $X$ and $Y$ are equivalent as metric spaces, and that this equivalence can be implemented by the bijection $f$. 

We now look at isometric isomorphisms in a different direction. Suppose that $f:X\rightarrow Y$ is a bijection of sets. Suppose that $Y$ is a metric space. Then there is unique metric $d_X$ on $X$ such that $f$ is an isometric isomorphism: one defines $d_X$ using \eqref{eq15}. We write such $d_X$ as $f^*d_Y$, \index{fdY@$f^*d_Y$: pullback metric}  i.e.,
\begin{align*}
f^*d_Y(x_1,x_2)=d_Y(f(x_1),f(x_2))
\end{align*}
and call $f^*d_Y$ the \textbf{pullback metric} \index{00@Pullback metrics} of $d_Y$ by $f$. \hfill\qedsymbol
\end{rem}

Pullback metrics are a very useful way of constructing metrics on a set. We consider some examples below. 


\begin{exe}\label{lb49}
Two metrics inducing the same topology are not necessarily equivalent metrics. For example, let $f:[0,1]\rightarrow [0,1]$ be $f(x)=x^2$. Let $X=[0,1]$, and let $d_X$ be the  Euclidean metric: $d_X(x,y)=|x-y|$. So
\begin{align*}
f^*d_X(x,y)=|x^2-y^2|
\end{align*}
is a metric on $X$. It is not hard to check that $f:(X,d_X)\rightarrow (X,d_X)$ is a homeomorphism.  So $d_X$ and $f^*d_X$ give the same topology on $[0,1]$ (cf. Exe. \ref{lb50}). Show that $f^*d_X$ and $d_X$ are not equivalent metrics.
\end{exe}

\begin{exe}\label{lb50}
Let $f:X\rightarrow Y$ be a bijection of sets with metrics $d_X,d_Y$. Show that $d_X$ and $f^*d_Y$ give the same topology on $X$ iff $f:(X,d_X)\rightarrow(Y,d_Y)$ is a homeomorphism. 

In particular, if $f:X\rightarrow X$ is a bijection, and $d_X$ is a metric on $X$. Then $d_X$ and $f^*d_X$ give the same topology on $X$ iff $f:(X,d_X)\rightarrow (X,d_X)$ is a homeomorphism. 
\end{exe}


\subsubsection{Convergence in $\ovl\Rbb$}


Our second application of pullback metrics is the convergence in $\ovl\Rbb$.




\begin{df}\label{lb47}
We say that a sequence $(x_n)$ in $\ovl\Rbb$ \textbf{converges to} \index{00@Convergence in $\ovl{\Rbb}$} $+\infty$ (resp. $-\infty$), if for every $A\in\Rbb$ there is $N\in\Zbb_+$ such that for all $n\geq N$ we have $x_n>A$ (resp. $x_n<A$). 

Suppose $x\in\Rbb$. We say that a sequence $(x_n)$ in $\ovl\Rbb$ \textbf{converges to} $x$, if there is $N\in\Zbb_+$ such that $x_n\in\Rbb$ for all $n\geq N$, and that the subsequence $(x_{k+N})_{k\in\Zbb_+}$ converges in $\Rbb$ to $x$.  \hfill\qedsymbol
\end{df}



This notion of convergence is weird: it is not defined by a metric. So one wonders if there is a metric $d$ on $\ovl\Rbb$ such that convergence of sequences under $d$ agrees with that in Def. \ref{lb47}. We shall now find such a metric.

\begin{lm}\label{lb59}
Let $-\infty\leq a<b\leq +\infty$ and $-\infty\leq c<d\leq +\infty$. Then there is a strictly increasing bijective map $[a,b]\rightarrow[c,d]$.
\end{lm}
Note that this map clearly sends $a$ to $c$ and $b$ to $d$. So it restricts to strictly increasing bijections $(a,b)\rightarrow(c,d)$, $(a,b]\rightarrow(c,d]$, $[a,b)\rightarrow[c,d)$.

\begin{proof}
We have a strictly increasing bijection $f:\Rbb\rightarrow(-1,1)$ defined by \eqref{eq20}.  $f$ can be extended to a strictly increasing bijective map $\ovl\Rbb\rightarrow[-1,1]$ if we set $f(\pm\infty)=\pm1$. Thus, $f$ restricts to a strictly increasing bijection $[a,b]\rightarrow [f(a),f(b)]$. Choose a linear function $g(x)=\alpha x+\beta$ (where $\alpha>0$) giving an increasing bijection $[f(a),f(b)]\rightarrow[0,1]$. Then $h=g\circ f:[a,b]\rightarrow[0,1]$ is a strictly increasing bijection. Similarly, we have a strictly increasing bijection $k:[c,d]\rightarrow[0,1]$. Then $k^{-1}\circ h:[a,b]\rightarrow[c,d]$ is a strictly increasing bijection.
\end{proof}


\begin{thm}\label{lb51}
Let $\varphi:\ovl\Rbb\rightarrow[a,b]$ be a strictly increasing bijective map where $[a,b]\subset\Rbb$ is equipped with the Euclidean metric $d_{[a,b]}$. Then a sequence $(x_n)$ in $\ovl\Rbb$ converges to $x\in\ovl\Rbb$ in the sense of Def. \ref{lb47} iff $\varphi(x_n)$ converges to $\varphi(x)$ under the metric $d_{[a,b]}$. In other words, the convergence in $\ovl\Rbb$ is given by the metric $\varphi^*d_{[a,b]}$.
\end{thm}

\begin{proof}
Let $y=\varphi(x)$ and $y_n=\varphi(x_n)$. We need to prove that $x_n\rightarrow x$ (in the sense of Def. \ref{lb47}) iff $y_n\rightarrow y$ (under the Euclidean metric). Write $\psi=\varphi^{-1}$, which is a strictly increasing map $[a,b]\rightarrow\ovl\Rbb$. Note that $\varphi(+\infty)=b$ and $\varphi(-\infty)=a$. 

Case 1: $x\in\Rbb$. By discarding the first several terms, we may assume that $(x_n)$ is always in $\Rbb$. If $x_n\rightarrow x$, then for every $\eps>0$, all but finitely many $x_n$ are inside the open interval $(\psi(y-\varepsilon),\psi(y+\varepsilon))$. So all but finitely many $y_n$ are inside $(y-\varepsilon,y+\varepsilon)$. So $y_n\rightarrow y$. That $y_n\rightarrow y$ implies $x_n\rightarrow x$ is proved in a similar way.

Case 2: $x=\pm\infty$. We consider $x=+\infty$ only; the other case is similar. Note that if $0<\eps<b-a$, then $B_{[a,b]}(b,\varepsilon)=(b-\varepsilon,b]$.  If $x_n\rightarrow+\infty$, then for each  $0<\eps<b-a$, all but finitely many $x_n$ are $>\psi(b-\eps)$. So all but finitely many $y_n$ are inside $(b-\eps,b]$. This proves $y_n\rightarrow b$. Conversely, if $y_n\rightarrow b$, then for each $A\in\Rbb$, all but finitely many $y_n$ are inside $(\varphi(A),b]$ and hence $>\varphi(A)$. So all but finitely many $x_n$ are $>A$.
\end{proof}


\begin{cv}\label{lb77}
Unless otherwise stated, a metric on $\ovl\Rbb$ is one that makes Def. \ref{lb47} true, for instance $\varphi^*d_{[a,b]}$ in Thm. \ref{lb51}. Unless otherwise stated, we do NOT view $\Rbb$ (or any subset of $\Rbb$) as a metric subspace of $\ovl\Rbb$. Namely, we do not follow Convention \ref{lb76} for $\Rbb\subset\ovl\Rbb$, or more generally for $\Rbb^N\subset\ovl\Rbb^N$. Instead, we choose Euclidean metrics on $\Rbb^N$, following Convention \ref{lb33}. 
\end{cv}




The main reason for not following Convention \ref{lb76} here is that metrics on $\ovl\Rbb$ are all bounded (by Prop. \ref{lb71}). Thus, every subset of $\Rbb$ is bounded if we view $\Rbb$ as a metric subspace of $\ovl\Rbb$. However, we want a subset of $\Rbb$ to be bounded precisely when it is contained in $[a,b]$ for some $-\infty<a<b<+\infty$. (Recall also Def. \ref{lb114}.)



After learning topological spaces, we shall forget about the metrics on $\ovl\Rbb$ and only care about its topology. (See Conv. \ref{lb173}.)






\begin{rem}\label{lb58}
By Thm. \ref{lb51}, the properties of $[a,b]$ about convergence of sequences and inequalities can be transported to $\ovl\Rbb$, for example:
\begin{enumerate}
\item If $(x_n),(y_n)$ are sequences in $\ovl\Rbb$ converging to $A,B\in\ovl\Rbb$, and if $x_n\leq y_n$ for all $n$, then $A\leq B$.
\item \textbf{Squeeze theorem}: \index{00@Squeeze theorem} Suppose that $(x_n),(y_n),(z_n)$ are sequences in $\ovl\Rbb$, $x_n\leq y_n\leq z_n$ for all $n$, and $x_n$ and $z_n$ both converge to $A\in\ovl\Rbb$. Then $y_n\rightarrow A$.
\item Prop. \ref{lb57} also holds for $[-\infty,+\infty]=\ovl\Rbb$: if $(x_n)$ is an increasing resp. decreasing sequence in $\ovl\Rbb$, then $\lim_n x_n$ exists in $\ovl\Rbb$ and equals $\sup_n x_n$ resp. $\inf_n x_n$.
\end{enumerate}
We will see more examples when studying $\limsup$ and $\liminf$ in the future.
\end{rem}


We have shown that there is a metric on $\ovl\Rbb$ which defines the convergence in Def. \ref{lb47}. However, there is no standard choice of such a metric on $\ovl\Rbb$. Even worse, two possible choices of metrics might not be equivalent: Let $\varphi,\psi:\ovl\Rbb\rightarrow[0,1]$ be a strictly increasing bijections where $\psi\circ\varphi^{-1}:[0,1]\rightarrow[0,1]$ is $x\mapsto x^2$. Then by Exe. \ref{lb49}, $\varphi^*d_{[0,1]}$ and $\psi^*d_{[0,1]}$ are non-equivalent but topologically equivalent metrics on $\ovl\Rbb$. This is the first example that metrics are not convenient for the description of convergence. When studying the convergence in $\ovl\Rbb$, thinking about metrics is distracting. In the future, we will see a better notion for the study of convergence: the notion of topological spaces.

We end this section with a generalization of Thm. \ref{lb51}.

\begin{thm}\label{lb65}
Let $\varphi$ be a strictly increasing bijection in one of the following forms
\begin{gather*}
[a,b]\rightarrow [c,d]\qquad (a,b)\rightarrow(c,d)\\
(a,b]\rightarrow (c,d]\qquad [a,b)\rightarrow [c,d)
\end{gather*}
where $-\infty\leq a\leq b\leq +\infty$ and $-\infty\leq c\leq d\leq +\infty$. Then $\varphi$ is a homeomorphism, i.e., if $(x_n)$ and $x$ are in the domain, then $x_n\rightarrow x$ iff $\varphi(x_n)\rightarrow \varphi(x)$ (in the sense of Def. \ref{lb47}).
\end{thm}

\begin{proof}
The case $a=b$ is obvious. So we consider $a<b$, and hence $c<d$. We consider the left-open-right-closed case for example. The other cases are treated in a similar way. If the theorem can be proved for $(-\infty,+\infty]\rightarrow(c,d]$, then it can also be proved $(-\infty,+\infty]\rightarrow(a,b]$. By composing the inverse of the second map with the first map, we see that the theorem holds for $(a,b]\rightarrow (c,d]$. 

Let us consider $\varphi:(-\infty,+\infty]\rightarrow(c,d]$. $\varphi$ can be extended to a strictly increasing bijection $\varphi:\ovl\Rbb\rightarrow[c,d]$ by letting $\varphi(-\infty)=c$. It suffices to prove that this $\varphi$ is a homeomorphism. When $-\infty<c<d<+\infty$, then the theorem holds by Thm. \ref{lb51}. If one of $c,d$ is $\pm\infty$, the same argument as in the proof of Thm. \ref{lb51} proves that $\varphi$ is a homeomorphism. We leave it to the readers to fill in the details.
\end{proof}



%% Record  #2  2023/9/20   three lectures  5


\subsection{Problems and supplementary material}

\begin{df}
Let $A$ be a subset of $\Rbb$  satisfying $x+y\in A$ for all $x,y\in A$. (Or more generally, let $A$ be an abelian semigroup.) We say that a function $f:A\rightarrow \Rbb$ is \textbf{subadditive} \index{00@Subadditivity} if for every $x,y\in A$ we have $f(x+y)\leq f(x)+f(y)$.
\end{df}

\begin{prob}\label{lb52}
Consider the following increasing functions:
\begin{gather*}
f_1:\Rbb_{\geq 0}\rightarrow[0,1)\qquad f_1(x)=\frac{x}{1+x}\\
f_2:\Rbb_{\geq 0}\rightarrow[0,1]\qquad f_2(x)=\min\{x,1\}
\end{gather*}
Prove that they are subadditive functions. 
\end{prob}

\begin{prob}\label{lb53}
Let $f:\Rbb_{\geq0}\rightarrow \Rbb_{\geq0}$ be an increasing subadditive function satisfying the following conditions:
\begin{itemize}
\item[(1)] $f^{-1}(0)=\{0\}$.
\item[(2)] For any $(x_n)_{n\in\Zbb_+}$ in $\Rbb_{\geq0}$ we have $x_n\rightarrow 0$ iff $f(x_n)\rightarrow 0$.
\end{itemize}
Let $(X,d)$ be a metric space. Define
\begin{align*}
\delta:X\times X\rightarrow [0,A)\qquad \delta(x,y)=f\circ d(x,y)
\end{align*}  
Prove that $\delta$ is a metric, and that $\delta$ and $d$ are topologically equivalent.
\end{prob}


\begin{pp}\label{lb195}
Let $(X,d)$ be a metric space. Then there is a bounded metric $\delta$ on $X$ such that $d$ and $\delta$ are topologically equivalent.
\end{pp}
\begin{proof}
Let $f$ be either $f_1$ or $f_2$ defined in Pb. \ref{lb52}. Then $f$ satisfies the assumptions in Pb. \ref{lb53}. So $\delta=f\circ d$ is a desired metric due to Pb. \ref{lb53}. We write down the formulas explicitly:
\begin{align*}
\delta_1(x,y)=\frac{d(x,y)}{1+d(x,y)}\qquad \delta_2(x,y)=\min\{d(x,y),1\}
\end{align*}
\end{proof}

\begin{prob}\label{lb78}
Let $(X_i,d_i)_{i\in\Zbb_+}$ be a sequence of metric spaces. Assume that $d_i\leq 1$ for each $i$. Let $\dps S=\prod_{i\in\Zbb_+} X_i$. For each elements $f=(f(i))_{i\in\Zbb_+}$ and $g=(g(i))_{i\in\Zbb_+}$ of $S$, define
\begin{align}
d(f,g)=\sup_{i\in\Zbb_+} \frac {d_i(f(i),g(i))}{i}  \label{eq16}
\end{align} 
Prove that $d$ is a metric on $S$. Let $f_n=(f_n(i))_{i\in\Zbb_+}$ be a sequence in $S$. Let $g\in S$. Prove that the following are equivalent:
\begin{enumerate}[label=(\alph*)]
\item $\dps \lim_{n\rightarrow\infty} f_n=g$ under the metric $d$.
\item $f_n$ \textbf{converges pointwise} to $g$, namely, $\dps\lim_{n\rightarrow\infty} f_n(i)=g(i)$ for every $i\in\Zbb_+$.
\end{enumerate}
\end{prob}

\begin{rem}
The above problem gives our first non-trivial example of function spaces  as metric spaces, where the domain of functions is a countable set. After learning series, the readers can check that 
\begin{align}
\delta(f,g)=\sum_{i\in\Zbb_+}2^{-i} d_i(f(i),g(i))   \label{eq61}
\end{align}
also defines a metric, and that (a) (with $d$ replaced by $\delta$) and (b) are equivalent. So $\delta$ and $d$ (defined by \eqref{eq16}) induce the same topology on $X$, called the \textbf{pointwise convergence topology} or simply \textbf{product topology}. Unfortunately, if the index set $\Zbb_+$ is replaced by an uncountable set, there is in general no metric inducing the product topology. We will prove this in Pb. \ref{lb203}.
\end{rem}













\begin{sprob}\label{lb194}
Let $X=\bigsqcup_{\alpha\in \scr A}X_\alpha$ be a disjoint union of metric spaces $(X_\alpha,d_\alpha)$. Assume that $d_\alpha\leq 1$ for all $i$. For each $x,y\in X$, define
\begin{gather*}
d(x,y)=\left\{
\begin{array}{ll}
d_\alpha(x,y)&\text{ if $x,y\in X_\alpha$ for some $\alpha\in\scr A$}\\[0.5ex]
\frac 12&\text{ otherwise}
\end{array}
\right.
\end{gather*}
\begin{enumerate}
\item Prove that $d$ defines a metric on $X$. 
\item Choose $(x_n)_{n\in\Zbb_+}$ in $X$ and $x\in X$. What does $\dps\lim_{n\rightarrow\infty}x_n=x$ mean in terms of the convergence in each $X_\alpha$?
\end{enumerate}
\end{sprob}

Think about the question: Let $X$ be a set. For each $x,y\in X$ define $d(x,y)=0$ if $x=y$, and $d(x,y)=1$ if $x\neq y$. What does convergence in $(X,d)$ mean?



\newpage



\section{Sequential compactness and completeness}



\subsection{Sequential compactness}


\subsubsection{Basic properties of sequentially compact spaces}

\begin{df}
Let $X$ be a metric space. We say that $X$ is \textbf{sequentially compact} \index{00@Sequentially compact} if every sequence in $X$ has a subsequence converging to some point of $X$.
\end{df}





The notion of sequential compactness is extremely useful for finding solutions in an analysis problem. In general, suppose we want to find a point $x\in X$ which makes a property $P(x)$ to be true. Suppose that we can find an ``approximate solution", i.e. an $y\in X$ such that $P(y)$ is close to being true. Thus, we can find a sequence $(x_n)$ in $X$ such that $P(x_n)$ is closer and closer to being true when $n\rightarrow\infty$. Now, if $X$ is sequentially compact, then $(x_n)$ has a subsequence $(x_{n_k})$ converging to $x\in X$. Then $P(x)$ is true, and hence $x$ is a solution for the problem. (See also Sec. \ref{lb55}.) Let us see an explicit example:

\begin{lm}[\textbf{Extreme value theorem}]\label{lb56} \index{00@EVT=Extreme value theorem}
Let $X$ be a sequentially compact metric space. Let $f:X\rightarrow\Rbb$ be a continuous function. Then $f$ attains its maximum and minimum at points of $X$. In particular, $f(X)$ is a bounded subset of $\Rbb$.
\end{lm}

This extremely important result is the main reason for introducing sequentially compact spaces. We call this a lemma, since we will substantially generalize this result later. (See Exe. \ref{lb63}.)

Note that the \textbf{boundedness} of subsets of $\Rbb$ (or more generally, of $\Rbb^N$) is always understood under the Euclidean metric of $\Rbb$, not under any metric of $\ovl\Rbb$ or $\ovl\Rbb^N$. (Recall Convention \ref{lb77}.)

\begin{proof}
We show that $f$ attains its maximum on $X$. The proof for minimum is similar. Let $A=\sup f(X)$. Then $A\in (-\infty,+\infty]$. If $A<+\infty$, then for each $n\in\Zbb_+$ there is $x_n\in X$ such that $A-1/n<f(x_n)\leq A$ (since $A-1/n$ is not an upper bound of $f(X)$). If $A=+\infty$, then for each $n$ there is $x_n\in X$ such that $f(x_n)>n$. In either case, we have a sequence $(x_n)$ in $X$ such that $f(x_n)\rightarrow A$ in $\ovl \Rbb$.

Since $X$ is sequentially compact, $(x_n)$ has a subsequence $(x_{n_k})_{k\in\Zbb_+}$ converging to some $x\in X$. Now, consider $f$ as a map $f:X\rightarrow\ovl\Rbb$, which is continuous (cf. Exp. \ref{lb54}). Since $f(x_n)\rightarrow A$, its subsequence $f(x_{n_k})$ also converges to $A$. But since $x_{n_k}\rightarrow x$ and $f$ is continuous at $x$, we have $A=f(x)$. So $f$ attains its maximum at $x$. Since $f(X)\subset\Rbb$, we have $A\in\Rbb$.
\end{proof}

The following are some elementary examples of sequential compactness:


\begin{exe}
Show that finite unions of sequentially compact spaces is sequentially compact. (In particular, a finite set is sequentially compact.) 

More precisely, let $X$ be a metric space. Assume $X=A_1\cup\cdots\cup A_N$ where each metric subspace $A_i$ is sequentially compact. Show that $X$ is sequentially compact.  
\end{exe}

\begin{pp}\label{lb72}
Let $X_1,\dots,X_N$ be sequentially compact metric spaces. Then $X=X_1\times\cdots\times X_N$ is sequentially compact.
\end{pp}

\begin{proof}
Since $X=(X_1\times\cdots\times X_{N-1})\times X_N$, by induction, it suffices to assume $N=2$. So we write $X=A\times B$ where $A,B$ are sequentially compact. Let $(a_n,b_n)$ be a sequence in $X$. Since $A$ is sequentially compact, $(a_n)$ has a convergent subsequence $(a_{n_k})$. Since $B$ is sequentially compact, $(b_{n_k})$ has a convergent subsequence $(b_{n_{k_l}})$. So $(a_{n_{k_l}},b_{n_{k_l}})$ is a convergent subsequence of $(a_n,b_n)$.
\end{proof}


\begin{pp}\label{lb62}
Let $f:X\rightarrow Y$ be a continuous  map of metric spaces. Assume that $X$ is sequentially compact. Then $f(X)$, as a metric subspace of $Y$, is sequentially compact.
\end{pp}

\begin{proof}
Choose any sequence $(y_n)$ in $f(X)$. We can write $y_n=f(x_n)$ where $x_n\in X$. Since $X$ is sequentially compact, $(x_n)$ has a subsequence $(x_{n_k})$ converging to some $x\in X$. Since $f$ is continuous, $y_{n_k}=f(x_{n_k})$ converges to $f(x)$.
\end{proof}


\begin{exe}\label{lb63}
Prove that if $Y$ is a sequentially compact subset of $\Rbb$, then $\sup Y\in Y$ and $\inf Y\in Y$. Therefore, Prop. \ref{lb62} generalizes Lem. \ref{lb56}.
\end{exe}

\begin{pp}\label{lb71}
Let $X$ be a sequentially compact metric space. Then $X$ is bounded under its metric $d$.
\end{pp}

\begin{proof}
Choose any $p\in X$. The function $d_p:x\in X\mapsto d(x,p)\in\Rbb_{\geq 0}$ is continuous by Exp. \ref{lb45}. So, by Lem. \ref{lb56}, $d_p$ is bounded by some $0<R<+\infty$. So $X=\ovl B_X(p,R)\subset B_X(p,2R)$.
\end{proof} 






\subsubsection{Limits inferior and superior, and Bolzano-Weierstrass}\label{lb69}

The goal of this subsection is to prove that closed intervals are sequentially compact. 


\begin{df}\label{lb266}
Let $(x_n)$ be a sequence in a metric space $X$. We say that $x\in X$ is a \textbf{cluster point} \index{00@Cluster point of a sequence} of $(x_n)$, if $(x_n)$ has a subsequence $(x_{n_k})$ converging to $x$.
\end{df}

Warning: In a general topological space, the cluster points of a sequence will be defined in a different way. (See Pb. \ref{lb223} and Rem. \ref{lb267}.)





\begin{df}\label{lb60}
Let $(x_n)$ be a sequence in $\ovl\Rbb$. Define
\begin{gather}
\alpha_n=\inf\{x_k:k\geq n \}\qquad \beta_n=\sup\{x_k:k\geq  n \}
\end{gather}
It is clear that $\alpha_n\leq x_n\leq \beta_n$, that $(\alpha_n)$ is increasing and $(\beta_n)$ is decreasing. Define \index{liminfsup@$\liminf,\limsup$}
\begin{subequations}
\begin{gather}
\liminf_{n\rightarrow\infty}x_n=\sup\{\alpha_n:n\in\Zbb_+\}=\lim_{n\rightarrow\infty} \alpha_n \label{eq18}\\
\limsup_{n\rightarrow\infty}x_n=\inf\{\beta_n:n\in\Zbb_+\}=\lim_{n\rightarrow\infty} \beta_n\label{eq19}
\end{gather}
\end{subequations}
(cf. Rem. \ref{lb58}), called respectively the \textbf{limit inferior} and the \textbf{limit superior} \index{00@Limit inferior and superior} of $(x_n)$.
\end{df}

\begin{rem}
Let $(x_n),(y_n)$ be sequences in $\ovl\Rbb$. Suppose that $x_n\leq y_n$ for every $n$. It is clear that
\begin{gather*}
\liminf_{n\rightarrow\infty}x_n\leq\limsup_{n\rightarrow\infty} x_n\qquad \liminf_{n\rightarrow\infty}x_n\leq \liminf_{n\rightarrow\infty}y_n\qquad \limsup_{n\rightarrow\infty} x_n\leq \limsup_{n\rightarrow\infty} y_n
\end{gather*}
\end{rem}


\begin{thm}\label{lb68}
Let $(x_n)$ be a sequence in $\ovl\Rbb$, and let $S$ be the set of cluster points of $(x_n)$. Then $\dps\liminf_{n\rightarrow\infty}x_n$ and $\dps\limsup_{n\rightarrow\infty}x_n$ belong to $S$. They are respectively the minimum and the maximum of $S$.
\end{thm}

In particular, every sequence in $\ovl\Rbb$ has at least one cluster point.

\begin{proof}
We use the notations in Def. \ref{lb60}. Let $A=\eqref{eq18}$ and $B=\eqref{eq19}$. If $x\in S$, pick a subsequence $(x_{n_k})$ converging to $x$. Since $\alpha_{n_k}\leq x_{n_k}\leq \beta_{n_k}$, we have $A\leq x\leq B$ by Rem. \ref{lb58}. It remains to show that $A,B\in S$. We prove $B\in S$ by constructing a subsequence $(x_{n_k})$ converging to $B$; the proof of $A\in S$ is similar. 

Consider first of all the special case that $(x_n)$ is bounded, i.e., is inside $[a,b]\subset\Rbb$. Choose an arbitrary $n_1\in\Zbb_+$. Suppose $n_1<\dots<n_k$ have been constructed. By the definition of $\beta_{1+n_k}$, there is $n_{k+1}\geq 1+n_k$ such that $x_{n_{k+1}}$ is close to $\beta_{1+n_k}$, say
\begin{align}
\beta_{1+n_k}-\frac 1k<x_{n_{k+1}}\leq \beta_{1+n_k}  \label{eq17}
\end{align}
Since the left most and the right most of \eqref{eq17} both converge to $B$ as $k\rightarrow\infty$, by Squeeze theorem (Cor. \ref{lb61}) we conclude $\lim_k x_{n_k}=B$. 

In general, by Lem. \ref{lb59} and Thm. \ref{lb65}, there is an increasing (i.e. order-preserving) homeomorphism (i.e. topopogy-preserving map) $\varphi:\ovl\Rbb\rightarrow[0,1]$. Then $\varphi(\beta_n)=\sup\{\varphi(x_k):k\geq n\}$ (cf. Exe. \ref{lb66}) and $\varphi(B)=\lim_n\varphi(\beta_n)$. So $\varphi(B)=\limsup_n \varphi(x_n)$. By the above special case, $(\varphi(x_n))$ has a subsequence $(\varphi(x_{n_k}))$ converging to $\varphi(B)$. So $(x_{n_k})$ converges to $B$. 
\end{proof}

\begin{rem}
One can also prove the above general case directly using a similar idea as in the special case. And you are encouraged to do so! (Pay attention to the case $B=\pm\infty$.) 

The proof given above belongs to a classical proof pattern: To prove that a space $X$ satisfies some property, one first prove it in a convenient case. Then, in the general case, one finds an ``isomorphism" (i.e. ``equivalence" in a suitable sense) $\varphi:X\rightarrow Y$ where $Y$ is in the convenient case. Then the result on $Y$ can be translated via $\varphi^{-1}$ to $X$, finishing the proof. 

For example, to solve a linear algebra problem about linear maps between finite-dimensional vector spaces $V,W$, one first proves it in the special case that $V=\Fbb^m$ and $W=\Fbb^n$. Then, the general case can be translated to the special case via an equivalence as in Exp. \ref{lb67}.  \hfill\qedsymbol
\end{rem}


\begin{exe}\label{lb66}
Let $X,Y$ be posets. Let $\varphi:X\rightarrow Y$ be an increasing bijection whose inverse is also increasing. (Namely, $\varphi$ induces an equivalence of posets). Suppose $E\subset X$ has supremum $\sup E$. Explain why $\varphi(E)$ has supremum $\varphi(\sup E)$.
\end{exe}


It is now fairly easy to prove the famous

\begin{thm}[Bolzano-Weierstrass]\index{00@Bolzano-Weierstrass theorem}
Let $[a_1,b_1],\dots,[a_N,b_N]$ be closed intervals in $\ovl\Rbb$. Then $[a_1,b_1]\times\cdots\times [a_N,b_N]$ is sequentially compact. 
\end{thm}

\begin{proof}
By Prop. \ref{lb72}, it suffices to assume $N=1$. Write $a_1=a,b_1=b$. Let $(x_n)$ be a sequence in $[a,b]$. By Thm. \ref{lb68}, $(x_n)$ has a subsequence $(x_{n_k})$ converging to some $x\in\ovl\Rbb$. (E.g. $x=\limsup_n x_n$.) Since $a\leq x_{n_k}\leq b$, we have $a\leq x\leq b$ by Rem. \ref{lb58}.
\end{proof}

Bolzano-Weierstrass theorem illustrates why we sometimes prefer to work with $\ovl\Rbb$ instead of $\Rbb$: $\ovl\Rbb$ is sequentially compact, while $\Rbb$ is not. That every sequence has limits superior and inferior in $\ovl\Rbb$ but not necessarily in $\Rbb$ is closely related to this fact. In the language of point-set topology, $\ovl\Rbb$ is a \textbf{compactification} of $\Rbb$.


Bolzano-Weierstrass theorem (restricted to $\Rbb^N$) will be generalized to \textbf{Heine-Borel theorem}, which says that a subset of $\Rbb^N$ is sequentially compact iff it is bounded and closed (cf. Def. \ref{lb99} for the definition of closed subsets). See Thm. \ref{lb98}.





\subsubsection{A criterion for convergence in sequentially compact spaces}

At the end of Sec. \ref{lb73}, we have raised the following question: Suppose that $(x_n)$ is a bounded sequence in a metric space $X$ such that any two convergent subsequences converge to the same point. Does $(x_n)$ converge?

When $X$ is sequentially compact, $(x_n)$ is automatically bounded due to Prop. \ref{lb71}. The answer to the above question is yes:

\begin{thm}\label{lb74}
Let $X$ be a sequentially compact metric space. Let $(x_n)$ be a sequence in $X$. Then the following are equivalent.
\begin{itemize}
\item[(1)] The sequence $(x_n)$ converges in $X$.
\item[(2)] Any two convergent subsequences of $(x_n)$ converge to the same point. In other words, $(x_n)$ has only one cluster point. 
\end{itemize}
\end{thm}


\begin{proof}
(1)$\Rightarrow$(2): By Prop. \ref{lb23}.

(2)$\Rightarrow$(1): Assume that $(x_n)$ has at most one cluster point. Since $X$ is sequentially compact, $(x_n)$ has at least one cluster point $x\in X$. We want to prove $\lim_{n\rightarrow\infty} x_n=x$. Suppose not. Then there exists $\eps>0$ such that for every $N\in\Zbb_+$ there is $n\geq N$ such that $d(x_n,x)\geq \eps$. Thus, one can inductively construct a subsequence $(x_{n_k})$ of $(x_n)$ such that $d(x_{n_k},x)\geq\eps$ for all $k$. Since $X$ is sequentially compact, $(x_{n_k})$ has a subsequence $x'_n$ converging to $x'\in X$. So $d(x'_n,x)\geq\eps$ for all $n$. Since the function $y\in X\mapsto d(y,x)$ is continuous (Exp. \ref{lb45}), we have $\lim_{n\rightarrow\infty}d(x_n',x)=d(x',x)$. This proves that $d(x',x)\geq\eps>0$. However, $x',x$ are both cluster points of $(x_n)$, and so $x=x'$. This gives a contradiction. 
\end{proof}

\begin{rem}
Thm. \ref{lb74} can be used in the following way. Suppose that we want to prove that a given sequence $(x_n)$ in a sequentially compact space $X$ converges to $x$. Then it suffices to prove that if $(x_n')$ is a subsequence of $(x_n)$ converging to some $y\in X$, then $y=x$. This is sometimes easier to prove than directly proving the convergence of $(x_n)$. We will use this strategy in the proof of L'H\^opital's rule, for example. (See Subsec. \ref{lb349}.)
\end{rem}





\begin{co}\label{lb75}
Let $(x_n)$ be a sequence in $\Rbb^N$. The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item The sequence $(x_n)$ converges in $\Rbb^N$.
\item The sequence $(x_n)$ is bounded. Moreover, any two convergent subsequences of $(x_n)$ converge to the same point of $\Rbb^N$.
\end{enumerate}
\end{co}

\begin{proof}
(1)$\Rightarrow$(2): By Prop. \ref{lb24} and \ref{lb23}. 

(2)$\Rightarrow$(1): Assume (2). Since $(x_n)$ is bounded, it can be contained in $X=I_1\times\cdots\times I_N$ where each $I_i$ is a closed interval in $\Rbb$. Clearly, any two cluster points of $(x_n)$ are inside $X$, and are equal by (2). By Bolzano-Weierstrass, $X$ is sequentially compact. Thus, by Thm. \ref{lb74}, $(x_n)$ converges in $X$ and hence in $\Rbb^N$.
\end{proof}

\begin{co}\label{lb113}
The following are true.
\begin{itemize}
\item[1.] Let $(x_n)$ be a sequence in $\ovl\Rbb$. Then $(x_n)$ converges in $\ovl\Rbb$ iff $\dps\limsup_{n\rightarrow\infty} x_n$ equals $\dps\liminf_{n\rightarrow\infty} x_n$. 
\item[2.] Let $(x_n)$ be a sequence in $\Rbb$. Then $(x_n)$ converges in $\Rbb$ iff $\dps\limsup_{n\rightarrow\infty} x_n$ equals $\dps\liminf_{n\rightarrow\infty} x_n$ and $(x_n)$ is bounded.
\end{itemize}
\end{co}

Note that if $(x_n)$ converges in $\ovl\Rbb$, we must have $\lim x_n=\limsup x_n=\liminf x_n$ by Thm. \ref{lb68}.

\begin{proof}
1. Let $A=\liminf x_n$ and $B=\limsup x_n$. Let $S$ be the set of cluster points of $(x_n)$.  By Thm. \ref{lb68}, $A=\min S,B=\max S $. So $A=B$ iff $S$ has only one element. This is equivalent to the convergence of $(x_n)$ in $\ovl\Rbb$ due to Thm. \ref{lb74} (since $\ovl\Rbb$ is sequentially compact by Bolzano-Weierstrass.)

2. If $(x_n)$ converges, then $A=B$ by part 1. And $(x_n)$ is bounded due to Prop. \ref{lb24}. Conversely, if $A=B$ and if $(x_n)$ is bounded, say $\alpha\leq x_n\leq \beta$ for all $n$ where $-\infty<\alpha<\beta<+\infty$. Then $\alpha\leq A\leq B\leq\beta$. So $A,B\in\Rbb$. By part 1, $(x_n)$ converges to $A\in\Rbb$.
\end{proof}



\subsection{Outlook: sequentially compact function spaces}


In Sec. \ref{lb55}, we mentioned that metric spaces and (more generally) point-set topology were introduced by mathematicians in order to study (typically infinite dimensional) function spaces with the help of the geometric intuition of $\Rbb^N$.  Now we have learned a couple of important results about sequentially compact spaces. But we have not met any example arising from function spaces. So let me show one example to the curious readers: The product space $[0,1]^{\Zbb_+}$, equipped with the metric defined in Pb. \ref{lb78}, is sequentially compact. We will prove this result at the end of this chapter. (Indeed, we will prove a slightly more general version. See  Thm. \ref{lb89}.) This is a famous result, not only because it has many important applications (some of which will be hinted at in this section), but also because its proof uses the clever  ``diagonal method".  

Moreover, we will later prove an even more surprising fact: every sequentially compact metric space is homeomorphic to a closed subset of $[0,1]^{\Zbb_+}$. (See Thm. \ref{lb261}.) Thus, all sequentially compact metric spaces can be constructed explicitly, in some sense.

The readers may still complain that functions on $\Zbb_+$ are very different from those we often see and use in analysis and (especially) in differential equations: We are ultimately interested in functions on $\Rbb$ or on $[a,b]$, but not on countable sets. This is correct. But $[0,1]^{\Zbb_+}$ (and its closed subsets) are in fact very helpful for the study of spaces of functions on $\Rbb$ and on $[a,b]$. In this course, we shall learn two major examples that the sequential compactness of $[0,1]^{\Zbb_+}$ helps with:
\begin{enumerate}
\item $A=\Qbb\cap[a,b]$ is a countable dense subset of $[a,b]$. Thus, if we let $C([a,b])$ denote the set of continuous $\Rbb$-functions on $[a,b]$, then the restriction map $f\in C([a,b])\mapsto f|_A\in\Rbb^A$ is injective. In many applications, we are interested in a subset $\mc X\subset C([a,b])$ of uniformly bounded functions, say all $f\in\mc X$ take values in $[-1,1]$. Then we have an injective map
\begin{align*}
\Phi:\mc X\rightarrow [-1,1]^A\qquad f\mapsto f|_A
\end{align*}
If $\mc X$ satisfies a condition called ``\textbf{equicontinuous}", then  a sequence $f_n$ in $\mc X$ converges \textit{uniformly} to $f\in C([a,b])$ iff $f_n|_A$ converges \textit{pointwise} to $f|_A$. (See Rem. \ref{lb145}.) Thus, from the sequential compactness of $[-1,1]^A$ under pointwise convergence topology, one concludes that every sequence in $\mc X$ has a subsequence converging uniformly in $C([a,b])$. This remarkable sequential compactness result on (the closure of) $\mc X$ is called \textbf{Arzel\`a-Ascoli theorem}, and will be used to prove the fundamental Peano existence theorem in ordinary differential equations. We also see that the fact that $[a,b]$ has a countable dense subset $A$ plays a crucial role. This property of metric spaces is called ``\textbf{separable}" and will be studied later.

\item \textbf{Fourier series} are powerful for the study of partial differential equations. A continuous function $f:[-\pi,\pi]\rightarrow\Cbb$ satisfying $f(-\pi)=f(\pi)$ has Fourier series expansion $f(x)=\sum_{n\in\Zbb}a_n e^{\im nx}$ where $a_n\in\Cbb$. However, for the sake of studying differential equations, one needs to consider series $\sum_{n\in\Zbb}a_n e^{\im nx}$ converging to a function much worse than a continuous function. For example, in the study of integral equations (which are closely related to certain partial differential equations), Hilbert and Schmidt discovered that one has to consider all $f(x)=\sum_{n\in\Zbb}a_n e^{\im nx}$ satisfying $\sum_n |a_n|^2\leq 1$. Therefore, one lets $\ovl B=\{z\in\Cbb:|z|\leq 1\}$ and considers $\wht f:n\in\Zbb\mapsto a_n\in\Cbb$ as an element of $\ovl B^\Zbb$. The sequential compactness of $\ovl B^\Zbb$ helps one find the $\wht f$ such that the corresponding $f(x)=\sum_n \wht f(n)\cdot e^{\im nx}$ is a desired solution of the integral equation. 
\end{enumerate}




\subsection{Complete metric spaces and Banach spaces}

In this section, we let $\Fbb\in\{\Rbb,\Cbb\}$, and assume that all vector spaces are over $\Fbb$.


\subsubsection{Cauchy sequences and complete metric spaces}

\begin{df}
A sequence $(x_n)$ in a metric space $X$ is called a \textbf{Cauchy sequence}, \index{00@Cauchy sequence}if:
\begin{itemize}
\item For every $\eps>0$  there exists $N\in\Zbb_+$ such that for all $m,n\geq N$ we have $d(x_m,x_n)<\eps$.
\end{itemize}
\end{df}

Here, ``$\eps>0$" can mean either ``$\eps\in\Rbb_{>0}$" or ``$\eps\in\Qbb_{>0}$". The choice of this meaning does not affect the definition. The above definition can be abbreviated to ``for every $\eps>0$, we have $d(x_m,x_n)<\eps$ for sufficiently large $m,n$". 

\begin{rem}
It is an easy consequence of triangle inequality that $(x_n)$ is a Cauchy sequence iff
\begin{itemize}
\item For every $\eps>0$ there exists $N\in\Zbb_+$ such that for all $n\geq N$ we have $d(x_n,x_N)<\eps$.
\end{itemize}
Also, it is clear that every Cauchy sequence is bounded.
\end{rem}


\begin{pp}\label{lb83}
Every convergent sequence in a metric space $X$ is a Cauchy sequence.
\end{pp}

\begin{proof}
Assume $(x_n)$ converges to $x$ in $X$. Then for every $\eps>0$ there is $N\in\Zbb_+$ such that $d(x,x_n)<\eps/2$ for all $n\geq N$. Since this is true for every $m\geq N$, we have $d(x_m,x_n)\leq d(x,x_n)+d(x,x_m)<\eps/2+\eps/2=\eps$.
\end{proof}


\begin{df}
A metric space $X$ is called \textbf{complete} \index{00@Complete metric space} if every Cauchy sequence in $X$ converges.
\end{df}




We have many examples of complete metric spaces:

\begin{thm}\label{lb79}
If $(x_n)$ is a Cauchy sequence in a metric space $X$ with at least one cluster point, then $(x_n)$ converges in $X$. Consequently, every sequentially compact metric space is complete.
\end{thm}

\begin{proof}
Let $(x_n)$ be a Cauchy sequence in $X$ with subsequence $(x_{n_k})$ converging to $x\in X$. Let us show that $x_n\rightarrow x$. 

Since $(x_n)$ is Cauchy, for every $\eps>0$ there is $N\in\Zbb_+$ such that $d(x_n,x_m)<\eps/2$ for all $m,n\geq N$. Since $x_{n_k}\rightarrow x$, there is $k\geq N$ such that $d(x_{n_k},x)<\eps/2$. Since $n_k$ is strictly increasing over $k$, we have $n_k\geq k$. So $n_k\geq N$. So we can let $m=n_k$. This gives $d(x_n,x_{n_k})<\eps/2$. Therefore $d(x_n,x)\leq d(x_n,x_{n_k})+d(x_{n_k},x)<\eps$ for all $n\geq N$.
\end{proof}

\begin{eg}\label{lb84}
Let  $X=[a_1,b_1]\times\cdots\times [a_N,b_N]$  where each $[a_i,b_i]$ is a closed interval in $\Rbb$, then $X$ is  sequentially compact by Bolzano-Weierstrass. Thus, by Thm. \ref{lb79}. $X$ is complete.
\end{eg}


\begin{co}\label{lb80}
$\Rbb^N$ and $\Cbb^N$ are complete (under the Euclidean metrics).
\end{co}

\begin{proof}
Since $\Cbb^N$ is isometrically isomorphic to $\Rbb^{2N}$, it suffices to prove that $\Rbb^N$ is complete. Choose a Cauchy sequence $(x_n)$ in $\Rbb^N$. Since $(x_n)$ is bounded, $(x_n)$ is contained inside $X=I_1\times\cdots\times I_N$ where each $I_i=[a,b]$ is in $\Rbb$. By Exp. \ref{lb84}, $X$ is complete. So $(x_n)$ converges to some $x\in X$.
\end{proof}





\begin{df}\label{lb99}
We say that a subset $A$ of a metric space $X$ is \textbf{closed} \index{00@Closed subset} if the following condition is true: For every sequence $(x_n)$ in $A$ converging to a point $x\in X$, we have $x\in A$.
\end{df}


Thus, the word ``closed" here means ``closed under taking limits".


\begin{pp}\label{lb86}
Let $A$ be a metric subspace of a metric space $X$. Recall  that the metric of $A$ inherits from that of $X$ (cf. Def. \ref{lb43}). Consider the statements:
\begin{enumerate}[label=(\arabic*)]
\item $A$ is complete.
\item $A$ is a closed subset of $X$.
\end{enumerate}
Then (1)$\Rightarrow$(2). If $X$ is complete, then (2)$\Rightarrow$(1).
\end{pp}


\begin{proof}
First, assume that $X$ is complete and (2) is true. Let $(x_n)$ be a Cauchy sequence in $A$. Then it is a Cauchy sequence in $X$. So $x_n\rightarrow x\in X$ because $X$ is complete. So $x\in A$ by the definition of closedness. This proves (1).

Next, we assume (1). Choose a sequence $(x_n)$ in $A$ converging to a point $x\in X$. By Prop. \ref{lb83}, $(x_n)$ is a Cauchy sequence in $X$, and hence a Cauchy sequence in $A$. Since $A$ is complete, there is $a\in A$ such that $x_n\rightarrow a$. So we must have $x=a$ because any sequence has at most one limit in a metric space. This proves $x\in A$. So (2) is proved.
\end{proof}

A similar result holds for sequential compactness. See Pb. \ref{lb90}.

\begin{eg}
Let $-\infty<a<b<+\infty$. Then $(a,b)$ is not complete (under the Euclidean metric), because $(a,b)$ is not closed in the metric space $\Rbb$. (For sufficiently large $n$, $b-1/n$ is in $(a,b)$, but $\lim_{n\rightarrow\infty} (b-1/n)=b$ is not in $b$.) 
\end{eg}

\begin{eg}
By Prop. \ref{lb2}, for each $x\in\Rbb\setminus\Qbb$, we can choose an increasing sequence in $\Qbb$ converging to $x$. So $\Qbb$ is not closed in $\Rbb$. So $\Qbb$ is not complete under the Euclidean topology.
\end{eg}


\begin{eg}\label{lb97}
Let $X$ be a metric space. Let $p\in X$ and $0\leq R<+\infty$. Then $\ovl B_X(x,R)$ is a closed subset of $X$. Therefore, if $X$ is complete, then $\ovl B_X(p,R)$ is complete by Prop. \ref{lb86}.
\end{eg}

\begin{proof}[Proof of closedness]
Let $(x_n)$ be a sequence in $\ovl B(p,R)$ converging to $x\in X$. Then $d(p,x_n)\leq R$. Since the function $y\in X\mapsto d(p,y)\in\Rbb$ is continuous (Exp. \ref{lb45}),  we have $d(p,x)=\lim_{n\rightarrow\infty}d(p,x_n)\leq R$. So $x\in\ovl B(p,R)$.
\end{proof}



\begin{exe}
Let $d,\delta$ be two equivalent metrics on a set $X$. Show that a sequence $(x_n)$ in $X$ is Cauchy under $d$ iff $(x_n)$ is Cauchy under $\delta$. 

Note that if, instead of assuming $d,\delta$ are equivalent, we only assume that $d,\delta$ are topologically equivalent. Then the above conclusion is not necessarily true:
\end{exe}


\begin{exe}
Find a non-complete metric $\delta$ on $\Rbb$ topologically equivalent to the Euclidean metric.
\end{exe}



%% Record  #3  2023/9/25  two lectures  7





\subsubsection{Normed vector spaces and Banach spaces}



A major application of complete metric spaces is to show that many series converge without knowing to what exact values these series converge. A typical example is the convergence of $\sum_{n\in\Zbb_+}\sin(\sqrt 2n)/n^2$ in $\Rbb$. We are also interested in the convergence of series in function spaces, for instance: the uniform convergence of $f(x)=\sum_{n\in\Zbb_+}\sin(\sqrt 2nx^3)/n^2$ on $\Rbb$; a suitable convergence of the Fourier series $\sum_{n\in\Zbb}a_ne^{\im n x}$. But we cannot take sum in a general metric space since it has no vector space structures. Therefore, we need a notion which combines complete metric spaces with vector spaces. Banach spaces are such a notion. 



\begin{df}\label{lb91}
Let $V$ be a vector space over $\Fbb$ with zero vector $0_V$. A function $\Vert\cdot\Vert:V\rightarrow\Rbb_{\geq 0}$ is called a \textbf{norm} \index{00@Norm} if for every $u,v\in V$ and $\lambda\in\Fbb$, the following hold:
\begin{itemize}
\item (Subadditivity) $\Vert u+v\Vert\leq \Vert u\Vert+\Vert v\Vert$. \index{00@Subadditivity}
\item (Absolute homogeneity) $\Vert\lambda v\Vert=|\lambda|\cdot \Vert v\Vert$. In particular, (by taking $\lambda=0$) we have $\Vert 0_V\Vert=0$.
\item If $\Vert v\Vert=0$ then $v=0_V$.
\end{itemize}
We call $(V,\Vert\cdot\Vert)$ (often abbreviated to $V$) a \textbf{normed vector space}. \index{00@Normed vector space}
\end{df}


\begin{rem}\label{lb367}
To check the absolute homogeneity, it suffices to check
\begin{align*}
\Vert\lambda v\Vert\leq |\lambda|\cdot\Vert v\Vert
\end{align*}
for all $\lambda$ and $v$. Then clearly $\Vert\lambda v\Vert=|\lambda|\cdot\Vert v\Vert$ when $\lambda=0$. Suppose $\lambda\neq 0$. Then
\begin{align*}
\Vert v\Vert=\Vert \lambda^{-1}\lambda v\Vert\leq |\lambda|^{-1}\Vert\lambda v\Vert
\end{align*}
which implies $\Vert\lambda v\Vert=|\lambda|\cdot\Vert v\Vert$.
\end{rem}

\begin{rem}
Let $V$ be a vector space. If $V$ is a normed vector space, then
\begin{align}
d(u,v)=\Vert u-v\Vert  \label{eq21}
\end{align} 
clearly defines a metric. (Note that triangle inequality follows from subadditivity.) Unless otherwise stated, we always assume that the metric of a normed vector space is defined by \eqref{eq21}.
\end{rem}






\begin{df}
Let $V$ be a normed vector space. We say that $V$ is a \textbf{Banach space} \index{00@Banach space} if $V$ is a complete metric space where the metric is the canonical one \eqref{eq21}. If $V$ is over the field $\Cbb$ (resp. $\Rbb$), we call $V$ a \textbf{complex} (resp. \textbf{real}) \textbf{Banach space}.
\end{df}




\begin{eg}
We always assume that the norm on $\Fbb^N$ is the \textbf{Euclidean norm} \index{00@Euclidean norm}
\begin{align}
\Vert (a_1,\dots,a_N)\Vert=\sqrt{|a_1|^2+\cdots+|a_N|^2}
\end{align}
The canonical metric it gives is the Euclidean metric. Thus, by Cor. \ref{lb80}, $\Fbb^N$ is a Banach space.
\end{eg}



If $(\lambda_n)$ is a sequence in $\Fbb$ converging to $\lambda$, and if $(x_n)$ is a sequence in $\Fbb^N$ converging to $x$, then one can show that $\lambda_nx_n$ converges to $\lambda x$ by checking that each component of $\lambda_nx_n$ converges to the corresponding component of $\lambda x$. This is due to Prop. \ref{lb38}. However, if $(x_n)$ is in general a sequence in a normed vector space, this method fails. So we need a different argument:

\begin{pp}\label{lb82}
Let $V$ be a normed vector space. The following maps are continuous
\begin{gather*}
+: V\times V\rightarrow V\qquad (u,v)\mapsto u+v\\
-: V\times V\rightarrow V\qquad (u,v)\mapsto u-v\\
\times_\Fbb: \Fbb\times V\rightarrow V\qquad (\lambda,v)\mapsto \lambda v\\
\Vert\cdot\Vert:V\rightarrow\Rbb_{\geq 0}\qquad v\mapsto \Vert v\Vert
\end{gather*}
\end{pp}

We didn't mention the continuity of the division map $(\lambda,v)\in\Fbb^\times\times V\mapsto\lambda^{-1}v$ since it follows from that of $\times_\Fbb$ and of the inversion map $\lambda\mapsto\lambda^{-1}$ by Exp. \ref{lb35}.

\begin{proof}
One can check that the addition map, the subtraction map, and the last map $\Vert\cdot\Vert$ are Lipschitz continuous. 

Define metric $d((\lambda,v),(\lambda',v'))=\max\{|\lambda-\lambda'|,\Vert v-v'\Vert \}$ on $\Fbb\times V$. Then $\Fbb\times V$ is covered by open balls of the form $B(0,r)=\{(\lambda,v)\in\Fbb\times V:|\lambda|<r,\Vert v\Vert<r\}$. Similar to the argument in \eqref{eq22}, one uses subadditivity (i.e. triangle inequality) and absolute homogeneity to show that $\times_\Fbb$ has Lipschitz constant $2r$ on $B(0,r)$. So $\times_\Fbb$ is continuous by Lem. \ref{lb30} and \ref{lb34}.
\end{proof}








\subsection{The Banach spaces $l^\infty(X,V)$ and $C(X,V)$}


In this section, we let $\Fbb\in\{\Rbb,\Cbb\}$ and assume that the vector spaces $V$ are over $\Fbb$. As the title suggests, in this section we shall introduce two important examples of Banach spaces: the space of bounded functions $l^\infty(X,V)$ and its  subspace of continuous functions $C(X,V)$ (when $X$ is a sequentially compact metric space).  In order for these two spaces to be Banach spaces, we must assume that $V$ is also Banach. 

In application, the main examples are $V=\Rbb,\Cbb,\Rbb^N,\Cbb^N$. Indeed, $C([a,b],\Rbb^N)$ is one of the main examples of function spaces considered by Fr\'echet when he defined metric spaces. Therefore, the readers can assume that $V$ is one of such spaces if they want to make life easier. Just keep in mind that we sometimes also consider the case where $V$ itself is a function space.

\begin{df}\label{lb150}
Let $X$ be a set and let $V$ be a vector space. The set $V^X$ \index{VX@$V^X$ as a vector space} is a vector space if we define for each $f,g\in V^X$ and $\lambda\in\Fbb$:
\begin{gather*}
f+g:X\rightarrow V\qquad (f+g)(x)=f(x)+g(x)\\
\lambda f:X\rightarrow V\qquad (\lambda f)(x)=\lambda f(x)
\end{gather*}
We also define the \textbf{absolute value function}\index{00@Absolute function $\lvert f\lvert$} \index{f@$\lvert f\lvert$}
\begin{align}
|f|:X\rightarrow\Rbb_{\geq 0}\qquad x\in X\mapsto \Vert f(x)\Vert
\end{align}
The symbol $|f|$ is sometimes also written as $\Vert f\Vert$ when it will not be confused with $\Vert f\Vert_{\infty}$ or other norms of $f$.
\end{df}



\begin{df}
Let $X$ be a set and let $V$ be a normed vector space. For each $f\in V^X$, define the \index{l@$\Vert\cdot\Vert_{l^\infty}=\Vert f\Vert_{l^\infty(X,V)}=\Vert\cdot\Vert_\infty$} \pmb{$l^\infty$}\textbf{-norm}
\begin{align}
\Vert f\Vert_{l^\infty(X,V)}\equiv\Vert f\Vert_{l^\infty}\equiv \Vert f\Vert_\infty=\sup_{x\in X}\Vert f(x)\Vert
\end{align}
where $\Vert f(x)\Vert$ is defined by the norm of $V$. Define the \pmb{$l^\infty$}\textbf{-space} \index{l@$l^\infty(X,V)$} 
\begin{align}
l^\infty(X,V)=\{f\in V^X:\Vert f\Vert_\infty<+\infty\}
\end{align}
which is a vector subspace of $V^X$. Then $l^\infty(X,V)$ is a normed vector space under the $l^\infty$-norm. A function $f:X\rightarrow V$ is called \textbf{bounded} \index{00@Bounded function} if $f\in l^\infty(X,V)$.
\end{df}





\begin{exe}\label{lb143}
Prove that for every $f,g\in V^X$ and $\lambda\in\Fbb$ we have
\begin{gather}\label{eq23}
\begin{gathered}
\Vert f+g\Vert_\infty\leq \Vert f\Vert_\infty+\Vert g\Vert_\infty\\
\Vert \lambda f\Vert_\infty=|\lambda|\cdot \Vert f\Vert_\infty
\end{gathered}
\end{gather}
(Note that clearly we have that $\Vert f\Vert_\infty=0$ implies $f=0$.) Here, we understand $0\cdot (+\infty)=0$. Use these relations to verify that $l^\infty(X,V)$ is a linear subspace of $V^X$ (i.e. it is closed under addition and scalar multiplication) and that $\Vert\cdot\Vert_\infty$ is a norm on $l^\infty(X,V)$.  \hfill\qedsymbol
\end{exe}

\begin{df}\label{lb148}
Let $V$ be a normed vector space. We say that a sequence $(f_n)$ in $V^X$ \textbf{converges uniformly} \index{00@Uniform convergence} to $f\in V^X$ if $\lim_{n\rightarrow\infty}\Vert f-f_n\Vert_\infty=0$. In this case, we write ${f_n}\rightrightarrows f$. \index{fnf@$f_n\rightrightarrows f$}

We say that $(f_n)$ \textbf{converges pointwise} \index{00@Pointwise convergence} to $f\in V^X$ if for every $x\in X$ we have $\lim_{n\rightarrow\infty} f_n(x)=f(x)$, i.e. $\lim_{n\rightarrow\infty} \Vert f_n(x)-f(x)\Vert=0$. 

The same definition will be applied to nets $(f_\alpha)_{\alpha\in I}$ in $V^X$ after learning net convergence in Sec. \ref{lb147}. \hfill\qedsymbol
\end{df}


In more details, the uniform convergence of $f_n$ to $f$ means that ``for every $\eps>0$ there is $N\in\Zbb_+$ such that for all $n\geq N$ and  \textit{for all $x\in X$}, we have $\Vert f_n(x)-f(x)\Vert<\eps$". If we place the words ``for all $x\in X$" at the very beginning of the sentence, we get pointwise convergence.


Uniform convergence implies pointwise convergence: If $\Vert f-f_n\Vert_\infty\rightarrow 0$, then for each $x\in X$ we have $\Vert f_n(x)-f(x)\Vert\rightarrow 0$ since $\Vert f(x)-f_n(x)\Vert\leq\Vert f-f_n\Vert_\infty$. 



\begin{eg}
Let $f_n:(0,1)\rightarrow\Rbb$ be $f_n(x)=x^n$. Then $f_n$ converges pointwise to $0$ (cf. Exp. \ref{lb110}). But $\sup_{x\in(0,1)}|x^n-0|=1$ does not converge to $0$. So $f_n$ does not converge uniformly to $0$.
\end{eg}



\begin{rem}
The uniform convergence of sequences in $l^\infty(X,V)$ is induced by the $l^\infty$-norm, and hence is induced by the metric $d(f,g)=\Vert f-g\Vert_\infty$. However, this formula cannot be extended to a metric on $V^X$, since for arbitrary $f,g\in V^X$, $\Vert f-g\Vert_\infty$ is possibly $+\infty$. 



In fact, it is true that the uniform convergence of sequences in $V^X$ is induced by a metric, see Pb. \ref{lb81}. When $X$ is countable, we have seen in Pb. \ref{lb78} that the pointwise convergence in $V^X$ is also given by a metric. \hfill\qedsymbol
\end{rem}












\begin{thm}\label{lb85}
Let $X$ be a set, and let $V$ be a Banach space (over $\Fbb$). Then $l^\infty(X,V)$ is a Banach space (over $\Fbb$).
\end{thm}


\begin{proof}
Let $(f_n)$ be a Cauchy sequence in $l^\infty(X,V)$. Then for every $\eps>0$ there is $N\in\Zbb_+$ such that for all $m,n\geq N$ we have that $\sup_{x\in X}\Vert f_n(x)-f_m(x)\Vert <\eps$, and hence $\Vert f_n(x)-f_m(x)\Vert <\eps$ for each $x\in X$. This shows that for each $x\in X$, $(f_n(x))$ is a Cauchy sequence in $V$, which converges to some element $f(x)\in V$ because $V$ is complete.

We come back to the statement that for each $\eps>0$, there exists $N\in\Zbb_+$ such that for all $n\geq N$ and all $x$,
\begin{align*}
\Vert f_n(x)-f_m(x)\Vert <\eps  
\end{align*}
for every $m\geq N$. Let $m\rightarrow\infty$. Then by the continuity of subtraction and taking norm (cf. Prop. \ref{lb82}.), we obtain $\Vert f_n(x)-f(x)\Vert\leq \eps$ for all $n\geq N$ and $x\in X$. In other words, $\Vert f_n-f\Vert_\infty\leq\eps$ for all $n\geq N$. In particular, $\Vert f\Vert_\infty\leq\Vert f_N\Vert_\infty +\Vert f_N-f\Vert_\infty<+\infty$ by \eqref{eq23}. This proves $f\in l^\infty(X,V)$ and $f_n\rightrightarrows f$.
\end{proof}


Mathematicians used to believe that ``if a sequence of continuous functions $f_n:[0,1]\rightarrow\Rbb$ converges pointwise to a function $f:[0,1]\rightarrow\Rbb$, then $f$ is continuous". Cauchy, one of the main figures in 19th century working on putting analysis on a rigorous ground, has given a problematic proof of this wrong statement. Counterexamples were later found in the study of Fourier series: Let $f:\Rbb\rightarrow\Rbb$ be a function with period $2\pi$ such that $f(x)=x$ when $-\pi<x<\pi$, and $f(x)=0$ when $x=\pm \pi$. Then the Fourier series  of this noncontinuous function $f$ converges pointwise to $f$, yet the partial sums of this series are clearly continuous functions. Later, it was realized that uniform convergence is needed to show the continuity of the limit function. (See Thm. \ref{lb87}.) This was the first time the importance of uniform convergence was realized.



The following discussions about (resp. sequentially compact) metric spaces also apply to general (resp. compact) topological spaces. The reader can come back and check the proofs for these more general spaces after studying them in the future.

\begin{df}
Let $X,Y$ be  metric spaces (resp. topological spaces). Then $C(X,Y)$ \index{CXY@$C(X,Y)$, the set of continuous functions $X\rightarrow Y$. See Conv. \ref{lb85} also} denotes the set of continuous functions from $X$ to $Y$. 
\end{df}


\begin{lm}
Let $X$ be a metric space (resp. a topological space), and let $V$ be a normed vector space. Then $C(X,V)$ is a linear subspace of $V^X$. If $X$ is sequentially compact (resp. compact), then $C(X,V)$ is a linear subspace of $l^\infty(X,V)$.
\end{lm}

\begin{proof}
Using Prop. \ref{lb82}, one checks easily that $C(X,V)$ is a linear subspace of $V^X$.  For any $f\in C(X,V)$, the absolute value function $|f|:x\in X\mapsto\Vert f(x)\Vert$ is continuous. Thus, assuming that $X$ is sequentially compact, then by Lem. \ref{lb56}, $|f|$ is bounded on $X$. This proves that $\Vert f\Vert_\infty<+\infty$. Thus $C(X,V)$ is a subset (and hence a linear subspace) of $l^\infty(X,V)$. 
\end{proof}



\begin{thm}\label{lb87}
Let $X$ be a metric space (resp. a topological space), and let $V$ be a normed vector space. Then $C(X,V)\cap l^\infty(X,V)$ is a closed linear subspace of $l^\infty(X,V)$. In particular, if $X$ is sequentially compact (resp. compact), then $C(X,V)$ is a closed linear subspace of $l^\infty(X,V)$.
\end{thm}



\begin{proof}
Choose a sequence $(f_n)$ in $C(X,V)\cap l^\infty(X,V)$ converging in $l^\infty(X,V)$ to $f$. Namely, $f_n\rightrightarrows f$. We want to prove that $f$ is continuous. We check that $f$ satisfies Def. \ref{lb31}-(2'). (One can also use Def. \ref{lb31}-(1). The proofs using these two definitions are not substantially different.)

Fix $p\in X$. Choose any $\eps>0$. Since $f_n\rightrightarrows f$, there exists $N\in\Zbb_+$ such that for all $n\geq N$ and we have $\Vert f-f_n\Vert_\infty<\eps$. Since $f_N$ is continuous, there exists $r>0$ such that for each $x\in B_X(p,r)$ we have $\Vert f_N(x)-f_N(p)\Vert<\eps$. Thus, for each $x\in B_X(p,r)$ we have
\begin{align*}
\Vert f(x)-f(p)\Vert\leq \Vert f(x)-f_N(x)\Vert +\Vert f_N(x)-f_N(p)\Vert+\Vert f_N(p)-f(p)\Vert<3\eps
\end{align*}
This finishes the proof.
\end{proof}





\begin{cv}\label{lb88}
Unless otherwise stated, if $X$ is sequentially compact metric space (or more generally, a compact topological space to be defined latter), and if $V$ is a normed vector space, the norm on $C(X,V)$ is chosen to be the $l^\infty$-norm.
\end{cv}


\begin{co}\label{lb101}
Let $X$ be a metric space  (resp. a topological space), and let $V$ be a Banach space. Then  $C(X,V)\cap l^\infty(X,V)$ is a Banach space under the $l^\infty$-norm. In particular, if $X$ is sequentially compact (resp. compact), then $C(X,V)$ is a Banach space.
\end{co}

\begin{proof}
This follows immediately from Prop. \ref{lb86}, Thm. \ref{lb87}, and the fact that $l^\infty(X,V)$ is complete (Thm. \ref{lb85}).
\end{proof}










\subsection{Problems and supplementary material}



\begin{prob}\label{lb64}
Let $(x_n)$ be a sequence in a metric space $X$. Let $x\in X$. Prove that the following are equivalent.
\begin{itemize}
\item[(1)] $x$ is a cluster point of $(x_n)$, i.e., the limit of a convergent subsequence of $(x_n)$.
\item[(2)] For each $\eps>0$ and each $N\in\Zbb_+$, there exists $n\geq N$ such that $d(x_n,x)<\eps$.
\end{itemize}
(Note: in a general topological space, these two statements are not equivalent.)
\end{prob}


\begin{rem}
Condition (2) is often abbreviated to ``for each $\eps>0$, the sequence $(x_n)$ is frequently in $B(x,\eps)$". In general, we say ``$(x_n)$ \textbf{frequently} satisfies P" if for each $N\in\Zbb_+$ there is $n\geq N$ such that $x_n$ satisfies P. We say that ``$(x_n)$ \textbf{eventually} satisfies P" if there exists $N\in\Zbb_+$ such that for every $n\geq N$, $x_n$ satisfies P. \index{00@Eventually} \index{00@Frequently}  

Thus ``$(x_n)$ eventually satisfies P" means the same as ``all but finitely many $x_n$ satisfies P". Its negation is ``$(x_n)$ frequently satisfies $\neg$P".   \hfill\qedsymbol
\end{rem}

\begin{rem}
Condition (2) of Pb. \ref{lb64} is sometimes easier to use than (1). For example, compared to the original definition of cluster points, it is much easier to find an explicit negation of (2) by using the rule suggested in Rem. \ref{lb100}: There exist $\eps>0$ and $N\in\Zbb_+$ such that $d(x_n,x)\geq\eps$ for all $n\geq N$. (Or simply: there exists $\eps>0$ such that $x_n$ is eventually not in $B(x,\eps)$.) 
\end{rem}



\begin{prob}\label{lb70}
Use Pb. \ref{lb64}-(2) to prove that if $(x_n)$ is a sequence in $\ovl\Rbb$, then $\dps\limsup_{n\rightarrow\infty} x_n$ is a cluster point of $(x_n)$.
\end{prob}

\begin{rem}
You will notice that your proof of Pb. \ref{lb70} is slightly simpler than the proof we gave for Thm. \ref{lb68}. This is because our construction of subsequence as in \eqref{eq17} has been incorporated into your proof of (2)$\Rightarrow$(1) in Pb. \ref{lb64}.
\end{rem}

\begin{prob}\label{lb235}
Let $f:X\rightarrow Y$ be a continuous map of metric spaces. Assume that $f$ is bijective and $X$ is sequentially compact.  Prove that $f$ is a homeomorphism using the following hint.
\end{prob}

\begin{proof}[Hint]
You need to prove that if $(y_n)$ is a sequence in $Y$ converging to $y\in Y$, then $x_n=f^{-1}(y_n)$ converges to $x=f^{-1}(y)$. Prove that $(x_n)$ has only one cluster point, and hence converges to some point $x'\in X$ (why?). Then prove $x'=x$. (In the future, we will use the language of open sets and closed sets to prove this result again. Do not use this language in your solution.)
\end{proof}




\begin{thm}[\textbf{Tychonoff theorem, countable version}]\index{00@Tychonoff theorem, countable version}  \label{lb89}
Let $(X_n)_{n\in\Zbb_+}$ be a sequence of sequentially compact metric spaces. Then the product space $\dps S=\prod_{n\in\Zbb_+} X_n$ is sequentially compact under the metric defined  as in Pb. \ref{lb78}.
\end{thm}

The method of choosing subsequence in the following proof is the reknowned \textbf{diagonal method}. \index{00@Diagonal method} A different method will be given in Pb. \ref{lb241}.

\begin{proof}
Let $(x_m)_{m\in\Zbb_+}$ be a sequence in $S$. Since $(x_m(1))_{m\in\Zbb_+}$ is a sequence in the sequentially compact space $X_1$, $(x_m)_{m\in\Zbb_+}$ has a subsequence $x_{1,1},x_{1,2},x_{1,3}\dots$ whose value at $n=1$ converges in $X_1$. Since $X_2$ is sequentially compact, we can choose a subsequence $x_{2,1},x_{2,2},x_{2,3},\dots$ of the previous subsequence such that its values at $n=2$ converge in $X_2$. Then pick a subsequence from the previous one whose values at $3$ converge in $X_3$. 

By repeating this process, we get an $\infty\times\infty$ matrix $(x_{i,j})_{i,j\in\Zbb_+}$:
\begin{equation}
\begin{tikzcd}[sep=0cm]
{x_{1,1}} & {x_{1,2}} & {x_{1,3}} & \cdots \\
{x_{2,1}} & {x_{2,2}} & {x_{2,3}} & \cdots \\
{x_{3,1}} & {x_{3,2}} & {x_{3,3}} & \cdots \\
\vdots    & \vdots    & \vdots    & \ddots
\end{tikzcd}
\end{equation}
such that the following hold:
\begin{itemize}
\item The $1$-st line is a subsequence of the original sequence $(x_m)_{m\in\Zbb_+}$.
\item The $(i+1)$-th line is a subsequence of the $i$-th line.
\item For each $n$, $\lim_{j\rightarrow\infty} x_{n,j}(n)$ converges in $X_n$.
\end{itemize}
Then the diagonal line $(x_{i,i})_{i\in\Zbb_+}$ is a subsequence of the original sequence $(x_m)_{m\in\Zbb_+}$. Moreover, for each $n$, $(x_{i,i})_{i\geq n}$ is a subsequence of the $n$-th line, whose value at $n$ therefore converges in $X_n$. Thus $\lim_{i\rightarrow\infty} x_{i,i}(n)$ converges in $X_n$. Thus, by Pb. \ref{lb78}, $(x_{i,i})_{i\in\Zbb_+}$ converges under any metric inducing the product topology.
\end{proof}




\begin{prob}\label{lb90}
Let $X$ be a sequentially compact metric space. Let $A\subset X$ be a metric subspace.  Consider the statements:
\begin{enumerate}[label=(\arabic*)]
\item $A$ is sequentially compact.
\item $A$ is a closed subset of $X$.
\end{enumerate}
Prove that (1)$\Rightarrow$(2). Prove that if $X$ is sequentially compact, then (2)$\Rightarrow$(1).
\end{prob}


The above problem implies immediately:

\begin{thm}[\textbf{Heine-Borel theorem}]\label{lb98}  \index{00@Heine-Borel theorem} 
Let $A$ be a subset of $\Rbb^N$. Then $A$ is sequentially compact iff $A$ is a bounded closed subset of $\Rbb^N$.
\end{thm}

\begin{proof}
Suppose that $A$ is sequentially compact. Then $A$ is bounded under the Euclidean metric by Prop. \ref{lb71}. By Pb. \ref{lb90}, $A$ is a closed subset of $\Rbb^N$.

Conversely, assume that $A$ is a bounded and closed subset of $\Rbb^N$. Then $A\subset B$ where $B$ is the product of $N$ pieces of closed intervals in $\Rbb$. Then $B$ is sequentially compact by Bolzano-Weierstrass. Since $A$ is closed in $\Rbb^N$, it is not hard to check that $A$ is closed in $B$.\footnote{If $Z$ is a metric space, if $X\subset Y\subset Z$, and if $X$ is closed in $Z$, then it is easy to check that $X$ is closed in $Y$.} Thus $A$ is sequentially compact by Pb. \ref{lb90}.
\end{proof}




\begin{eg}
Choose any $p\in \Rbb^N$ and $0\leq R<+\infty$. Then $\ovl B_{\Rbb^N}(p,R)$ is a bounded closed subset of $\Rbb^N$ (Exp. \ref{lb97}), and hence is sequentially compact by Heine-Borel.
\end{eg}

\begin{rem}
Think about the question: Equip $\Rbb^{\Zbb_+}$ with metric
\begin{align*}
d(x,y)=\sup_{n\in\Zbb_+}\frac {\min\{|x(n),y(n)|,1\}}{n}
\end{align*} 
What are the sequentially compact subsets of $\Rbb^{\Zbb_+}$? (Namely, think about how to generalize Heine-Borel theorem to $\Rbb^{\Zbb_+}$.)
\end{rem}



\begin{prob}
Do Exercise \ref{lb143}.
\end{prob}




\begin{prob}\label{lb81}
Let $V$ be a normed vector space. For every $f,g\in V^X$ define
\begin{align}
d(f,g)=\min\{1,\Vert f-g\Vert_\infty \}\label{eq55}
\end{align}
\begin{enumerate}
\item Show that $d$ defines a metric on $V^X$. 
\item Show that for every sequence $(f_n)$ in $V^X$ and every $g\in V^X$, we have $f_n\rightarrow g$ under the metric $d$ iff $f_n\rightrightarrows g$.
\end{enumerate}
\end{prob}

\begin{df}\label{lb146}
Let $X$ be a set, and let $V$ be a normed vector space. A metric on $V^X$ is called a \textbf{uniform convergence metric} \index{00@Uniform convergence metric} if it is equivalent to \eqref{eq55}. Thus, by Def. \ref{lb144}, a uniform convergence metric is one such that a sequence $(f_n)$ in $V^X$ converges to $f$ under this metric iff $f_n\rightrightarrows f$.
\end{df}




\begin{prob}\label{lb103}
Let $X,Y$ be metric spaces, and assume that $Y$ is sequentially compact. Let $V$ be a normed vector space. Choose $f\in C(X\times Y,V)$, i.e., $f:X\times Y\rightarrow V$ is continuous. For each $x\in X$, let
\begin{align*}
f_x:Y\rightarrow V\qquad y\mapsto f(x,y)
\end{align*}
Namely $f_x(y)=f(x,y)$. It is easy to check that $f_x\in C(Y,V)$. Define a new function
\begin{gather}
\begin{gathered}
\Phi(f):X\rightarrow C(Y,V)\qquad x\mapsto f_x
\end{gathered}
\end{gather}
Recall that $C(Y,V)$ is equipped with the $l^\infty$-norm. 
\begin{enumerate}
\item Prove that $\Phi(f)$ is continuous. In other words, prove that if $(x_n)$ is a sequence in $X$ converging to  $x\in X$, then $f_{x_n}\rightrightarrows f_x$ on $Y$, i.e.
\begin{align*}
\lim_{n\rightarrow\infty}\Vert f_{x_n}-f_x\Vert_{l^\infty(Y,V)}=0
\end{align*}
\item[$\star$ 2.]  Give an example of $f\in C(X\times Y,\Rbb)$ where $Y$ is not sequentially compact, $(x_n)$ converges to $x$ in $X$, and $f_{x_n}$ does not converge uniformly to $f_x$. (Note: you may consider $X=Y=\Rbb$.)
\end{enumerate}

\end{prob}

\begin{proof}[Hint]
In part 1, to prove that $\Phi(f)$ is continuous,   one can  prove the equivalent fact that for every fixed $x\in X$ the following is true:
\begin{itemize}
\item For every $\eps>0$ there exists $\delta>0$ such that for all $p\in B_X(x,\delta)$, we have $\sup_{y\in Y}\Vert f(p,y)-f(x,y)\Vert<\eps$. 
\end{itemize}
(Cf. Def. \ref{lb31}.) Prove this by contradiction and by using the sequential compactness of $Y$ appropriately.
\end{proof}


\begin{rem}\label{lb102}
Let $X=\Zbb_+\cup\{\infty\}$, equipped with the metric
\begin{align*}
d(m,n)=|m^{-1}-n^{-1}|
\end{align*} 
In other words, the metric on $X$ is $\tau^*d_\Rbb$ where $d_\Rbb$ is the Euclidean metric on $\Rbb$, and $\tau:X\rightarrow \Rbb,n\mapsto n^{-1}$. It is not hard to show that $X$ is sequentially compact: either prove it directly, or apply Heine-Borel to $\tau(X)$.

Let $Y$ be a metric space. Let $(y_n)_{n\in\Zbb_+}$ be a sequence in $Y$, and let $y_\infty\in Y$. It is not hard to see that the following two statements are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item The function $F:X\rightarrow Y,n\mapsto y_n$ is continuous.
\item The sequence $(y_n)_{n\in\Zbb_+}$ converges to $y_\infty$.
\end{enumerate}
The following problem is a generalization of this equivalence.  \hfill\qedsymbol
\end{rem}



\begin{sprob}\label{lb104}
Let $V$ be a normed vector space. Let $Y$ be a metric space. Let $X=\Zbb_+\cup\{\infty\}$ with metric defined as in Rem. \ref{lb102}. Let $(f_n)_{n\in\Zbb_+}$ be a sequence in $C(Y,V)$. Let $f_\infty\in V^Y$. Prove that the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item The following function is continuous:
\begin{align}
F:X\times Y\rightarrow V\qquad (n,y)\mapsto f_n(y)
\end{align}
In particular, by restricting $F$ to $\infty\times Y$, we see that $f_\infty\in C(Y,V)$.
\item $(f_n)_{n\in\Zbb_+}$ converges pointwise to $f_\infty$. Moreover, $(f_n)_{n\in\Zbb_+}$ is \textbf{pointwise equicontinuous}, \index{00@Pointwise equicontinuous} which means the following:
\begin{itemize}
\item For every $y\in Y$ and every $\eps>0$, there exists $\delta>0$ such that for all $p\in B_Y(y,\delta)$ we have
\begin{align*}
\sup_{n\in\Zbb_+}\Vert f_n(p)-f_n(y)\Vert<\eps
\end{align*}
\end{itemize}
\end{enumerate}
\end{sprob}

\begin{proof}[Note]\renewcommand{\qedsymbol}{}
In part (1), the only nontrivial thing to prove  is that $F$ is continuous at $(\infty,y)$ for every $y\in Y$.
\end{proof}





\begin{rem}
There is a concise way to define pointwise equicontinuity: a sequence $(f_n)_{n\in\Zbb_+}$ in $V^Y$ is pointwise equicontinuous iff the function
\begin{gather}
Y\mapsto V^{\Zbb_+}\qquad y\mapsto (f_1(y),f_2(y),\dots)
\end{gather}
is continuous, where $V^{\Zbb_+}$ is equipped with any uniform convergence metric (cf. Def. \ref{lb146}). 
\end{rem}

\begin{srem}
In Pb. \ref{lb104}, there is a quick and tricky way to conclude (1)$\Rightarrow$(2): Use Pb. \ref{lb103} and the sequential compactness of $X$. (Do not use this method in your solution. Prove (1)$\Rightarrow$(2) directly; it is a good exercise and is not difficult.)
\end{srem}



\begin{srem}\label{lb145}
Pb. \ref{lb103} and \ref{lb104}, together with Thm. \ref{lb87}, imply the following fact (can you see why?): 
\begin{itemize}
\item Let $Y$ be a sequentially compact metric space. Let $V$ be a normed vector space. Let $(f_n)_{n\in\Zbb_+}$ be a pointwise equicontinuous sequence of functions $Y\rightarrow V$ converging pointwise to some $f:Y\rightarrow V$. Then $f_n\rightrightarrows f$ on $Y$. 
\end{itemize}
You can also try to give a straightforward proof of this fact without using Pb. \ref{lb103} and \ref{lb104}.
\end{srem}








\newpage




\section{Series}

In this chapter, we assume that vector spaces are over $\Fbb\in\{\Rbb,\Cbb\}$ unless otherwise stated.





\subsection{Definitions and basic properties}


\begin{df}
Let $V$ be a Banach space (over $\Fbb$). A \textbf{series} \index{00@Series in a Banach space} in $V$ is an expression of the form
\begin{align}
\sum_{i=1}^\infty v_i  \label{eq25}
\end{align}
where $(v_i)_{i\in\Zbb_+}$ is a sequence in $V$. If $s\in V$, we say that the series \eqref{eq25} \textbf{converges to $s$} if
\begin{align*}
s=\lim_{n\rightarrow\infty} \sum_{i=1}^n v_i
\end{align*}
namely, $s_n\rightarrow s$ where $s_n$ is the \textbf{partial sum} \index{00@Partial sum} $s_n=\sum_{i=1}^n v_i$. In this case, we write
\begin{align*}
s=\sum_{i=1}^\infty v_i
\end{align*}
\end{df}


\begin{rem}\label{lb93}
Since $V$ is complete, the series \eqref{eq25} converges iff the sequence of partial sum $(s_n)$ is a Cauchy sequence: for every $\eps>0$ there exists $N\in\Zbb_+$ such that for all $n> m\geq N$ we have $\Vert s_n-s_m\Vert<\eps$, i.e.,
\begin{align}
\Big\Vert \sum_{i=m+1}^n v_i\Big\Vert<\eps  \label{eq28}
\end{align}
\end{rem}


\begin{pp}\label{lb92}
Suppose that $\sum_{i=1}^\infty v_i$ is a convergent series in a Banach space $V$. Then $\dps\lim_{n\rightarrow\infty} v_n=0$.
\end{pp}

\begin{proof}
Let $s_n=v_1+\cdots+v_n$, which converges to $s\in V$. Then $\lim_{n\rightarrow\infty} s_{n+1}=s$. So $v_n=s_{n+1}-s_n\rightarrow s-s=0$ since subtraction in continuous (Prop. \ref{lb82}).
\end{proof}

Thus, for example, $\sum_{n=1}^\infty (-1)^n$ diverges in the Banach space $\Rbb$ since $\lim_{n\rightarrow\infty} (-1)^n$ does not converge to $0$.





\begin{df}
Consider a \textbf{series} in $\ovl\Rbb_{\geq0}$: \index{00@Series in $\ovl\Rbb_{\geq0}$}
\begin{align}
\sum_{i=1}^\infty a_i \label{eq26}
\end{align}
namely, each $a_i$ is in $\ovl\Rbb_{\geq 0}$. Note that the partial sum $s_n=\sum_{i=1}^n a_i$ is increasing. We say that $\lim_{n\rightarrow\infty} s_n$ (which exists in $\ovl\Rbb_{\geq0}$ and equals $\sup\{s_n:n\in\Zbb_+\}$, cf. Rem. \ref{lb58}) is the value of the series \eqref{eq26} and write
\begin{align*}
\sum_{i=1}^\infty a_i=\lim_{n\rightarrow\infty} s_n
\end{align*}
\end{df}


\begin{df}
We say that a series $\sum_{i=1}^\infty a_i$ in $\Rbb_{\geq 0}$ \textbf{converges} if it converges in $\Rbb$ (but not just converges in $\ovl\Rbb_{\geq 0}$, which is always true). Clearly, $\sum_{i=1}^\infty a_i$ converges iff
\begin{align*}
\sum_{i=1}^\infty a_i<+\infty
\end{align*}
More generally, we say that a series $\sum_{i=1}^\infty v_i$ in a Banach space $V$ \textbf{converges absolutely}, \index{00@Absolute convergent series} if
\begin{align*}
\sum_{i=1}^\infty~ \Vert v_i\Vert <+\infty
\end{align*}
\end{df}

\begin{rem}
By the Cauchy condition of convergence, $\sum_{i=1}^\infty v_i$ converges absolutely iff for every $\eps>0$ there exists $N\in\Zbb_+$ such that for all $n> m\geq N$ we have 
\begin{align}
\sum_{i=m+1}^n~\Vert v_i\Vert<\eps \label{eq27}
\end{align}
By comparing \eqref{eq27} with \eqref{eq28} and using the subadditivity of the norm (recall Def. \ref{lb91}), we immediately see:
\end{rem}

\begin{pp}\label{lb94}
Let $\sum_{i=1}^\infty v_i$ be a series in a Banach space. The following are true.
\begin{enumerate}
\item If $\sum_{i=1}^\infty v_i$ converges absolutely, then it converges.
\item For each $i$ we choose $a_i\in\Rbb_{\geq0}$ satisfying $\Vert v_i\Vert\leq a_i$. Suppose that $\sum_{i=1}^\infty a_i<+\infty$. Then $\sum_{i=1}^\infty v_i$ converges absolutely.
\end{enumerate}
\end{pp}

\begin{proof}
Part 1 has been explained above. In part 2, we have $\sum\Vert v_i\Vert \leq \sum a_i<+\infty$. So $\sum v_i$ converges absolutely.
\end{proof}


\begin{exe}
Suppose that $\sum_{i=1}^\infty u_i$ and $\sum_{i=1}^\infty v_i$ are convergent (resp. absolutely convergent) series in a Banach space $V$. Let $\lambda\in\Fbb$. Show that the LHS of the following equations converges (resp. converges absolutely) in $V$, and that the following equations hold:
\begin{gather*}
\sum_{i=1}^\infty (u_i+v_i)=\sum_{i=1}^\infty u_i+\sum_{i=1}^\infty v_i\\
\sum_{i=1}^\infty \lambda v_i=\lambda\cdot\sum_{i=1}^\infty v_i
\end{gather*}
\end{exe}


\begin{rem}
We have seen that absolute convergence implies convergence. In fact, at least when $V=\Fbb^N$, absolute convergence is in many ways more natural than convergence. For example, we will learn that if a series $\sum_i v_i$ in $\Fbb^N$ converges absolutely, then the value of $\sum_i v_i$ is invariant under rearrangement of the series: for every bijection $\varphi:\Zbb_+\rightarrow\Zbb_+$ we have $\sum_i v_i=\sum_i v_{\varphi(i)}$. In the next semester, we shall learn Lebesgue integral theory and, more generally, measure theory. When applying measure theory to infinite sums over the countable set $\Zbb_+$, many good results (e.g. dominated convergence theorem, Fubini's theorem)  hold only for absolute convergence series, but not for arbitrary convergent series in general. In fact, there is no analog of convergent (but not absolutely convergent) series in measure theory at all!

When $V$ is not necessarily finite-dimensional, the situation is subtler: there is a version of convergence which lies between the usual convergence and absolute convergence, and which coincides with absolute convergence when $V=\Fbb^N$. This version of convergence is defined using nets instead of sequences. Moreover, many good properties (as mentioned above) hold for this convergence, and these properties can be proved in a very conceptual way (rather than using brute-force computation). We will learn this convergence in the next chapter.   \hfill\qedsymbol
\end{rem}



\subsection{Basic examples}


Let us study the \textbf{geometric series} $\sum_{n=0}^\infty z^n$ where $z\in\Cbb$. We first note the famous \textbf{binomial formula}: \index{00@Binomial formula} for each $z,w\in\Cbb$ and $n\in\Nbb$,
\begin{align}
(z+w)^n=\sum_{k0}^n{n\choose k}z^kw^{n-k}  \label{eq60}
\end{align}
In particular,
\begin{align}
(1+z)^n=1+nz+\frac{n(n-1)}{2}z^2+\frac{n(n-1)(n-2)}6 z^3+\cdots+nz^{n-1}+z^n \label{eq29}
\end{align}


\begin{eg}\label{lb110}
Assume $z\in\Cbb$ and $|z|<1$. Then $\lim_{n\rightarrow\infty}z^n=0$. 
\end{eg}

\begin{proof}
If $z=0$ then it is obvious. Assume that $0<|z|<1$. Choose $\delta>0$ such that $|z|=1/(1+\delta)$. By \eqref{eq29}, $(1+\delta)^n\geq 1+n\delta$. So
\begin{align*}
0\leq |z^n|\leq (1+n\delta)^{-1}
\end{align*}
Since $\dps\lim_{n\rightarrow\infty} (1+n\delta)^{-1}=0$, we have $|z^n|\rightarrow0$ by squeeze theorem. Hence $z^n\rightarrow0$.
\end{proof}

\begin{eg}\label{lb106}
Let $z\in\Cbb$. If $|z|<1$, then $\dps\sum_{n=0}^\infty z^n$ converges absolutely, and
\begin{align}
\sum_{n=0}^\infty z^n=\frac 1{1-z}
\end{align}
where $0^0$ is understood as $1$. If $|z|\geq 1$, then $\dps\sum_{n=0}^\infty z_n$ diverges in $\Cbb$.
\end{eg}


\begin{proof}
The partial sum $s_n=1+z+z^2+\cdots +z^n$ equals $(1-z^{n+1})/(1-z)$ when $z\neq 1$. Therefore, when $|z|<1$, $s_n\rightarrow 1/(1-z)$. When $|z|\geq 1$, we have $|z^n|\geq 1$ and hence $z^n\nrightarrow 0$. So $\sum_{n=0}^\infty z^n$ diverges by Prop. \ref{lb92}.
\end{proof}


\begin{eg}\label{lb95}
The \textbf{harmonic series} $\dps\sum_{n=1}^\infty \frac 1n$ diverges (in $\Rbb$).
\end{eg}


\begin{proof}
We want to show that the Cauchy condition (cf. Rem. \ref{lb93}) does not hold. Thus, we want to prove that there exists $\eps>0$ such that for every $N\in\Zbb_+$ there exist $n>m\geq N$ such that $|(m+1)^{-1}+(m+2)^{-1}+\cdots+n^{-1}|\geq\eps$.

To see this, for each $N$ we choose  $m=2^N$ and $n=2^{N+1}$. Then $n>m>N$, and
\begin{align*}
&\Big|\sum_{i=m+1}^n i^{-1}\Big|=\Big|\frac 1{2^N+1}+\frac 1{2^N+2}+\cdots +\frac 1{2^N+2^N}  \Big|\\
\geq&\underbrace{\Big|\frac 1{2^{N+1}}+\frac 1{2^{N+1}}+\cdots +\frac 1{2^{N+1}}  \Big|}_{2^N\text{ terms}}=\eps
\end{align*}
where $\eps=\frac 12$.
\end{proof}

\begin{comment}
\begin{rem}
For every $p\in\Rbb$, assume that $n^p$ (where $n\in\Zbb_+$) is defined and that the properties we learned in high school mathematics are satisfied. If $p\leq 1$, then clearly $\sum_{n=1}^\infty n^{-p}=+\infty$ since $1/n^p\geq 1/n$. It is in fact true that for every $p>1$ we have $\sum_{n=1}^\infty n^{-p}<+\infty$. The easiest (but not the most elementary) way to see this is by using integrals: This series equals $1+\int_{x=1}^{+\infty}f(x)dx$ where $f:[1,+\infty)\rightarrow\Rbb_{\geq0}$ equals $(n+1)^{-p}$ when restricted to $[n,n+1)$. So $f(x)\leq x^{-p}$ on $[1,+\infty)$. Hence $\int_{x=1}^{+\infty}f(x)dx\leq \int_{x=1}^{+\infty}x^{-p}dx=(1-p)^{-1}x^{1-p}|_{x=1}^{+\infty}=(p-1)^{-1}<+\infty$.
\end{rem}
\end{comment}


\begin{exe}\label{lb96}
Choose any $p\in\Zbb$. Prove that $\dps\sum_{n=1}^\infty n^{-p}$ converges iff $p\geq 2$.
\end{exe}

\begin{proof}[Hint]
Use Prop. \ref{lb94} and Exp. \ref{lb95} to reduce the problem to the case $p=2$. Prove this case by proving $\sum_{n=1}^\infty 1/n(n+1)=1<+\infty$.
\end{proof}



\begin{df}
Let $V$ be a Banach space, let $X$ be a set, and let $(f_n)$ be a sequence in $l^\infty(X,V)$, and let $g\in l^\infty(X,V)$. We say that the series of functions $\dps\sum_{i=1}^\infty f_i$ \textbf{converges uniformly to $g$} \index{00@Uniform convergence of series of functions} (on $X$) if it converges to $g$ as a series in the Banach space $l^\infty(X,V)$ and under the $l^\infty$-norm. Equivalently, this means that the partial sum function $s_n=f_1+\cdots+f_n$ converges uniformly to $g$ as $n\rightarrow\infty$.
\end{df}


\begin{eg}
The series of functions $\dps\sum_{n=1}^\infty \frac{\sin|nz^3|}{n^2}$ converges uniformly on $\Cbb$ to a continuous function $g:\Cbb\rightarrow\Rbb$ which is bounded (i.e. $\dps\sup_{z\in\Cbb}|g(z)|<+\infty$).
\end{eg}

\begin{proof}
Let $f_n(z)=\sin|nz^3|/n^2$. Then each $f_n$ is in $\fk X=C(\Cbb,\Rbb)\cap l^\infty(\Cbb,\Rbb)$ where $\fk X$ is a real Banach space under the $l^\infty$-norm by Cor. \ref{lb101}. Note that $\Vert f_n\Vert_\infty\leq n^{-2}$. By Exe. \ref{lb96}, $\sum_{n=1}^\infty n^{-2}<+\infty$. Therefore, by Prop. \ref{lb94}, the series $\sum_n f_n$ converges in $\fk X$, i.e., it converges uniformly to an element $g\in \fk X$. (In particular, $\sum_n f_n(z)=g(z)$ for all $z\in\Cbb$.)
\end{proof}


\subsection{Root test and ratio test; power series; construction of $e^z$}\label{lb218}


Root test and ratio test are useful criteria for proving the convergence or divergence of series, especially  power series. In addition, the method of power series provides a unified and elegant proof for many useful formulas about limit (see Prop. \ref{lb109} and Exp. \ref{lb111}). We begin our discussion with the following easy observation:

\begin{rem}\label{lb105}
Let $(x_n)$ be a sequence in $\ovl\Rbb$, and let $A\in\ovl\Rbb$. The following are true.
\begin{enumerate}
\item If $\dps\limsup_{n\rightarrow\infty} x_n<A$, then $x_n<A$ is eventually true.
\item If $\dps\limsup_{n\rightarrow\infty} x_n>A$, then $x_n>A$ is frequently true.
\end{enumerate}
By taking negative, we obtain similar statements for $\liminf$.
\end{rem}


\begin{proof}
Recall that $\limsup x_n=\inf_{n\in\Zbb_+}\alpha_n$ where where $\alpha_n=\sup\{x_n,x_{n+1},\dots\}$. 



Assume that  $\inf_{n\in\Zbb_+}\alpha_n<A$. Then $A$ is not a lower bound of $\{\alpha_n:n\in\Zbb_+\}$. Thus, there exists $N\in\Zbb_+$ such that $\alpha_N<A$. Then $x_n<A$ for all $n\geq N$.

Assume that $\inf_{n\in\Zbb_+}\alpha_n>A$. Then for each $N\in\Zbb_+$ we have $\alpha_N>A$. So $A$ is not an upper bound of $\{x_n,x_{n+1},\dots\}$.  So there is $n\geq N$ such that $x_n>A$.
\end{proof}


We will heavily use $\sqrt[n]{x}$ (where $x\geq0$ and $n\in\Zbb_+$) in the following discussions. $\sqrt[n]{x}$ will be rigorously constructed in Exp. \ref{lb216}, whose proof does not rely on the results of this section.


\begin{pp}[\textbf{Root test}]\index{00@Root test} 
Let $\dps\sum_{n=1}^\infty v_n$ be a series in a Banach space $V$. Let $\dps\beta=\limsup_{n\rightarrow\infty}\sqrt[n]{\Vert v_n\Vert}$. Then:
\begin{enumerate}
\item If $\beta<1$, then $\sum v_n$ converges absolutely, and hence converges in $V$.
\item If $\beta>1$, then $\sum v_n$ diverges in $V$.
\end{enumerate}
\end{pp}


\begin{proof}
Suppose $\beta<1$. Then we can choose $\gamma$ such that $\beta<\gamma<1$. So $\limsup \sqrt[n]{\Vert v_n\Vert}<\gamma$. By Rem. \ref{lb104}, there exists $N\in\Zbb_+$ such that for all $n\geq N$, we have $\sqrt[n]{\Vert v_n\Vert}<\gamma$, and hence $\Vert v_n\Vert <\gamma^n$. Since $\sum_{n=0}^\infty \gamma^n=(1-\gamma)^{-1}<+\infty$ (Exp. \ref{lb106}), the series $\sum_{n=N}^\infty v_n$ converges absolutely by Prop. \ref{lb94}. So the original series converges absolutely.

Assume that $\beta>1$. Then by Rem. \ref{lb105}, for each $N$ there is $n\geq N$ such that $\sqrt[n]{\Vert v_n\Vert}>1$ and hence $\Vert v_n-0\Vert>1$. So $v_n\nrightarrow 0$. So $\sum v_n$ diverges by Prop. \ref{lb92}.
\end{proof}

\begin{eg}
Let $V=\Rbb$ and $v_n=1/n$ resp. $v_n=1/n^2$. Then $\beta=1$, and $\sum v_n$ diverges resp. converges absolutely due to Exe. \ref{lb96}. So Root test gives no information on the convergence of series when $\beta=1$. The same can be said about ratio test.
\end{eg}




\begin{pp}[\textbf{Ratio test}]\index{00@Ratio test}  
Let $\dps\sum_{n=1}^\infty v_n$ be a series in a Banach space $V$ such that $v_n\neq 0$ for all $n$. Let $\dps\alpha=\liminf_{n\rightarrow\infty}\frac{\Vert v_{n+1}\Vert}{\Vert v_n\Vert}$ and $\dps\beta=\limsup_{n\rightarrow\infty}\frac{\Vert v_{n+1}\Vert}{\Vert v_n\Vert}$. Then:
\begin{enumerate}
\item If $\beta<1$, then $\sum v_n$ converges absolutely, and hence converges in $V$.
\item If $\alpha>1$, then $\sum v_n$ diverges in $V$.
\end{enumerate}
\end{pp}

\begin{proof}
Suppose $\beta<1$. Choose $\gamma$ such that $\beta<\gamma<1$. Then by Rem. \ref{lb105}, there is $N$ such that for all $n\geq N$ we have $\Vert v_{n+1}\Vert/\Vert v_n\Vert<\gamma$. So $\Vert v_n\Vert <\gamma^{n-N}\Vert v_N\Vert$. So $\sum_{n\geq N}\Vert v_n\Vert \leq \Vert v_N\Vert \cdot\sum_{n\geq N}\gamma^{n-N}=\Vert v_N\Vert\cdot (1-\gamma)^{-1}<+\infty$. So $\sum v_n$ converges absolutely.

Suppose $\alpha>1$. Then by Rem. \ref{lb105}, there is $N$ such that for all $n\geq N$ we have $\Vert v_{n+1}\Vert/\Vert v_n\Vert>1$. So $\Vert v_n\Vert\geq\Vert v_N\Vert>0$ for all $n\geq N$. So $v_n\nrightarrow 0$ and hence $\sum v_n$ diverges, as in the proof of root test.
\end{proof}









\begin{df}
A \textbf{power series} in a complex Banach space $V$ \index{00@Power series} is an expression of the form $\dps\sum_{n=0}^\infty v_nz^n$ where the \textbf{coefficients} $v_0,v_1,v_2,\dots$ are elements of $V$, and $z$ is a \textbf{complex variable},\index{00@Complex variable} i.e., a symbol which can take arbitrary values in $\Cbb$. If the power series $\sum v_nz^n$ converges at $z_0\in\Cbb$, we often let $\sum v_n z_0^n$ denote this limit.
\end{df}


\begin{pp}\label{lb108}
Let $\sum v_n z^n$ be a power series in a complex Banach space $V$. Then there is a unique $0\leq R\leq+\infty$ satisfying the following properties:
\begin{enumerate}[label=(\alph*)]
\item If $z\in\Cbb$ and $|z|<R$, then $\sum v_n z^n$ converges absolutely in $V$.
\item If $z\in\Cbb$ and $|z|>R$, then $\sum v_n z^n$ diverges in $V$.
\end{enumerate}
Such $R$ is called the \textbf{radius of convergence} \index{00@Radius of convergence} of $\sum v_nz^n$. Moreover, we have
\begin{align}
R=\frac 1{\dps\limsup_{n\rightarrow\infty}\sqrt[n]{\Vert v_n\Vert}}=\liminf_{n\rightarrow\infty}\frac 1{\sqrt[n]{\Vert v_n\Vert}}  \label{eq30}
\end{align}
\end{pp}



\begin{proof}
Clearly, there are at most one $R$ satisfying (a) and (b). Let us define $R$ using \eqref{eq30} (note that the second and the third terms of \eqref{eq30} are clearly equal), and prove that $R$ satisfies (a) and (b). Let
\begin{gather*}
\beta(z)=\limsup_{n\rightarrow\infty} \sqrt[n]{\Vert v_n z^n\Vert}
\end{gather*}
Then $\beta(z)=|z|/R$. So (a) and (b) follow immediately from root test.
\end{proof}


\begin{rem}
Note that if one can find $0\leq r\leq R$ such that $\sum v_nz^n$ converges whenever $|z|<r$, then $r\leq R$ where $R$ is the radius of convergence: otherwise, the series diverges for any positive $z$ satisfying $R<z<r$, impossible.

It follows that if  $\sum v_nz^n$ converges for all $|z|<r$, then $\sum v_nz^n$ converges \textit{absolutely} for all $|z|<r$.  \hfill\qedsymbol
\end{rem}


Prop. \ref{lb108} provides a useful method for computing limits of a positive sequence:

\begin{pp}\label{lb109}
Let $(\lambda_n)$ be a sequence in $\Rbb_{>0}$. Then
\begin{align}
\liminf_{n\rightarrow\infty}\frac{\lambda_{n+1}}{\lambda_n}\leq \liminf_{n\rightarrow\infty}\sqrt[n]{\lambda_n}\leq \limsup_{n\rightarrow\infty}\sqrt[n]{\lambda_n}\leq \limsup_{n\rightarrow\infty} \frac{\lambda_{n+1}}{\lambda_n}  \label{eq35}
\end{align}
In particular, (by Cor. \ref{lb113}) we have
\begin{align}
\lim_{n\rightarrow\infty}\sqrt[n]{\lambda_n}=\lim_{n\rightarrow\infty} \frac{\lambda_{n+1}}{\lambda_n}\label{eq31}
\end{align}
provided that the limit on the RHS of \eqref{eq31} exists in $\ovl\Rbb$.
\end{pp}

The four numbers in \eqref{eq35} can be completely different. See \cite[Exp. 3.35]{Rud-P}.

\begin{proof}
Let $R$ be the radius of convergence of $\sum \lambda_n z^n$. Then $R=1/\limsup\sqrt[n]{\lambda_n}$ by \eqref{eq31}. Thus, by Prop. \ref{lb108}, if $|z|>R$ then $\sum \lambda_n z^n$ diverges, and hence $\limsup|\lambda_{n+1}z^{n+1}|/|\lambda_n z^n|\geq 1$ by ratio test. Therefore, 
\begin{align*}
|z|>\frac 1{\dps\limsup\sqrt[n]{\lambda_n}}\qquad\Longrightarrow \qquad |z|\cdot\limsup\frac{\lambda_{n+1}}{\lambda_n}\geq 1
\end{align*}
This proves
\begin{align*}
\limsup\sqrt[n]{\lambda_n}\leq \limsup \frac{\lambda_{n+1}}{\lambda_n}
\end{align*}
Replacing $\lambda_n$ by $\lambda_n^{-1}$, we get
\begin{align*}
\frac 1{\dps\liminf \sqrt[n]{\lambda_n}}=\limsup\sqrt[n]{\lambda_n^{-1}}\leq \limsup \frac{\lambda_n}{\lambda_{n+1}}=\frac 1{\dps \liminf\frac{\lambda_{n+1}}{\lambda_n}}
\end{align*}
This proves \eqref{eq35}.
\end{proof}

\begin{eg}\label{lb111}
Let $a\in\Rbb_{>0}$ and $p\in\Zbb$. The following formulas follow immediately from Prop. \ref{lb109} (especially, from \eqref{eq31}):
\begin{subequations}
\begin{gather}
\lim_{n\rightarrow\infty} \sqrt[n]{a}=1 \label{eq32}\\
\lim_{n\rightarrow\infty}\sqrt[n]{n!}=+\infty  \label{eq33}\\
\lim_{n\rightarrow\infty}\sqrt[n]{n^p}=1  \label{eq34}
\end{gather}
Note that \eqref{eq34} follows from 
\begin{align}
\lim_{n\rightarrow\infty}\Big(\frac n{n+1}\Big)^p=1 
\end{align}
(This is clearly true when $p=\pm1$, and hence is true for any $p$ by induction.) By \eqref{eq34}, the radius of convergence of $\sum_n n^p z^n$ is $1$. Therefore, by Prop. \ref{lb108},  $\sum n^pA^{-n}$ converges absolutely when $A>1$. Thus, by Prop. \ref{lb92},
\begin{align}
\lim_{n\rightarrow\infty} \frac{n^p}{A^n}=0 \qquad(\text{if }A>1)
\end{align}
This means that ``polynomials grow slower than exponentials".
\end{subequations}

The same conclusions hold for arbitrary $p\in\Rbb$ once we know how to define $x^p$ and prove the continuity of $x\in\Rbb_{>0}\mapsto x^p$. (See Sec. \ref{lb219}.) \hfill\qedsymbol
\end{eg}


\begin{sexe}
Prove \eqref{eq32} directly. Then use \eqref{eq32} to give a direct proof of Prop. \ref{lb109}. Do not use root test, ratio test, or any results about power series.
\end{sexe}


%% Record #4 2023/09/27   three lectures  10





\begin{df}\label{lb107}
By \eqref{eq33}, the power series \index{exp@$e^z=\exp(z)$}
\begin{align*}
\exp(z)\equiv e^z=\sum_{n=0}^\infty \frac{z^n}{n!}
\end{align*}
has radius of convergence $+\infty$, and hence converges absolutely on $\Cbb$. (In particular, $\lim_{n\rightarrow\infty} z^n/n!=0$ for all $z\in\Cbb$.) This gives a function $\exp:\Cbb\rightarrow\Cbb$, called the \textbf{exponential function}. \index{00@Exponential function} 
\end{df}















Part (a) of Prop. \ref{lb108} can be strengthened in the following way.

\begin{thm}\label{lb112}
Let $\sum v_n z^n$ be a power series with coefficients in a complex Banach space $V$. Let $R$ be its radius of convergence, and assume that $0<R\leq+\infty$. For each $z\in B_\Cbb(0,R)$, let $f(z)$  denote the value of this series at $z$ (which is an element of $V$). Then $f:B_\Cbb(0,R)\rightarrow V$ is continuous. Moreover,  for each $0<\rho<R$, the series of functions $\sum v_n z^n$ converges uniformly on $\ovl B_\Cbb(0,\rho)$ to $f$. 
\end{thm}

Note that by calling $\sum v_n z^n$ a series of functions, we understand each term $v_nz^n$ as a function $\Cbb\rightarrow V$.

\begin{proof}
For each $0<\rho<R$, let $X_\rho=\ovl B_\Cbb(0,\rho)$. Then $X_\rho$ is clearly a bounded closed subset of $\Cbb$, and hence is sequentially compact by Heine-Borel Thm. \ref{lb98}. Let $g_n=v_nz^n$, which is a continuous function $X_\rho\rightarrow V$. We view $g_n$ as an element of the Banach space (cf. Cor. \ref{lb101}) $C(X_\rho,V)$. Then $\Vert g_n\Vert_\infty=\rho^n\Vert v_n\Vert$. Thus
\begin{align*}
\limsup_{n\rightarrow\infty}\sqrt[n]{\Vert g_n\Vert_\infty}=\limsup_{n\rightarrow\infty}\rho\sqrt[n]{\Vert v_n\Vert}=\rho/R<1
\end{align*}
Therefore, by root test, the series $\sum g_n$ converges in the Banach space $C(X_\rho,V)$ to some $f_\rho\in C(X_\rho,V)$.

We have proved that for each $0<\rho<R$, the series of functions $\sum v_nz^n$ converges uniformly on $X_\rho$ to a continuous function $f_\rho$. Let $f:B_\Cbb(0,R)\rightarrow V$ whose value at each $z$ is the value of the original series at $z$. Thus, if $|z|\leq\rho$, then $f_\rho(z)=f(z)$. Namely, $f|_{X_\rho}=f_\rho$. This shows that $\sum v_nz^n$ converges uniformly on $X_\rho$ to $f$. It also shows that $f|_{B_\Cbb(0,\rho)}$ is continuous (because $f_\rho$ is continuous). Therefore, since $B_\Cbb(0,R)$ is covered by all open disks $B_\Cbb(0,\rho)$ (where $0<\rho<R$), we conclude from Lem. \ref{lb30} that $f$ is continuous on $B_\Cbb(0,R)$.
\end{proof}

\begin{eg}\label{lb214}
By Thm. \ref{lb112}, the exponential function $\exp:\Cbb\rightarrow\Cbb$ is continuous; moreover, $\sum_{n=0}^\infty z^n/n!$ converges uniformly to $e^z$ on $\ovl B_{\Cbb}(0,R)$ for every $0<R<+\infty$, and hence on every bounded subset of $\Cbb$.
\end{eg}



\subsection{Problems and supplementary material}


Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$. 

\begin{prob}\label{lb566}
Let $W$ be a normed vector space. Prove that $W$ is complete iff every absolutely convergent series in $W$ is convergent (i.e. if $\sum_{n=1}^\infty \Vert w_n\Vert<+\infty$ then $\sum_{n=1}^\infty w_n$ converges).
\end{prob}

\begin{proof}[Hint]
``$\Rightarrow$" was proved in Prop. \ref{lb94}. To prove ``$\Leftarrow$", for each Cauchy sequence $(w_n)$ in $W$, choose a subsequence $(w_{n_k})$ such that $\Vert w_{n_k}-w_{n_{k+1}}\Vert\leq 2^{-k}$. Apply Thm. \ref{lb79}.
\end{proof}

\begin{prob}
Use the following discrete version of ``integration by part" 
\begin{align}
\sum_{k=m+1}^n f_kg_k=F_ng_n-F_mg_m+\sum_{k=m}^{n-1}F_k (g_k-g_{k+1})
\end{align}
(where $F_n=\sum_{j=1}^n f_j$) to prove the following Dirichlet's test.
\end{prob}


\begin{thm}[\textbf{Dirichlet's test}]\index{00@Dirichlet's test}\label{lb481}
Let $X$ be a set. Assume that $(f_n)$ is a sequence in $l^\infty(X,V)$ such that $\sup_{n\in\Zbb_+}\Vert F_n\Vert_{l^\infty}<+\infty$ (where $F_n=\sum_{j=1}^n f_j$). Assume that $(g_n)$ is a decreasing sequence (i.e. $g_1\geq g_2\geq g_3\geq\cdots$) in $l^\infty(X,\Rbb_{\geq0})$ converging uniformly to $0$. Then $\sum_{n=1}^\infty f_ng_n$ converges uniformly on $X$.
\end{thm}


\begin{prob}\label{lb394}
Let $e_n:\Rbb\rightarrow\Cbb$ be $e_n(x)=e^{\im nx}=\cos(nx)+\im\sin(nx)$. Show that for each $x\in\Rbb$, $\sum_{n=1}^\infty e_n(x)/n$ does not converge absolutely. Use Dirichlet's test to show that the series of functions $\sum_{n=1}^\infty e_n/n$ converges pointwise on $\Rbb\setminus\{2k\pi:k\in\Zbb\}$, and uniformly on $[\delta,2\pi-\delta]$ for every $0<\delta<\pi$.
\end{prob}











\begin{comment}

\subsection{Problems and supplementary material}

\begin{sprob}
Consider a power series $\sum a_nz^n$ where $a_n\in\Rbb_{\geq 0}$ for each $n$. Let $R$ be its radius of convergence. Prove that the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\sum a_n<+\infty$.
\item $\sum a_nz^n$ converges uniformly on $\ovl B_\Cbb(0,R)$ to a continuous function.
\item $\sum a_nz^n$ converges uniformly on $B_\Cbb(0,R)$.
\end{enumerate}
\end{sprob}
\end{comment}


\newpage


\section{Nets and discrete integrals}


\subsection{Introduction: why do we need nets?}



Nets were introduced by Moore and Smith in 1922 as a generalization of sequences. The most well-known motivation for introducing nets is that sequences are not enough for the study of non-metrizable topological spaces (i.e. topological spaces whose topologies are not induced by metrics). Here are two examples:
\begin{itemize}
\item In a general topological space, the definition of continuous maps using sequential convergence (as in Def. \ref{lb31}-(1)) is weaker than the definition using interior points and open sets (as in Def. \ref{lb31}-(2'), see also Rem. \ref{lb109}). Therefore, the dynamic intuition of sequences is not equivalent to the static intuition of open sets. 
\item Some important  topological spaces are compact (i.e. every open cover has a finite subcover) but not sequentially compact. $[0,1]^I$ (where $I$ is uncountable), equipped with the ``product topology" (i.e. ``pointwise convergence topology"), is such an example. 
\end{itemize}
As we shall see, nets provide a remedy for these issues: For a general topological space, the definition of continuity using net convergence is equivalent to that using open sets; compactness is equivalent to ``net-compactness", where the latter means that every net has a convergent subset. Thus, by generalizing sequences to nets, the dynamic intuition and the static and geometric intuition are unified again.


Nevertheless, the most common topological spaces appearing in analysis are metrizable. This raises the question: Why should we care about nets, given that our primary interest is in metrizable topological spaces? Here is my answer: Even though we are mainly interested in metrizable spaces, we can still find nets helpful in the following aspects. 

First of all, many convergence processes cannot be described by sequential convergence, but can be described by net convergence. For example, the following limits can be formulated and understood in the language of net convergence:
\begin{enumerate}[label=(\arabic*)]
\item The limit of a function $\lim_{x\rightarrow x_0}f(x)$ where $f:X\rightarrow Y$ is a map of metric spaces and $x_0\in X$.
\item The limit $\dps\lim_{m,n\rightarrow\infty}a_{m,n}$ where $(a_{m,n})_{m,n\in\Zbb_+}$ is a \textbf{double sequence} in a metric space $X$. Note that this is not the same as (but is more natural than) the \textbf{iterated limit} $\dps\lim_{n\rightarrow\infty}\lim_{m\rightarrow\infty} a_{m,n}$. Moreover, the limit $\dps\lim_{m,n\rightarrow\infty}a_{m,n}$ is the key to understanding the problem of commutativity of iterated integrals:
\begin{align*}
\lim_{n\rightarrow\infty}\lim_{m\rightarrow\infty} a_{m,n}\xlongequal{?}\lim_{m\rightarrow\infty}\lim_{n\rightarrow\infty} a_{m,n}
\end{align*}
\item The Riemann integral $\int_a^b f(x)dx$. This is the limit of the Riemann sum $\lim\sum f(\xi_i)(a_i-a_{i-1})$  as the partition of the interval $[a,b]$ is getting finer and finer.
\end{enumerate} 
Moreover, as for (3), we shall see that the net version of Cor. \ref{lb113} provides a quick and conceptual proof of the following fact: If the upper and lower Darboux integrals are equal, then the Riemann integral exists and are equal to the two Darboux integrals. Indeed, the upper and lower Darboux integrals are respectively the $\limsup$ and $\liminf$ of a net in $\Rbb$.

Second, nets provide a conceptual solution to many problems about \textbf{double series}. Let $(a_{m,n})_{m,n\in\Zbb_+}$ be a double sequence in $\Rbb$. Think about the following questions, which arise naturally when one is trying to prove $e^ze^w=e^{z+w}$.
\begin{enumerate}[label=(\alph*)]
\item When is it true that $\dps \sum_{n=1}^\infty\sum_{m=1}^\infty a_{m,n}=\sum_{m=1}^\infty\sum_{n=1}^\infty a_{m,n}$ ?
\item Since $\card(\Zbb_+\times\Zbb_+)=\card(\Zbb_+)$, why not use an ordinary series to study a double series? So let us \textbf{parametrize} $\Zbb_+\times\Zbb_+$ by $\Zbb_+$: choose a bijection $\varphi:\Zbb_+\rightarrow \Zbb_+\times\Zbb_+$. When is it true that $\dps  \sum_{k=1}^\infty a_{\varphi(k)}=\sum_{n=1}^\infty\sum_{m=1}^\infty a_{m,n}$ ?
\item Choose another parametrization (i.e. bijection) $\psi:\Zbb_+\rightarrow\Zbb_+\times\Zbb_+$. When is it true that $\dps  \sum_{k=1}^\infty a_{\varphi(k)}=\sum_{k=1}^\infty a_{\psi(k)}$ ?
\item More generally, let $X$ be a countably infinite set, and let $f:X\rightarrow\Rbb$. Intuitively, we can take an infinite sum $\dps\sum_{x\in X}f(x)$. How to define it rigorously? One may think about choosing a parametrization, i.e., a bijection $\varphi:\Zbb_+\rightarrow X$. Then one defines the infinite sum by $\sum_{k=1}^\infty f(\varphi(k))$. Is this definition independent of the choice of parametrization?
\item As a special case of (d), when is a series invariant under \textbf{rearrangement}? Namely, choose a bijection $\varphi:\Zbb_+\rightarrow\Zbb_+$, and choose a sequence $(a_n)$ in $\Rbb$, when is it true that $\dps\sum_{n=1}^\infty a_n=\sum_{n=1}^\infty a_{\varphi(n)}$ ?
\end{enumerate}

Modern differential geometry (whose ``intrinsic" spirit stems from Gauss's Theorema Egregium) teaches us that in order to answer these questions, one should first define the infinite sum $\sum_{x\in X}f(x)$ in a parametrization-independent way. (The reason we call a bijection $\varphi:\Zbb_+\rightarrow X$ a parametrization is that we want the readers to compare it with the parametrizations of curves, surfaces, and more generally manifolds.)  We will call this sum a \textbf{discrete integral}. Then, one tries to answer when this definition agrees with those that depend on parametrizations (such as the sums in (a)-(e) above). These goals can be achieved with the help of nets.



\subsection{Nets}\label{lb147}


\subsubsection{Directed sets and nets}

\begin{df}\label{lb116}
A relation $\leq$ on a set $I$ is called a \textbf{preorder} \index{00@Preorder, and preordered set} if for all $\alpha,\beta,\gamma\in I$, the following are satisfied:
\begin{itemize}
\item (Reflexivity) $\alpha\leq \alpha$.
\item (Transitivity) If $\alpha\leq \beta$ and $\beta\leq \gamma$ then $a\leq \gamma$.
\end{itemize}
The pair $(I,\leq)$ (or simply $I$) is called a \textbf{preordered set}.
\end{df}

Therefore, a partial order is a preorder satisfying antisymmetry: $(\alpha\leq \beta)\land(\beta\leq\alpha)\Rightarrow (\alpha=\beta)$.

\begin{df}
A preordered set $(I,\leq)$ is called a \textbf{directed set} \index{00@Direct set} if 
\begin{align}
\forall\alpha,\beta\in I~~~\exists\gamma\in I~~\text{ such that }\alpha\leq \gamma,\beta\leq\gamma  \label{eq37}
\end{align}
If $I$ is a directed set and $X$ is a set, then a function $x:I\rightarrow X$ is called a \textbf{net} \index{00@Net $(x_\alpha)_{\alpha\in I}$} with directed set/index set $I$. We often write $x(\alpha)$ as $x_\alpha$ if $\alpha\in I$, and write $x$ as $(x_\alpha)_{\alpha\in I}$.
\end{df}


\begin{eg}
$(\Zbb_+,\leq)$ is a directed set. A net with index set $\Zbb_+$ in a set $X$ is precisely a sequence in $X$.
\end{eg}

\begin{df}\label{lb117}
Suppose that $(I,\leq_I )$ and $(J,\leq_J)$ are preordered set (resp. directed set), then the \textbf{product} \index{00@Product preordered/directed set} $(I\times J,\leq)$ is a preordered set (resp. directed set) if for every $\alpha,\alpha'\in I,\beta,\beta'\in J$ we define
\begin{align}\label{eq36}
(\alpha,\beta)\leq (\alpha',\beta')\qquad\Longleftrightarrow\qquad \alpha\leq_I \alpha'~~\text{and}~~\beta\leq_J\beta'
\end{align}
Unless otherwise stated, the preorder on $I\times J$ is assumed to be defined by \eqref{eq36}.
\end{df}


\begin{eg}
$\Zbb_+\times\Zbb_+$ (or similarly, $\Nbb\times\Nbb$) is naturally a directed set whose preorder is defined by \eqref{eq36}. A net $(x_{m,n})_{(m,n)\in\Zbb_+\times\Zbb_+}$ with index set $\Zbb_+\times\Zbb_+$ is called a \textbf{double sequence} \index{00@Double sequence} and is written as $(x_{m,n})_{m,n\in\Zbb_+}$ or simply $(x_{m,n})$. (We will even write it as $(x_{mn})$ when no confusion arises.)

More generally, we call $(x_{\alpha,\beta})_{(\alpha,\beta)\in I\times J}=(x_{\alpha,\beta})_{\alpha\in I,\beta\in J}$ a \textbf{double net} \index{00@Double net} if its index set is $I\times J$ for some directed sets $I,J$. \hfill\qedsymbol
\end{eg}




\begin{eg}\label{lb130}
If $X$ is a set, then $(2^X,\subset)$ and $(\fin(2^X),\subset)$ \index{fin@$\fin(2^X)$} are directed sets where
\begin{align}
\fin(2^X)=\{A\subset X:A\text{ is a finite set}\}
\end{align}
We will use nets with index set $\fin(2^X)$ to study infinite sums.
\end{eg}


\begin{eg}
Let $X$ be a metric space and $x\in X$. Then $X_x=(X,\leq)$ is a directed set if for each $p_1,p_2\in X$ we define
\begin{align}
p_1\leq p_2~~\text{in }X_x\qquad\Longleftrightarrow\qquad d(p_1,x)\geq d(p_2,x)
\end{align}
(Namely, a larger element of $X_x$ is one closer to $x$.) Nets with this directed set can be used to study the limits of functions (cf. Rem. \ref{lb269}). Note that $X_x$ is our first example of directed set which is not a poset! ($d(p_1,x)=d(p_2,x)$ does not imply $p_1=p_2$.)
\end{eg}


\subsubsection{Limits of nets}

If $I$ is an preordered set and $\beta\in I$, we write \index{I@$I_{\geq\beta}$}
\begin{gather}
I_{\geq\beta}=\{\alpha\in I:\alpha\geq\beta\}
\end{gather}
\begin{df}
Let $P$ be a property about elements of a set $X$, i.e., $P$ is a function $X\rightarrow\{\text{true, false}\}$. Let $(x_\alpha)_{\alpha\in I}$ be a net in $X$. 

We say that $x_\alpha$ \textbf{eventually} \index{00@Eventually} satisfies $P$ (equivalently, we say that $x_\alpha$ satisfies $P$ for \textbf{sufficiently large} \index{00@Sufficiently large} $\alpha$) if:
\begin{itemize}
\item There exists $\beta\in I$ such that for every $\alpha\in I_{\geq\beta}$, the element $x_\alpha$ satisfies $P$.
\end{itemize}
``Sufficiently large" is also called ``\textbf{large enough}". \index{00@Large enough=sufficiently large}

We say that $x_\alpha$ \textbf{frequently} \index{00@Frequently} satisfies $P$ if:
\begin{itemize}
\item For every $\beta\in I$ there exists $\alpha\in I_{\geq\beta}$ such that $x_\alpha$ satisfies $P$.
\end{itemize}
\hfill\qedsymbol
\end{df}


\begin{rem}
Note that unlike sequences, for a general net, ``$x_\alpha$ eventually satisfies $P$" does not imply ``all but finitely many $x_\alpha$ satisfy $P$" because the complement of $I_{\geq\beta}$ is not necessarily a finite set.  
\end{rem}


\begin{rem}
Let $P$ and $Q$ be two properties about elements of $X$. Then
\begin{subequations}
\begin{gather}
\neg(\text{$x_\alpha$ eventually satisfies $P$})~~=~~(\text{$x_\alpha$ frequently satisfies $\neg P$})
\end{gather}
By the crucial condition \eqref{eq37} for directed sets, we have
\begin{gather}\label{eq38}
\begin{gathered}
(x_\alpha\text{ eventually satisfies }P)\land(x_\alpha\text{ eventually satisfies }Q)\\
\Downarrow \\
x_\alpha\text{ eventually satisfies }P\land Q
\end{gathered}
\end{gather}
By taking contraposition and replacing $P,Q$ by $\neg P,\neg Q$, we have
\begin{gather}\label{eq103}
\begin{gathered}
x_\alpha\text{ frequently satisfies }P\lor Q\\
\Downarrow\\
(x_\alpha\text{ frequently satisfies }P)\lor(x_\alpha\text{ frequently satisfies }Q)
\end{gathered}
\end{gather}
\end{subequations}
\end{rem}

\begin{df}\label{lb174}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a metric space $X$. Let $x\in X$. We say that $(x_\alpha)$ \textbf{converges to} $x$ and write \index{lim@$\lim_{\alpha\in I}x_\alpha\equiv \lim_\alpha x_\alpha$}
\begin{align*}
\lim_{\alpha\in I}x_\alpha\equiv \lim_\alpha x_\alpha=x
\end{align*}
or simply $x_\alpha\rightarrow x$ if the following statement holds:
\begin{itemize}
\item For every $\eps>0$, $x_\alpha$ is eventually in $B_X(x,\eps)$.
\end{itemize}
Clearly, $x_\alpha\rightarrow x$ iff $d(x_\alpha,x)\rightarrow 0$.
\end{df}

\begin{df}
Let $(x_{m,n})_{m,n\in\Zbb_+}$ be a double sequence in a metric space. Then we write
\begin{align}
\lim_{(m,n)\in\Zbb_+\times\Zbb_+} x_{m,n}\equiv \lim_{m,n\rightarrow\infty} x_{m,n}
\end{align}
and call it the \textbf{(double) limit} \index{00@Double limit} of $(x_{m,n})$.
\end{df}

\begin{rem}
Let us spell out the meaning of $\lim_{m,n\rightarrow\infty} x_{m,n}=x$: For each $\eps>0$ there exists $M,N\in\Zbb_+$ such that $d(x_{m,n},x)<\eps$ for all $m\geq M$ and $n\geq N$. Clearly, this is equivalent to the statement:
\begin{itemize}
\item For each $\eps>0$ there exists $N\in\Zbb_+$ such that $d(x_{m,n},x)<\eps$ for all $m,n\geq N$.
\end{itemize}
Therefore, if $(x_n)$ is a sequence in $X$, then
\begin{align}
\text{$(x_n)$ is a Cauchy sequence}\qquad\Longleftrightarrow\qquad \lim_{m,n\rightarrow\infty} d(x_m,x_n)=0
\end{align}
Thus, the Cauchyness of sequences can be studied in terms of double limits, and hence in terms of nets.
\end{rem}




\begin{pp}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a metric space $X$ converging to $x,y$. Then $x=y$.
\end{pp}

\begin{proof}
Suppose that $x\neq y$. Then there are $r,\rho>0$ such that $B(x,r)\cap B(y,\rho)=\emptyset$, say $r=\rho=d(x,y)/2$. Since $x_\alpha\rightarrow x$, the point $x_\alpha$ is eventually in $B(x,r)$. Since $x_\alpha\rightarrow y$, the point $x_\alpha$ is eventually in $B(y,\rho)$. Therefore, by the logic \eqref{eq38}, $x_\alpha$ is eventually in $B(x,r)\cap B(y,\rho)$, impossible.
\end{proof}

\begin{thm}\label{lb121}
Let $f:X\rightarrow Y$ be map of metric spaces continuous at $x\in X$. Let $(x_\alpha)_{\alpha\in I}$ be a net in $X$ converging to $x$. Then  $\dps\lim_\alpha f(x_\alpha)=f(x)$.
\end{thm}

\begin{proof}
Choose any $\eps>0$. By Def. \ref{lb31}-(2) and the continuity of $f$ at $x$, there exists $\delta>0$ such that for all $p\in B(x,\delta)$ we have $f(p)\in B(f(x),\eps)$. Since $x_\alpha\rightarrow x$, $x_\alpha$ is eventually in $B(x,\delta)$. Therefore $f(x_\alpha)$ is eventually in $B(f(x),\eps)$.
\end{proof}


This theorem implies, for example, that if $(v_\alpha)$ is a net in a complex normed vector space converging to $v$, and if $(\lambda_\alpha)$ is a net in $\Cbb$ converging to $\lambda$, then $\lambda_\alpha v_\alpha$ converges to $\lambda v$ because the scalar multiplication map is continuous (Prop. \ref{lb82}).



\begin{exe}\label{lb123}
Prove the generalization of Rem. \ref{lb58}:
\begin{enumerate}
\item If $(x_\alpha)_{\alpha\in I},(y_\alpha)_{\alpha\in I}$ are nets in $\ovl\Rbb$ converging to $A,B\in\ovl\Rbb$, and if $x_\alpha\leq y_\alpha$ for all $\alpha$, then $A\leq B$.
\item \textbf{Squeeze theorem}: \index{00@Squeeze theorem} Suppose that $(x_\alpha)_{\alpha\in I},(y_\alpha)_{\alpha\in I},(z_\alpha)_{\alpha\in I}$ are nets in $\ovl\Rbb$, $x_\alpha\leq y_\alpha\leq z_\alpha$ for all $\alpha$, and $x_\alpha$ and $z_\alpha$ both converge to $A\in\ovl\Rbb$. Then $y_\alpha\rightarrow A$.
\item If $(x_\alpha)$ is an increasing resp. decreasing net in $\ovl\Rbb$, then $\lim_\alpha x_\alpha$ exists in $\ovl\Rbb$ and equals $\sup_\alpha x_\alpha$ resp. $\inf_\alpha x_\alpha$.
\end{enumerate}
\end{exe}








\subsubsection{Subnets (in the sense of Willard)}




\begin{df}
A subset $E$ of a directed set $I$ is called \textbf{cofinal} \index{00@Cofinal subset} if:
\begin{align*}
\forall\alpha\in I~~~\exists\beta\in E~~~\text{such that }\alpha\leq\beta
\end{align*}
By the transitivity in Def. \ref{lb116} and property \eqref{eq37}, we clearly have
\begin{align*}
\forall\alpha_1,\dots,\alpha_n\in I~~~\exists\beta\in E~~~\text{such that }\alpha_1\leq\beta,\dots,\alpha_n\leq\beta
\end{align*}
\end{df}



\begin{df}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a set $X$. A \textbf{subnet} \index{00@Subnet} of $(x_\alpha)_{\alpha\in I}$ is, by definition, of the form $(x_{\alpha_s})_{s\in S}$ where $S$ is a directed set, and
\begin{align*}
(\alpha_s)_{s\in S}:S\rightarrow I\qquad s\mapsto \alpha_s
\end{align*}
is an increasing function whose range $\{\alpha_s:s\in S\}$ is cofinal in $I$.
\end{df}



\begin{rem}
There are several different definitions of subnets that are equivalent for proving the main results in point-set topology. Unfortunately, there is no common agreement on the standard definition of subnets. The definition we gave is due to Willard \cite{Wil}, and is also the one given in the famous textbook of Munkres \cite{Mun}. Some famous analysis and topology textbooks (e.g. \cite{Fol,Kel,RS}) use a weaker definition, which does not assume that the map $S\rightarrow I$ is increasing.
\end{rem}




\begin{eg}
A subsequence of a sequence is a subnet of that sequence.
\end{eg}

\begin{eg}\label{lb118}
Let $(x_{m,n})_{m,n\in\Zbb_+}$ be a net with index set $\Zbb_+\times\Zbb_+$. Then $(x_{k,k})_{k\in\Zbb_+}$ and $(x_{2k,k})_{k\in\Zbb_+}$ are subnets. $(x_{k,1})_{k\in\Zbb_+}$ is not a subnet, because the cofinal condition is not satisfied. More generally, it is not hard to show that for every function $\varphi,\psi:\Zbb_+\rightarrow\Zbb_+$, $(x_{\varphi(k),\psi(k)})_{k\in\Zbb_+}$ is a subnet iff $\varphi,\psi$ are increasing and $\lim_{k\rightarrow\infty}\varphi(k)=\lim_{k\rightarrow\infty}\psi(k)=+\infty$.
\end{eg}

\begin{exe}
Prove the following facts:
\begin{itemize}
\item The cofinal subset of a cofinal subset of a directed set $I$ is a cofinal subset of $I$.
\item  The subnet of a subnet of a net $(x_\alpha)$ is a subnet of $(x_\alpha)$. 
\end{itemize}
Note that in your proof you need to use the transitivity in Def. \ref{lb116}.
\end{exe}


The biggest difference between subnets and subsequences is that the index set of a subnet is not necessarily a subset of the index set of the original net. Indeed, subnets are defined in this way mainly because we want to have a net version of Pb. \ref{lb64} in any topological space. (This will be achieved in Pb. \ref{lb223}.) Let us see an elementary example of subnet whose index set is larger than that of the original net. Its importance is justified by the proofs of Exp. \ref{lb125} and Prop. \ref{lb126}.


\begin{eg}\label{lb124}
Let $J$ be a directed set. Then every net $(x_\alpha)_{\alpha\in I}$ has subnet $(x_\alpha)_{(\alpha,\beta)\in I\times J}$. The corresponding increasing map of directed sets is the projection $I\times J\rightarrow I$ onto the first component. 
\end{eg}






To appreciate the importance of cofinalness (as well as transitivity), we prove the following generalization of Prop. \ref{lb23}. This result has a wide range of surprising applications that are unavailable when one only considers sequences. (We will see them soon in this chapter. For instance, this result explains why the values of absolutely convergent series are invariant under rearrangement.) So I call this result a theorem, even though its proof is simple.


\begin{thm}\label{lb120}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a metric space (or more generally, a topological space) $X$ converging to $x\in X$. Then every subnet $(x_{\alpha_s})_{s\in S}$ converges to $x$.
\end{thm}

The following proof for metric spaces can be generalized straightforwardly to topological spaces. The readers can come back and check the details after learning topological spaces.

\begin{proof}
Choose any $\eps>0$. Since $x_\alpha\rightarrow x$, there exists $\beta\in I$ such that for all $\alpha\geq\beta$ we have $d(x_\alpha,x)<\eps$. By the cofinalness, there exists $t\in S$ such that $\alpha_t\geq\beta$. Thus, since $s\in S\mapsto \alpha_s\in I$ is increasing, for every $s\geq t$, we have $\alpha_s\geq\alpha_t\geq\beta$ and hence $\alpha_s\geq\beta$ by the transitivity in Def. \ref{lb116}. So $d(x_{\alpha_s},x)<\eps$ for all $s\geq t$. This finishes the proof.
\end{proof}

This proposition does not hold if one does not assume cofinalness in the definition of subnets:

\begin{eg}
Let $(x_n)$ be a sequence in $\Rbb$ converging to $x\in\Rbb$. Since $(x_n)$ is a Cauchy sequence, we know that $\lim_{m,n\rightarrow\infty}x_m-x_n=0$. We have seen in Exp. \ref{lb118} that $(x_{2k}-x_k)_{k\in\Zbb_+}$ is a subnet of $(x_{m,n})$. Therefore, $\lim_{k\rightarrow\infty} x_{2k}-x_k=0$. But $(x_k-x_1)_{k\in\Zbb_+}$ is not a subnet since the cofinal condition is not satisfied. And if $x\neq x_1$, then $\lim_k (x_k-x_1)=x-x_1\neq 0$, i.e.,
\begin{align*}
\lim_{k\rightarrow\infty}(x_k-x_1)\neq \lim_{m,n\rightarrow\infty} (x_m-x_n)
\end{align*}
\end{eg}


In Subsec. \ref{lb119}, we have seen two criteria for the divergence of sequence: a sequence diverges if it is unbounded, or if it has two subsequences converging to different points. By Thm. \ref{lb120}, the second criterion can be generalized to nets. However, the following example shows that the first criterion does not has its net version:


%% Record #5 2023/10/7 two lectures  12



\begin{eg}
A convergent net $(x_\alpha)_{\alpha\in I}$ in a metric space $X$ is not necessarily \textbf{bounded}. Namely, it is not necessarily true that $\{x_\alpha:\alpha\in I\}$ is a bounded subset of $X$. Let $f:\Rbb_{>0}\rightarrow\Rbb$ be $f(x)=1/x$. Then $f$ is net in $\Rbb$ with directed set $(\Rbb_{> 0},\leq)$. This net is not bounded, although $\lim f(x)=0$.
\end{eg}





\begin{eg}
The double sequence $x_{m,n}=n/(m+n)$ in $\Rbb$ has subnets $x_{n,n}=n/(n+n)=1/2$ and $x_{2n,n}=1/3$. Since these two subnets converge to different values, Thm. \ref{lb120} implies that $\lim_{m,n}x_{m,n}$ does not exist. However, the \textbf{iterated limits}\index{00@Iterated limit} exist and take different values:
\begin{gather*}
\lim_{m\rightarrow\infty}\lim_{n\rightarrow\infty}\frac{n}{m+n}=1\qquad \lim_{n\rightarrow\infty}\lim_{m\rightarrow\infty}\frac{n}{m+n}=0
\end{gather*}
As we shall see, this gives another criterion for the divergence of double series: If the two iterated limits exist and are different, then the double series diverge.
\end{eg}


Finally, we do an example of convergent double sequence:

\begin{eg}\label{lb125}
Let $\dps x_{m,n}=(m^{-2}-n^{-1})\sin\frac{\pi(m+\sqrt n)}{4}$. Then $\dps\lim_{m,n\rightarrow\infty}x_{m,n}=0$.
\end{eg}

\begin{proof}
The sequence $(m^{-2})_{m\in\Zbb_+}$ converges to $0$. By Exp. \ref{lb124}, the double sequence $(m^{-2})_{m,n\in\Zbb_+}$ is its subnet, and hence converges to $0$ by Thm. \ref{lb120}. Similarly, the double sequence $(n^{-1})_{m,n\in\Zbb_+}$ converges to $0$. Therefore, $m^{-2}+n^{-1}$ converges to $0$ due to Thm. \ref{lb121} and the continuity of the addition map $(x,y)\in\Rbb^\mapsto x+y\in\Rbb$ (Prop. \ref{lb41}). Since $0\leq |x_{m,n}|\leq m^{-2}+n^{-1}$, we conclude $|x_{m,n}|\rightarrow 0$ (and hence $x_{m,n}\rightarrow0$) by squeeze theorem (Exe. \ref{lb123}).
\end{proof}







\subsubsection{Double limits and iterated limits}

\begin{thm}\label{lb122}
Let $(x_{\alpha,\beta})_{\alpha\in I,\beta\in J}$ be a double net in a metric space $X$. Assume that the following are true:
\begin{enumerate}[label=(\arabic*)]
\item The limit $\dps\lim_{(\alpha,\beta)\in I\times J}x_{\alpha,\beta}$ exists in $X$.
\item For each $\alpha\in I$, the limit $\dps\lim_{\beta\in J}x_{\alpha,\beta}$ exists in $X$.
\end{enumerate}
Then the LHS limit in the following equation exists and equals the RHS:
\begin{align}
\lim_{\alpha\in I}\lim_{\beta\in J}x_{\alpha,\beta}=\lim_{(\alpha,\beta)\in I\times J}x_{\alpha,\beta}
\end{align}
In particular, suppose that the following is also true:
\begin{itemize}
\item[(3)] For each $\beta\in J$, the limit $\dps\lim_{\alpha\in J}x_{\alpha,\beta}$ exists in $X$. 
\end{itemize}
Then the following limits exist and are equal:
\begin{align}
\lim_{\alpha\in I}\lim_{\beta\in J}x_{\alpha,\beta}=\lim_{\beta\in J}\lim_{\alpha\in I}x_{\alpha,\beta}
\end{align}
\end{thm}


\begin{proof}
Let $\dps x_\alpha=\lim_\beta x_{\alpha,\beta}$ and $\dps x=\lim_{\alpha,\beta}x_{\alpha,\beta}$. We want to show that $\dps\lim_\alpha x_\alpha=x$. Choose any $\eps>0$. Then there exist $A\in I,B\in J$ such that for every $\alpha\geq A$ and $\beta\geq B$ we have $d(x_{\alpha,\beta},x)<\eps/3$. In particular,  $d(x_{\alpha,\beta},x)\leq \eps/2$. Using Thm. \ref{lb121} and the fact that $p\in X\mapsto d(p,x)\in\Rbb$ is continuous (Exp. \ref{lb45}), we see that for every $\alpha\geq A$ we have $\dps d(x_\alpha,x)=\lim_{\beta\in J_{\geq B}} d(x_{\alpha,\beta},x)\leq \eps/2<\eps$.
\end{proof}

The readers may skip the next remark and proof and come back to them when they have learned about topological spaces.

\begin{srem}
Thm. \ref{lb122} can be generalized to the case that $X$ is a regular topological space. By saying that the topological space $X$ is \textbf{regular}, \index{00@Regular topological space} we mean that for every $x\in X$ and every open set $U$ containing $x$, there is a smaller open set $V$ containing $x$ such that the closure $\ovl V$  (cf. Def. \ref{lb183}) is contained in $U$.
\end{srem}

\begin{proof}[$\star$ Proof]
Let $\dps x_\alpha=\lim_\beta x_{\alpha,\beta}$ and $\dps x=\lim_{\alpha,\beta}x_{\alpha,\beta}$. Choose any open set $U$ containing $x$. We want to prove that $x_\alpha$ is eventually in $U$. Choose an open set $V$ containing $x$ such that $\ovl V\subset U$. Then there are $A\in I,B\in J$ such that for all $\alpha\geq A$ and $\beta\geq B$ we have $x_{\alpha,\beta}\in V$. Thus, for each $\alpha\geq A$, since $x_{\alpha,\beta}$ approaches $x_\alpha$, we have $x_\alpha\in \ovl V$ and hence $x_\alpha\in U$.
\end{proof}


\begin{co}
Let  $(x_{\alpha,\beta})_{\alpha\in I,\beta\in J}$ be a double net in $\ovl\Rbb$. Assume that $x_{\blt,\blt}$ is increasing, i.e., $x_{\alpha,\beta}\leq x_{\alpha',\beta'}$ if $\alpha\leq\alpha'$ and $\beta\leq \beta'$. Then the following equation \eqref{eq39} hold, where all the limits \eqref{eq39} exist in $\ovl\Rbb$:
\begin{align}
\lim_{\alpha\in I}\lim_{\beta\in J}x_{\alpha,\beta}=\lim_{\beta\in J}\lim_{\alpha\in I}x_{\alpha,\beta}=\lim_{(\alpha,\beta)\in I\times J}x_{\alpha,\beta}=\sup\{x_{\alpha,\beta}:\alpha\in I,\beta\in J\}  \label{eq39}
\end{align}
\end{co}

Clearly, a similar result holds for decreasing double nets in $\ovl\Rbb$.

\begin{proof}
By Exe. \ref{lb123}, the three limits $\dps\lim_\alpha x_{\alpha,\beta}$, $\dps\lim_\beta x_{\alpha,\beta}$, and $\dps\lim_{\alpha,\beta}x_{\alpha,\beta}$ exist in $\ovl\Rbb$. Therefore, by Thm. \ref{lb122}, the three limits in \eqref{eq39} exist and are equal. The last equality in \eqref{eq39} is also due to Exe. \ref{lb123}.
\end{proof}


\subsubsection{Cauchy nets}

\begin{df}
A net $(x_\alpha)_{\alpha\in I}$ in a metric space $X$ is called a Cauchy net \index{00@Cauchy net} if
\begin{align*}
\lim_{\alpha,\beta\in I}d(x_\alpha,x_\beta)=0
\end{align*}
Equivalently, this means that 
\begin{align}
\forall\eps>0~~~\exists \gamma\in I~~~\text{such that }\forall\alpha,\beta\geq\gamma~~~\text{we have }d(x_\alpha,x_\beta)<\eps
\end{align}
\end{df}

\begin{exe}
Show that the subnet of a Cauchy net is Cauchy.
\end{exe}


\begin{pp}\label{lb126}
A convergent net in a metric space is a Cauchy net.
\end{pp}

\begin{proof}
Let $(x_\alpha)_{\alpha\in I}$ converge to $x$ in a metric space $X$. Then $\lim_\alpha d(x_\alpha,x)=0$. Since $(d(x_\alpha,x))_{\alpha,\beta\in I}$ is a subnet (cf. Exp. \ref{lb124}), we have $\lim_{\alpha,\beta} d(x_\alpha,x)=0$ by Thm. \ref{lb120}. Similarly, we have $\lim_{\alpha,\beta}d(x,x_\beta)=0$. Since $0\leq d(x_\alpha,x_\beta)\leq d(x_\alpha,x)+d(x,x_\beta)$, by Squeeze theorem (Exe. \ref{lb123}) we have $\lim_{\alpha,\beta}d(x_\alpha,x_\beta)=0$.
\end{proof}

\begin{comment}
\begin{exe}
Prove Prop. \ref{lb126} directly using the definitions of convergent nets and Cauchy nets.
\end{exe}
\end{comment}


\begin{pp}\label{lb127}
Let $(x_\alpha)_{\alpha\in I}$ be a Cauchy net in a metric space $X$. Suppose that $(x_\alpha)_{\alpha\in I}$ has a convergent subnet $(x_{\alpha_s})_{s\in S}$ converging to $x\in X$. Then $(x_\alpha)_{\alpha\in I}$ converges to $x$.
\end{pp}

\begin{proof}
Choose any $\eps>0$. Since $(x_\alpha)$ is a Cauchy net, there exists $\gamma\in I$ such that $d(x_\alpha,x_\beta)\leq \eps$ for all $\alpha,\beta\geq \gamma$. Since $(\alpha_s)_{s\in S}$ has cofinal range, $\alpha_{s_0}\geq \gamma$ for some $s_0\in S$. Thus $\alpha_s\geq \gamma$ for all $s\geq s_0$ because $(\alpha_s)_{s\in S}$ is increasing and because of the transitivity in Def. \ref{lb116}. Thus, for every $\beta\geq\gamma$, $d(x_{\alpha_s},x_{\beta})\leq\eps$ for sufficiently large $s$. By taking limit over $s$ and using the continuity of $y\in X\mapsto d(y,x_\beta)$ as well as Thm. \ref{lb121}, we get $d(x,x_\beta)\leq \eps$ for all $\beta\geq\gamma$.
\end{proof}

\begin{df}\label{lb155}
Two nets $(x_\alpha)_{\alpha\in I}$ and $(y_\alpha)_{\alpha\in I}$ in a metric space $X$ are called \textbf{Cauchy-equivalent} \index{00@Cauchy-equivalent} if
\begin{align*}
\lim_{\alpha\in I}d(x_\alpha,y_\alpha)=0
\end{align*}
Two Cauchy nets are simply called \textbf{equivalent} if they are Cauchy-equivalent. It is not hard to see that Cauchy-equivalence is an equivalence relation (recall Def. \ref{lb156}) on $X^I$.
\end{df}


\begin{exe}\label{lb128}
Let $(x_\alpha)_{\alpha\in I}$ and $(y_\alpha)_{\alpha\in I}$ be nets in a metric space $X$. 
\begin{enumerate}
\item Assume that $(x_\alpha)_{\alpha\in I}$ and $(y_\alpha)_{\alpha\in I}$ are Cauchy-equivalent. Prove that $(x_\alpha)$ is a Cauchy net iff $(y_\alpha)$ is a Cauchy net.
\item Assume that $(x_\alpha)_{\alpha\in I}$ converges to $x$. Prove that $(y_\alpha)_{\alpha\in I}$ converges to $x$ iff $(x_\alpha)_{\alpha\in I}$ and $(y_\alpha)_{\alpha\in I}$ are Cauchy-equivalent.
\end{enumerate}
\end{exe}



\begin{thm}\label{lb129}
Every Cauchy net $(x_\alpha)_{\alpha\in I}$ in a complete metric space $X$ is convergent.
\end{thm}

We give a hint of the proof and leave the details to the readers as an exercise.

\begin{proof}[Hint]
Construct an increasing sequence $(\alpha_n)_{n\in\Zbb_+}$ in $I$ such that for every $\beta,\gamma\geq\alpha_n$ we have $d(x_\beta,x_\gamma)<1/n$. Prove that $(x_{\alpha_n})_{n\in\Zbb_+}$ is a Cauchy sequence, and hence converges to some $x\in X$. Prove that $(x_\alpha)_{\alpha\in I}$ converges to $x$. (Warning: $(x_{\alpha_n})_{n\in\Zbb_+}$ is not necessarily a subnet of $(x_\alpha)_{\alpha\in I}$.)
\end{proof}

\begin{comment}
\begin{proof}
Let $(x_\alpha)_{\alpha\in I}$ be a Cauchy net in a complete metric space $X$. Then $I\times \Nbb$ is a directed set. Define
\begin{align*}
S=\{(\alpha,n)\in I\times\Nbb:\forall \beta,\gamma\geq\alpha\text{ we have }d(x_\beta,x_\gamma)<1/n\}
\end{align*}
Then $(x_\alpha)_{(\alpha,n)\in S}$ is a subnet of $X$. Recall that every subnet of a Cauchy net is Cauchy. By Prop. \ref{lb127}, it suffices to show that the Cauchy net $(x_\alpha)_{(\alpha,n)\in S}$ converges.

For each $n\in\Zbb_+$, choose $\beta_n$ such that $(\beta_n,n)\in S$: the existence of $\beta_n$ is due to the Cauchyness of $(x_\alpha)_{\alpha\in I}$. Thus, if $(\alpha,n)\in S$, we have $d(x_\alpha,x_{\beta_n})<1/n$, and hence
\begin{align*}
\lim_{(\alpha,n)\in S}d(x_\alpha,x_{\beta_n})=0
\end{align*}
Therefore, by Exe. \ref{lb128}, it suffices to prove that the equivalent Cauchy net $(x_{\beta_n})_{(\alpha,n)\in S}$ is convergent. But this is a subnet of the sequence $(x_{\beta_n})_{n\in\Nbb}$. And clearly, the Cauchyness of $(x_{\beta_n})_{(\alpha,n)\in S}$ implies that of $(x_{\beta_n})_{n\in\Nbb}$. So the Cauchy sequence $(x_{\beta_n})_{n\in\Nbb}$ converges because $X$ is complete. So its subnet $(x_{\beta_n})_{(\alpha,n)\in S}$ converges.
\end{proof}
\end{comment}



\subsection{Discrete integrals $\sum_{x\in X}f(x)$}

In this section, we fix $V$ to be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$. We fix a (non-necessarily countable) set $X$. Note that if $f:X\rightarrow V$ is a function and $X$ is finite, then $\sum_{x\in X}f(x)$ can be understood in its most obvious way.



\begin{df}\label{lb131}
Let $f:X\rightarrow V$ be a map. The expression
\begin{align*}
\sum_{x\in X}f(x)
\end{align*}
(or simply $\sum_X f$) is called a \textbf{discrete integral}.\index{00@Discrete integral} If $v\in V$, we say that $\sum_{x\in X}f(x)$ equals (or \text{converges} to) $v$, if
\begin{align}
\lim_{A\in\fin(2^X)}\sum_{x\in A}f(x)=v
\end{align}
In this case, we write
\begin{align}
\sum_{x\in X}f(x)=v \label{eq40}
\end{align}
\end{df}

\begin{rem}
Recall from Exp. \ref{lb130} that $\fin(2^X)$ is the directed set of finite subsets of $2^X$. Its preorder is ``$\subset$". So \eqref{eq40} means more precisely that:
\begin{itemize}
\item For every $\eps>0$, there exists a finite set $B\subset X$ such that for every finite set $A$ satisfying $B\subset A\subset X$, we have $\Vert v-\sum_{x\in A}f(x)\Vert<\eps$.
\end{itemize}
\end{rem}


\begin{rem}
Discrete integrals are one of the most important and representative examples in Moore and Smith's original paper on nets (cf. \cite{MS22}), explaining why nets are called nets: Imagine an infinitely large \textit{fishing net} whose vertices form the set $X=\Zbb^2$. You grab the net with your hands and pull it up. As you pull it up, the lifted part $A\in\fin(2^X)$ becomes larger and larger.
\end{rem}


\begin{rem}\label{lb141}
One of the advantages of discrete integrals over series is that  discrete integrals are clearly invariant under rearrangement: For every bijection $\varphi:X\rightarrow X$, if one side of the following equation converges in $V$, then the other side converges, and the equation holds true:
\begin{align}
\sum_{x\in X}f(x)=\sum_{x\in X}f(\varphi(x))
\end{align} 
or simply $\sum_Xf=\sum_Xf\circ\varphi$.
\end{rem}


\begin{rem}\label{lb133}
Let us spell out what Cauchyness means for the net $(\sum_A f)_{A\in\fin(2^X)}$:
\begin{itemize}
\item[(1)] For every $\eps>0$, there exists a finite set $B\subset X$ such that for any finite sets $A_1,A_2$ satisfying $B\subset A_1\subset X,B\subset A_2\subset X$, we have
\begin{align*}
\Big\Vert \sum_{A_1\setminus A_2}f-\sum_{A_2\setminus A_1}f \Big\Vert<\eps
\end{align*} 
\end{itemize}
Note that the term inside the norm is $\sum_{A_1}f-\sum_{A_2}f$. This is also equivalent to:
\begin{itemize}
\item[(2)] For every $\eps>0$, there exists a finite set $B\subset X$ such that for any finite set $E\subset X\setminus B$, we have 
\begin{align*}
\Big\Vert \sum_Ef\Big\Vert<\eps
\end{align*}
\end{itemize}
We shall mainly use (2) as the Cauchy criterion for the convergence of $\sum_Xf$.
\end{rem}

\begin{proof}[Proof of the equivalence]
(2) follows from (1) by taking $A_1=B$ and $A_2=B\cup E$. (1) follows from (2) by taking $E_1=A_1\setminus A_2$ and $E_2=A_2\backslash A_1$ and then concluding $\Vert \sum_{E_1}f-\sum_{E_2}f\Vert<2\eps$.
\end{proof}





\begin{df}\label{lb132}
Let $g:X\rightarrow\ovl\Rbb_{\geq0}$ be a map. Note that the net $(\sum_A g)_{A\in\fin(2^X)}$ is increasing. Hence, its limit exists in $\ovl\Rbb$ and equals $\sup_{A\in\fin(2^X)}\sum_Ag$ (by Exe. \ref{lb123}). We write this as $\sum_Xg$, or more precisely:
\begin{align}
\sum_Xg\equiv\sum_{x\in X}g(x)\xlongequal{\mathrm{def}} \lim_{A\in \fin(2^X)}\sum_A g=\sup_{A\in \fin(2^X)}\sum_A g
\end{align}
We say that $\sum_Xg$ \textbf{converges} or \textbf{converges absolutely}, if $\sum_Xg<+\infty$. 
\end{df}

It is clear that $\sum_Xg<+\infty$ iff there exists $C\in\Rbb_{\geq0}$ such that $\sum_Ag<C$ for all $A\in\fin(2^X)$.


\begin{rem}
Note that when $g:X\rightarrow\Rbb_{\geq0}$, the convergence in Def. \ref{lb132} agrees with that in Def. \ref{lb131}. Therefore, Rem. \ref{lb133} still gives a Cauchy criterion for convergence.
\end{rem}






\begin{df}
Let $f:X\rightarrow V$. We say that $\sum_Xf$ \textbf{converges absolutely} \index{00@Absolutely convergent discrete integral} if 
\begin{align*}
\sum_{x\in X}\Vert f(x)\Vert<+\infty
\end{align*}
\end{df}



\begin{pp}\label{lb142}
Let $f:X\rightarrow V$. If $\sum_Xf$ converges absolutely, then it converges, and
\begin{align}
\Big\Vert \sum_{x\in X} f(x) \Big\Vert\leq\sum_{x\in X} \Vert f(x)\Vert \label{eq41}
\end{align}
We write this simply as $\Vert\sum_X f\Vert\leq\sum_X |f|$. (Recall Def. \ref{lb150}.)
\end{pp}

\begin{proof}
\eqref{eq41} clearly holds when $X$ is finite. In the general case, assume that $\sum_Xf $ converges absolutely. Then by Cauchy criterion (Rem. \ref{lb133}-(2)), for every $\eps>0$ there is $A\in \fin(2^X)$ such that for each finite $E\subset X\setminus A$ we have $\sum_E|f|<\eps$, and hence $\Vert \sum_E f\Vert<\eps$. Therefore $\sum_Xf$ converges by Cauchy criterion again.

By the continuity of the norm function $v\in V\mapsto \Vert v\Vert\in\Rbb_{\geq0}$, and by Thm. \ref{lb121}, we have
\begin{align*}
\Big\Vert \sum_X f \Big\Vert=\Big\Vert \lim_A \sum_Af \Big\Vert=\lim_A \Big\Vert \sum_Af \Big\Vert
\end{align*}
Since $\Vert \sum_A f\Vert\leq\sum_A|f|$, by Exe. \ref{lb123}, the above expression is no less than
\begin{align*}
\lim_A\sum_A|f|=\sum_X|f|
\end{align*}
\end{proof}

The following proposition gives another demonstration that discrete integrals are more natural than series. We leave the proof to the readers.

\begin{pp}\label{lb134}
Let $f:X\rightarrow\Rbb^N$ where $N\in\Zbb_+$. Then
\begin{align*}
\sum_{x\in X}f(x)~~\textrm{converges}\qquad\Longleftrightarrow\qquad \sum_{x\in X}f(x)~~\text{converges absolutely}
\end{align*}

\end{pp}

\begin{proof}[Hint]
Reduce to the case $N=1$. Consider $A=\{x\in X:f(x)\geq 0\}$ and $B=X\setminus A$. 
\end{proof}

When $\Rbb^N$ is replaced by an infinite-dimensional Banach space, the convergence of a discrete integral may not imply absolute convergence. See Pb. \ref{lb149}.







\subsection{Fubini's theorem for discrete integrals}\index{00@Fubini's theorem for discrete integrals}\label{lb138}

Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Let $X,Y$ be sets.

\begin{thm}[\textbf{Fubini's theorem-A}]\label{lb135} 
Let $f:X\times Y\rightarrow V$. Assume that $\sum_{X\times Y}f$ converges. Then $\sum_Y f(x,\cdot)$ converges for each $x\in X$, and $\sum_X f(\cdot,y)$ converges for each $y\in Y$, and 
\begin{align}\label{eq45}
\sum_{x\in X}\sum_{y\in Y}f(x,y)=\sum_{y\in Y}\sum_{x\in X}f(x,y)=\sum_{(x,y)\in X\times Y}f(x,y)
\end{align}
where all discrete integrals converge in $V$.
\end{thm}

We abbreviate \eqref{eq45} to $\sum_X\sum_Yf=\sum_Y\sum_Xf=\sum_{X\times Y}f$.

\begin{proof}
For each $x\in X$, let $f_x(y)=f(x,y)$. Let us prove that $\sum_Y f_x$ converges.
Choose any $\eps>0$. Since $\sum_{X\times Y}f$ converges, by Cauchy criterion (Rem. \ref{lb133}-(2)), there exists a finite $S\subset X\times Y$ such that the sum of $f$ over any finite subset outside $S$ has norm $<\eps$. The projection $X\times Y\rightarrow Y$ maps $S$ to a finite set $B\subset Y$. Thus, for each finite $E\subset Y\setminus B$, we have $\Vert\sum_E f_x\Vert<\eps$ since $x\times E$ is outside $S$. Therefore $\sum_Y f_x$ converges. By the same reasoning, $\sum_Xf(\cdot,y)$ converges for all $y$.

Recall that $\sum_{X\times Y}f$ is the limit of the net $(\sum_S f)_{S\in\fin(2^{X\times Y})}$. This net has subnet
\begin{align*}
\Big(\sum_{(x,y)\in A\times B} f(x,y)\Big)_{A\in \mc I,B\in \mc J}\qquad \text{ where }\mc I=\fin(2^X)~~\mc J=\fin(2^Y)
\end{align*}
(Its index set is $\mc I\times\mc J$.) Thus, by Thm. \ref{lb120},
\begin{align}
\sum_{(x,y)\in X\times Y}f(x,y)=\lim_{A\in\mc I,B\in\mc J} \sum_{(x,y)\in A\times B} f(x,y) \label{eq43}
\end{align}
We are now going to use Thm. \ref{lb122} to show that
\begin{align}
\lim_{A\in\mc I,B\in\mc J} \sum_{(x,y)\in A\times B} f(x,y)=\lim_{A\in\mc I}\lim_{B\in\mc J} \sum_{(x,y)\in A\times B} f(x,y) \label{eq42}
\end{align}
where the RHS limit exists. For that purpose, we need to check for each $A\in\mc I$ the convergence of $\lim_B\sum_{(x,y)\in A\times B}f(x,y)$. Since $\sum_Yf_x$ converges, we have
\begin{align}
&\lim_{B\in\mc J}\sum_{(x,y)\in A\times B}f(x,y)=\lim_{B\in\mc J}\sum_{x\in A}\sum_{y\in B}f(x,y)\nonumber\\
=&\sum_{x\in A}\lim_{B\in\mc J}\sum_{y\in B}f(x,y)=\sum_{x\in A}\sum_{y\in Y}f(x,y)\label{eq44}
\end{align}
(Note that $\sum_A$ is a finite sum and hence commutes with $\lim_B$.) Thus, the assumption in Thm. \ref{lb122} ensuring \eqref{eq42} has now been proved true. So \eqref{eq42} is true. Moreover, combining \eqref{eq43}, \eqref{eq42}, \eqref{eq44}  together, we get
\begin{align*}
\sum_{(x,y)\in X\times Y}f(x,y)=\lim_{A\in\mc I}\sum_{x\in A}\sum_{y\in Y}f(x,y)=\sum_{x\in X}\sum_{y\in Y}f(x,y)
\end{align*}
where the second and the third limits exist. This proves a half of \eqref{eq45}. The other half can be proved in the same way.
\end{proof}








\begin{thm}[\textbf{Fubini's theorem-B}]\label{lb137}
Let $g:X\times Y\rightarrow\ovl\Rbb_{\geq0}$. Then the five discrete integrals in \eqref{eq46} exist in $\ovl\Rbb_{\geq0}$, and equations \eqref{eq46} hold in $\ovl\Rbb_{\geq0}$:
\begin{align}\label{eq46}
\sum_{x\in X}\sum_{y\in Y}f(x,y)=\sum_{y\in Y}\sum_{x\in X}f(x,y)=\sum_{(x,y)\in X\times Y}f(x,y)
\end{align}
\end{thm}

\begin{proof}
The existence in $\ovl\Rbb_{\geq0}$ of the five discrete integrals is clear. (Recall Def. \ref{lb132}.) Formula \eqref{eq46} can be proved in the same way as \eqref{eq45}. Note that when applying Thm. \ref{lb122} to prove \eqref{eq46}, the assumption in Thm. \ref{lb122} on the existence of limits is satisfied because all nets involved are increasing in $\ovl\Rbb$. (Recall Exe. \ref{lb123}.)
\end{proof}

\begin{co}[\textbf{Fubini's theorem-C}]
Let $f:X\times Y\rightarrow V$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\sum_{X\times Y}f$ converges absolutely.
\item $\sum_{x\in X}\sum_{y\in Y}\Vert f(x,y)\Vert<+\infty$.
\item $\sum_{y\in Y}\sum_{x\in X} \Vert f(x,y)\Vert<+\infty$.
\end{enumerate}
\end{co}

\begin{proof}
Immediate from Thm. \ref{lb137}. It is also not hard to prove it directly.
\end{proof}

%% Record #6 2023/10/9 two lectures  14



\subsection{Parametrization theorem for discrete integrals}

We fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. In the following sections, we shall apply the results about discrete integrals to the study of series and double series. For the convenience of applications (e.g. the proof of  $e^ze^w=e^{z+w}$), we enlarge the concept of series a little:

\subsubsection{Series over $\Zbb$}\label{lb151}


\begin{df}
A \textbf{series over $\Zbb$} \index{00@Series over $\Zbb$} is an expression
\begin{align*}
\sum_{n=-\infty}^{+\infty} f(n)
\end{align*}
where $f$ is a function from $\Zbb$ to either $V$ or $\ovl\Rbb_{\geq0}$. We say that this series \textbf{converges to}  (or equals) $\mu\in V$ resp.  equals $\mu\in\ovl\Rbb_{\geq0}$, if
\begin{align}
\lim_{m,n\rightarrow+\infty} \sum_{i=-m}^n f(i)=\mu \label{eq47}
\end{align}
In this case, we write
\begin{align*}
\sum_{n=-\infty}^{+\infty} f(n)=\mu
\end{align*}
We say that $\sum_{n=-\infty}^{+\infty}f(n)$ \textbf{converges absolutely} \index{00@Absolute convergent series over $\Zbb$} if $\sum_{n=-\infty}^{+\infty}\Vert f(n)\Vert<+\infty$.
\end{df}

\begin{rem}
Note that in the case of $\ovl\Rbb_{\geq0}$, the limit on the LHS of \eqref{eq47} must exist in $\ovl\Rbb_{\geq0}$. Again, this is due to the fact that the involved net is increasing, and so one can use Exe. \ref{lb123}.
\end{rem}



\begin{exe}\label{lb152}
Let $\sum_{n=-\infty}^{+\infty} f(n)$ be a series in either $V$ or $\ovl\Rbb_{\geq0}$.
\begin{enumerate}
\item Fix $k\in\Zbb$. Prove that $\sum_{n=-\infty}^{+\infty} f(n)$ converges iff the following limits converge:
\begin{subequations}
\begin{gather}
\sum_{n=k}^{+\infty} f(n)=\lim_{n\rightarrow+\infty} \sum_{i=k}^nf(i)\\
\sum_{n=-\infty}^{k-1} f(n)=\lim_{m\rightarrow+\infty} \sum_{i=-m}^{k-1}f(i)
\end{gather}
\end{subequations}
Moreover, if these limits converge, then
\begin{align}
\sum_{n=-\infty}^{+\infty}f(n)=\sum_{n=k}^{+\infty} f(n)+\sum_{n=-\infty}^{k-1} f(n)
\end{align}
\item In the case that $f$ has codomain $V$, prove that $\sum_{n=-\infty}^{+\infty} f(n)$ converges if it converges absolutely.
\item Prove that if $f$ is zero outside $\Zbb_+$, then
\begin{align}
\sum_{n=-\infty}^{+\infty}f(n)=\sum_{n=1}^{+\infty}f(n)  \label{eq48}
\end{align}
\end{enumerate}
\end{exe}

Thus, by \eqref{eq48}, our following results about series over $\Zbb$ can be directly applied to series over $\Zbb_+$ (or over $\Zbb_{\geq k}$ where $k\in\Zbb$).



\subsubsection{Parametrization theorem}

The following theorem relates series and discrete integrals. The structure of this theorem is similar to that of Fubini's theorem-A,B,C in Sec. \ref{lb138}.

\begin{thm}[\textbf{Parametrization theorem}]\label{lb136}
Let $X$ be an infinite countable set. Let $\varphi:\Zbb\rightarrow X$ be a bijection (called a \textbf{parametrization} of $X$). \index{00@Parametrization (in discrete integrals)} The following are true.
\begin{enumerate}
\item Let $f:X\rightarrow V$. If the RHS of \eqref{eq49} converges in $V$, then the LHS converges, and \eqref{eq49} holds:
\begin{align}
\sum_{n=-\infty}^{+\infty}f\circ\varphi(n)=\sum_{x\in X}f(x)\label{eq49}
\end{align}
\item Let $f:X\rightarrow\ovl\Rbb_{\geq0}$. Then \eqref{eq49} holds in $\ovl\Rbb_{\geq0}$.
\item Let $f:X\rightarrow V$. Then the discrete integral $\dps\sum_{x\in X} f(x)$ converges absolutely iff the series $\dps\sum_{n=-\infty}^{+\infty} f\circ\varphi(n)$ converges absolutely.
\end{enumerate}
The same conclusions hold if we assume that $\varphi:\Zbb_+\rightarrow X$ is a bijection.
\end{thm}



\begin{proof}
We prove the case $\varphi:\Zbb\rightarrow X$; the other case is similar. 

Assume that $\sum_Xf$ converges, which means that the limit of the net $(\sum_Af)_{A\in\fin(2^X)}$ converges to some $v\in V$. Therefore, by Thm. \ref{lb120}, the subnet
\begin{align*}
\Big(\sum_{x\in A_{m,n}}f(x)\Big)_{m,n\in\Zbb_+}=\Big(\sum_{i=-m}^n f\circ\varphi(i)\Big)_{m,n\in\Zbb_+}
\end{align*}
converges to $v$, where $A_{m,n}=\{\varphi(i):i\in\Zbb,-m\leq i\leq n\}$. This proves part 1. The same method proves part 2. Part 3 follows directly from part 2.
\end{proof}

\subsection{Application to (double) series and power series; $e^ze^w=e^{z+w}$}

Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$.

\subsubsection{General results about series and double series}

\begin{co}\label{lb140}
Let $f:\Zbb\rightarrow V$, and let $\psi:\Zbb\rightarrow\Zbb$ be a bijection. Suppose that
\begin{align}
\sum_{n=-\infty}^{+\infty}\Vert f(n)\Vert<+\infty \label{eq54}
\end{align}
Then \eqref{eq50} holds true, where the RHS of \eqref{eq50} converges absolutely:
\begin{align}
\sum_{n=-\infty}^{+\infty} f(n)=\sum_{n=-\infty}^{+\infty}f\circ\psi(n) \label{eq50}
\end{align}
\end{co}
The same conclusion clearly holds if $\Zbb$ is replaced by $\Zbb_+$.

\begin{proof}
By \eqref{eq54} and Thm. \ref{lb136}-3, the discrete integral $\sum_\Zbb f$ converges absolutely, and hence converges. By Thm. \ref{lb136}-1, the LHS resp. RHS of \eqref{eq50} converges to the value of $\sum_\Zbb f$ if we choose the parametrization to be $\id_\Zbb$ resp. $\psi$. This proves \eqref{eq50} and the convergence of the RHS of \eqref{eq50}. Applying the same conclusion to $\Vert f(\cdot)\Vert$ proves the absolute convergence of the RHS of \eqref{eq50}.
\end{proof}


\begin{co}\label{lb139}
Let $f:\Zbb^2\rightarrow V$. Let $\Phi:\Zbb^2\rightarrow\Zbb^2$ be a bijection. Suppose that
\begin{align}\label{eq51}
\sum_{m=-\infty}^{+\infty}\sum_{n=-\infty}^{+\infty}\Vert f(m,n)\Vert<+\infty
\end{align}
Then \eqref{eq52} holds true, where the six series involved in \eqref{eq52} converge absolutely.
\begin{align}\label{eq52}
\sum_{m=-\infty}^{+\infty}\sum_{n=-\infty}^{+\infty} f(m,n)=\sum_{n=-\infty}^{+\infty}\sum_{m=-\infty}^{+\infty}f(m,n)=\sum_{k=-\infty}^{+\infty}\sum_{l=-\infty}^{+\infty} f\circ\Phi(k,l)
\end{align}
\end{co}

Similar results clearly hold if $\Zbb^2$ is replaced by $\Zbb_+^2$: One extends the domain of $f$ and the domain and codomain of $\Phi$ from $\Zbb_+^2$ to $\Zbb^2$. Then one apply Cor. \ref{lb139}. 

Also, note that the second term of \eqref{eq52} is redundant: it follows from the equality of the first and the third terms of \eqref{eq52} if we choose $\Phi(k,l)=(l,k)$.


\begin{proof}
By Thm. \ref{lb136}-2 and Thm. \ref{lb137}, we have
\begin{align}
\sum_{m=-\infty}^{+\infty}\sum_{n=-\infty}^{+\infty}\Vert f(m,n)\Vert=\sum_{x\in\Zbb}\sum_{y\in \Zbb}\Vert f(x,y)\Vert=\sum_{(x,y)\in\Zbb^2}\Vert f(x,y)\Vert\label{eq53}
\end{align}
where all the limits exist in $\ovl\Rbb_{\geq0}$. Therefore, by \eqref{eq51}, the discrete integral $\sum_{\Zbb^2}f$ is absolutely convergent and hence convergent. 

Similar to the argument for \eqref{eq53}, Thm. \ref{lb136}-1 and Thm. \ref{lb135} imply that the first two terms of \eqref{eq52}  exist and are both equal to the discrete integral $\sum_{\Zbb^2}f$. Since $\sum_{\Zbb^2}f=\sum_{\Zbb^2}f\circ\Phi$ (recall Rem. \ref{lb141}), by Thm. \ref{lb136}-1 and Thm. \ref{lb135} again, the last term of \eqref{eq52} converges to $\sum_{\Zbb^2}f\circ\Phi$.

We have proved that the six series in \eqref{eq52} converge, and \eqref{eq52} holds. Replacing $f(\cdot,\cdot)$ with $\Vert f(\cdot,\cdot)\Vert$ and applying a similar argument, we see that the six series in \eqref{eq52} converge absolutely. %(Note that one needs Prop. \ref{lb142} to show that $\sum\Vert \sum(\cdots)\Vert\leq\sum\sum\Vert(\cdots)\Vert<+\infty$.)
\end{proof}


\begin{rem}
Using the same method as in the above proof, one can easily prove a more general version of Cor. \ref{lb139}: Let $N\in\Zbb_+$. Let $f:\Zbb^2\rightarrow V$ such that \eqref{eq51} holds true. Let $\Psi:\Zbb^N\rightarrow\Zbb^2$ be a bijection. Then the $N$ series involved in the expression of \eqref{eq58} (from innermost to outermost) converge absolutely:
\begin{align}\label{eq58}
\sum_{n_1=-\infty}^{+\infty}\cdots \sum_{n_N=-\infty}^{+\infty}f\circ\Psi(n_1,\dots,n_N)
\end{align}
Moverover, the outermost series of \eqref{eq58} converges to \eqref{eq52}. And of course, a similar result holds if $\Zbb^2$ is replaced by $\Zbb^M$ for every $M\in\Zbb_+$. We leave it to the readers to fill in the details.
\end{rem}



%We remark that \eqref{eq52} is the most important reason we introduced series over $\Zbb$: If we restrict to series over $\Zbb_+$, the proof of \eqref{eq52} will involve more technical discussions.

\begin{co}\label{lb153}
Assume that
\begin{align*}
A=\sum_{n=-\infty}^{+\infty} a_n\qquad B=\sum_{n=-\infty}^{+\infty}b_n
\end{align*}
are absolutely convergent series in $\Cbb$. Then for each $k\in\Zbb$, the series
\begin{align*}
c_k=\sum_{l=-\infty}^{+\infty}a_{k-l}b_l
\end{align*}
converges absolutely. Moreover, the LHS of the \eqref{eq59} converges absolutely to the RHS:
\begin{align}\label{eq59}
\sum_{k=-\infty}^{+\infty}c_k=AB
\end{align}
\end{co}


\begin{proof}
Apply Cor. \ref{lb139} to the case that $f(m,n)=a_mb_n$ and $\Phi(k,l)=(k-l,l)$.
\end{proof}


\subsubsection{Application to power series}


\begin{co}\label{lb362}
Let $\dps f(z)=\sum_{n=0}^{+\infty}a_nz^n$ and $\dps g(z)=\sum_{n=0}^{+\infty}b_nz^n$ be power series in $\Cbb$ with radii of convergence $R_1,R_2$ respectively. Let $R=\min\{R_1,R_2\}$. For each $k\in\Zbb_+$, let
\begin{align*}
c_k=\sum_{l=0}^k a_{k-l}b_l
\end{align*}
Then the power series $\dps h(z)=\sum_{k=0}^{+\infty}c_kz^k$ has radius of convergence $\geq R$. Moreover, for each $z\in\Cbb$ satisfying $0\leq |z|<R$, we have
\begin{align*}
h(z)=f(z)\cdot g(z)
\end{align*}
\end{co}

\begin{proof}
For each $0\leq |z|<R$, apply Cor. \ref{lb153} by replacing the $a_n,b_n,c_k$ of Cor. \ref{lb153} with $a_nz^n,b_nz^n,c_kz^k$. This shows that $h(z)$ converges absolutely to $f(z)\cdot g(z)$. Since this is true for all $|z|<R$, $h(z)$ must have radius of convergence at least $R$ by Rem. \ref{lb108}.
\end{proof}

The above result also holds more generally for Laurent series. See Exe. \ref{lb154}.

\begin{co}\label{lb215}
For each $z,w\in\Cbb$ we have 
\begin{align*}
e^ze^w=e^{z+w}
\end{align*}
\end{co}

\begin{proof}
Apply Cor. \ref{lb153} to the case $a_n=z^n/n!$ and $b_n=w^n/n!$. (We set $a_n=b_n=0$ if $n<0$.) Then
\begin{align*}
c_k=\sum_{l=0}^{k} \frac{z^{k-l}}{(k-l)!}\cdot\frac{w^l}{l!}=\sum_{l=0}^k {k\choose l}\frac{z^{k-l}w^l}{k!}=\frac{(z+w)^k}{k!}
\end{align*}
by \eqref{eq60}.
\end{proof}




\subsection{Summary}


The following are some fundamental questions about series and double series:
\begin{itemize}
\item[(a)] Are they invariant under rearrangement? (Cf. \eqref{eq50}.) 
\item[(b)] Does the value of an interated double series remain unchanged if the order of the two infinite sums is changed? (Cf. the first equality in \eqref{eq52}.)
\item[(c)] A mixture of the above two questions. (Cf. the last term of \eqref{eq52}.)
\end{itemize}
We address these questions by relating them to discrete integral, a version of infinite sums which is parametrization-independent. The following are some key features of this theory.
\begin{enumerate}
\item (General principle)  A discrete integral is to a series (defined by parametrization) as a net to a subnet.
\item (Net $\Rightarrow$ Subnet) All subnets of a convergent net converge to the same value: the limit of the original net.
\item (Discrete integral $\Rightarrow$ Series) Therefore, different series converge to the same value if they are different parametrizations of the same convergent discrete integral.
\item (Discrete integrals $\Rightarrow$ Series) Fubini-type theorems (any theorems about exchanging the orders of iterated sums/integrals) hold for convergent double discrete integrals. Therefore, they hold when passing to subnets, in particular, when passing to double series.
\item (Subnet $\Rightarrow$ Net) Every increasing net in $\ovl\Rbb_{\geq0}$  has a limit in $\ovl\Rbb_{\geq0}$. Therefore, if an increasing net in $\Rbb_{\geq0}$ has a subnet converging to a number $<+\infty$, then the original net converges in $\Rbb_{\geq 0}$ (to a finite number).
\item (General principle) The discrete integral $\sum_{x\in X}\Vert f(x)\Vert$ is defined by the limit of an increasing net in $\ovl\Rbb_{\geq0}$.
\item (Series $\Rightarrow$ Discrete integral) Therefore, if any series or double series corresponds in a reasonable way to a discrete integral, then the absolute convergence of this (double) series (more specifically: \eqref{eq54} or \eqref{eq51}) implies the absolute convergence (and hence convergence) of the original discrete integral. This implies the absolute convergence of any other (double) series arising from that discrete integral.
\item (Conclusion) Thus, when a (double) series converges absolutely (in the form of \eqref{eq54} or \eqref{eq51}), the three problems (a), (b), (c) have satisfying answers. The reason absolutely convergent (double) series are so good is because increasing nets in $\ovl\Rbb_{\geq0}$ are very good!
\item (Counterexamples) Non-absolutely convergent series in $\Rbb$ have rearrangements converging to different values. This is because non-convergent nets may have two subnets converging to different values, cf. Pb. \ref{lb204}. (Recall from Prop. \ref{lb134} that for discrete integrals in $\Rbb$, absolute convergence is equivalent to convergence.)
\end{enumerate}











\subsection{Problems and supplementary material}


Let $X$ be a set, and let $V$ be a Banach space over $\Rbb\in\{\Rbb,\Cbb\}$.





\begin{comment}
\begin{prob}
Prove Thm. \ref{lb129}, namely, prove that every Cauchy net $(x_\alpha)_{\alpha\in I}$ in a complete metric space $X$ is convergent.
\end{prob}


\begin{sprob}\label{lb266}
Let $f:X\rightarrow\Rbb$ be a function. Recall that $\fin(2^X)$ is the set of finite subsets of $X$. For each $A\in\fin(2^X)$, define
\begin{align*}
s_A=\sum_{x\in A}f(x)
\end{align*}
We say that $s\in\Rbb$ is a cluster point of the net $(s_A)_{A\in\fin(2^X)}$ if the following condition holds: 
\begin{itemize}
\item[($\varstar$)] For every $\eps>0$, $s_A$ is frequently in $B_\Rbb(s,\eps)$. (Namely, for every $\eps>0$ and $A\in\fin(2^X)$ there exists $B\in\fin(2^X)$ such that $A\subset B$ and $|s-s_B|<\eps$.) 
\end{itemize}
Answer the following questions.
\begin{enumerate}
\item Prove that if there is a bijection $\varphi:\Zbb_+\rightarrow X$ such that the series $\sum_{n=1}^\infty f\circ\varphi(n)$ converges to $s$, then $s$ is a cluster point of the net $(s_A)_{A\in\fin(2^X)}$.
\item Suppose that $s$ is a cluster point of $(s_A)_{A\in\fin(2^X)}$. Define $(I,\leq)$ to be
\begin{gather*}
I=\big\{(A,\eps)\in\fin(2^X)\times\Rbb_{>0}:|s-s_A|<\eps \big\}\\[0.5ex]
(A,\eps)\leq (A',\eps')\qquad\Longleftrightarrow\qquad A\subset A'\text{ and }\eps\geq\eps'
\end{gather*}
For each $\alpha=(A,\eps)\in I$, let $s_\alpha=s_A$. Prove that $I$ is a directed set. Prove that $(s_\alpha)_{\alpha\in I}$ is a subnet of $(s_A)_{A\in\fin(2^X)}$ if the increasing map $I\rightarrow \fin(2^X)$ is defined to be $(A,\eps)\mapsto A$. Prove that $(s_\alpha)_{\alpha\in I}$ converges to $s$.
\end{enumerate}
\end{sprob}
\end{comment}


\begin{prob}
Compute $\lim_{p,q\rightarrow+\infty}a_{p,q}$ where $(a_{p,q})_{p,q\in\Zbb_+}$ are given below. Or explain why the limit does not exist.
\begin{gather*}
a_{p,q}=\frac{(-1)^p\cdot p}{p+q} \qquad 
a_{p,q}=\frac{(-1)^p}{p}\qquad
a_{p,q}=\frac{\cos(p\pi/4)}{p+q}
\end{gather*}
\end{prob}


\begin{prob}
Prove Thm. \ref{lb129}. (Every Cauchy net in a complete metric space converges.)
\end{prob}



\begin{prob}\label{lb413}
Let $f:X\rightarrow V$. Define the \textbf{support} \index{00@Support $\Supp(f)$} \index{Supp@$\Supp(f)$} of $f$ to be
\begin{align}
\Supp(f)=\{x\in X:f(x)\neq 0\}
\end{align}
Prove that if $\sum_Xf$ converges absolutely, then $\Supp(f)$ is a countable set.
\end{prob}

\begin{proof}[Hint]
Consider $\{x\in X:|f(x)|\geq\eps\}$ where $\eps>0$.
\end{proof}




\begin{prob}
Prove Prop. \ref{lb134}. 
\end{prob}


\begin{sprob}\label{lb204}
Prove \textbf{Riemann rearrangement theorem}, which says the following: Let $\sum_{n=1}^{+\infty} x_n$ be a series in $\Rbb$ which converges and which does not converge absolutely. Choose any  $A\in\ovl\Rbb$. Then $\sum_{n=1}^{+\infty} x_n$ has a rearrangement converging to $A$ (i.e., there is bijection $\varphi:\Zbb_+\rightarrow\Zbb_+$ such that $\sum_{n=1}^{+\infty} f\circ\varphi(n)=A$).
\end{sprob}


\begin{rem}
By Riemann rearrangement theorem, it is clear that every convergent series in $\Rbb^N$ which is not absolutely convergent must have two  rearrangements converging to two different points. However, when $\Rbb^N$ is replaced by an infinite dimensional Banach space, one may find a series $\sum_{n=1}^{+\infty} v_n$ which does not converge absolutely but converge to some $v$, and every rearrangement of $\sum_{n=1}^{+\infty} v_n$ converges to $v$. See Pb. \ref{lb149}.
\end{rem}


\begin{prob}\label{lb149}
Consider the case that $V$ is the real Banach space $V=l^\infty(\Zbb_+,\Rbb)$. For each $n\in\Zbb_+$, let $e_n\in V$ be the characteristic function $\chi_{\{n\}}$. Namey, $e_n$ takes value $1$ at $n$, and takes $0$ at the other points. Prove that the discrete integral
\begin{align}
\sum_{n\in\Zbb_+}\frac 1{n}e_n \label{eq57}
\end{align}
converges in $V$, and find the limit. Prove that \eqref{eq57} does not converge absolutely.
\end{prob}


\begin{rem}
A more important example that will be considered later is $V=l^2(\Zbb_+,\Cbb)$, the set of all functions $f:\Zbb_+\rightarrow\Cbb$ satisfying that the \pmb{$l^2$}\textbf{-norm} $\Vert f\Vert_{l^2}=\sqrt{\sum_{n\in\Zbb_+} |f(n)|^2}$ is finite. Then $V$ is in fact a Banach space. (Actually, it is a so-called \textbf{Hilbert space}.) Again, let $e_n=\chi_{\{n\}}$. (These $e_n$ will be called an \textbf{orthonormal basis} of $V$.) Then for each $f\in V$, the discrete integral $\sum_{n\in\Zbb_+}f(n)\cdot e_n$ converges to $f$. But it does not converge absolutely if $\sum_{n\in\Zbb_+}|f(n)|=+\infty$. Take for example $f(n)=n^{-1}$. We will study these objects in the second semester.
\end{rem}


\begin{prob}
Define $(x_{j,k})_{(j,k)\in\Zbb_+\times\Zbb_+}$ to be
\begin{align*}
x_{j,k}=\left\{
\begin{array}{ll}
\dps\frac 1{j^2-k^2}
 & \text{if }j\neq k\\[2ex]
0&\text{if }j=k
\end{array}\right.
\end{align*}
Prove that the discrete integral $\dps\sum_{(j,k)\in\Zbb_+^2}x_{j,k}$ does not converge in $\Rbb$.
\end{prob}

\begin{proof}[Hint]
Consider $(x_{j,k})$ as a net over $\Zbb^2$. Find a good bijection $\Phi:\Zbb^2\rightarrow\Zbb^2$.
\end{proof}




\begin{df}
For each $f\in V^X$, define the \pmb{$l^1$}\textbf{-norm} \index{l1@$\Vert\cdot\Vert_{l^1}=\Vert\cdot\Vert_{1}$}
\begin{align*}
\Vert f\Vert_{l^1(X,V)}\equiv\Vert f\Vert_{l^1}\equiv\Vert f\Vert_1=\sum_{x\in X}\Vert f(x)\Vert
\end{align*}
Define the $l^1$-space \index{l1XV@$l^1(X,V)$}
\begin{gather*}
l^1(X,V)=\{f\in V^X:\Vert f\Vert_{l^1}<+\infty \}
\end{gather*}
Namely, $l^1(X,V)$ is the set of all $f\in V^X$ where $\sum_Xf$ converges absolutely. In particular, $\sum_Xf$ converges for such $f$. 
\end{df}


\begin{exe}
Prove that for each $f,g\in V^X$ and $\lambda\in\Fbb$, we have
\begin{gather}
\Vert f+g\Vert_1\leq \Vert f\Vert_1+\Vert g\Vert_1\qquad \Vert\lambda f\Vert_1=|\lambda|\cdot\Vert f\Vert_1
\end{gather}
Show that $l^1(X,V)$ is a linear subspace of $l^\infty(X,V)$, and that $\Vert\cdot\Vert_{l^1}$ is a norm on $l^1(X,V)$
\end{exe}

\begin{prob}
Prove that $l^1(X,V)$ is a Banach space. Namely, prove that the metric on $l^1(X,V)$ defined by the $l^1$-norm is complete.
\end{prob}


\begin{prob}
Prove the \textbf{dominated convergence theorem} for discrete integrals: Let $(f_\alpha)_{\alpha\in I}$ be a net in $V^X$ satisfying the following conditions:
\begin{enumerate}[label=(\arabic*)]
\item There exists $g\in l^1(X,\Rbb)$ satisfying $g\geq0$ (i.e. $g(x)\geq0$ for all $x\in X$) such that for every $\alpha\in I,x\in X$ we have
\begin{align*}
\Vert f_\alpha(x)\Vert\leq g(x)
\end{align*}
We simply write the above condition as $|f_\alpha|\leq g$.
\item $(f_\alpha)_{\alpha\in I}$ converges pointwise some $f\in V^X$. Namely, $\lim_\alpha f_\alpha(x)=f(x)$ for every $x\in X$.
\end{enumerate}
Prove that $f\in l^1(X,V)$. Prove that the LHS of \eqref{eq56} exists and equals the RHS: 
\begin{align}\label{eq56}
\lim_{\alpha\in I}\sum_{x\in X}f_\alpha(x)=\sum_{x\in X}f(x)
\end{align}
\end{prob}


\begin{sprob}
Assume $v_n\in V$ for each $n$. Let $z$ be a complex variable. Then the expression
\begin{align*}
f(z)=\sum_{n=-\infty}^{+\infty}v_nz^n
\end{align*}
is called a \textbf{Laurent series} \index{00@Laurent series} in $V$.

Prove that there exist unique $r,R\in\ovl\Rbb_{\geq0}$ such that $f(z)$ converges absolutely when $|r|<z<|R|$, and that $f(z)$ diverges when $|z|<r$ or $|z|>R$. Prove that
\begin{align}
r=\limsup_{n\rightarrow+\infty} \sqrt[n]{\Vert v_{-n}\Vert}\qquad R=\frac{1}{\dps\limsup_{n\rightarrow+\infty} \sqrt[n]{\Vert v_n\Vert}}
\end{align}
(Recall that by Exe. \ref{lb152}, $f(z)$ diverges iff either $\sum_{n=0}^\infty v_nz^n$ or $\sum_{n=-\infty}^{-1} v_nz^n$ diverges.) We call $r$ and $R$ the \textbf{radii of convergence} of $f(z)$. \hfill\qedsymbol
\end{sprob}

\begin{sexe}\label{lb154}
Consider Laurent series $f(z)=\sum_{n=-\infty}^{+\infty}a_nz^n$ (with radii of convergence $r_1<R_1$) and $g(z)=\sum_{n=-\infty}^{+\infty}b_nz^n$ (with radii of convergence $r_2<R_2$) in $\Cbb$. Let
\begin{align*}
r=\max\{r_1,r_2\}\qquad R=\min\{R_1,R_2\}
\end{align*}
Assume that $r<R$. Prove that for each $k\in\Zbb$, the series
\begin{align*}
c_k=\sum_{l=-\infty}^{+\infty}a_{k-l}b_l
\end{align*}
converges absolutely. Prove that for each $z\in\Cbb$ satisfying $r<|z|<R$, the LHS of the following equation converges absolutely to the RHS:
\begin{align}
\sum_{k=-\infty}^{+\infty}c_kz^k= f(z)g(z)
\end{align}
\end{sexe}





\newpage


\section{$\star$ Construction of $\Rbb$ from $\Qbb$}  \label{lb167}


The goal of this chapter is to construct real numbers from rationals. More precisely, our goal is to prove Thm. \ref{lb3}. We use the method of Cantor to construct real numbers using equivalence classes of Cauchy sequences in $\Qbb$. The idea is quite simple: If we admit the existence of $\Rbb$ satisfying Thm. \ref{lb3}, then by Prop. \ref{lb2}, each $x\in\Rbb$ is the limit of a sequence $(x_n)$ in $\Qbb$, which must be a Cauchy sequence. Moreover, if $(y_n)$ is a sequence in $\Qbb$ converging to $y\in\Rbb$, then by Exe. \ref{lb128} we have $x=y$ iff $(x_n)$ and $(y_n)$ are Cauchy equivalent. Motivated by this, we now do not assume the existence of $\Rbb$, and make the following definition:

\begin{df}
We let $\scr R$ be the set of Cauchy sequences in $\Qbb$, \footnote{Only in this chapter do we use $\scr R$ for this meaning.} namely, the set of $(x_n)_{n\in\Zbb_+}\in\Qbb^{\Zbb_+}$ satisfying 
\begin{align*}
\lim_{m,n\rightarrow+\infty} (x_m-x_n)=0
\end{align*}
We say that two elements $(x_n),(y_n)$ of $\scr R$ are Cauchy-equivalent and write $(x_n)\sim(y_n)$ if $\lim_{n\rightarrow\infty}(x_n-y_n)=0$.
\end{df}


Note that the above definition does not rely on the existence of $\Rbb$, because the limit of nets in $\Qbb$ can be defined using only rational numbers: a net $(\xi_\alpha)_{\alpha\in I}$ converges to $\xi$ iff for every $\eps\in\Qbb_{>0}$, $\xi_\alpha$ is eventually satisfies $|\xi_\alpha-\xi|<\eps$. The readers can check that all the properties about limit used in this chapter does not rely on the existence of $\Rbb$.

Cauchy-equivalence is clearly an equivalence condition on $\scr R$: For example, if $\lim (x_n-y_n)=\lim(y_n-z_n)=0$ then $|x_n-z_n|\leq |x_n-y_n|+|y_n-z_n|\rightarrow 0$. So $|x_n-z_n|\rightarrow0$. This proves the transitivity. The other two conditions are obvious. Therefore, we can define:

\begin{df}
We let $\Rbb=\scr R/\sim$ where $\sim$ is the Cauchy-equivalence relation. (Recall Def. \ref{lb157}). The equivalence class of $(x_n)_{n\in\Zbb_+}$ is denoted by $[x_n]_{n\in\Zbb_+}=[x_1,x_2,\dots]$, simply written as $[x_n]$. The zero element $0$ of $\Rbb$ is defined to be $[0,0,\dots]$. 
\end{df}




\begin{exe}\label{lb160}
Choose $[x_n]\in\Rbb$ (i.e. $(x_n)\in\scr R$) . The following are equivalent:
\begin{itemize}
\item[(1)] $[x_n]\neq 0$. Namely, $\lim_n x_n$ does not converge to $0$.
\item[(2)] There exists $\eps\in\Qbb_{>0}$ such that either $x_n>\eps$ eventually, or $x_n<-\eps$ eventually. In particular, $x_n\neq 0$ eventually.
\end{itemize}
Consequently, the map $a\in\Qbb\mapsto [a,a,\dots]\in\Rbb$ is injective. With the help of this injective map, $\Qbb$ can be viewed as a subset of $\Rbb$.
\end{exe}


\begin{exe}\label{lb159}
Let $[x_n]\in\Rbb$ and $a\in\Qbb$. Suppose that $x_n\geq a$ (resp. $x_n\leq a$) frequently.  Then for every $\eps\in\Qbb_{>0}$, we have that $x_n\geq a-\eps$ (resp. $x_n\leq a+\eps$) eventually.
\end{exe}





\begin{df}\label{lb158}
If $\xi,\eta\in\Rbb$, write $\xi=[x_n]$ and $\eta=[y_n]$. In the case that $\eta\neq0$, we assume $y_n\neq 0$ for all $n$, which is possible by Exe. \ref{lb160}. Define
\begin{gather*}
[x_n]+ [y_n]=[x_n+ y_n]\\
-[x_n]=[-x_n]\\
[x_n]\cdot [y_n]=[x_ny_n]\\
1/[y_n]=[1/y_n]\qquad(\text{if }[y_n]\neq0)
\end{gather*}
\end{df}


\begin{exe}
Prove that the above formulas are well-defined: For example, if $(x_n)\sim(x_n')$ in $\scr R$, then $(x_ny_n)\sim (x_n'y_n)$. (You may need the easy fact that every Cauchy sequence is bounded.)
\end{exe}

\begin{rem}\label{lb166}
It is clear that Def. \ref{lb158} makes $\Rbb$ a \textbf{field}, \index{00@Field} which means that for every $\alpha,\beta,\gamma\in\Rbb$, the following are satisfied:
\begin{gather*}
\alpha+\beta=\beta+\alpha\qquad  (\alpha+\beta)+\gamma=\alpha+(\beta+\gamma)\qquad 0+\alpha=\alpha\qquad \alpha+(-\alpha)=0\\
\alpha\beta=\beta\alpha\qquad (\alpha\beta)\gamma=\alpha(\beta\gamma)\qquad 1\cdot\alpha=\alpha\qquad(\alpha+\beta)\gamma=\alpha\gamma+\beta\gamma\\
\alpha\cdot \frac 1\alpha=1\qquad(\text{if }\alpha\neq0)
\end{gather*}
Moreover, $\Qbb$ is a subfield of $\Rbb$ where the addition, taking negative, multiplication, and inverse of $\Rbb$ restrict to those of $\Qbb$.
\end{rem}

\begin{df}\label{lb164}
Let $[x_n],[y_n]\in\Rbb$. We write $[x_n]<[y_n]$ if one of the following equivalent (due to Exe. \ref{lb159}) statements hold:
\begin{itemize}
\item There exists $\eps\in\Qbb_{>0}$ such that $y_n-x_n>\eps$ eventually. 
\item There exists $\eps\in\Qbb_{>0}$ such that $y_n-x_n>\eps$ frequently. 
\end{itemize}
It is not hard show that $``<"$ is well-defined, and that (by Exe. \ref{lb160}) if $[x_n]<[y_n]$ then $[x_n]\neq [y_n]$. We write $[x_n]\leq [y_n]$ if $[x_n]<[y_n]$ or $x_n=y_n$.
\end{df}

\begin{lm}
$(\Rbb,\leq)$ is a totally ordered set.
\end{lm}

\begin{proof}
Choose $[x_n],[y_n],[z_n]\in\Rbb$. If $[x_n]<[y_n]$ and $[y_n]<[z_n]$, then clearly $[x_n]<[z_n]$. This proves that $\leq$ is a preorder. 

Suppose $[x_n]\leq[y_n]$ and $[y_n]\leq[x_n]$. Let us prove $[x_n]=[y_n]$. Suppose not. Then $[x_n]<[y_n]$ and $[y_n]<[x_n]$ by the definition of ``$\leq$". So there is $\eps>0$ such that $y_n-x_n>\eps$ eventually, and $x_n-y_n>\eps$ eventually. Impossible. So $\leq$ is a partial order.

Suppose $[x_n]\neq [y_n]$. Then $(x_n-y_n)\nsim 0$. So Exe. \ref{lb160} implies that either $[x_n]<[y_n]$ or $[y_n]<[x_n]$. So $\leq$ is a total order.
\end{proof}

\begin{lm}\label{lb165}
Let $[x_n],[y_n]\in\Rbb$. Then the following are equivalent.
\begin{itemize}
\item $[x_n]\geq[y_n]$.
\item For every $\eps\in\Qbb_{>0}$, $x_n-y_n\geq-\eps$ frequently.
\item For every $\eps\in\Qbb_{>0}$, $x_n-y_n\geq-\eps$ eventually.
\end{itemize}
\end{lm}

\begin{proof}
Since $\leq$ is a total order, the negation of $>$ is $\leq$. So the statements follow immediately by negating Def. \ref{lb164}.
\end{proof}



\begin{lm}
$\Rbb$ is an ordered field extension of $\Qbb$, and $\Rbb$ is Archimedean.
\end{lm}

\begin{proof}
The order of $\Rbb$ clearly restricts to that of $\Qbb$. We want to prove that $\Rbb$ is an ordered field. (Recall Def. \ref{lb161}). Clearly, if $[x_n]<[y_n]$ and $[z_n]\in\Rbb$, then $[x_n]+[z_n]=[x_n+z_n]<[y_n+z_n]=[y_n]+[z_n]$. If $[x_n]>0$ and $[y_n]>0$, then there are $\eps>0$ such that $x_n>\eps$ eventually and $y_n>\eps$ eventually. So $x_ny_n>\eps^2$ eventually. So $[x_n][y_n]>0$. This proves that $\Rbb$ is an ordered field.

Now let $[x_n]>0$ and $[y_n]\in\Rbb$. So there exist $\eps\in\Qbb_{>0}$ such that $x_n>\eps$ eventually. Since $(y_n)$ is Cauchy, one checks easily that $(y_n)$ is bounded. So there is $M\in\Qbb_{>0}$ such that $|y_n|\leq M$ for all $n$. Since $\Qbb$ is Archimedean, there exists $k\in\Zbb_+$ such that $k\eps>M+1$. So $kx_n>M+1$ eventually. So $k[x_n]>M$. This proves that $\Rbb$ is Archimedean.
\end{proof}




To finish the proof of Thm. \ref{lb3}, it remains to prove that $\Rbb$ satisfies the least-upper-bound property.


\begin{lm}\label{lb162}
Thm. \ref{lb3} holds if every bounded increasing sequence in $\Rbb$ converges.
\end{lm}

\begin{proof}
Suppose that every bounded increasing sequence in $\Rbb$ converges. Choose any nonempty $E\subset \Rbb$ bounded from above. We shall show that $E$ has a least upper bound.

Let $F$ be the set of upper bounds of $E$. Namely, $F=\{\eta\in\Rbb:\eta\geq\xi,\forall\xi\in E\}$. So $F\neq\emptyset$. We construct an increasing sequence $(\xi_k)$ in $E$ and an decreasing sequence $(\eta_k)$ in $F$ as follows. Since $E,F$ are nonempty, we choose arbitrary $\xi_1\in E$ and $\eta_1\in F$. Then $\xi_1\leq \eta_1$. Suppose $\xi_1\leq\cdots\leq\xi_k\in E$ and $\eta_1\geq\cdots\geq\eta_k\in F$ have been constructed. Let $\psi_k=(\xi_k+\eta_k)/2$. Let
\begin{gather*}
\left\{
\begin{array}{ll}
\xi_{k+1}=\psi_k,\eta_{k+1}=\eta_k &\text{ if }\psi_k\in E\\
\xi_{k+1}=\xi_k,\eta_{k+1}=\psi_k &\text{ if }\psi_k\in F
\end{array}
\right.
\end{gather*}
Then the sequences we have constructed satisfy $\lim_{k\rightarrow\infty}(\eta_k-\psi_k)=0$.

By assumption, $\alpha=\lim_{k\rightarrow\infty}\xi_k$ exists, and it equals $\lim_k \eta_k$.  So $\alpha$ is an upper bound of $E$. (If $\lambda\in E$, then $\lambda\leq \eta_k$ for all $k$ since $\eta_k\in F$. So $\lambda\leq\lim_k\eta_k=\alpha$.) We now show that $\alpha$ is the least upper bound. Let $\eps>0$. Since $\xi_k\rightarrow\alpha$, there is $k$ such that $\alpha-\xi_k<\eps$. So $\xi_k>\alpha-\eps$, and hence $\alpha-\eps$ is not an upper bound of $E$.
\end{proof}


\begin{lm}\label{lb163}
Thm. \ref{lb3} holds if every bounded increasing sequence in $\Qbb$ converges to an element of $\Rbb$.
\end{lm}

\begin{proof}
Suppose that every increasing sequence in $\Qbb$ converges in $\Rbb$. By Lem. \ref{lb162}, it suffices to prove that every increasing sequence $(\xi_k)$ in $\Rbb$ converges. If $\{\xi_{k}:k\in\Zbb_+\}$ is a finite subset of $\Rbb$, then $(\xi_k)$ clearly converges. If $\{\xi_{k}:k\in\Zbb_+\}$ is infinite, then $(\xi_k)$ clearly has a strictly increasing subsequence $(\xi_{k_l})$. If we can prove that $(\xi_{k_l})$ converges to some $\psi\in\Rbb$, then $(\xi_k)$ converges to $\psi$. (Choose any $\eps>0$. Choose $L\in\Zbb_+$ such that $|\psi-\xi_{k_L}|<\eps$ and hence $0\leq \psi-\xi_{k_L}<\eps$. Then for all $k\geq k_L$ we have $0\leq\psi-\xi_k<\eps$.)

Thus, it remains to prove that every strictly increasing sequence $(\eta_k)$ in $\Rbb$ converges. Since we have proved that $\Rbb$ is an Archimedean ordered field extension of $\Qbb$, by Prop. \ref{lb2}, for each $k$, there exists $a_k\in\Qbb$ such that $\xi_k< a_k< \xi_{k+1}$. By assumption, $(a_k)$ converges to some $\alpha\in\Rbb$. Since $a_{k-1}<\xi_k< a_k$, by squeeze theorem, $(\xi_k)$ converges to $\alpha$.
\end{proof}




\begin{proof}[\textbf{Proof of Thm. \ref{lb3}}]
By Lem. \ref{lb163}, it suffices to show that every bounded increasing sequence $(a_k)$ in $\Qbb$ converges in $\Rbb$. Let $M\in\Qbb$ such that $a_k\leq M$ for all $k$.

We first prove that $(a_k)$ is a Cauchy sequence. If not, then there exists $\eps\in\Qbb_{>0}$ such that for every $K\in\Zbb_+$ there is $k>K$ such that $|a_k-a_K|>\eps$, and hence $a_k-a_K>\eps$. Thus, we can find a subsequence $(a_{k_l})$ such that $a_{k_{l+1}}-a_{k_l}>\eps$. By the Archimedean property for $\Qbb$, there is $l\in\Zbb_+$ such that $a_{k_1}+l\cdot\eps>M$. So $a_{k_{l+1}}>M$, impossible.



Note that each $a_k$ is identified with $\xi_k=[a_k,a_k,\dots]$. Let $\psi=[a_1,a_2,a_3,\dots]$, which is an element of $\Rbb$ since we just proved that $(a_n)\in\scr R$. Then for each $k$, $\psi-\xi_k=[a_1-a_k,a_2-a_k,\dots]$, where the terms are eventually $\geq0$. So $\xi_k\leq\psi$ by Lem. \ref{lb165}. We have proved that $\psi$ is an upper bound for the sequence $(\xi_k)$.

Let us prove that $\lim_k\xi_k=\psi$. Choose any $\eps\in\Qbb_{>0}$. Let us prove that there exists $k$ such that $\psi-\eps<\xi_k$. Then for every $k'\geq k$ we have $\psi-\eps<\xi_{k'}\leq\psi$, finishing the proof of $\lim_k\xi_k=\psi$.

We have proved that $a_1,a_2,\dots$ is a Cauchy sequence in $\Qbb$. So there exists $k$ such that $a_l-a_k<\eps/2$ for all $l\geq k$. Thus, for all $l\geq k$ we have $a_k-(a_l-\eps)>\eps/2$. Thus, the $l$-th term of $\xi_k=[a_k,a_k,\dots]$ minus that of $\psi-\eps=[a_1-\eps,a_2-\eps,\dots]$ is $>\eps/2$ for sufficiently large $l$. By Def. \ref{lb164}, we have that $\psi-\eps<\xi_k$.
\end{proof}

\newpage

\section{Topological spaces}\label{lb350}


\subsection{The topologies of metric spaces}\label{lb169}

In this chapter, we begin our study of topological spaces, which were introduced by Hausdorff in 1914 \cite{Hau14} as a generalization of metric spaces. As we have seen, focusing on metrics in order to study convergence and continuity is often distracting. For example, in $\ovl\Rbb$, we only care about how the convergence of sequences look like, but not about the particular metrics. The same is true about the countable product of metric spaces $S=\prod_{i\in\Zbb_+}X_i$: the metrics \eqref{eq16} and \eqref{eq61} give the same topology, although they look very different. Moreover, the shapes of the open balls defined by these two metrics are not very simple. This makes it more difficult to study the continuity of functions on $S$ by using (2) or (2') of Def. \ref{lb31}.




Topological spaces generalize metric spaces by giving a set of axioms satisfied by the open sets of the spaces.

\begin{df}\label{lb168}
Let $X$ be a metric space, and let $E\subset X$. A point $x\in E$ is called an \textbf{interior point} of $E$ if $B_X(x,r)\subset E$ for some $r>0$. We say that $E$ is an \textbf{open (sub)set} of $X$, \index{00@Open set of a metric spaces} if every point of $E$ is an interior point.
\end{df}

\begin{df}
Let $\mc T$ be the set of open sets of $X$. We call $\mc T$ the \textbf{topology of the metric space} $X$. \index{00@Topology of a metric space}
\end{df}

\begin{eg}
By triangle inequality, every open ball of a metric space $X$ is open. $\emptyset$ and $X$ are open subsets of $X$. If $p,q\in\Rbb^N$ and $d(p,x)=r$ (where $0\leq r<+\infty$), then $p$ is not an interior point of $\ovl B_{\Rbb^N}(x,r)$. So the closed balls of $\Rbb^N$ are not open sets. In particular, $[a,b]$ are not open subsets of $\Rbb$ since $a,b$ are not interior points.
\end{eg}

\begin{eg}
It is not hard to see that a finite intersection of open sets is open.
\end{eg}



In topological spaces, open sets play the role of open balls in metric spaces due to the following facts:

\begin{exe}
Let $(x_n)$ be a sequence in a metric space $X$. Let $x\in X$. Show that the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $(x_n)$ converges to $x$.
\item For every \textbf{neighborhood $U$ of $x$} (i.e. every open set containing $x$) there is $N\in\Zbb_+$ such that for every $n\geq N$ we have $x_n\in U$.
\end{enumerate}
\end{exe}
\begin{exe}
Let $f:X\rightarrow Y$ be a map of metric spaces. Let $x\in X$ and $y=f(x)$. Prove that the following are equivalent. 
\begin{enumerate}[label=(\arabic*)]
\item $f$ is continuous at $x$.
\item For every neighborhood $V$ of $y$ there is a neighborhood $U$ of $x$ such that $f(U)\subset V$ (equivalently, $U\subset f^{-1}(V)$.)
\end{enumerate}
\end{exe}


But there is an important difference between the intuitions of open sets and open balls: We want the open balls at a point $x$ to be small so that they can be used to describe the approximation to $x$. However, an arbitrary open set can be very large. For example, when studying convergence and continuity in $\Rbb$, we really want a neighborhood of $1$ to be $(1-\eps,1+\eps)$ but not the more complicated and bigger one $(-\infty,-2)\cup (0,100-\eps)$. Indeed, open sets can be very big:


\begin{lm}
Let $X$ be a metric space. If $(U_\alpha)_{\alpha\in I}$ is a family of open subsets of $X$, then $W=\bigcup_{\alpha\in I}U_\alpha$ is open in $X$.
\end{lm}

\begin{proof}
Choose $x\in W$. Then $x\in U_\alpha$ for some $\alpha$. So $B_X(x,r)\subset U_\alpha$ for some $r>0$. So $x$ is an interior point of $W$.
\end{proof}



Thus, people very often choose a class $\mc B$ of smaller open sets (such as the set of open balls) to study the analytic properties of a topological space. 
\begin{df}\label{lb170}
Let $\mc B$ be a set of open sets of a metric space (or more generally, a topological space) $X$. We say that $\mc B$ is a \textbf{basis for the topology} \index{00@Basis for topology} $\mc T$ of $X$ if one of the following (clearly) equivalent statements holds:
\begin{itemize}
\item For every point $x\in X$ and every neighborhood $W$ of $x$ there exists $U\in\mc B$ such that $x\in U$ and $U\subset W$.
\item Every open subset of $X$ is a union of some members of $\mc B$.
\end{itemize}
\end{df}

Thus, according to Def. \ref{lb168}, the set of open balls of a metric space $X$ form a basis for the topology of $X$. Nevertheless, even in the case of metric spaces, we sometimes consider more convenient bases than the set of open balls. We will see this when we study the topologies of infinite product spaces.


\subsection{Topological spaces}


\subsubsection{Definitions and basic examples}

\begin{df}\label{lb178}
We say that a pair $(X,\mc T)$ (or simply $X$) is a \textbf{topological space} \index{00@Topological space} if $X$ is a set, and if $\mc T$ (called the \textbf{topology} of $X$) is a set of subsets of $X$ satisfying the following conditions
\begin{itemize}
\item $\emptyset\in\mc T$ and $X\in \mc T$.
\item (Union property) If $(U_\alpha)_{\alpha\in I}$ is a family of elements of $\mc T$, then $\bigcup_{\alpha\in I}U_\alpha$ is an element of $\mc T$.
\item (Finite intersection property) If $n\in\Zbb_+$ and $U_1,\dots,U_n\in\mc T$, then $U_1\cap\cdots\cap U_n$ is an element of $\mc T$.
\end{itemize}
Elements of $\mc T$ are called \textbf{open (sub)sets} \index{00@Open set} of $X$.
\end{df}


\begin{df}
Let $X$ be a topological space, and $x\in X$. A subset $U\subset X$ is called  a \index{00@Neighborhood=open set containing the point} \textbf{neighborhood} of $x$, if $U$ is an open subset of $X$ containing $x$.\footnote{We are following the convention in \cite{Mun,Rud-R}. But many people refer to the word "neighborhood" with slightly different meaning: a subset $A$ is called a neighborhood of $x$ if there is an open set $U$ such that $x\in U\subset A$. And our neighborhoods are called ``open neighborhoods" by them.} We define $(\Nbh_X(x),\leq)$, \index{Nbh@$\Nbh_X(x)=\Nbh(x)$}  the \textbf{directed set of neighborhoods of $x$}, \index{00@Directed set of neighborhoods of a point} to be 
\begin{gather}
\begin{gathered}
\Nbh_X(x)=\{\text{neighborhoods of }x\text{ in }X\}\\
U\leq U'\qquad\Longleftrightarrow \qquad U\supset U'
\end{gathered}
\end{gather} 
(Note that one needs the finite intersection property to show that $\Nbh_X(x)$ is a directed set.) We abbreviate this set to $\Nbh_X(x)$ or simply $\Nbh(x)$.
\end{df}


\begin{eg}\label{lb529}
In Subsec. \ref{lb169}, we have proved that the topology of a metric space satisfies the above axioms of a topological space. 

In particular,   if $X$ is a normed vector space, the topology induced by the metric $d(x,x')=\Vert x-x'\Vert$ is called the \textbf{norm topology}. \index{00@Norm topology} If $X$ is a subset of $\Rbb^N$ or $\Cbb^N$, the topology on $X$ induced by the Euclidean metric is called the \textbf{Euclidean topology}. \index{00@Euclidean topology}  \hfill\qedsymbol
\end{eg}

\begin{df}
A topological space $(X,\mc T)$ is called \textbf{metrizable}, \index{00@Metrizable topological space} if there is a metric on $X$ inducing the topology $\mc T$. 
\end{df}


We have seen that the open balls of a metric space generate a topology. In general, one may ask what possible subsets of $2^X$ generate a topology on a set $X$. Here is a description, whose proof is left to the readers as an exercise.

\begin{pp}\label{lb171}
Let $X$ be a set, and let $\mc B\subset 2^X$. Define
\begin{align}
\mc T=\{\text{Unions of elements of }\mc B\}
\end{align}
The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $(X,\mc T)$ is a topological space.
\item The following are satisfied:
\begin{itemize}
\item[(2-a)] $X=\bigcup_{U\in\mc B}U$.
\item[(2-b)] If $U_1,U_2\in\mc B$, then $U_1\cap U_2\in\mc T$ (i.e., for each $x\in U_1\cap U_2$ there exists $V\in\mc B$ such that $x\in V$ and $V\subset U_1\cap U_2$).
\end{itemize} 
\end{enumerate}
\end{pp}

When (1) or (2) holds, we call $\mc T$ the \textbf{topology generated by $\mc B$}. \index{00@Topology generated by the basis} Clearly, $\mc B$ is a basis for $\mc T$ (cf. Def. \ref{lb170}).





\begin{exe}\label{lb172}
Let $X$ be a set. Let $\mc B,\mc B'$ be subsets of $2^X$ generating topologies $\mc T,\mc T'$ respectively. Prove that the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\mc T=\mc T'$.
\item Each $U\in\mc B$ is a union of elements of $\mc B'$. Each $U'\in\mc B'$ is a union of elements of $\mc B$.
\item For each $U\in\mc B$ and $x\in U$, there exists $U'\in\mc B'$ such that $x\in U'\subset U$. For each $U'\in\mc B$ and $x\in U'$, there exists $U\in\mc B$ such that $x\in U\subset U'$.
\end{enumerate} 
\end{exe}

\begin{eg}
If $(X,\mc T)$ is a topological space, then $\mc T$ is a basis for $\mc T$.
\end{eg}

\begin{eg}
Let $(X,d)$ be a metric space with topology $\mc T$. Let $\mc B=\{B_X(x,r):x\in X,0<r<+\infty\}$. Then $\mc B$ is a basis for $\mc T$. For each $\eps>0$, the set $\mc B'=\{B_X(x,r):x\in X,0<r<\eps\}$ is also a basis for $\mc T$.
\end{eg}

\begin{eg}\label{lb691}
Let $\mc B\subset 2^{\ovl\Rbb}$ be defined by
\begin{gather}
\mc B=\{(a,b),(c,+\infty],[-\infty,d):a,b,c,d\in\Rbb \}  \label{eq62}
\end{gather}
Using Prop. \ref{lb171}, one easily checks that $\mc B$ is a basis for a topology $\mc T$. We call this the \textbf{standard topology} of $\ovl\Rbb$. \index{00@Topology of $\ovl\Rbb$}

Let $\varphi:\ovl\Rbb\rightarrow[u,v]$ be a strictly increasing bijection where $-\infty<u<v<+\infty$. Let $d_{[u,v]}$ be the Euclidean metric, and let $\mc T'$ be the topology on $\ovl\Rbb$ defined by $d_{\ovl\Rbb}=\varphi^*d_{[u,v]}$. Then the set of open balls under $\mc T'$ is
\begin{align*}
\mc B'=\{&(\varphi^{-1}(y-\eps),\varphi^{-1}(y+\eps)), (\varphi^{-1}(v-\eps'),+\infty],[-\infty,\varphi^{-1}(u+\eps'')):\\
&y\in(u,v)\text{ and } \eps,\eps',\eps''\in\Rbb_{>0} \}
\end{align*}
(Note that the three types of intervals in the definition of $\mc B'$ are open balls centered at $\varphi^{-1}(y),+\infty,-\infty$ respectively.) Using Exe. \ref{lb172}, one easily checks $\mc T=\mc T'$. \hfill\qedsymbol 
\end{eg}

\begin{cv}\label{lb173}
Unless otherwise stated, the topology on $\ovl\Rbb$ is defined to be the standard one, i.e., the one generated by \eqref{eq62}. We shall forget about the metric on $\ovl\Rbb$, and view $\ovl\Rbb$ only as a (metrizable) topological space.
\end{cv}



\begin{df}\label{lb180}
Let $A$ be a subset of a topological space $(X,\mc T_X)$. Then
\begin{align*}
\mc T_A=\{U\cap A:U\in \mc T_X\}
\end{align*} 
is clearly a topology on $A$, called the \textbf{subspace topology}. \index{00@Subspace topology} Unless otherwise stated, when viewing a subset as a topological subspace, we always choose the subspace topology for the subset.
\end{df}




\begin{exe}
Let $(X,d_X)$ be a metric space, inducing a topology $\mc T_X$. Let $A$ be a metric subspace of $X$. (So $A\subset X$, and $d_X$ restricts to $d_A$.) Prove that the topology on $A$ induced by $d_A$ is the subspace topology.
\end{exe}


According to the above exercise, if $X$ is a metric space, then viewing a subset $A$ as a topological subspace is compatible with viewing $A$ as a metric subspace.

\begin{exe}\label{lb209}
Let $A$ be a subset of a topological space $X$. Let $\mc B$ be a basis for the topology of $X$. Show that $\{U\cap A:U\in\mc B\}$ is a basis for the subspace topology of $A$.
\end{exe}




\subsubsection{Convergence of nets}



\begin{df}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a topological space $X$. Let $x\in X$. We say that $(x_\alpha)$ \textbf{converges to} $x$ and write \index{lim@$\lim_{\alpha\in I}x_\alpha\equiv \lim_\alpha x_\alpha$}
\begin{align*}
\lim_{\alpha\in I}x_\alpha\equiv \lim_\alpha x_\alpha=x
\end{align*}
or simply write $x_\alpha\rightarrow x$, if the following statement holds:
\begin{itemize}
\item For every $U\in\Nbh_X(x)$, we have that $x_\alpha$ is eventually in $U$.
\end{itemize}
Clearly, if $\mc B$ is a basis for the topology, then $x_\alpha\rightarrow x$ iff:
\begin{itemize}
\item For every $U\in\mc B$ containing $x$, we have that $x_\alpha$ is eventually in $U$.
\end{itemize}
In the case that $X$ is a metric space (and the topology of $X$ is induced by the metric), the definition here agrees with Def. \ref{lb174}.
\end{df}

\begin{exe}
Let $(x_\alpha)$ be a net in $X$ converging to $x\in X$. Prove that every subnet of $(x_\alpha)$ converges to $x$.
\end{exe}

\begin{exe}
Let $A$ be a subset of a topological space $X$, equipped with the subspace topology. Let $(x_\alpha)$ be a net in $A$, and let $x\in A$. Show that $x_\alpha\rightarrow x$ in $A$ iff $x_\alpha\rightarrow x$ in $X$.
\end{exe}




\begin{eg}
Let $X$ be a set. Let $\mc T=\{\emptyset,X\}$. Then every net in $X$ converges to every point of $X$. Thus, if $X$ has at least two elements, then the limit of a net in $X$ is not unique. Therefore, a general topological space might be very pathological. To avoid this uniqueness issue, we introduce the following notion:
\end{eg}



\begin{df}\label{lb268}
Let $X$ be a topological space with a basis for the topology $\mc B$. We say that $X$ is a \textbf{Hausdorff space} if the following equivalent conditions are satisfied:
\begin{itemize}
\item[(1)] (Hausdorff condition) If $x,y\in X$ and $x\neq y$, then there exist neighborhoods $U$ of $x$ and $V$ of $y$ such that $U\cap V=\emptyset$.
\item[(1')] If $x,y\in X$ and $x\neq y$, then there exist $U\in\mc B$ containing $x$ and $V\in\mc B$ containing $y$ such that $U\cap V=\emptyset$.
\item[(2)] If $(x_\alpha)_{\alpha\in I}$ is a net in $X$ converging to both $x$ and $y$, then $x=y$.
\end{itemize}
\end{df}

\begin{proof}[Proof of the equivalence]
(1)$\Leftrightarrow$(1'): Obvious.

(1)$\Rightarrow$(2): Suppose that $(x_\alpha)$ converges to $x$ and $y$. Suppose $x\neq y$. By (1),  we have disjoint neighborhoods $U\ni x$ and $V\ni y$. Since $x_\alpha\rightarrow x$, $x_\alpha$ is eventually in $U$. Similarly, $x_\alpha$ is eventually in $V$. Therefore, by the logic \eqref{eq38}, $x_\alpha$ is eventually in $U\cap V=\emptyset$, impossible.

$\neg$(1)$\Rightarrow$ $\neg$(2): Suppose that (1) is not true. Then there exist $x\neq y$ such that every neighborhood of $x$ intersects every neighborhood of $y$. Let $I=\Nbh_X(x)\times\Nbh_X(y)$. For each $\alpha=(U,V)\in I$, by assumption, there exists $x_\alpha\in U\cap V$. Then $(x_\alpha)_{\alpha\in I}$ is a net in $X$. We leave it to the readers to check that $x_\alpha\rightarrow x$ and $x_\alpha\rightarrow y$.
\end{proof}


\begin{rem}
In Hausdorff's 1914 paper introducing topological spaces, the Hausdorff condition is one of the axioms of topological spaces. Non-Hausdorff topological spaces were studied much later. The reason that Hausdorff spaces appeared first may be as follows: The original motivation for topological spaces lies in the study of analysis (especially functional analysis). But in analysis, most spaces are Hausdorff, because we want the limits of sequences or nets to be unique. 

In differential geometry and in topology\footnote{Here, I mean genuine topology, such as algebraic topology, differential topology, geometric topology, etc., but not point-set topology, which is analysis under the guise of topology.}, people are also mainly concerned with topological spaces that are Hausdorff. This is related to the fact that in these areas people often use tools from analysis. But in algebraic geometry, the main examples of topological spaces (e.g. varieties and schemes, whose topologies are called \textbf{Zariski topology}) are not Hausdorff.  As a related fact, sequences and nets are not effective tools in the study of algebraic geometry.  \hfill\qedsymbol
\end{rem}

\subsection{Closures, interiors, and closed sets}


In this section, we fix a topological space $X$.


\subsubsection{Closure points; dense subsets}





\begin{df}\label{lb183}
Let $A$ be a subset of $X$. We say that $x\in X$ is a \textbf{closure point} \index{00@Closure, closure point} of $A$, if the following equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item There is a net $(x_\alpha)_{\alpha\in I}$ in $A$ converging to $x$.
\item Each $U\in \Nbh_X(x)$ intersects $A$. 
\end{enumerate}
The \textbf{closure} of $A$ is defined to be \index{Acl@$\ovl A=\Cl_X(A)$, closure}
\begin{align*}
\ovl A\equiv\Cl(A)\equiv\Cl_X(A)=\{\text{closure points of }A\}
\end{align*}
Clearly $A\subset \ovl A$. Clearly, if $A\subset B\subset X$, then $\ovl A\subset\ovl B$.
\end{df}

Unless otherwise stated, if several subsets are involved, we always understand $\ovl A$ as $\Cl_X(A)$ where $X$ is the ambient topological space.

\begin{proof}[Proof of equivalence]
(1)$\Rightarrow$(2): Assume (1). Choose any $U\in\Nbh_X(x)$. Since $x_\alpha\rightarrow x$, we have that $x_\alpha$ is eventually in $U$. So $U$ must contain some $x_\alpha$. But $x_\alpha\in A$. So $U\cap A\neq\emptyset$.

(2)$\Rightarrow$(1):  By (2), for each $U\in\Nbh_X(x)$ we can choose $x_U\in U\cap A$. Then $(x_U)_{U\in\Nbh_X(x)}$ is a net in $A$ converging to $x$. 
\end{proof}







\begin{exe}
Let $\mc B$ be a basis for the topology of $X$. Show that $x\in X$ is a closure point of $A$ iff every $U\in\mc B$ containing $x$ must intersect $A$.
\end{exe}

\begin{exe}
Let $A$ be a subset of a metric space. Show that $x\in X$ is a closure point of $A$ iff there is a sequence $(x_n)_{n\in\Zbb_+}$ in $A$ converging to $x$.
\end{exe}

\begin{exe}
Recall that if $X$ is a metric space, then $\ovl B_X(x,r)=\{y\in X:d(x,y)\leq r\}$. Show that 
\begin{align}
\ovl{B_X(x,r)}\subset \ovl B_X(x,r)
\end{align}
and that these two sets are not necessarily equal.
\end{exe}



\begin{rem}\label{lb270}
Our proof of (2)$\Rightarrow$(1) in Def. \ref{lb183} is an indirect proof, because it uses axiom of choice. (Given $U\in\Nbh_X(x)$, the choice of $x_U\in U\cap A$ is highly arbitrary.) Here is a direct proof: Assume (2). Define  a direct set $(I,\leq)$ where
\begin{gather*}
I=\{(p,U):U\in\Nbh_X(x),p\in U\cap A\}\\[0.5ex]
(p,U)\leq(p',U')\qquad\Longleftrightarrow\qquad U\supset U'
\end{gather*} 
The fact that $I$ is a directed set is due to (2). Then $(p)_{(p,U)\in I}$ is a net in $A$ converging to $x$.

We will often prove results about nets in topological spaces using axiom of choice, not only because it is simpler than direct proofs (as above), but also because it is parallel to our use of sequences in metric spaces. (For example, see the proof of (1)$\Rightarrow$(2) in Def. \ref{lb31}.) However, it is important to know how to give a direct proof. This is because the studies of topological spaces using nets and using open sets are often equivalent, and direct proofs using nets can be more easily translated into proofs using open sets and vice versa.   \hfill \qedsymbol
\end{rem}


\begin{exe}
Prove $\neg$(1)$\Rightarrow$ $\neg$(2) of Def. \ref{lb268} without using axiom of choice.
\end{exe}



\begin{rem}\label{lb176}
There is a notion closely related to closure points, called accumulation points. Let $A$ be a subset of $X$. A point $x\in X$ is called a \textbf{accumulation point} \index{00@Accumulation point of a subset}  (or \textbf{limit point} or \textbf{cluster point}) of $A$, if $x$ is a closure point of $A\setminus\{x\}$.

We will not use the notion of accumulation points, although this concept is widely used in many textbooks on analysis or point-set topology. We use closure points instead. (But note that if $x\notin A$, then $x$ is a closure point iff $x$ is an accumulation point.) On the other hand, the following opposite notion of accumulation points is important and has a clear geometric picture:   \hfill\qedsymbol
\end{rem}

\begin{df}
We say that $x\in X$ is an \textbf{isolated point} of $X$, if the following (clearly) equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item $x\notin\ovl {X\setminus\{x\}}$.
\item There is no net in $X\setminus\{x\}$ converging to $x$.
\item There is a neighborhood of $x$ disjoint from $X\setminus\{x\}$.
\end{enumerate}
If $X$ is a metric space, then $x$ is an isolated point iff there is no sequence in $X\setminus\{x\}$ converging to $x$.
\end{df}


We return to the study of closures.

\begin{pp}\label{lb177}
Let $A$ be a subset of $X$. Then $\ovl{\ovl A}=\ovl A$.
\end{pp}

\begin{proof}
Choose any $x\in \ovl{\ovl A}$. To prove $x\in \ovl A$, we choose any  $U\in\Nbh_X(x)$, and try to prove $U\cap A\neq\emptyset$. Since $x$ is a closure point of $\ovl A$, $U$ intersects $\ovl A$. Pick $y\in U\cap\ovl A$. Then $y$ is a closure point of $A$, and $U\in\Nbh_X(y)$. So $U$ intersects $A$. 
\end{proof}


One should think of $\ovl{\ovl A}=\ovl A$ not only as a ``geometric" fact about closures. Instead, one should also understand its analytic content: A closure point of $A$ is a point which can be approximated by elements of $A$. Thus, $\ovl{\ovl A}=\ovl A$ says that ``approximation is transitive": If $x$ can be approximated by some elements which can be approximated by elements of $A$, then $x$ can be approximated by elements of $A$. Alternatively, one can use the language of density:

\begin{df}
A subset $A$ of $X$ is called \textbf{dense} (in $X$) \index{00@Dense subset} if $\ovl A= X$.
\end{df}

\begin{exe}
Show that $A$ is dense in $X$ iff every nonempty open subset of $X$ intersects $A$.
\end{exe}



\begin{rem}\label{lb182}
Let $A\subset B\subset X$.  From Def. \ref{lb183}-(1), it is clear that
\begin{align}
\Cl_B(A)=\Cl_X(A)\cap B \label{eq64}
\end{align}
Thus, $A$ is dense in $B$ iff $B\subset \Cl_X(A)$.
\end{rem}

Thus, the following property has the same meaning as $\ovl{\ovl A}=A$.
\begin{co}
Let $A\subset B\subset X$. Assume that $A$ is dense in $B$, and $B$ is dense in $X$, then $A$ is dense in $X$.
\end{co}
\begin{proof}
Choose any $x\in X$. Then $x\in\Cl_X(B)$ since $B$ is dense in $X$. Since $A$ is dense in $B$, we have $B\subset \Cl_X(A)$. Therefore $x\in\Cl_X(\Cl_X(A))$, and hence $x\in\Cl_X(A)$ by Prop. \ref{lb177}.
\end{proof}

\begin{eg}
Let $X=C([0,1],\Rbb)$, equipped with the $l^\infty$-norm. Let $B$ be the set of polynomials with real coefficients, regarded as continuous functions on $[0,1]$. By Weierstrass approximation theorem (which will be studied in the future), $B$ is a dense subset of $X$. Then the set $A$ of polynomials with rational coefficients is clearly a dense subset of $B$ under the $l^\infty$-norm. (Proof: Let $f(x)=a_0+a_1x+\cdots+a_{k}x^k$. For each $0\leq i\leq k$, choose a sequence $(a_{i,n})_{n\in\Zbb_+}$ in $\Qbb$ converging to $a_i$. Let $f_n(x)=a_{0,n}+a_{1,n}x+\cdots+a_{k,n}x^k$. Then $f_n\rightrightarrows f$ on $[0,1]$.) Therefore, $A$ is dense in $X$. To summarize:
\begin{itemize}
\item Since each continuous function on $[0,1]$ can be uniformly approximated by polynomials with $\Rbb$-coefficients, and since each polynomial can be uniformly approximated polynomials with $\Qbb$-coefficients, therefore each continuous function on $[0,1]$ can be uniformly approximated by polynomials with $\Qbb$-coefficients.
\end{itemize}
\end{eg}


%% Record #7 2023/10/11 three lectures  17



\subsubsection{Interior points}

Interior points are dual to closure points:

\begin{df}\label{lb187}
Let $A$ be a subset of $X$. A point $x\in X$ is called an \textbf{interior point} \index{00@Interior point} of $A$ if the following equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item There exists $U\in \Nbh_X(x)$ such that $U\subset A$.
\item $x$ is not a closure point of $X\setminus A$. 
\end{enumerate}
The set of interior points of $A$ is called the \textbf{interior} \index{00@Interior} of $A$ and is denoted by $\Int_X(A)$ or simply $\Int(A)$. \index{Int@$\Int_X(A)=\Int(A)$} So
\begin{align}
X\setminus\Int(A)=\ovl{X\setminus A}\qquad(\text{or simply }\Int(A)^c=\ovl{A^c})
\end{align}
according to (2). In particular, $\Int(A)\subset A$.
\end{df}

\begin{proof}[Proof of equivalence]
$A$ contains no neighborhoods of $x$ with respect to $X$ iff $A^c$ intersects every neighborhood of $x$ iff $x$ is a closure point of $A^c$.
\end{proof}

It is clear that if $\mc B$ is a basis for the topology, then $x\in\Int(A)$ iff there exists $U\in\mc B$ such that $x\in U\subset A$.

In analysis, interior points are not as commonly used as closure points. The following property is an important situation where interior points are used:

\begin{pp}\label{lb179}
Let $U$ be a subset of $X$. Then $U$ is open iff every point of $U$ is an interior point.
\end{pp}

In other words, $U$ is open iff $U=\Int_X(U)$.

\begin{proof}
If $U$ is open and $x\in U$, then $U\in \Nbh_X(x)$. So $x$ is an interior point of $U$.

Conversely, suppose that each $x\in U$ is interior. Choose $V_x\in\Nbh_X(x)$. Then $U=\bigcup_{x\in U}V_x$. So $U$ is open by the union property in Def. \ref{lb178}.
\end{proof}

Note that this is the first time we seriously use the fact that a union of open sets is open. 




\subsubsection{Closed sets and open sets}\label{lb228}


\begin{df}
We say that $A\subset X$ is a \textbf{closed (sub)set} \index{00@Closed subset} of $X$ if $\ovl A=A$.
\end{df}

\begin{exe}
Show that the above definition of closed subsets agrees with Def. \ref{lb99} when $X$ is a metric space.
\end{exe}

\begin{exe}
Show that a finite subset of a Hausdorff space is closed. Give an example of non-closed finite subset of a non-Hausdorff topological space.
\end{exe}


\begin{rem}\label{lb242}
The closure $\ovl A$ is the smallest closed set containing $A$. (Proof: By Prop. \ref{lb177},  $\ovl A$ is closed. If $B$ is closed and contains $A$, then $\ovl A\subset\ovl B=B$.)
\end{rem}



\begin{thm}\label{lb181}
Let $A$ be a subset of $X$. Then $A$ is closed iff $X\setminus A$ is open.
\end{thm}

\begin{proof}
Let $B=X\setminus A$. Then $A$ is closed iff every closure point of $A$ is in $A$, iff every non-interior point of $B$ is not in $B$, iff every point in $B$ is an interior point of $B$. By Prop. \ref{lb179}, this is equivalent to that $B$ is open.
\end{proof}

\begin{co}\label{lb186}
$\emptyset$ and $X$ are closed subsets of $X$. An intersection of closed subsets is closed. A finite union of closed subsets is closed. 
\end{co}

\begin{proof}
Take the complement of Def. \ref{lb178}, and apply Thm. \ref{lb181}. (Of course, they can also be proved directly using the condition $A=\ovl A$ for closedness.)
\end{proof}


\begin{co}\label{lb243}
$X$ is Hausdorff iff for every distinct $x,y\in X$ there exists $U\in\Nbh_X(x)$ such that $y\notin \ovl U$.
\end{co}

\begin{proof}
``$\Leftarrow$": Let $x\neq y$. Choose $U\in\Nbh(x)$ such that $y\notin\ovl U$. Then $X\setminus \ovl U\in\Nbh(y)$ by Thm. \ref{lb181}. So $x$ and $y$ are separated by neighborhoods $U,X\setminus\ovl U$.

``$\Rightarrow$": Let $x\neq y$. Choose disjoint $U\in\Nbh(x)$ and $V\in\Nbh(y)$. Then $X\setminus V$ is closed by Thm. \ref{lb181}. So $\ovl U\subset X\setminus V$ by Rem. \ref{lb242}. So $y\notin\ovl U$.
\end{proof}

\begin{co}\label{lb190}
Let $Y$ be a subset of $X$, and let $A\subset Y$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $A$ is a closed subset of $Y$.
\item $A=B\cap Y$ for some closed subset $B$ of $X$.
\end{enumerate}
\end{co}

Note that the ``open subset" version of this corollary is true due to the definition of the subspace topology of $Y$ (cf. Def. \ref{lb180}).

\begin{proof}[First proof]
$A$ is closed in $Y$ iff $Y\setminus A=Y\cap A^c$ is open in $Y$, iff $Y\cap A^c$ equals $Y\cap U$ for some open subset $U\subset X$, iff $Y\cap A$ (which is $A$) equals $Y\cap U^c$ for some open subset $U\subset X$. This finishes the proof, thanks to Thm. \ref{lb181}.
\end{proof}

\begin{proof}[Second proof]
Recall by \eqref{eq64} that $\ovl A\cap Y$ is the closure of $A$ in $Y$. Then $A$ is closed in $Y$ iff $A=\ovl A\cap Y$. This proves (1)$\Rightarrow$(2) since $\ovl A$ is closed by Prop. \ref{lb177}. Assume (2). Then $A=B\cap Y$ where $B=\ovl B$. So $\ovl A\cap Y=\ovl{B\cap Y}\cap Y\subset \ovl B\cap Y=B\cap Y=A$. This proves (1).
\end{proof}


As an immediate consequence of Def. \ref{lb178} and Cor. \ref{lb190}, we have:

\begin{exe}\label{lb341}
Let $A\subset B\subset X$. 
\begin{enumerate}
\item Prove that if $B$ is open in $X$, then $A$ is open in $B$ iff $A$ is open in $X$.
\item Prove that if $B$ is closed in $X$, then $A$ is closed in $B$ iff $A$ is closed in $X$.
\end{enumerate}
\end{exe}







\begin{rem}
Many people define a closed set to be the complement of an open set, and then proves that a set $A$ is closed iff $A=\ovl A$. I went the other way because I believe that $A=\ovl A$ is more essential for understanding of closedness from the viewpoint of analysis. In Thm. \ref{lb87}, we have already seen a classical example of closed set in analysis: $C([0,1],\Rbb)$ is a closed subset of $l^\infty([0,1],\Rbb)$, which has the clear analytic meaning that the uniform limit of a sequence/net of continuous functions $[0,1]\rightarrow\Rbb$ is continuous. And we will see many more examples of this type in the future.
\end{rem}

\begin{rem}\label{lb229}
I defined closedness using $A=\ovl A$, and hence using the limits of nets. This is because the intuition of closed sets is very closely related to the intuition of limits of nets/sequences. On the other hand, the intuition of open sets is very different. Let me say a few words about this.

Without a doubt, the keyword I give for the intuition of limits of nets is ``\uline{approximation}": Limit is not only a dynamic process, but also gives an impression of "getting smaller and smaller". When dealing with closed sets, we often do the same thing! We take an intersection of possibly infinitely many closed subsets, and the result we get is still a closed set (cf. Cor. \ref{lb185}). 

The keyword I give for open sets is ``local", or more precisely, ``\uline{local-to-global}" (as opposed to ``getting smaller and smaller"!). This is not only because a union of open sets is open, but also because open sets are very often used to prove a global result by reducing to local problems. One easy example is Exe. \ref{lb184}, which says that in order to prove that a function is continuous on the whole space $X$, it suffices to prove this locally. (We have already used this strategy in Sec. \ref{lb185}.) Here is a more advanced example: to define the integral for a function on a large set, one can first define it locally (i.e. on small enough open subsets), and then patch these local values together. We will see many examples in the future, for example, in the following chapter about compactness.
\end{rem}

\begin{rem}
Very often, a theorem is an important result establishing two seemingly different (systems of) intuitions, and hence two different ways of mathematical thinking. This is why I call ``closed sets are the complements of open sets" a theorem. The term ``complement" implies that this theorem often manifests itself in the following way: If solving a problem using open sets is a direct proof, then solving the problem using limits of sequences/nets is a proof by contradiction/contrapositive. And vise versa.
\end{rem}


\subsection{Continuous maps and homeomorphisms}



Unless otherwise stated,  $X$ and $Y$ are topological spaces.


\subsubsection{Continuous maps}



\begin{df}\label{lb188}
Let $f:X\rightarrow Y$ be a map. Let $x\in X$. We say that $f$ is \textbf{continuous at} \index{00@Continuity} $x$ if the following equivalent conditions hold:
\begin{enumerate}
\item[(1)] For every net $(x_\alpha)_{\alpha\in I}$ in $X$ converging to $x$, we have $\lim_{\alpha\in I}f(x_\alpha)=f(x)$.
\item[(2)] For every $V\in \Nbh_Y(f(x))$, there exists $U\in\Nbh_X(x)$ such that for every $p\in U$ we have $f(p)\in V$.
\item[(2')] For every $V\in \Nbh_Y(f(x))$, the point $x$ is an interior point of $f^{-1}(V)$.
\end{enumerate}
We say that $f$ is a \textbf{continuous} function/map, if $f$ is continuous at every point of $X$. 
\end{df}

It is clear that ``for every $V\in\Nbh_{Y}(f(x))$" in (2) and (2') can be replaced by ``for every $V\in\mc B$ containing $f(x)$" if $\mc B$ is a basis for the topology of $Y$. 

Note that in the case that $(x_\alpha)$ or $(f(x_\alpha))$ has more than one limits (which could happen when $X$ or $Y$ is not Hausdorff), condition (1) means that $f(x)$ is one of the limits of $(f(x_\alpha))_{\alpha\in I}$ if $x$ is one of the limits of $(x_\alpha)$.


\begin{proof}[Proof of equivalence]
Clearly (2) is equivalent to (2'). The proof of (2)$\Rightarrow$(1) is similar to the case of sequences in metric spaces. (See Def. \ref{lb31}.) We leave the details to the reader.

$\neg$(2)$\Rightarrow$ $\neg$(1): Assume that (2) is not true. Then there is a neighborhood $V$ of $f(x)$ such that for every neighborhood $U$ of $x$ there exists $x_U\in U$ such that $f(x_U)\notin V$. Then $(x_U)_{U\in\Nbh_X(x)}$ is a net in $X$, and $\lim_Ux_U=x$ since $x_U\in U$. However, for each $U$ we have $f(x_U)\in Y\setminus V$. So $\lim_U f(x_U)$ cannot converge to $f(x)$.
\end{proof}

\begin{exe}
Show that when $X,Y$ are metric spaces, Def. \ref{lb188} agrees with Def. \ref{lb31}.
\end{exe}

\begin{exe}\label{lb321}
Let $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ be maps of topological spaces. Assume that $f$ is continuous at $x\in X$, and $g$ is continuous at $f(x)$. Prove that $g\circ f:X\rightarrow Z$ is continuous at $x$.
\end{exe}








The proof of (1)$\Rightarrow$(2) in Def. \ref{lb188} is indirect, since it uses the axiom of choice. (The merit of this proof is that it is parallel to the proof for metric spaces in Sec. \ref{lb185}.) One can also give a direct proof. Indeed, there is a particular net $(x_\alpha)$ converging to $x$ such that  $\lim f(x_\alpha)=f(x)$ iff (2) is true:

\begin{exe}\label{lb198}
Define $(\Pnbh_X(x),\leq)$, \index{PNbh@$\Pnbh_X(x)$} the \textbf{directed set of pointed neighborhoods} of $x$,  to be
\begin{gather}
\begin{gathered}
\Pnbh_X(x)=\big\{(p,U):U\in\Nbh_X(x),p\in U  \big\}\\
(p,U)\leq(p',U')\qquad\Longleftrightarrow\qquad U\supset U'
\end{gathered}
\end{gather}
For each $\alpha=(p,U)\in\Pnbh_X(x)$, let $x_\alpha=p$. Then $(x_\alpha)_{\alpha\in\Pnbh_X(x)}$ is a net in $X$ converging to $x$. Prove that $f$ is continuous at $x$ iff $\lim_\alpha f(x_\alpha)=f(x)$.
\end{exe}



\begin{pp}\label{lb191}
Let $f:X\rightarrow Y$ be a map. The following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $f$ is continuous.
\item If $V\subset Y$ is open in $Y$, then $f^{-1}(V)$ is open in $X$.
\item If $F\subset Y$ is closed in $Y$, then $f^{-1}(F)$ is closed in $Y$.
\end{enumerate}
\end{pp}

\begin{proof}
(1)$\Leftrightarrow$(2): By Def. \ref{lb188}-(2') and Prop. \ref{lb179}. (2)$\Leftrightarrow$(3): By Thm. \ref{lb181} and the fact that $f^{-1}(B^c)=f^{-1}(B)^c$ for every $B\subset Y$.
\end{proof}



\begin{rem}
We first defined the continuity of $f$ at a point, and then used this to define a continuous function $f$ to be one continuous at every point. However, it seems that the notion of continuity at a point is used only in analysis. In geometry and in topology, only continuous maps (but not a map continuous at a point) are used, and they are defined by Prop. \ref{lb191}-(2).

One might think that continuous functions are special cases of functions  which are continuous at given points. But in fact, the latter notion can also be derived from the former: \hfill\qedsymbol
\end{rem}

\begin{exe}\label{lb308}
Let $f:(X,\mc T)\rightarrow (Y,\mc T')$ be a map of topological spaces. Let $x\in X$. Define a new topological space $(X_x,\mc T_x)$ as follows. $X_x$ equals $X$ as a set. The topology $\mc T_x$ of $X_x$ is generated by the basis
\begin{align}
\mc B_x=\Nbh_X(x)\cup\big\{\{p\}:p\neq x\big\}
\end{align}
Prove that if $X$ is Hausdorff, then $X_x$ is Hausdorff. Prove that the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $f:X\rightarrow Y$ is continuous at $x$.
\item $f:X_x\rightarrow Y$ is continuous.
\end{enumerate}
\end{exe}

Continuous functions are determined by their values on a dense subset: 

\begin{pp}\label{lb196}
Let $A$ be a subset of $X$, and let $f,g:\ovl A\rightarrow Y$ be continuous. Then
\begin{subequations}
\begin{gather}
f(\ovl A)\subset\ovl{f(A)}  \label{eq80}
\end{gather}
If $Y$ is moreover Hausdorff, then
\begin{gather}
f=g\qquad\Longleftrightarrow\qquad f|_A=g|_A
\end{gather}
\end{subequations}
\end{pp}
\begin{proof}
If $y\in f(\ovl A)$, then $y=f(x)$ for some $x\in\ovl A$. Choose a net $(x_\alpha)$ in $A$ converging to $x$. Then $\lim_\alpha f(x_\alpha)=f(x)$, and hence $f(x)\in\ovl {f(A)}$.

Assume that $Y$ is Hausdorff. If $f=g$ then clearly $f|_A=g|_A$. Assume that $f|_A=g|_A$. For each $x\in\ovl A$, choose a net $(x_\alpha)$ in $A$ converging to $x$. Then $f(x)=\lim f(x_\alpha)=\lim g(x_\alpha)=g(x)$. So $f=g$.
\end{proof}


You are encouraged to prove Prop. \ref{lb196} using open sets instead of using nets.



\subsubsection{Homeomorphisms}

\begin{df}
A map $f:X\rightarrow Y$ is called \textbf{open} (resp. \textbf{closed}) \index{00@Open map} \index{00@Closed map} if for every open (resp. closed) subset $A\subset X$, the image $f(A)$ is open (resp. closed) in $Y$.
\end{df}


\begin{df}
A bijection $f:X\rightarrow Y$ is called a \textbf{homeomorphism} \index{00@Homeomorphism} if the following clearly equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item $f$ and $f^{-1}$ are continuous.
\item For every net $(x_\alpha)$ in $X$ and each $x\in X$, we have that  $\lim_\alpha x_\alpha=x$ iff $\lim_\alpha f(x_\alpha)=f(x)$.
\item $f$ is continuous and open.
\item $f$ is continuous and closed.
\end{enumerate}
If a homeomorphism $f:X\rightarrow Y$ exists, we say that $X,Y$ are \textbf{homeomorphic}.
\end{df}

Recall from Def. \ref{lb189} that when $X,Y$ are metric spaces, the sequential version of (2) holds. 

\begin{rem}
Let $\mc T_1,\mc T_2$ be two topologies on a set $X$. Clearly, we have $\mc T_1=\mc T_2$ iff
\begin{gather}
\varphi:(X,\mc T_1)\rightarrow(X,\mc T_2)\qquad x\mapsto x
\end{gather}
is a homeomorphism. Thus, the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\mc T_1=\mc T_2$.
\item For every net $(x_\alpha)$ in $X$ and $x\in X$, we have that $\lim_\alpha x_\alpha=x$ under $\mc T_1$ iff $\lim_\alpha x_\alpha=x$ under $\mc T_2$.
\end{enumerate}
This equivalence implies that
\begin{align*}
\boxed{\text{ topologies are determined by net convergence }}
\end{align*}
Therefore, instead of using open sets or bases of topologies to describe a topology, one can also describe a topology $\mc T$ on a set $X$ in the following way:
\begin{gather}\label{eq65}
\begin{gathered}
\text{$\mc T$ is the unique topology on $X$ such that}\\
\text{a net $(x_\alpha)$ in $X$ converges to $x\in X$ iff ...}
\end{gathered}
\end{gather}
Similarly, by Def. \ref{lb189}, metrizable topologies are determined by sequential convergence. Therefore, metrizable topologies can be described in the following way:
\begin{gather}
\begin{gathered}
\text{$\mc T$ is the unique metrizable topology on $X$ such that}\\
\text{a sequence $(x_n)$ in $X$ converges to $x\in X$ iff ...}
\end{gathered}
\end{gather}
\end{rem}


%% Record #8 2023/10/16 two lectures  19



\subsection{Examples of topological spaces described by net convergence}

\begin{eg}
Let $A$ be a subset of a topological space $(X,\mc T_X)$. Then the subspace topology $\mc T_A$ of $A$ is the unique topology such that a net $(x_\alpha)$ in $A$ converges to $x\in A$ under $\mc T_A$ iff it converges to $x$ under $\mc T_X$.
\end{eg}

\begin{seg}\label{lb193}
Let $X=\bigsqcup_{\alpha\in \scr A}X_\alpha$ be a disjoint union where each $(X_\alpha,\mc T_\alpha)$ is a topology space. Then
\begin{align*}
\mc B=\bigcup_{\alpha\in\scr A}\mc T_\alpha
\end{align*}
is clearly a basis generating a topology $\mc T$ on $X$, called \textbf{disjoint union topology}. \index{00@Disjoint union of topological spaces}   $\mc T$ is the unique topology on $X$ such that for every net $(x_\mu)_{\mu\in I}$ in $X$ and any $x\in X$, the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $(x_\mu)_{\mu\in I}$ converges to $x$ under $\mc T$.
\item There exists $\nu\in I$ such that for every $\mu\geq\nu$, the element $x_\mu$ belongs to the unique $X_\alpha$ containing $x$. Moreover, $\lim_{\mu\in I_{\geq\nu}}x_\mu=x$ in $X_\alpha$.
\end{enumerate}
We call $(X,\mc T)$ the \textbf{disjoint union topological space} \index{00@Disjoint union topological space} of $(X_\alpha)_{\alpha\in\scr A}$.
\end{seg}

The following exercise says that ``disjoint union of topological spaces" is synonymous with ``disjoint union of open subsets".

\begin{sexe}
Assume that $X=\bigsqcup_{\alpha\in\scr A}X_\alpha$. Assume that $X$ has a topology $\mc T$, and equip each $X_\alpha$ with the subspace topology. Show that $(X,\mc T)$ is the disjoint union topological space of $(X_\alpha)_{\alpha\in\scr A}$ iff each $X_\alpha$ is an open subset of $(X,\mc T)$.
\end{sexe}

Thus, for example, $\bigcup_{n\in\Nbb} [2n,2n+1)$ (under the Euclidean topology) is the disjoint union topological space of the family $\big([2n,2n+1)\big)_{n\in\Nbb}$.


\begin{sexe}
In Exp. \ref{lb193}, assume that each $(X_\alpha,\mc T_\alpha)$ is metrizable. Prove that $(X,\mc T)$ is metrizable. More precisely: Choose a metric $d_\alpha$ inducing $\mc T_\alpha$, and assume that $d_\alpha\leq 1$ (cf. Prop. \ref{lb195}). Define a metric $d$ on $X$ as in Pb. \ref{lb194}. Solve \textit{the net version} of part 2 of Pb. \ref{lb194}. Conclude from this that $d$ induces the topology $\mc T$. (Warning: we cannot conclude this from the original sequential version of Pb. \ref{lb194}-2.)
\end{sexe}


\begin{df}\label{lb454}
Let $(X_\alpha)_{\alpha\in\mc A}$ be a family of topological spaces. Elements of the product space
\begin{align*}
S=\prod_{\alpha\in\scr A}X_\alpha
\end{align*}
are denoted by $x=(x(\alpha))_{\alpha\in\scr A}$. One checks easily that
\begin{align}
\begin{aligned}
\mc B=\Big\{&\prod_{\alpha\in\scr A} U_\alpha: \text{each $U_\alpha$ is open in $X_\alpha$},\\
& \text{$U_\alpha=X_\alpha$ for all but finitely many $\alpha$}\Big\}
\end{aligned}
\end{align}
is a basis for a topology $\mc T$, called the \textbf{product topology} \index{00@Product topology} or \textbf{pointwise convergence topology} \index{00@Pointwise convergence topology} of $S$. We call $(S,\mc T)$ the \textbf{product topological space}. \index{00@Product topological space} 

Equivalently, let
\begin{gather}
\pi_\alpha:S\rightarrow X_\alpha \qquad x\mapsto x(\alpha)
\end{gather}
be the \textbf{projection map onto the $X_\alpha$ component}. Then
\begin{align}
\mc B=\Big\{\bigcap_{\alpha\in E} \pi_\alpha^{-1}(U_\alpha):E\in\fin(2^{\scr A}), \text{ $U_\alpha$ is open in $X_\alpha$ for each $\alpha\in E$}    \Big\}
\end{align}
Unless otherwise stated, a product of topological spaces is equipped with the product topology.  \hfill\qedsymbol
\end{df}



\begin{eg}\label{lb226}
Let $S=X_1\times\cdots\times X_N$ be a finite product of topological spaces. Then the product topology has a basis
\begin{align}
\mc B=\{ U_1\times\cdots \times U_N:\text{ each $U_i$ is open in $X_i$}\}
\end{align}
\end{eg}


\begin{thm}\label{lb192}
Let $S=\prod_{\alpha\in\scr A}X_\alpha$ be a product of topological spaces, equipped with the product topology. Then each projection map $\pi_\alpha$ is continuous. Moreover, for every net $(x_\mu)_{\mu\in I}$ in $S$ and every $x\in S$, the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\dps\lim_{\mu\in I}x_\mu=x$ in $S$.
\item For every $\alpha\in\scr A$, we have $\dps\lim_{\mu\in I}x_\mu(\alpha)=x(\alpha)$ in $X_\alpha$.
\end{enumerate}
\end{thm}

If $(x_\mu)$ satisfies (1) or (2), we say that $x_\mu$ \textbf{converges pointwise} \index{00@Pointwise convergence} to $x$ if we view $(x_\mu)$ as a net of functions with domain $\scr A$. 

\begin{proof}
We leave the proof to the readers. Note that the continuity of $\pi_\alpha$ follows easily from the basis-for-topology version of Prop. \ref{lb191}-(2). And from the continuity of $\pi_\alpha$ one easily deduce (1)$\Rightarrow$(2).
\end{proof}

\begin{rem}
In the spirit of \eqref{eq65}, one says that:
\begin{itemize}
\item The product topology $\mc T$ on $S=\prod_{\alpha\in\scr A}X_\alpha$ is the unique topology such that a net $(x_\mu)$ converges to $x$ under $\mc T$ iff $x_\mu$ converges pointwise to $x$ as a net of functions on $\scr A$.
\end{itemize}
\end{rem}


\begin{co}
If each $X_\alpha$ is a Hausdorff space, then $S=\prod_{\alpha\in\scr A}X_\alpha$ is Hausdorff.
\end{co}

\begin{proof}
Either prove this directly using the basis for the topology, or prove that any net cannot converge to two different values using Thm. \ref{lb192}.
\end{proof}

\begin{co}\label{lb260}
Let $X_1,X_2,\dots$ be a possibly finite sequence of metric spaces. Then $S=\prod_i X_i$ is metrizable. More precisely, for each $i$, choose a metric $d_i$ on $X_i$ topologically equivalent to the original one such that $d_i\leq 1$ (cf. Prop. \ref{lb194}). Then the metric $d$ on $S$ defined by
\begin{align}
d(f,g)=\sup_{i} \frac {d_i(f(i),g(i))}{i} 
\end{align}
induce the product topology.
\end{co}

We note that the product topology is also induced by
\begin{align}
\delta(f,g)=\sum_{i}2^{-i} d_i(f(i),g(i))
\end{align}

\begin{proof}
The same method for solving Pb. \ref{lb78} also applies to its net version: One shows that a net $(f_\alpha)$ in $S$ converges to $f$ under $d$ (or under $\delta$) iff $(f_\alpha)$ converges pointwise to $f$. Thus, by Thm. \ref{lb192}, $d$ and $\delta$ induce the product topology.
\end{proof}

The next example discusses the topologies induced by uniform convergence metrics. (Recall Def. \ref{lb146}.) In this example, $Y$ is usually a normed vector space.

\begin{eg}\label{lb272}
Let $X$ be a set, and let $(Y,d_Y)$ be a metric space. Then there is a unique topology $\mc T$ on $Y^X$ such that for every net $(f_\alpha)_{\alpha\in I}$ in $Y^X$ and every $f\in Y^X$, the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item The net $(f_\alpha)$ converges to $f$ under $\mc T$.
\item We have $\dps\lim_{\alpha\in I}~\sup_{x\in X} d_Y(f_\alpha(x),f(x))=0$.
\end{enumerate}
(If $(f_\alpha)$ satisfies (2), we say that $f_\alpha$  \textbf{converges uniformly} \index{00@Uniform convergence} to $f$.) For example, one checks easily that $\mc T$ is induced by any \textbf{uniform convergence metric}, \index{00@Uniform convergence metric} i.e., any metric on $Y^X$ equivalent to $d$ where
\begin{align}\label{eq78}
d(f,g)=\min\Big\{1,\sup_{x\in X} d_Y(f(x),g(x))\Big\}
\end{align}
So $\mc T$ is metrizable. We call $\mc T$ the \textbf{uniform convergence topology} \index{00@Uniform convergence topology} on $Y^X$. 
\end{eg}

\begin{comment}
According to \eqref{eq78}, $\mc T$ has a basis
\begin{gather}\label{lb275}
\begin{gathered}
\mc B=\{U_{f,\eps}:f\in Y^X,\eps\in\Rbb_{>0}\}\quad\text{where}\\
U_{f,\eps}=\Big\{g\in Y^X:\sup_{x\in X}d_Y(f(x),g(x))<\eps   \Big\}
\end{gathered}
\end{gather}
\end{comment}

\begin{thm}\label{lb339}
Let $Y$ be a complete metric space. Let $X$ be a set. Then $Y^X$, equipped with the metric \eqref{eq78}, is complete.
\end{thm}

\begin{proof}
It can be proved in a similar way as Thm. \ref{lb85}. We leave the details to the readers.
\end{proof}



\begin{thm}\label{lb279}
Let $V$ be a normed vector space over $\Rbb$ or $\Cbb$. Let $X$ be a topological space. Equip $V^X$ with the uniform convergence topology. Then $C(X,V)$ is a closed subset of $V^X$.
\end{thm}

\begin{proof}
This is similar to the proof of Thm. \ref{lb87}. Let $(f_\alpha)$ be a net $C(X,V)$ converging uniformly to $f:X\rightarrow V$. Choose any $x\in X$ and $\eps>0$. Then there is $\alpha\in I$ such that $\sup_{p\in X}\Vert f(x)-f_\alpha(x)\Vert<\eps$. Since $f_\alpha$ is continuous, there is $U\in\Nbh_X(x)$ such that for each $p\in U$ we have $\Vert f_\alpha(x)-f_\alpha(p)\Vert<\eps$. Thus, for each $p\in U$ we have 
\begin{align*}
\Vert f(x)-f(p)\Vert\leq \Vert f(x)-f_\alpha(x)\Vert +\Vert f_\alpha(x)-f_\alpha(p)\Vert+\Vert f_\alpha(p)-f(p)\Vert<3\eps
\end{align*}
So $f$ is continuous.
\end{proof}


\begin{rem}
Note that the uniform convergence topology depends on the equivalence class (not just the topological equivalence class) of $d_Y$. Thus, one needs metrics when talking about uniform convergence. On the other hand, the study of pointwise convergence does not require metrics.
\end{rem}

\subsection{Limits of functions}\label{lb290}




By Prop. \ref{lb196}, if $A\subset X$,  and if $f:\ovl A\rightarrow Y$ is continuous, then the value of $f$ is uniquely determined by $f|_A$ provided that $Y$ is Hausdorff (cf. Prop. \ref{lb196}). We now consider the opposite question of \uwave{extension of continuous functions}: Suppose that $f:A\rightarrow Y$ is continuous. Can we extend $f$ to a continuous function $f:\ovl A\rightarrow Y$? (We know that such extension must be unique if it exists.) The classical concept of the limits of functions can be understood in this light.



\begin{df}\label{lb197}
Let $A$ be a subset of $X$. Let $f:A\rightarrow Y$ be a map. Let $x\in\ovl A\setminus A$. Let $y\in Y$. We say that the \textbf{limit of the function} \index{00@Limit of a function} $f$ at $x$ is $y$ and write \index{lim@$\lim_{p\rightarrow x}f(p)$}
\begin{align*}
\lim_{
\begin{subarray}{c}
p\in A\\
p\rightarrow x
\end{subarray}
} f(p)\equiv\lim_{p\rightarrow x}f(p)=y
\end{align*}
if the following equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item If we extend $f$ to a function $A\cup\{x\}\rightarrow Y$ satisfying $f(x)=y$, then $f:A\cup\{x\}\rightarrow Y$ is continuous at $x$.
\item For every $V\in\Nbh_Y(y)$, there exists $U\in\Nbh_X(x)$ such that for every $p\in U\cap A$, we have $f(p)\in V$.
\item For every net $(x_\alpha)_{\alpha\in I}$ in $A$ converging to $x$, we have $\lim_{\alpha\in I} f(x_\alpha)=y$.
\end{enumerate}
When $X,Y$ are metric spaces, the above three conditions and the following two are equivalent:
\begin{itemize}
\item[(2m)] For every $\eps>0$, there exists $\delta>0$ such that for every $p\in A$, if $d(p,x)<\delta$ then $d(f(p),y)<\eps$.
\item[(3m)] For every sequence $(x_n)_{n\in\Zbb_+}$ in $A$ converging to $x$, we have $\lim_{n\rightarrow\infty}f(x_n)=y$.
\end{itemize}
\end{df}


Recall that by the definition of subspace topology, we have
\begin{align}
\Nbh_A(x)=\{U\cap A:A\in\Nbh_X(x)\}  \label{eq104}
\end{align}

\begin{proof}[\textbf{Proof of equivalence}]
Extend $f$ to $\wtd f:A\cup\{x\}\rightarrow Y$ by setting $\wtd f(x)=y$. Then by Def. \ref{lb188}-(2), condition (1) of Def. \ref{lb197} means that for every $V\in\Nbh_Y(y)$ there is a neighborhood of $x$ in $A\cup\{x\}$ (which, by \eqref{eq104}, must be of the form $U\cap (A\cup\{x\})$ where $U\in\Nbh_X(x)$) such that for every $p\in U\cap (A\cup\{x\})$ we have $\wtd f(p)\in V$. This is clearly equivalent to (2), since $\wtd f(x)=y\in V$. The equivalence (2)$\Leftrightarrow$(3) can be proved in a similar way as the equivalence of (1) and (2) in Def. \ref{lb188}. We leave the details to the readers. When $X,Y$ are metric spaces, (2) is clearly equivalent to (2m). The equivalence (2m)$\Leftrightarrow$(3m) can be proved in a similar way as the equivalence of (1) and (2) in Def. \ref{lb31}.
\end{proof}






The following remarks show that the limit of a function at a point is the limit of a single net, rather than the limit of many nets (as in Def. \ref{lb197}-(3)).

\begin{rem}\label{lb275}
Assume the setting of Def. \ref{lb197}. In the same spirit of Exe. \ref{lb198}, define a directed set $(\Pnbh_A(x),\leq)$ where
\begin{gather}
\begin{gathered}
\Pnbh_A(x)=\big\{(p,U):U\in\Nbh_X(x),p\in U\cap A  \big\}\\[0.5ex]
(p,U)\leq(p',U')\qquad\Longleftrightarrow\qquad U\supset U'
\end{gathered}
\end{gather}
(That it is a directed set is due to $x\in \ovl A$.) We have seen this directed set in Rem. \ref{lb270}. Then $(p)_{(p,U)\in\Pnbh_A(x)}$ is a net converging to $x$, and
\begin{align}
\lim_{p\rightarrow x} f(p)=\lim_{(p,U)\in\Pnbh_A(x)}f(p)
\end{align}
where the convergence of the LHS is equivalent to that of the RHS.
\end{rem}

\begin{rem}\label{lb269}
In the setting of Def. \ref{lb197}, assume moreover that $X$ is a metric space, then $\lim_{p\rightarrow x}f(p)$ can be described by the limit of a simpler net. Define a directed set $(A_x,\leq)$ where
\begin{gather}
\begin{gathered}
A_x=A\text{ as sets}\\[0.5ex]
p\leq p'\qquad\Longleftrightarrow \qquad d(p',x)\geq d(p,x)
\end{gathered}
\end{gather}
Then $(p)_{p\in A_x}$ is a net in $A$ converging to $x$, and
\begin{align}
\lim_{p\rightarrow x} f(p)=\lim_{p\in A_x}f(p)
\end{align}
where the convergence of the LHS is equivalent to that of the RHS.
\end{rem}


\begin{rem}\label{lb318}
Thanks to the above two remarks, limits of functions enjoy all the properties that limits of nets enjoy. For example, they satisfy Squeeze theorem; if $f:A\rightarrow \Fbb$ and $g:A\rightarrow V$ (where $V$ is a normed vector space over $\Fbb\in\{\Rbb,\Cbb\}$), if $x\in\ovl A\setminus A$,  and if $\lambda=\lim_{p\rightarrow x}f(p)$ and $v=\lim_{p\rightarrow x}g(p)$ exist, then $\lim_{p\rightarrow x}f(p)g(p)$ converges to $\lambda v$. 

Of course, you can also conclude them by using Def. \ref{lb197}-(3) instead of Rem. \ref{lb275}. In practice, it makes no difference whether you view $\lim_{p\rightarrow x}f(p)$ as the limit of $f(x_\alpha)$ for an arbitrary net $x_\alpha$ in $A$ converging to $x$, or whether you view  $\lim_{p\rightarrow x}f(p)$ as the limit of the particular net in Rem. \ref{lb275} or Rem. \ref{lb269}. The explicit constructions of nets in these two remarks are not important for proving results about limits of functions.   \hfill\qedsymbol
\end{rem}




\begin{rem}\label{lb202}
Let $f:A\rightarrow Y$, and let $x\in\ovl A\setminus A$. Suppose that $Y$ is Hausdorff. Assume that there exist two nets $(x_\alpha)_{\alpha\in I}$ and $(y_\beta)_{\beta\in J}$ in $A$ converging to $x$ such that $(f(x_\alpha))$ and $(f(y_\beta))$ converge to two different values. Then by Def. \ref{lb197}-(2), the limit $\lim_{p\rightarrow x}f(p)$ does not exist.
\end{rem}


The above remark gives a useful criterion for the non-convergence of limits of functions. The following proposition, on the other hand, gives a method of computing limits of functions by decomposing the domain into (non-necessarily mutually disjoint) subsets.


\begin{pp}\label{lb199}
Assume the setting of Def. \ref{lb197}. Assume that $A=A_1\cup\cdots\cup A_N$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item We have $\dps \lim_{p\rightarrow x} f(p)=y$.
\item For every $1\leq i\leq N$ such that $x\in\ovl {A_i}$, we have $\dps\lim_{p\rightarrow x} f|_{A_i}(p)=y$
\end{enumerate}
\end{pp}

\begin{proof}
(1)$\Rightarrow$(2): Assume (1). Extend $f:A\rightarrow Y$ to a function $\wtd f:A\cup\{x\}\rightarrow Y$ by setting $\wtd f(x)=y$. Then $\wtd f$ is continuous by (1). Thus, if $x\in\ovl{A_i}$, then $\wtd f|_{A_i\cup\{x\}}$ is continuous. This proves (2).

(2)$\Rightarrow$(1): Assume (2). Choose any $V\in\Nbh_Y(y)$. By (2), for each $i$, either $x\in\ovl{A_i}$ so that there is $U_i\in\Nbh_X(x)$ satisfying $U_i\cap A_i\subset f^{-1}(V)$ (recall \eqref{eq104}), or that $x\notin\ovl{A_i}$ so that there is $U_i\in\Nbh_X(x)$ disjoint from $A_i$. In either case, we have $U_i\cap A_i\subset f^{-1}(V)$. Let $U=U_1\cap\cdots\cap U_N$. Then
\begin{align*}
U\cap A=U\cap(A_1\cup\cdots\cup A_N)=\bigcup_i U\cap A_i\subset\bigcup_i U_i\cap A_i
\end{align*}
which is therefore a subset of $f^{-1}(V)$.

Another proof of $\neg$(1)$\Rightarrow$ $\neg$(2): Assume (1) is not true. Then there is a net $(x_\alpha)_{\alpha\in I}$ in $A$ converging to $x$ such that $f(x_\alpha)$ does not converge to $y$. So there exists $V\in\Nbh_Y(y)$ such that $f(x_\alpha)$ is not eventually in $V$, i.e., $f(x_\alpha)$ is frequently in $V^c$. Then $(f(x_\alpha))$ has a subnet $(f(x_{\beta}))_{\beta\in J}$ which is always in $V^c$. For example, take
\begin{align*}
J=\{\beta\in I:f(x_\beta)\in V^c\}
\end{align*}
Since $(x_\beta)_{\beta\in J}$ is always in $A$, by the logic \eqref{eq103}, there is $1\leq i\leq N$ such that $(x_\beta)$ is frequently in $A_i$. Thus, by the same argument as above, $(x_\beta)$ has a subnet $(x_\gamma)_{\gamma\in K}$ which is always in $A_i$. Since $x_\alpha\rightarrow x$, we have $x_\gamma\rightarrow x$, and hence $x\in\ovl{A_i}$. But $f(x_\gamma)\in V^c$. So we have found a net $(x_\gamma)$ in $A_i$ converging to $x$ such that $(f(x_\gamma))$ does not converge to $y$. This disproves (2).
\end{proof}








\begin{rem}
In many textbooks, $\lim_{p\rightarrow x}f(x)$ is also defined more generally when $x$ is an accumulation point of $A$, i.e., when $x\in\ovl{A\setminus\{x\}}$. In this case, the limit of $f$ at $x$ simply means \index{lim@$\lim_{p\rightarrow x}f(p)$}
\begin{align}\label{eq66}
\lim_{p\rightarrow x}f(p)
\xlongequal{\mathrm{def}} \lim_{p\rightarrow x}f|_{A\setminus\{x\}}(p)
\end{align}
This more general case is important in classical analysis, but is less useful in abstract analysis. (As a matter of fact, accumulation points are less convenient than closure points.) In order not to deviate too far from the traditional analysis textbooks, let's take a look at some examples.
\end{rem}


\begin{df}\label{lb200}
Let $A\subset\Rbb$ and $x\in\Rbb$. Let $f:A\rightarrow Y$ be a function. If $x$ is a closure point of $A\cap\Rbb_{<x}$ resp. $A\cap \Rbb_{>x}$, we define the \textbf{left limit} resp. \textbf{right limit} \index{00@Left and right limit} \index{lim@$\lim_{t\rightarrow x^-}$ and $\lim_{t\rightarrow x^+}$} to be
\begin{subequations}
\begin{gather}
\lim_{t\rightarrow x^-}f(t)=\lim_{t\rightarrow x}f|_{A\cap \Rbb_{<x}}(t)\\
\lim_{t\rightarrow x^+}f(t)=\lim_{t\rightarrow x}f|_{A\cap \Rbb_{>x}}(t)
\end{gather}
\end{subequations}
If $x\in\ovl\Rbb$ and $x$ is a closure point of $A\setminus \{x\}$, then $\lim_{t\rightarrow x}f(t)$ is understood by \eqref{eq66}.
\end{df}

\begin{rem}\label{lb201}
In Def. \ref{lb200}, if $x\in\Rbb$ is a closure point of $A\setminus\{x\}$, then by Prop. \ref{lb199},
\begin{align}
\lim_{p\rightarrow x}f(p)=y\qquad\Longleftrightarrow\qquad \lim_{p\rightarrow x^-}f(p)=\lim_{p\rightarrow x^+}f(p)=y
\end{align}
In particular, the existence of the limit on the LHS is equivalent to the existence and the equality of the two limits on the RHS.
\end{rem}



\begin{eg}
Let $g,h:\Rbb\rightarrow \Rbb$ be continuous functions. Let $c\in\Rbb$. Define $f:\Rbb\rightarrow\Rbb$ to be
\begin{align*}
f(x)=\left\{
\begin{array}{ll}
g(x)&\text{ if }x<0\\
c&\text{ if }x=0\\
h(x)&\text{ if }x>0\\
\end{array}
\right.
\end{align*}
Since $g|_{(-\infty,0]}$ is continuous, by Def. \ref{lb197}-(1) we have that $\lim_{x\rightarrow 0^-}f(x)=\lim_{x\rightarrow 0,t<0}g(t)=g(0)$. Similarly, $\lim_{x\rightarrow 0^+}f(x)=h(0)$. Therefore, by Rem. \ref{lb201}, $\lim_{x\rightarrow0}f(x)$ exists iff $g(0)=h(0)$, and it converges to $g(0)$ if $g(0)=h(0)$. The value $c$ is irrelevant to the limits.
\end{eg}


\begin{eg}
Let $f:X=\Rbb^2\setminus\{(0,0)\}\rightarrow \Rbb$ be $f(x,y)=\frac{x}{x+y}$. Then $(1/n,0)$ and $(0,1/n)$ are sequences in $X$ converging to $0$. But $f(1/n,0)=1$ and $f(0,1/n)=0$. So $\lim_{(x,y)\rightarrow(0,0)}f(x,y)$ does not exist by Rem. \ref{lb202}.
\end{eg}
















\subsection{Connected spaces}

Let $X$ be a topological space. In this section, we shall define a notion of connected space. Based on our usual geometric intuition, one might attempt to define a connected space as one satisfying that any two points can be linked by a path. Such spaces are actually called \textbf{path-connected spaces} and is stronger than connected spaces. In fact, connected spaces arise from the study of intermediate value problem.

\subsubsection{Connected $\Leftrightarrow$ IVP}

\begin{df}\label{lb212}
We say that the topological space $X$ is \textbf{connected} \index{00@Connected topological space} if $X$ can not be written as the disjoint of two nonempty open sets. Namely, if $X=U\sqcup V$ where $U,V$ are open subsets of $X$, then either $U=\emptyset$ (and hence $V=X$) or $V=\emptyset$ (and hence $U=X$).
\end{df}

Equivalently (by Thm. \ref{lb181}), $X$ is connected iff every $U\subset X$ which is both closed and open must be $\emptyset$ or $X$.


\begin{df}
We say that $X$ satisfies the \textbf{intermediate value property} (abbreviated to \textbf{IVP}) \index{00@IVP=Intermediate value property} if for every continuous function $f:X\rightarrow\Rbb$ and every $x,y\in X$ we have
\begin{align}
f(x)<f(y)\qquad\Longrightarrow\qquad [f(x),f(y)]\subset f(X)  \label{eq67}
\end{align}
\end{df}


\begin{thm}\label{lb206}
$X$ is connected iff $X$ satisfies IVP. Moreover, if $X$ is not connected, then there is a continuous $f:X\rightarrow\Rbb$ such that $f(X)=\{0,1\}$.
\end{thm}

\begin{proof}
First, assume that $X$ does not satisfy IVP. Choose a continuous $f:X\rightarrow\Rbb$ with real numbers $a<b<c$ such that $a,c\in f(X)$ but $b\notin f(X)$. So $U=f^{-1}(-\infty,b)$ and $V=f^{-1}(b,+\infty)$ are disjoint non-empty open subsets of $X$, and $X=U\sqcup V$. They are open, because $f$ is continuous (cf. Prop. \ref{lb191}). So $X$ is not connected.

Next, assume that $X$ is not connected. Then $X=U\sqcup V$ where $U,V$ are open subsets of $X$. Define $f:X\rightarrow\Rbb$ to be constantly $0$ on $U$ and constantly $1$ on $V$. It is easy to check that $f$ is continuous. (See also Rem. \ref{lb205}.) That $f(X)=\{0,1\}$ means that $X$ does not satisfy IVP.
\end{proof}



We now give a couple of elementary examples.


\subsubsection{Connected subsets of $\ovl\Rbb$ are precisely intervals}


\begin{pp}\label{lb207}
Let $A$ be a dense subset of $X$. Assume that $A$ is connected. Then $X$ is connected.
\end{pp}

\begin{proof}
If $X=\ovl A$ is not connected, then by Thm. \ref{lb206}, there exists a continuous surjection $f:\ovl A\rightarrow\{0,1\}$. By Prop. \ref{lb196}, $\ovl{f(A)}$ contains $f(\ovl A)$. So $f(A)$ has closure $\{0,1\}$. So $f(A)=\{0,1\}$. $A$ does not satisfy IVP, and hence is not connected.
\end{proof}









\begin{thm}\label{lb211}
Let $A$ be a nonempty subset of $\ovl\Rbb$. Then $A$ is connected iff $A$ is an interval.
\end{thm}

\begin{proof}
Step 1. Suppose that $A$ is connected. Let $a=\inf A$ and $b=\sup B$. To show that $A$ is one of $(a,b)$, $(a,b]$, $[a,b)$, $[a,b]$, it suffices to show that every $c\in (a,b)$ belongs to $A$. Suppose that some $c\in (a,b)$ does not belong to $A$. Then $A$ is the disjoint union of two nonempty open subsets $A\cap[-\infty,c)$ and $A\cap (c,+\infty]$, impossible.\\[-1ex]


Step 2. Every single point is clearly connected. Since every interval containing at least two points is homeomorphic to one of $[0,1]$, $(0,1]$, $[0,1)$, $(0,1)$, it suffices to prove that these four intervals are connected. Since $(0,1)$ is dense in the other three intervals, by Prop. \ref{lb207}, it suffices to prove that $(0,1)$ is connected.

Suppose that $(0,1)$ is not connected. Then $(0,1)=U\sqcup V$ where $U,V$ are disjoint open nonempty subsets. Choose $x_1\in U$ and $y_1\in V$, and assume WLOG that $x_1<y_1$. In the following, we construct an increasing sequence $(x_n)$ in $U$ and a decreasing one $(y_n)$ in $V$ satisfying $x_n<y_n$ for all $n$ by induction.  Suppose $x_n,y_n$ has been constructed. Let $z_n=(x_n+y_n)/2$. 
\begin{itemize}
\item If $z_n\in U$, then let $x_{n+1}=z_n$ and $y_{n+1}=y_n$.
\item If $z_n\in V$, then let $x_{n+1}=x_n$ and $y_{n+1}=z_n$.
\end{itemize}
Then $y_n-x_n$ converges to $0$. So $x_n$ and $y_n$ converge to the same point $\xi\in\Rbb$. We have $\xi\in(0,1)$ since $x_1<\xi<y_1$. Since $V$ is open, $U=(0,1)\setminus V$ is closed in $(0,1)$ by Thm. \ref{lb181}. So $\xi\in\Cl_{(0,1)}(U)=U$. Similarly, $\xi\in\Cl_{(0,1)}(V)=V$. This is impossible.
\end{proof}







\subsubsection{More examples of connected spaces}


\begin{df}
Let $x,y\in X$. A \textbf{path} \index{00@Path in a topological space} in $X$ from $x$ to $y$ is defined to be a continuous map $\gamma:[a,b]\rightarrow X$ where $-\infty<a<b<+\infty$, such that $\gamma(a)=x$ and $\gamma(b)=y$. Unless otherwise stated, we take $[a,b]$ to be $[0,1]$. We call $x$ and $y$ respectively the \textbf{initial point} and the \textbf{terminal point} of $\gamma$. 
\end{df}

\begin{df}
We say that $X$ is \textbf{path-connected} \index{00@Path-connected space} if for every $x,y\in X$ there is a path in $X$ from $x$ to $y$.
\end{df}


\begin{eg}
$\Rbb^N$ is path-connected. $B_{\Rbb^N}(0,R)$ and $\ovl B_{\Rbb^N}(0,R)$ (where $R<+\infty$) are path connected. $\{x\in \Rbb^N:r<x<R\}$ (where $0\leq r<R<+\infty$) are path connected. The region enclosed by a triangle is a connected subset of $\Rbb^2$. $[0,1]^N$ is connected.
\end{eg}

\begin{thm}\label{lb220}
Assume that $X$ is path-connected. Then $X$ is connected.
\end{thm}

\begin{proof}
If $X$ is not connected, then $X=U\sqcup V$ where $U,V$ are nonempty open subsets of $X$. Since $X$ is path-connected, there is a path $\gamma$ from a point of $U$ to a point of $V$. So $[0,1]=\gamma^{-1}(U)\sqcup\gamma^{-1}(V)$ where $\gamma^{-1}(U),\gamma^{-1}(V)$ are open (by Prop. \ref{lb191}) and nonempty. This contradicts the fact that $[0,1]$ is connected (cf. Thm. \ref{lb211}).
\end{proof}



\begin{pp}\label{lb210}
Let $f:X\rightarrow Y$ be a continuous map of topological spaces. Suppose that $X$ is connected. Then $f(X)$ is connected.
\end{pp}

\begin{proof}
By replacing $f:X\rightarrow Y$ by the restricted continuous map $f:X\rightarrow f(X)$, it suffices to assume $Y=f(X)$. If $Y$ is not connected, then $Y=U\sqcup V$ where $U,V$ are open and nonempty. Then $X=f^{-1}(U)\sqcup f^{-1}(V)$ are open (by Prop. \ref{lb191}) and nonempty subsets of $X$. So $X$ is not connected, impossible. (One can also use Thm. \ref{lb206} to prove that $f(X)$ is connected.)
\end{proof}

\begin{rem}
When $Y=\Rbb$, Prop. \ref{lb210} and Thm. \ref{lb211} imply that $f(X)$ is an interval. So $f$ satisfies \eqref{eq67}. Therefore, Prop. \ref{lb210} can be viewed as a generalization of IVP for connected spaces.
\end{rem}



\begin{co}\label{lb213}
Let $I$ be an interval, and let $f:I\rightarrow\ovl\Rbb$ be a strictly increasing continuous map. Then $J=f(I)$ is an interval, and the restriction $f:I\rightarrow J$ is a homeomorphism.
\end{co}


\begin{proof}
By Thm. \ref{lb211} and Prop. \ref{lb210}, $J$ is connected and hence is an interval. Therefore, by Thm. \ref{lb65}, $f$ is a homeomorphism.
\end{proof}


\begin{seg}
Not all connected spaces are path-connected. Let $f:(0,1]\rightarrow\Rbb^2$ be defined by $f(x)=(x,\sin(x^{-1}))$. Then the range $f((0,1])$ is connected by Prop. \ref{lb210}. Since $f((0,1])$ is a dense subset of $X=f((0,1])\cup\{(0,0)\}$, by Prop. \ref{lb207}, $X$ is connected. However, it can be checked that $X$ is not path-connected. (Prove it yourself, or see \cite[Sec. 24]{Mun}.) $X$ is called the \textbf{topologist's sine curve}.
\end{seg}




The following proposition can be used to decompose (for example) an open subset of $\Rbb^N$ into open connected subsets. (See Pb. \ref{lb221}.)


\begin{pp}\label{lb208}
Assume that $X=\bigcup_{\alpha\in\scr A}X_\alpha$ where each $X_\alpha$ is connected. Assume that $\bigcap_{\alpha\in\scr A}X_\alpha\neq\emptyset$. Then $X$ is connected.
\end{pp}

\begin{proof}
Suppose that $X$ is not connected. By Thm. \ref{lb206}, there is a continuous surjection $f:X\rightarrow\{0,1\}$. Let $p\in\bigcap_\alpha X_\alpha$. Then $f(p)$ is $0$ or $1$. Assume WLOG that $f(p)=0$. Choose $x\in X$ such that $f(x)=1$. Choose $\alpha$ such that $x\in X_\alpha$. Then $f|_{X_\alpha}:X_\alpha\rightarrow\{0,1\}$ is a continuous surjection. So $X_\alpha$ does not satisfy IVP, and hence is not connected.
\end{proof}

\begin{exe}
Prove a path-connected version of Prop. \ref{lb208}.
\end{exe}

\begin{exe}
Prove Prop. \ref{lb207} and \ref{lb208} directly using Def. \ref{lb212} (but not using IVP).
\end{exe}



%% Record #9 2023/10/18 three lectures  22





\subsection{Rigorous constructions of $\sqrt[n]{x}$, $\log x$, and $a^x$}\label{lb219}


With the help of Cor. \ref{lb213}, one can construct a lot of well-known functions rigorously. 

%Our first construction is $\sqrt[n]{x}$. This expression was used in Sec. \ref{lb218} to prove root test and hence to construct $e^x$. Therefore, $\sqrt[n]{x}$ will also be needed in the construction of its generalization $a^x=e^{x\log a}$. Thus, we shall construct $\sqrt[n]{x}$ before we construct $a^x$.


\begin{eg}\label{lb216}
Let $f:\Rbb_{\geq0}\rightarrow\Rbb_{\geq0}$ be $f(x)=x^n$ where $n\in\Zbb_+$. Then by Cor. \ref{lb213}, $J=f(\Rbb_{\geq 0})$ is an interval. Clearly $J\subset[0,+\infty)$. Since $0\in J$ and $\sup J=+\infty$, we have $J=[0,+\infty)$. Therefore $f$ is a homeomorphism. Its inverse function is a homeomorphism: the \textbf{$n$-th root} function
\begin{gather*}
\sqrt[n]{~}:\Rbb_{\geq0}\rightarrow\Rbb_{\geq0} \qquad x\mapsto\sqrt[n]x
\end{gather*}
This gives the rigorous construction of $\sqrt[n]{x}$.
\end{eg}

A similar method gives the rigorous construction of $\log$. 
\begin{eg}\label{lb217}
By Exp. \ref{lb214}, the exponential function $\exp:\Rbb\rightarrow\Rbb$ is continuous. We claim that $\exp$ is a strictly increasing homeomorphism from $\Rbb$ to $\Rbb_{>0}$. Its inverse function is called the \textbf{logarithmic function} \index{log@$\log$} 
\begin{align*}
\log:\Rbb_{>0}\rightarrow\Rbb \qquad x\mapsto \log x
\end{align*}
\end{eg}

\begin{proof}
From $e^x=\sum_{n=0}^\infty x^n/n!$ we clearly have $e^0=1$ and $e^x>1$ if $x>0$. From $e^{x+y}=e^xe^y$ proved in Cor. \ref{lb215}, we have $e^xe^{-x}=e^0=1$, which shows that $e^x\in\Rbb_{>0}$ for all $x\in\Rbb$. If $x<y$, then $e^y>e^x>0$ since $e^y=e^{y-x}e^x$ and $e^{y-x}>1$. So $\exp$ is strictly increasing. Thus, by Cor. \ref{lb213}, $\exp$ is a homeomorphism from $\Rbb$ to $J=\exp(\Rbb)$, and $J$ is an interval. 

When $x\geq 0$, we have $e^x\geq x$ from the definition of $e^x$. So $\sup_{x\geq 0}e^x=+\infty$. When $x\leq 0$, since $e^xe^{-x}=1$, we have $\inf_{x\leq 0}e^x=1/\sup_{x\geq 0}e^x=0$. So $\sup J=+\infty$ and $\inf J=0$. Since $0\notin\exp(\Rbb)$ (if $e^x=0$, then $1=e^xe^{-x}=0$, impossible), we have $J=\Rbb_{>0}$.
\end{proof}

\begin{eg}
Let $a\in\Rbb_{>0}$. For each $z\in\Cbb$, define
\begin{align}
a^z=e^{z\log a}
\end{align}
By Exp. \ref{lb217}, if $a>1$ (resp. $0<a<1$), the map
\begin{align}
\Rbb\rightarrow\Rbb_{>0}\qquad x\mapsto a^x
\end{align}
is an increasing (resp. decreasing) homeomorphism, since it is the composition of the increasing (resp. decreasing) homeomorphism $x\in\Rbb\mapsto x\log a\in\Rbb$ and the increasing one $\exp:\Rbb\rightarrow\Rbb_{>0}$. By the proof of Exp. \ref{lb217}, we have 
\begin{align}
a^0=1\qquad a^xa^{-x}=1\qquad a^xa^y=a^{x+y}
\end{align}
And clearly
\begin{align}
a^1=a.
\end{align}
It follows that for every $n\in\Zbb_+$, $a^n=a^{1+\cdots+1}=a\cdots a$. Namely, $a^n=e^{n\log a}$ agrees with the usual understanding of $a^n$. Thus, since $(a^{1/n})^n$ equals $a^{1/n}\cdots a^{1/n}=a^{1/n+\cdots+1/n}=a^1=a$, we conclude
\begin{align*}
a^{\frac 1n}=\sqrt[n]{a}
\end{align*}
\end{eg}



\begin{eg}
By Exp. \ref{lb217}, if $p>0$ (resp. $p<0$), then
\begin{align}
\Rbb_{>0}\rightarrow\Rbb_{> 0} \qquad x\mapsto x^p=e^{p\log x}\label{eq102}
\end{align}
is an increasing (resp. decreasing) homeomorphism, since it is the composition of the increasing (resp. decreasing)  homeomorphism $x\in\Rbb_{>0}\rightarrow p\log x\in\Rbb$ and the increasing homeomorphism $\exp:\Rbb\rightarrow\Rbb_{>0}$. 
\end{eg}







\subsection{Problems and supplementary material}


Let $X$ and $Y$ be topological spaces.

\subsubsection{Open sets, closed sets, closures}



\begin{prob}
Let $A,B\in X$. Let $(A_\alpha)_{\alpha\in \scr A}$ be a family of subsets of $X$. Prove that
\begin{subequations}
\begin{gather}
\ovl{A\cup B}=\ovl A\cup \ovl B\label{eq73}\\
\ovl{\bigcap_{\alpha\in \scr A}A_\alpha}\subset\bigcap_{\alpha\in\scr A}\ovl{A_\alpha}\label{eq74}
\end{gather}
\end{subequations}
\end{prob}

The following problem is crucial to the study of compactness. (See Sec. \ref{lb254} for instance.)

\begin{prob}\label{lb223}
Let $(x_\alpha)_{\alpha\in I}$ be a net in $X$. Let $x\in X$. Prove that the following statements are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $(x_\alpha)_{\alpha\in I}$ has a subnet converging to $x$.
\item For every neighborhood $U$ of $x$, we have that $x_\alpha$ is frequently in $U$.
\item $x$ belongs to $\dps\bigcap_{\alpha\in I}\ovl{\{x_\beta:\beta\geq\alpha\}}$.
\end{enumerate}
Any $x\in X$ satisfying one of these three conditions is called a \textbf{cluster point} \index{00@Cluster point of a net} of $(x_\alpha)_{\alpha\in I}$. (Compare Pb. \ref{lb64}.)
\end{prob}

\begin{proof}[Hint]
(2)$\Leftrightarrow$(3) is a direct translation. Assume (2). To prove (1), show that $(J,\leq)$ is a directed set, where
\begin{subequations}
\begin{gather}
\begin{gathered}
J=\{(\alpha,U)\in I\times\Nbh_X(x):x_\alpha\in U \}\\[0.5ex]
(\alpha,U)\leq (\alpha',U')\qquad\Longleftrightarrow\qquad \alpha\leq \alpha'\text{ and }U\supset U'
\end{gathered}
\end{gather}
Prove that $(x_\mu)_{\mu\in J}$ is a subnet of $(x_\alpha)_{\alpha\in I}$ if for each $(\alpha,U)\in J$ we set
\begin{align}
x_{(\alpha,U)}=x_\alpha
\end{align}
\end{subequations}
(Namely, the increasing map $J\rightarrow I$ is defined to be $(\alpha,U)\mapsto \alpha$.) Prove that $(x_\mu)_{\mu\in J}$ converges to $x$. You should point out where (2) is used in your proofs.
\end{proof}





\begin{rem}\label{lb238}
By Pb. \ref{lb223}-(3), the set of cluster points of $(x_\alpha)$ is a closed subset, since intersections of closed subsets are closed (cf. Cor. \ref{lb186}, or by \eqref{eq74}).
\end{rem}

For the reader's convenience, we present below the sequential version of Pb. \ref{lb223}. ``(1)$\Leftrightarrow$(2)" is due to Pb. \ref{lb64}. ``(2)$\Leftrightarrow$(3)" is due to Pb. \ref{lb223}.

\begin{pp}\label{lb256}
Assume that $X$ is a metric space. Let $(x_n)_{n\in\Zbb_+}$ be a sequence in $X$. Let $x\in X$. The following statements are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $(x_n)_{n\in\Zbb_+}$ has a subsequence converging to $x$.
\item For every neighborhood $U$ of $x$, we have that $x_n$ is frequently in $U$.
\item $x$ belongs to $\dps\bigcap_{n\in\Zbb_+}\ovl{\{x_k:k\geq n\}}$.
\end{enumerate}
Any $x\in X$ satisfying one of these three conditions is called a \textbf{cluster point} of  $(x_n)_{n\in\Zbb_+}$.
\end{pp} 

Prop. \ref{lb256}-(3) should remind you of the definitions of $\limsup$ and $\liminf$.



\begin{srem}\label{lb263}
It is not hard to show that \textit{the (1,2,3) of Prop. \ref{lb256} are equivalent in the more general case that $X$ is a first countable topological space} (see below for the definition). The proof is similar to that for metric spaces, and is left to the readers as an exercise.
\end{srem}

\begin{df}\label{lb262}
Let $X$ be a topological space. A subset $\mc B_x$ of $\Nbh_X(x)$ is called a \textbf{neighborhood basis} \index{00@Neighborhood basis} of $x$, if for every $U\in\Nbh_X(x)$ there exists $V\in\mc B_x$ such that $V\subset U$. We say that $X$ \textbf{first countable} \index{00@First countable} if every point $x$ has a neighborhood basis $\mc B_x$ which is a countable set.  
\end{df}


\begin{eg}
If $X$ is a metric space, then $X$ is first countable, since for every $x\in X$, $\{B_X(x,1/n):n\in\Zbb_+\}$ is a neighborhood basis of $x$.
\end{eg}

\begin{rem}\label{lb267}
By Pb. \ref{lb223}-(2) and Prop. \ref{lb256}-(2), if $(x_n)$ is a sequence in a metric space $X$, then $(x_n)$ has a subsequence converging to $x\in X$ iff $(x_n)$ has a subnet converging to $x$. This is not necessarily true when $X$ is a general topological space. Note that in the general case, cluster points of a sequence $(x_n)$ mean cluster points of $(x_n)$ as a net. Thus, they are not the limits of convergent \textit{subsequences} of $(x_n)$. 
\end{rem}


\begin{comment}
\begin{rem}
Although the word ``cluster point" is also used for a subset $A$ of $X$ (cf. Rem. \ref{lb176}), it is not parallel to the notion of cluster points of net. The true analogous concept of ``cluster points of a net" is ``closure points of a subset".
\end{rem}
\end{comment}



\begin{prob}\label{lb251}
Assume that $X$ is a metric space. Let $E\subset X$. Recall that $d(x,E)=d(E,x)=\inf_{e\in E}d(e,x)$. Prove that
\begin{align}
\{x\in X:d(x,E)=0\}=\ovl E
\end{align}
\end{prob}





\begin{rem}\label{lb257}
If $E,F$ are disjoint subsets of a metric space $X$, a continuous function $f:X\rightarrow[0,1]$ is called an \textbf{Urysohn function} \index{00@Urysohn function of a metric space} with respect to $E,F$, if
\begin{align*}
f^{-1}(1)=E\qquad f^{-1}(0)=F
\end{align*} 
For example, it is easy to check that
\begin{gather}
f:X\rightarrow[0,1]\qquad f(x)=\frac{d(x,F)}{d(x,E)+d(x,F)}
\end{gather}
is an Urysohn function.
\end{rem}


\begin{sprob}
A topological space $X$ is called \textbf{normal} \index{00@Normal topological space} if for every closed disjoint $E,F\subset X$, there exist disjoint open subsets $U,V\subset X$ such that $E\subset U$ and $F\subset V$. 
\begin{enumerate}
\item Prove that $X$ is normal iff for each $E\subset W\subset X$ where $E$ is closed and $W$ is open, there exists an open subset $U\subset X$ such that $E\subset U\subset \ovl U\subset W$.
\item Prove that if $X$ is metrizable, then $X$ is normal.
\end{enumerate}
\end{sprob}








\subsubsection{Continuous maps}


\begin{exe}\label{lb184}
Let $f:X\rightarrow Y$ be a map.
\begin{enumerate}
\item Suppose that $F$ is a subset of $Y$ containing $f(X)$. Show that $f:X\rightarrow Y$ is continuous iff $f:X\rightarrow F$ is continuous.
\item (Local to global principle) Suppose that $X=\bigcup_{\alpha\in I}U_\alpha$ where each $U_\alpha$ is an open subset of $X$, Prove that $f$ is continuous iff $f|_{U_\alpha}:U_\alpha\rightarrow Y$ is continuous for every $\alpha$. 
\end{enumerate}
\end{exe}

\begin{rem}\label{lb205}
The above local-to-global principle for continuous functions can be rephrased in the following way. Suppose that $X=\bigcup_{\alpha\in I}U_\alpha$ where each $U_\alpha$ is an open subset of $X$. Suppose that for each $\alpha$ we have a continuous map $f_\alpha:U_\alpha\rightarrow Y$. Assume that for each $\alpha,\beta\in I$ we have
\begin{align*}
f_\alpha|_{U_\alpha\cap U_\beta}=f_\beta|_{U_\alpha\cap U_\beta}
\end{align*}
Then there is a (necessarily unique) continuous function $f:X\rightarrow Y$ such that $f|_{U_\alpha}=f_\alpha$ for every $\alpha$.
\end{rem}



\begin{prob}
Let $f:X\rightarrow Y$ be a map. Suppose that $X=A_1\cup\cdots \cup A_N$ where $N\in\Zbb_+$ and $A_1,\dots,A_N$ are closed subsets of $X$. Suppose that $f|_{A_i}:A_i\rightarrow Y$ is continuous for each $1\leq i\leq N$. Prove that $f$ is continuous. 

Does the conclusion remain true if $A_1,\dots,A_N$ are not assumed closed? If no, find a counterexample.  \hfill\qedsymbol
\end{prob}

\begin{proof}[Note]
Do not use Prop. \ref{lb199} in your proof. But you can think about how this problem is related to Prop. \ref{lb199}.
\end{proof}




\begin{df}\label{lb175}
Let $(I,\leq)$ be a directed set. Let $\infty_I$ (often abbreviated to $\infty$) be a new symbol not in $I$. Then
\begin{align*}
I^*=I\cup\{\infty_I\}
\end{align*}
is also a directed set if we extend the preorder $\leq$ of $I$ to $I^*$ by setting
\begin{align*}
\alpha\leq\infty_I\qquad(\forall\alpha\in I^*)
\end{align*}
For each $\alpha\in I$, let
\begin{align*}
I^*_{\geq\alpha}=\{\beta\in I^*:\beta\geq\alpha\}
\end{align*}
The \textbf{standard topology} \index{00@Topology of $\mc I^*=I\cup\{\infty_I\}$ where $I$ is an index set} on $I^*$ is defined to be the one induced by the basis
\begin{align}\label{eq63}
\mc B=\big\{\{\alpha\}:\alpha\in I \big\}\cup \big\{I^*_{\geq\alpha}:\alpha\in I \big\}
\end{align}
\end{df}



\begin{prob}\label{lb278}
Let $(I,\leq)$ be a directed set. Let $I^*$ be as in Def. \ref{lb175}.
\begin{enumerate}
\item Check that $\mc B$ (defined by \eqref{eq63}) is a basis for a topology. (Therefore, $\mc B$ generates a topology $\mc T$ on $I^*$.) 
\item Let $(x_\alpha)_{\alpha\in I}$ be a net in a topological space $X$. Let $x_\infty\in X$. (So we have a function $x:I^*\rightarrow X$.) Prove that 
\begin{align}
x\text{ is a continuous function}\qquad\Longleftrightarrow\qquad \lim_{\alpha\in I} x_\alpha=x_\infty
\end{align}
\item Is $I^*$ Hausdorff? Prove it, or find a counterexample.
\end{enumerate} 
\end{prob}




\subsubsection{Product spaces}













\begin{prob}
Prove Thm. \ref{lb192}.
\end{prob}


\begin{prob}\label{lb258}
Let $(X_\alpha)_{\alpha\in\scr A}$ and $(Y_\alpha)_{\alpha\in\scr A}$ be families of nonempty topological spaces. Let $Z$ be a nonempty topological space. For each $\alpha\in\scr A$, choose maps $f_\alpha:X_\alpha\rightarrow Y_\alpha$ and $g_\alpha:Z\rightarrow X_\alpha$. 
\begin{enumerate}
\item Use Thm. \ref{lb192} to prove that  \index{zz@$\prod_{\alpha\in\scr A} f_\alpha$}
\begin{align}
\prod_{\alpha\in\scr A}f_\alpha:\prod_\alpha X_\alpha\rightarrow\prod_\alpha Y_\alpha\qquad (x(\alpha))_{\alpha\in\scr A}\mapsto (f_\alpha(x(\alpha)))_{\alpha\in\scr A}
\end{align}
is continuous iff each $f_\alpha$ is continuous.
\item Use Thm. \ref{lb192} to prove that \index{zz@$\bigvee_{\alpha\in\scr A} f_\alpha$}
\begin{align}
\bigvee_{\alpha\in\scr A}g_\alpha:Z\rightarrow \prod_\alpha X_\alpha \qquad z\mapsto (g_\alpha(z))_{\alpha\in\scr A}
\end{align}
is continuous iff each $g_\alpha$ is continuous.
\end{enumerate}
\end{prob}







\begin{prob}\label{lb203}
Let $(X_\alpha)_{\alpha\in\scr A}$ be an uncountable family of metric spaces, where each $X_\alpha$ has at least two elements. Let $S=\prod_{\alpha\in\scr A}X_\alpha$ be the product space, equipped with the product topology. Prove that $S$ is not first countable (recall Def. \ref{lb262}), and hence not metrizable.
\end{prob}

\subsubsection{Limits of functions}

\begin{prob}
Prove the equivalence of (2) and (3) in Def. \ref{lb197}.
\end{prob}


\begin{prob}
Find $\dps\lim_{(x,y)\rightarrow(0,0)}f(x,y)$, or explain why it does not exist:
\begin{gather*}
f(x,y)=\frac{x^2-y^2}{x^2+y^2}\\
f(x,y)=\frac{(xy)^2}{(xy)^2+(x-y)^2}\\
f(x,y)=\frac{x^6y^2}{(x^4+y^2)^2}
\end{gather*}
\end{prob}








\subsubsection{Connectedness}

\begin{prob}
Assume that $X,Y$ are not empty. Prove that $X$ and $Y$ are connected iff $X\times Y$ is connected.
\end{prob}

\begin{proof}[Hint]
Write $X\times Y$ as a union of sets of the form $(X\times\{y\})\cup (\{x\}\times Y)$.
\end{proof}


\begin{df}
A topological space $X$ is called \textbf{locally connected} \index{00@Locally connected} if every $x\in X$ has a neighborhood basis $\mc B_x$ (recall Def. \ref{lb262}) whose members are all connected.
\end{df}

\begin{eg}
Every open subset of a locally connected space is clearly locally connected.
\end{eg}

\begin{eg}
$\Rbb^N$ is locally connected, since open balls are path-connected and hence connected (by Thm. \ref{lb220}). Therefore, every open subset of $\Rbb^N$ is locally connected.
\end{eg}


\begin{prob}\label{lb221}
Suppose that $X$ is locally connected. Prove that $X$ has a unique (disjoint) decomposition $X=\bigsqcup_{\alpha\in\scr A}X_\alpha$ where each $X_\alpha$ is a nonempty connected open subset of $X$. (Each $X_\alpha$ is called a \textbf{connected component} of $X$.) \index{00@Connected component}
\end{prob}

\begin{proof}[Hint]
For each $x\in X$, consider the union of all connected neighborhoods containing $x$.
\end{proof}




\newpage

\section{Compactness}\label{lb351}


\subsection{Overture: two flavors of compactness}

Let $X$ be a topological space.



\begin{df}
An \textbf{open cover} \index{00@Open cover} of $X$ means a family $\fk U=(U_\alpha)_{\alpha\in\scr A}$ of open subsets of $X$ such that $X=\bigcup_{\alpha\in\scr A} U_\alpha$. $\fk U$ is called \textbf{finite} resp. \textbf{countable} if $\scr A$ is finite resp. countable. A \textbf{subcover} \index{00@Subcover} of $\fk U$ means an open cover $\fk V=(V_\beta)_{\beta\in\scr B}$ of $X$ such that each $V_\beta$ equals $U_\alpha$ for some $\alpha\in\scr A$.
\end{df}

\begin{df}
We say that $X$ is a \textbf{compact space} \index{00@Compact space} if every open cover of $X$ has a finite subcover.  We say that $X$ is a \textbf{Lindel\"of space} \index{00@Lindel\"of space} if every open cover of $X$ has a countable subcover.
\end{df}

\begin{rem}
For the purpose of this chapter, it suffices to consider open covers $\fk U=(U_\alpha)_{\alpha\in\scr A}$ such that $\alpha\in\scr A\mapsto U_\alpha\in 2^X$ is injective. (Namely, one can throw away repeated open sets.) In this case, we write
\begin{align*}
\fk U\subset 2^X
\end{align*}
and view $\fk U$ as a subset of $2^X$. A \textbf{subcover} of such $\fk U$ is an open cover $\fk V$ such that $\fk V\subset\fk U$. The readers can check that this assumption does not affect the definition of compact spaces and Lindel\"of spaces.
\end{rem}

\begin{rem}\label{lb244}
Compactness can also be formulated in a relevant version: If $A$ is a compact subset of $X$, and if $\fk U$ is a set of open subsets of $X$ such that $A\subset\bigcup_{U\in\fk U}U$, then since $\{U\cap A:U\in\fk U\}$ gives an open cover of $A$, we know that
\begin{align*}
A\subset U_1\cup\cdots\cup U_n
\end{align*}
some $U_1,\dots,U_n\in\fk U$. Conversely, any $A$ satisfying such property is a compact subspace of $X$.
\end{rem}


\begin{exe}
Show that a finite union of compact spaces is compact. Show that a finite set is compact.
\end{exe}

\begin{exe}
Show that $X$ is compact iff $X$ satisfies the \textbf{finite intersection property}: \index{00@Finite intersection property} The intersection of a family of non-empty closed subsets is nonempty.
\end{exe}



A main goal of this chapter is to prove the following theorem. Its proof will be finished at \hyperlink{target1}{the end} of Sec. \ref{lb253}. In fact, the formal proof will only be in Sec. \ref{lb254} and Sec. \ref{lb253}. 

\begin{thm}\label{lb222}
Let $X$ be a metric space. Then $X$ is sequentially compact iff $X$ is compact.
\end{thm}
This is the first fundamental theorem we prove in this course. Its significance lies in the fact that it connects two seemingly very different notions of compactness, and hence two strikingly different intuitions. We hope that the reader can not only follow the logical chains of the proofs, but also understand the pictures behind the proof. More precisely, we hope that the readers can have an intuitive understanding of the following questions:
\begin{itemize}
\item Why are these two compactness both powerful in solving certain problems? What roles do they play in the proof? How are the roles played by these two compactness related? (This is more important than just knowing why these notions are \textit{logically} equivalent.)
\item Why is one version of compactness more powerful than the other one in solving certain problems?
\end{itemize}




Thus, I feel that it is better to look at some applications of compactness before we prove Thm. \ref{lb222} rigorously. For pedagogical purposes, we also introduce two related notions of compactness:

\begin{df}
We say that $X$ is \textbf{net-compact}, \index{00@Net-compact} if every net $(x_\alpha)_{\alpha\in I}$ in $X$ has at least one cluster point (recall Pb. \ref{lb223}), equivalently, at least one convergent subnet. We say that $X$ is \textbf{countably compact}, \index{00@Countably compact} if every countable open cover of $X$ has a finite subcover.
\end{df}



Sequential compactness and net-compactness clearly share the same intuition. Countable compactness is intuitively similar to compactness. Also, compactness clearly implies countable compactness. But net-compactness does not imply sequential compactness in general: In a net-compact space, every sequence has a convergent subnet, but not necessarily a convergent subsequence. 

The relationship between these four versions of compactness is as follows. (We will prove this in the course of proving Thm. \ref{lb222}.)
\begin{subequations}\label{eq68}
\begin{align}
&\text{Metric spaces:} & \text{all the four versions}&\text{ of compactness are equivalent}\\
&\text{Topological spaces:} & \text{net-compact}&\Longleftrightarrow\text{compact}
\end{align}
\end{subequations}
After proving \eqref{eq68}, we will not use the notions of countable compactness and net-compactness. This is because net-compactness is equivalent to compactness, and countable compactness is more difficult to use than compactness. (Nevertheless, proving/using compactness by proving/using the existence of cluster points is often helpful.)




\subsection{Act 1: case studies}\label{lb293}



\subsubsection{Extreme value theorem (EVT)}

\begin{lm}[\textbf{Extreme value theorem}]\label{lb224} \index{00@EVT=Extreme value theorem}
Let $X$ be a compact topological space. Let $f:X\rightarrow\Rbb$ be a continuous function. Then $f$ attains its maximum and minimum at points of $X$. In particular, $f(X)$ is a bounded subset of $\Rbb$.
\end{lm}



We have seen the sequential compactness version of \textbf{EVT} (extreme value theorem) in Lem. \ref{lb56}. There, we find the point $x\in X$ at which $f$ attains its maximum by first finding a sequence $(x_n)$ such that $f(x_n)$ converges to $\sup f(X)$. Then we choose any convergent subsequence $(x_{n_k})$, which converges to the desired point $x$. The same method can be used to prove EVT for net-compact spaces if we replace sequences by nets.


For compact spaces, EVT is proved in a completely different way. In fact, without the tools of sequences and nets, one can not easily find $x$ at which $f$ attains it maximum. The argument is rather indirect:

\begin{proof}[Proof of Lem. \ref{lb224}]
It suffices to prove that $f(X)$ is bounded for all continuous maps $f:X\rightarrow\Rbb$. Then $a=\sup f(X)$ is in $\Rbb$. If $a\notin f(X)$, we choose a homeomorphism $\varphi:(-\infty,a)\rightarrow\Rbb$. So $\varphi\circ f:X\rightarrow\Rbb$ is  continuous but has no upper bound. This is impossible.
\end{proof}

Thus, to prove EVT, it remains to prove:
\begin{eg}\label{lb227}
Assume that $X$ is compact and $f:X\rightarrow\Rbb$ is continuous. Then $f(X)$ is a bounded subset of $\Rbb$.
\end{eg}

\begin{proof}
For each $x\in X$, since $f$ is continuous at $x$, there exists $U_x\in\Nbh(x)$ such that $|f(p)-f(x)|<1$ for all $p\in U_x$. In particular, $f(U_x)$ is bounded. Since $X=\bigcup_{x\in X}U_x$ is an open cover of $X$, by compactness, $X=U_{x_1}\cup\cdots\cup U_{x_n}$ for some $x_1,\dots,x_n\in X$. Thus $X= f(U_{x_1})\cup\cdots\cup f(U_{x_n})$ is bounded.
\end{proof}



The above proof is typical. It suggests that compactness is powerful for \uline{proving finiteness properties} rather than finding solutions of functions satisfying certain requirements. Thus, if you want to prove a finiteness property using sequential or net-compactness, you have to prove it indirectly. For example, you need to prove by contradiction:

\begin{eg}
Assume that $X$ is net-compact or sequentially compact. Assume that $f:X\rightarrow\Rbb$ is continuous. Then $f(X)$ is a bounded subset of $\Rbb$.
\end{eg}


\begin{proof}
Assume that $X$ is net-compact. If $f(X)$ is not bounded above, then there is a sequence $(x_\alpha)$ in $X$ (viewed as a net)  such that $\lim_\alpha f(x_\alpha)=+\infty$. By net-compactness, $(x_\alpha)$ has a subnet $(x'_\beta)$ converging to $x\in X$. So $f(x)=\lim_\beta f(x'_\beta)=+\infty$, impossible. So $f$ is bounded above, and hence bounded below by a similar argument. The case where $X$ is sequentially compact can be proved by a similar method.
\end{proof}




Let us look at a more complicated example.













\subsubsection{Uniform convergence in multivariable functions}\label{lb271}


\begin{eg}\label{lb225}
Let $X,Y$ be topological spaces. Assume that $Y$ is compact. Let $V$ be a normed vector space. Choose $f\in C(X\times Y,V)$. For each $x\in X$, let
\begin{align*}
f_x:Y\rightarrow V\qquad y\mapsto f(x,y)
\end{align*}
Equip $C(Y,V)$ with the $l^\infty$-norm. Then the following map is continuous:
\begin{align}
\Phi(f):X\rightarrow C(Y,V)\qquad x\mapsto f_x
\end{align}
\end{eg}



\begin{rem}
Note that since $Y$ is compact, each $g\in C(Y,V)$ is bounded by EVT (applied to $|g|$). So $C(Y,V)\subset l^\infty(Y,V)$.
\end{rem}


The continuity of $\Phi(f)$ means that for each $x\in X$, the following statement holds:
\begin{itemize}
\item[\textleaf] For every $\eps>0$ there exists $U\in\Nbh_X(x)$ such that for all $p\in U$ and all $y\in Y$ we have $\Vert f(p,y)-f(x,y)\Vert<\eps$.
\end{itemize}
This is clearly a finiteness property. Thus, its sequentially compact version or net-compact version should be proved indirectly. Indeed, when $X,Y$ are metric spaces and $Y$ is sequentially compact, we have proved this in Pb. \ref{lb103} by contradiction. The same method (with sequences replaced by nets) also works for the net-compact case:

\begin{eg}
Example \ref{lb225} is true, assuming that $Y$ is net-compact rather than compact.
\end{eg}


\begin{proof}
Suppose ``\textleaf" is not true. Then there is $\eps>0$ such that for every $U\in\Nbh_X(x)$ there is $x_U\in U$ and $y_U\in Y$ such that $\Vert f(x_U,y_U)-f(x,y_U)\Vert\geq\eps$. Then $(x_\alpha)_{\alpha\in\Nbh_X(x)}$ is a net converging to $x$, where $x_\alpha=x_U$ if $\alpha=U$. Since $Y$ is net-compact, $(y_\alpha)$ has a subnet $(y_\beta)$ converging to  some $y\in Y$. Since the subnet $(x_\beta)$ also converges to $x$, we have $\lim_\beta f(x_\beta,y_\beta)=f(x,y)$ and $\lim_\beta f(x,y_\beta)=f(x,y)$ by the continuity of $f$. This contradicts the fact that $\Vert f(x_\beta,y_\beta)-f(x,y_\beta)\Vert\geq\eps$ for all $\beta$.
\end{proof}



On the other hand, the solution of Exp. \ref{lb225} using open covers is a direct proof:

\begin{proof}[\textbf{Proof of Exp. \ref{lb225}}]
``\textleaf" is a finiteness property global over $Y$. We prove ``\textleaf" by first proving it locally, and then passing to the global space $Y$ using the compactness of $Y$.

Fix $x\in X$. Choose any $\eps>0$. For each $y\in Y$, since $f$ is continuous at $(x,y)$, there is a neighborhood $W$ of $(x,y)$ such that for every $(p,q)\in W$ we have $\Vert f(p,q)-f(x,y)\Vert<\eps/2$. By Exp. \ref{lb226}, we can shrink $W$ to a smaller neighborhood of the form $U_y\times V_y$ where $U_y\in\Nbh_X(x)$ and $V_y\in\Nbh_Y(y)$. Then for each $p\in U_y$ and $q\in U_y$ we have $\Vert f(p,q)-f(x,y)\Vert<\eps/2$, and hence $\Vert f(p,q)-f(x,q)\Vert<\eps$. This proves the special case of ``\textleaf" where $Y$ is replaced by $U_y$.

Now we pass from local to global in the same way as in the proof of Exp. \ref{lb227}. Since $Y=\bigcup_{y\in Y}V_y$ is an open cover of $Y$, by the compactness of $Y$, there is a finite subset $F\subset Y$ such that $Y=\bigcup_{y\in F}V_y$. Then ``\textleaf" is true if we let $U=\bigcap_{y\in F}U_y$.
\end{proof}


\subsubsection{Conclusions}

\begin{enumerate}%[label=(\arabic*)]
\item Sequential compactness and net-compactness are useful for finding solutions of a function satisfying some given conditions. 
\item Compactness is useful for proving finiteness properties. The proof is usually a  local-to-global argument. It is usually a direct argument (rather than proof by contradiction).
\item If one uses sequential/net-compactness to prove a finiteness property, one usually proves it by contradiction: Assume that this finiteness is not true. Find a sequence/net $(x_\alpha)$ that violates this finiteness property, and pass to a convergent subsequence/subnet to find a contradiction.
\item Therefore, for sequential/net-compact spaces, the argument is in the direction of ``getting smaller and smaller", opposite to the argument for compact spaces.
\end{enumerate}

Let me emphasize that the proof for sequentially/net-compact spaces is opposite to the one for compact spaces in two aspects: (1) If one argument is direct, the other is a proof by contradiction for the same problem. (2) The former has the intuition of ``getting smaller", while the latter local-to-global argument has the intuition of ``getting larger". 

I have already touched on this phenomenon in Rem. \ref{lb229}: \textit{The reason that sequences and nets run in the opposite direction to that of open sets is because closed sets are opposite to open sets}, as proved in Thm. \ref{lb181}.  Thus, you can expect that the transition between closed and open sets plays a crucial role in the following proof of Thm. \ref{lb222}. 

\begin{comment}

Now, we have a more vivid sense of the following sentence: 
\begin{itemize}
\item[$\Sun/\Moon$] The seemingly simple fact that ``closed sets are the complements of open sets" is the golden key that allows you to walk between the worlds of dark and light.
\end{itemize}



Let me close this section with a final remark.

\begin{rem}
Logically equivalent concepts are usually not intuitively equivalent. Proving directly is sometimes more transparent and intuitive than proving by contradiction, and sometime vice versa. For example: You will never prove a property by contradiction unless you already know what should be proved, and what statement is expected true. However, a direct argument can be more helpful in finding an unknown property and setting the goal.
\end{rem}
\end{comment}


\subsection{Act 2: ``sequentially compact $\Leftrightarrow$ countably compact'' for metric spaces, just as ``net-compact $\Leftrightarrow$ compact''}\label{lb254}

\epigraph{The road up and the road down is one and the same.}{Heraclitus}


As mentioned before, our goal of this chapter is to prove ``sequentially compact $\Leftrightarrow$ compact" for metric spaces. Our strategy is as follows: We reformulate the sequential compactness condition in terms of decreasing chains of closed sets, and reformulate the compactness condition in terms of increasing chains of open sets. Then we relate these two pictures easily using Thm. \ref{lb181}.

The following difficulty arises when carrying out this strategy. Sequences are countable by nature, whereas open covers can have arbitrarily large cardinality. Thus, sequences are related to countable decreasing chains, and hence countable open covers. Therefore, the above idea only implies the equivalence
\begin{subequations}\label{eq69}
\begin{align}\label{eq70}
\text{sequentially compact}\quad\Longleftrightarrow\quad \text{countably compact}\qquad(\text{for metric spaces})
\end{align}
Accordingly, it will only imply
\begin{align}\label{eq71}
\text{net-compact}\qquad\Longleftrightarrow\qquad \text{compact}
\end{align}
\end{subequations}
since there are no constraints on the cardinalities of indexed sets of nets. We will prove \eqref{eq69} in this section, and leave the proof of ``countably compact $\Leftrightarrow$ compact" for metric spaces to Sec. \ref{lb253}.


Since the proofs of \eqref{eq70} and \eqref{eq71} are similar, we first discuss \eqref{eq71}.


\begin{pp}\label{lb230}
Let $X$ be a topological space. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is compact.
\item (\textbf{Increasing chain property}) If $(U_\mu)_{\mu\in I}$ is an increasing net of open subsets of $X$ satisfying $\bigcup_{\mu\in I}U_\mu=X$, then $U_\mu=X$ for some $\mu$.
\item (\textbf{Decreasing chain property}) If $(E_\mu)_{\mu\in I}$ is a decreasing net of nonempty closed subsets of $X$, then $\bigcap_{\mu\in I}E_\mu\neq\emptyset$.
\end{enumerate}
\end{pp}

Here, ``increasing net" means $U_\mu\subset U_\nu$ if $\mu\leq \nu$, and ``decreasing net" means the opposite.

\begin{proof}
(1)$\Rightarrow$(2): Assume (1). Then $X=\bigcup_\mu U_\mu$ is an open cover of $X$. So, by the compactness of $X$, we have $X=U_{\mu_1}\cup\cdots\cup U_{\mu_n}$ for some $\mu_1,\dots,\mu_n\in I$. Choose $\mu\in I$ which is $\geq \mu_1,\dots,\mu_n$. Then $X=U_\mu$.

(2)$\Rightarrow$(1): Assume (2). Let $X=\bigcup_{\alpha\in\scr A} W_\alpha$ be an open cover of $X$. Let $I=\fin(2^{\scr A})$. For each $\mu=\{\alpha_1,\dots,\alpha_n\}\in I$, let $U_\mu=W_{\alpha_1}\cup\cdots\cup W_{\alpha_n}$. Then $(U_\mu)_{\mu\in I}$ is an increasing net of open sets covering $X$. Thus, by (2), we have $U_\mu=X$ for some $\mu$. This proves (1).

(2)$\Leftrightarrow$(3): If we let $E_\mu=X\setminus U_\mu$, then (2) says that if $(E_\mu)$ is a decreasing net of closed sets whose intersection is $\emptyset$, then $E_\mu=\emptyset$ for some $\mu$. This is the contraposition of (3).
\end{proof}


We now relate decreasing chain property and cluster points of nets using \eqref{eq76}.




%% Record #10 2023/10/23 two lectures  24


\begin{thm}\label{lb232}
Let $X$ be a topological space. Then $X$ is net-compact iff $X$ is compact. 
\end{thm}


\begin{proof}
Assume that $X$ is net-compact. By Prop. \ref{lb230}, it suffices to prove that $X$ satisfies the decreasing chain property. Let $(E_\mu)_{\mu\in I}$ be a decreasing net of nonempty closed subsets of $X$. For each $\mu$ we choose $x_\mu\in E_\mu$, which gives a net $(x_\mu)_{\mu\in I}$ in $X$. The fact that $(E_\mu)$ is decreasing implies that $F_\mu\subset E_\mu$ if we set
\begin{subequations}\label{eq76}
\begin{align}
F_\mu=\{x_\nu:\nu\in I,\nu\geq\mu \}\label{eq72}
\end{align}
Thus, the closure $\ovl F_\mu$ is a subset of $E_\mu$ since $E_\mu$ is closed. It suffices to prove that $\bigcap_{\mu\in I}\ovl F_\mu\neq\emptyset$. By Pb. \ref{lb223}, 
\begin{align}
\bigcap_\mu \ovl F_\mu=\big\{\text{the cluster points of the net $(x_\mu)_{\mu\in I}$}\big\}
\end{align}
\end{subequations}
which is nonempty because $X$ is net-compact. This finishes the proof that $X$ is compact.

Now we assume that $X$ is compact. Let $(x_\mu)_{\mu\in I}$ be a net in $X$. Define $F_\mu$ by \eqref{eq72}. Then $(\ovl F_\mu)_{\mu\in I}$ is a decreasing net of nonempty closed subsets. So $\bigcap_\mu\ovl F_\mu$, the set of cluster points of $(x_\mu)_{\mu\in I}$, is nonempty by the decreasing chain property (cf. Prop. \ref{lb230}). So $X$ is net-compact.
\end{proof}



\begin{pp}\label{lb231}
Let $X$ be a topological space. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is countably compact.
\item (\textbf{Increasing chain property}) If $(U_n)_{n\in\Zbb_+}$ is an increasing sequence of open subsets of $X$ satisfying $\bigcup_{n\in\Zbb_+}U_n=X$, then $U_n=X$ for some $n$.
\item (\textbf{Decreasing chain property}) If $(E_n)_{n\in\Zbb_+}$ is a decreasing sequence of nonempty closed subsets of $X$, then $\bigcap_{n\in\Zbb_+}E_n\neq\emptyset$.
\end{enumerate}
\end{pp}

\begin{proof}
Similar to the proof of Prop. \ref{lb230}.
\end{proof}

\begin{exe}
Fill in the details of the proof of Prop. \ref{lb231}.
\end{exe}



\begin{lm}\label{lb247}
Let $X$ be a metric space. Then $X$ is sequentially compact iff $X$ is countably compact. 
\end{lm}

\begin{proof}
This lemma can be proved in a similar way as Thm. \ref{lb231}. The only difference is that one should use the sequential version of Pb. \ref{lb223}, namely, Prop. \ref{lb256}. Note that the ``(1)$\Leftrightarrow$(2)" of Prop. \ref{lb256} does not hold for general topological spaces. Thus, sequential compactness is not equivalent to countable compactness for general topological spaces. 
\end{proof}

\begin{srem}\label{lb264}
As pointed out in Rem. \ref{lb263}, Prop. \ref{lb256} holds more generally for first countable topological spaces (Def. \ref{lb262}). Therefore, Lem. \ref{lb247} also holds for such spaces:
\begin{gather}
\begin{gathered}
\text{sequentially compact}\quad\Longleftrightarrow\quad \text{countably compact}\\
(\text{for first countable spaces})
\end{gathered}
\end{gather}
\end{srem}



\subsection{Intermezzo: elementary properties about compactness}






\begin{pp}\label{lb233}
Suppose that $X\subset Y$ where $Y$ is a topological space. The following are true.
\begin{enumerate}
\item Assume that $Y$ is compact and $X$ is closed in $Y$. Then $X$ is compact.
\item Assume that $Y$ is Hausdorff and $X$ is compact, then $X$ is a closed subset of $Y$. 
\end{enumerate} 
\end{pp}

\begin{proof}
Part 1. Let $(x_\alpha)$ be a net in $X$. Since $Y$ is compact, $(x_\alpha)$ has a subnet $(x'_\beta)$ converging to some $p\in Y$. Since $(x'_\beta)$ is in $X$, we have $p\in \ovl X=X$. So $X$ is compact.

Part 2. To prove $\ovl X=X$, we choose any net $(x_\alpha)$ in $X$ converging to $p\in Y$ and show that $p\in X$. Indeed, since $X$ is compact, $(x_\alpha)$ has a subnet $(x_\beta')$ converging to some $x\in X$. Since $(x_\beta')$ also converges to $p$, we have $p=x$ because $Y$ is Hausdorff. So $p\in X$.
\end{proof}

I have mentioned that non-Hausdorff spaces are not often used in analysis. Thus, we mainly use the following special case of Prop. \ref{lb233}:


\begin{co}\label{lb234}
Let $Y$ be a Hausdorff space and $X\subset Y$. If $X$ is compact, then $X$ is closed in $Y$. If $X$ is closed in $Y$ and if $Y$ is compact, then $X$ is compact.
\end{co}

Recall that a similar property holds for complete metric spaces, cf. Prop. \ref{lb86}.







\begin{thm}\label{lb236}
Suppose that $f:X\rightarrow Y$ is a continuous map of topological spaces where $X$ is compact. Then $f(X)$ is compact. Moreover, if $f$ is injective and $X,Y$ are Hausdorff, then $f$ restricts to a homeomorphism $f:X\rightarrow f(X)$.
\end{thm}

\begin{proof}
Choose any net $(f(x_\alpha))$ in $f(X)$ where $x_\alpha\in X$. Since $X$ is compact, $(x_\alpha)$ has a subnet $(x'_\beta)$ converging to some $x\in X$. Then $f(x'_\beta)$ converges to $f(x)$. So $f(X)$ is compact.

Now assume that $f$ is injective and $Y$ is Hausdorff. Then the subspace $f(X)$ is also Hausdorff. By replacing $Y$ by $f(X)$, we assume that $f$ is bijective. To show that $f^{-1}$ is continuous, by Prop. \ref{lb191}, it suffices to prove that $f$ is a closed map, i.e., $f$ sends every closed $E\subset X$ to a closed subset $f(E)$. Indeed, since $X$ is compact, $E$ is also compact by Cor. \ref{lb234}. So $f(E)$ is compact by the first paragraph. So $f(E)$ is closed in $X$ by Cor. \ref{lb234}.
\end{proof}

The second part of Thm. \ref{lb236} can also be proved in a similar way as Pb. \ref{lb235} by replacing sequences with nets. But that argument relies on the fact that every net in a compact Hausdorff space with only one cluster point is convergent. We leave the proof of this fact to the readers (cf. Pb. \ref{lb237}).


\begin{exe}
The first part of Thm. \ref{lb236} can be viewed as a generalization of extreme value theorem. Why?
\end{exe}



\begin{pp}\label{lb239}
Suppose that $X,Y$ are compact topological spaces. Then $X\times Y$ is compact.
\end{pp}

\begin{proof}
Take a net $(x_\alpha,y_\alpha)$ in $X\times Y$. Since $X$ is compact, $(x_\alpha)$ has a convergent subnet $(x_{\alpha_\beta})$. Since $Y$ is compact,  $(y_{\alpha_\beta})$ has a convergent subnet $(y_{\alpha_{\beta_\gamma}})$. So $(x_{\alpha_{\beta_\gamma}},y_{\alpha_{\beta_\gamma}})$ is a convergent subnet of $(x_\alpha,y_\alpha)$.
\end{proof}

\begin{rem}
Note that if $X\times Y$ is compact, then $X$, as the image of $X\times Y$ under the projection map, is compact by Thm. \ref{lb236}. Therefore, we conclude that $X\times Y$ is compact iff $X$ and $Y$ are compact.
\end{rem}










\subsection{Act 3: ``countably compact $\Leftrightarrow$ compact" for Lindel\"of spaces}\label{lb253}


Assume in this section that $X$ is a topological space. Recall that $X$ is Lindel\"of iff every open cover has a countable subcover. Thus, it is obvious that
\begin{align}
\text{countably compact}\quad\Longleftrightarrow\quad \text{compact}\qquad(\text{for Lindel\"of spaces})
\end{align}
Thus, by Lem. \ref{lb247}, to prove that sequentially/countably compact metric spaces are compact, it suffices to prove that they are Lindel\"of.

We introduce two related concepts that are more useful than Lindel\"of spaces:
\begin{df}
We say that a topological space $X$ is \textbf{separable} \index{00@Separable} if $X$ has a countable dense subset. We say that $X$ is \textbf{second countable} \index{00@Second countable} if the topology of $X$ has a countable basis (i.e., a basis $\mc B$ with countably many elements).
\end{df}

\begin{eg}\label{lb248}
$\Rbb^N$ is separable, since $\Qbb^N$ is a dense subset. 
\end{eg}


As we shall immediately see, these two notions are equivalent for metric spaces. It is often easier to visualize and prove separability for concrete examples (such as Exp. \ref{lb248}). Indeed, Exp. \ref{lb248} is the typical example that helps us imagine more general separable spaces. However, for general topological spaces, second countability behaves better than separability.  The following property gives one reason.

\begin{pp}\label{lb475}
If $Y$ is a subset of a second countable space $X$, then $Y$ is second countable.
\end{pp}

\begin{proof}
Let $\mc B$ be a countable basis for the topology of $X$. Then $\{Y\cap U:U\in\mc B\}$ is a countable basis for the topology of $Y$.
\end{proof}
Another reason that second countability is better is because it implies Lindel\"of property. This is mainly due to the following fact:

\begin{pp}\label{lb249}
Let $\mc B$ be a basis for the topology of $X$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is Lindel\"of (resp. compact).
\item If $X$ has open cover $\fk U$ where each member of $\fk U$ is an element of $\mc B$, then $\fk U$ has a countable (resp. finite) subcover.
\end{enumerate}
\end{pp}

\begin{proof}
``(1)$\Rightarrow$(2)" is obvious.  Assume (2). Let $\fk W$ be an open cover of $X$. Let
\begin{align*}
\fk U=\{U\in \mc B:U\subset W\text{ for some }W\in\fk W\}
\end{align*} 
For each $x\in X$, since there is $W\in\fk W$ containing $x$, and since $\mc B$ is a basis, there is $U\in\mc B$ such that $x\in U\subset W$. This proves that $\fk U$ is an open cover of $X$. So $\fk U$ has a countable (resp. finite) subcover $\fk U_0$. For each $U\in\fk U_0$, choose $W_U\in\fk W$ containing $U$. Then $\{W_U:U\in\fk U_0\}$ is a countable (resp. finite) subcover of $\fk W$.
\end{proof}

\begin{co}\label{lb265}
Every second countable topological space $X$ is Lindel\"of.
\end{co}

\begin{proof}
Let $\mc B$ be a countable basis for the topology of $X$. Let $\fk U$ be an open cover of $X$ such that each member of $\fk U$ is in $\mc B$. Then by discarding duplicated terms, $\fk U$ becomes countable. This verifies (2) of Prop. \ref{lb249}.
\end{proof}


\begin{thm}\label{lb250}
Consider the following statements:
\begin{enumerate}[label=(\arabic*)]
\item $X$ is second countable.
\item $X$ is separable.
\end{enumerate}
Then (1)$\Rightarrow$(2). If $X$ is metrizable, then (1)$\Leftrightarrow$(2).
\end{thm}



\begin{proof}
Assume that $X$ is second countable with countable basis $\mc B$. For each $U\in\mc B$, choose $x_U\in U$. Then one checks easily that $\{x_U:U\in\mc B\}$ is dense by checking that it intersects every nonempty open subset of $X$. This proves (1)$\Rightarrow$(2).

Assume that $X$ is a metric space with countable dense subset $E$. Let us prove that the countable set
\begin{align*}
\mc B=\{B_X(e,1/n):e\in E,n\in\Zbb_+\}
\end{align*}
is a basis for the topology of $X$. Choose any open $W\subset X$ with $x\in W$. We want to show that some member of $\mc B$ contains $x$ and is in $W$. By shrinking $W$, we may assume that $W=B_X(x,1/n)$ for some $n\in\Zbb_+$. Since $E$ is dense, $B(x,1/2n)$ contains some $e\in E$. So $d(x,e)<1/2n$. Therefore,  $B(e,1/2n)$ contains $x$  and is inside $W$ by triangle inequality.
\end{proof}


It can be proved that Lindel\"of metric spaces are separable. (Therefore, the three notions agree for metric spaces.) We will not use this fact. So we leave its proof to the readers as an exercise (cf. Pb. \ref{lb255}). The following chart is a summary of the relationships between the various topological properties about countability.





\begin{align}
\begin{aligned}
&\text{Topological spaces:} &  \text{second countable }&\Longrightarrow \left\{
{\begin{array}{l}
\text{subset is second countable}\\
\text{separable}\\
\text{Lindel\"of}
\end{array}}
\right.\\[0.5ex]
&\text{Metric spaces:}  & \text{second countabl}&\text{e }\Longleftrightarrow\text{ separable}\Longleftrightarrow \text{ Lindel\"of} 
\end{aligned}
\end{align}

\begin{eg}
Since $\Rbb^N$ is separable and hence second countable, every subset of $\Rbb^N$ is second countable, and hence is separable.
\end{eg}



\begin{thm}\label{lb252}
Let $X$ be a sequentially compact metric space. Then $X$ is separable, and hence second countable and Lindel\"of.
\end{thm}

\begin{proof}
We claim that for every $\delta>0$, there is a finite set $E\subset X$ such that for every $x\in E$, the distance $d(x,E)<\delta$. Suppose that there is no such a finite set for some given number $\delta>0$. Pick any $x_1\in X$. Suppose $x_1,\dots,x_k\in X$ have been constructed. Then there is a point $x_{k+1}\in X$ whose distance to $\{x_1,\dots,x_k\}$ is $\geq \delta$. This defines inductively a sequence $(x_k)_{k\in\Zbb_+}$ in $X$ such that any two elements have distance $\geq \delta$. So $(x_k)$ has no convergent subsequence, contradicting the sequential compactness of $X$.

Thus, for each $n\in\Zbb_+$, we can choose a finite  $E_n\subset X$ satisfying  $d(x,E_n)<1/n$ for all $x\in X$. Let $E=\bigcup_{n\in\Zbb_+}E_n$, which is countable. Then for each $x\in X$, $d(x,E)\leq d(x,E_n)<1/n$ for every $n$, which implies $d(x,E)=0$ and hence $x\in\ovl E$ by Pb. \ref{lb251}. So $E$ is dense in $X$.
\end{proof}


\begin{rem}\label{lb483}
The above proof is indirect because it proves the existence of $E_n$ by contradiction but not by explicit construction. However, if $X$ is a bounded closed subset of $\Rbb^N$, one can find an explicit countable basis for the topology of $X$:
\begin{align*}
\mc B=\{X\cap B(x,1/n):n\in\Zbb_+,x\in\Qbb^N\}
\end{align*}
and hence has an explicit countable dense subset $\{x_U:U\in\mc B,U\neq\emptyset\}$ where for each $U$ we choose some $x_U\in U$. More generally, if $X$ is a closed subset of the sequentially compact space $[0,1]^{\Zbb_+}$, one can find a countable basis for the topology of $X$ and hence a countable dense subset of $X$ in a similar way. (You will be asked to construct them in Pb. \ref{lb305}.) 


We shall see in Thm. \ref{lb261} that every sequentially compact metric space is homeomorphic to a closed subset of $[0,1]^{\Zbb_+}$. Therefore, for any sequentially compact metric space $X$ you will see in the real (mathematical) life, you don't need the indirect construction in the proof of Thm. \ref{lb252} to prove the separability of $X$. So what is the point of giving an indirect proof of Thm. \ref{lb252}? Well, you need Thm. \ref{lb252} to prove Thm. \ref{lb261}.  \hfill\qedsymbol
\end{rem}





\begin{proof}[\textbf{Proof of Thm. \ref{lb222}}] \hypertarget{target1}{}
Let $X$ be a metric space. Assume that $X$ is compact. Then $X$ is clearly countably compact, and hence sequentially compact by Lem. \ref{lb247}. Conversely, assume that $X$ is sequentially compact. Then by Lem. \ref{lb247} and Thm. \ref{lb252}, $X$ is countably compact and Lindel\"of, and hence compact.
\end{proof}




\begin{srem}
Since second countable spaces are first countable, by Rem. \ref{lb264} and Cor. \ref{lb265}, we have
\begin{gather}\label{eq75}
\begin{gathered}
\text{sequentially compact}\quad\Longleftrightarrow\quad \text{countably compact}\quad\Longleftrightarrow\quad \text{compact}\\[0.5ex]
(\text{for second countable topological spaces})
\end{gathered}
\end{gather}
Relation \eqref{eq75} not only generalizes Thm. \ref{lb222}, but also tells us what are the crucial properties that ensure the equivalence of compactness and sequential compactness for metric spaces. 
\end{srem}






















\subsection{Problems and supplementary material}


Let $X,Y$ be topological spaces.


\subsubsection{Compactness}

\begin{prob}\label{lb237}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a compact Hausdorff space $X$. Prove that $(x_\alpha)$ is convergent iff $(x_\alpha)$ has exactly one cluster point.
\end{prob}


\begin{prob}\label{lb346}
Let $(x_\alpha)_{\alpha\in I}$ be a net in the compact space $\ovl\Rbb$. Let $S$ be the (automatically nonempty) set of cluster points of $(x_\alpha)$ in $\ovl\Rbb$. Recall that $S$ is a closed subset by Rem. \ref{lb238}. For each $\alpha\in I$, define  \index{00@Limit inferior and superior}
\begin{gather}
A_\alpha=\inf\{x_\beta:\beta\geq \alpha \}\qquad  B_\alpha=\sup\{x_\beta:\beta\geq  \alpha \}
\end{gather}
Then $(A_\alpha)$ is increasing and $(B_\alpha)$ is decreasing. So they converge in $\ovl\Rbb$. Define \index{liminfsup@$\liminf,\limsup$}
\begin{subequations}
\begin{gather}
\liminf_{\alpha\in I}x_\alpha=\sup\{A_\alpha:\alpha\in I\}=\lim_{\alpha\in I} A_\alpha \\
\limsup_{\alpha\in I}x_\alpha=\inf\{B_\alpha:\alpha\in I\}=\lim_{\alpha\in I} B_\alpha
\end{gather}
\end{subequations}
Prove that
\begin{align}
\liminf_{\alpha\in I}x_\alpha=\inf S\qquad \limsup_{\alpha\in I}x_\alpha=\sup S
\end{align}
\end{prob}

\begin{proof}[Note]
You will get a quick proof by choosing the right one of the three equivalent definitions of cluster points in Pb. \ref{lb223}. A wrong choice will take you much more effort.
\end{proof}


\begin{co}\label{lb385}
Let $(x_\alpha)$ be a net in $\ovl\Rbb$. Then $(x_\alpha)$ converges in $\ovl\Rbb$ iff $\limsup_\alpha x_\alpha=\liminf_\alpha x_\alpha$. 
\end{co}


\begin{proof}
$\ovl\Rbb$ is compact. Therefore, $\limsup_\alpha x_\alpha=\liminf_\alpha x_\alpha$ iff $(x_\alpha)$ has only one cluster point (by Pb. \ref{lb346}), iff $(x_\alpha)$ converges (by Pb. \ref{lb237}).
\end{proof}




\begin{prob}
Given a general topological space $X$, which one implies the other between the conditions of ``sequential compactness" and ``countable compactness"? Prove your conclusion with details.
\end{prob}

\begin{proof}[Hint]
Check the proof of Thm. \ref{lb232}, and think about the question: If $(x_n)$ is a sequence in $X$, what is the inclusion relation between $\bigcap_n\ovl{\{x_k:k\geq n\}}$ and the set of limits of the convergent subsequences (rather than subnets) of $(x_n)$?
\end{proof}







\begin{prob}\label{lb246}
Prove Prop. \ref{lb233} using the original definition of compact spaces (i.e. every open cover has a finite subcover) instead of using nets.
\end{prob}

\begin{sprob}
Prove Prop. \ref{lb239} using the original definition of compact spaces instead of using nets.
\end{sprob}

\begin{prob}\label{lb240}
Assume that $Y$ is compact. Let $(x_\alpha,y_\alpha)_{\alpha\in I}$ be a net in $X\times Y$. Assume that $x\in X$ is a cluster point of $(x_\alpha)$. Prove that there exists $y\in Y$ such that $(x,y)$ is a cluster point of $(x_\alpha,y_\alpha)$.
\end{prob}

\begin{prob}\index{00@Tychonoff theorem, countable version} \label{lb241}
(\textbf{Tychonoff theorem, countable version}) Let $(X_n)_{n\in\in\Zbb_+}$ be a sequence of compact topological spaces. Prove that the product space $S=\prod_{n\in\Zbb_+}X_n$ (equipped with the product topology) is compact using the following hint.
\end{prob}

\begin{proof}[Hint]
Let $(f_\alpha)_{\alpha\in I}$ be a net in $S$ where $f_\alpha=(f_\alpha(1),f_\alpha(2),\dots)$. Use Pb. \ref{lb240} to construct inductively an element $x=(x(1),x(2),\dots)\in S$ such that for every $n\in\Zbb_+$, the element $(x(1),\dots,x(n))$ is a cluster point of $(f_\alpha(1),\dots,f_\alpha(n))_{\alpha\in I}$ in $X_1\times\cdots\times X_n$. Prove that $x$ is a cluster point of $(f_\alpha)_{\alpha\in I}$ in $S$.
\end{proof}


\begin{rem}
The same idea as above can be used to prove the general Tychonoff theorem (the version where the index set $\Zbb_+$ in Pb. \ref{lb241} is replaced by an arbitrary set) by replacing mathematical induction by Zorn's lemma. 
\end{rem}



\begin{sprob}
Let $X$ be the set of two elements: $X=\{0,1\}$, viewed as a metric subspace of $\Rbb$. Let $S=X^{[0,1]}$, the product space of uncountably many $X$, where the index set is the  interval $[0,1]$. $S$ is equipped with the product topology. According to Tychonoff theorem (to be proved in the future), $S$ is compact. Prove that $S$ is not sequentially compact.
\end{sprob}


\begin{proof}[Hint]
Use binary representations in $[0,1]$.
\end{proof}




\subsubsection{LCH spaces}


\begin{df}\label{lb458}
A subset $A$ of a Hausdorff space $X$ is called \textbf{precompact} \index{00@Precompact subset} if its closure $\ovl A$ is compact. This is equivalent to saying that $A$ is contained in a compact subset of $X$.   
\end{df}

\begin{proof}[Proof of equivalence]
If $\ovl A$ is compact, then $\ovl A$ is a compact subset of $X$ (cf. Cor. \ref{lb234}) containing $A$. Conversely, if $A\subset K$ where $K$ is a compact subset of $X$, then $K$ is closed by Cor. \ref{lb234}. Since $\ovl A$ is the smallest closed subset of $X$ containing $A$, we have $\ovl A\subset K$. By Exe. \ref{lb341}, $\ovl A$ is closed in $K$. By Cor. \ref{lb234}, $\ovl A$ is compact.
\end{proof}



It is clear that a subset of a precompact set is precompact.

\begin{prob}\label{lb287}
Suppose that $X$ is metric space. Let $A\subset X$. Prove that the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $A$ is precompact, i.e., $\ovl A$ is compact.
\item Every sequence in $A$ has a subsequence converging to some point of $X$.
\end{enumerate}
\end{prob}



\begin{df}
A Hausdorff space $X$ is called a \textbf{locally compact Hausdorff (LCH) space} \index{00@LCH=Locally compact Hausdorff} if every point has a precompact neighborhood.
\end{df}






\begin{pp}\label{lb245}
Let $X$ be an LCH space. Then the closed subsets and the open subsets of $X$ are LCH. 
\end{pp}

\begin{proof}
Let $E\subset X$ be closed. Let $x\in E$. Then there is $U\in\Nbh_X(x)$ with compact closure $\Cl_X(U)$. So $\Cl_X(U)\cap E$ is compact by Cor. \ref{lb234}. Note that $U\cap E\in \Nbh_E(x)$, and $U\cap E$ is a subset of $\Cl_X(U)\cap E$. So $U\cap E$ is precompact in $E$. This proves that $E$ is LCH.

Next, we let $W$ be an open subset of $X$. Let $x\in W$. We want to show that there exists $U\in\Nbh_X(x)$ such that $\ovl U=\Cl_X(U)$ is compact and $\ovl U\subset W$. Then $U$ is clearly precompact in $W$.

Since $X$ is LCH, there is $\Omega\in\Nbh_X(x)$ with compact closure $\ovl\Omega=\Cl_X(\Omega)$. Then every open subset $U$ of $\Omega$ containing $x$ has compact closure in $X$. Thus, it suffices to prove that there exists $U\in\mc I=\Nbh_\Omega(x)$  such that $\ovl U\subset W$. Suppose that this is not true. Then for each $U\in\mc I$ there is $x_U\in \ovl U\setminus W$. So $(x_U)_{U\in\mc I}$ is a net in $\ovl\Omega\setminus W$, where $\ovl\Omega\setminus W$ is compact by Cor. \ref{lb234}. So $(x_U)_{U\in\mc I}$ has a cluster point $y\in\ovl\Omega\setminus W$. In particular, $y\neq x$.

Since $X$ is Hausdorff, by Cor. \ref{lb243}, there is $V_0\in\Nbh_X(x)$ such that $y\notin\ovl V_0$. Let $V=V_0\cap\Omega$. Then $V\in\mc I$ and $y\notin \ovl V$. For every $U\in\mc I$ satisfying $U\subset V$ we have $x_U\in\ovl V$. So $(x_U)_{U\in\mc I}$ is eventually not in the neighborhood $X\setminus \ovl V$ of $y$. This is impossible.
\end{proof}


\begin{comment}


Case 1: Assume that $X$ is compact Hausdorff.  Then $B=X\setminus W$ is compact by Cor. \ref{lb234}. Since $X$ is Hausdorff, for each $y\in B$ there exists $V_y\in\Nbh_X(y)$ such that $x\notin \ovl V_y$. (Recall Cor. \ref{lb243}.) Since $B$ is compact, there are $y_1,\dots,y_n\in B$ such that $B\subset V$ where $V=V_{y_1}\cup\cdots\cup V_{y_n}$. By \eqref{eq73}, $\ovl V=\ovl V_{y_1}\cup\cdots\cup \ovl V_{y_n}$. So $x\notin\ovl V$. Now $X\setminus V$ is a closed subset of $X$, and hence 

Let $U=X\setminus \ovl V$. Then $x\in U$. Since $U\subset X\setminus V$ and $X\setminus V$ is closed, $\ovl U\subset X\setminus V$, and hence $\ovl U\subset W$. Since $X$ is compact, $U$ is precompact.


Case 2: $X$ is LCH. Choose a precompact $\Omega\in\Nbh_X(x)$. By case 1 (applied to $\ovl\Omega$), and by the definition of subspace topology, there exists $V\in\Nbh_X(x)$ such that $x\in V\cap \ovl\Omega\subset\Cl_{\ovl\Omega}(V\cap \ovl\Omega)\subset W\cap\ovl\Omega$. By Rem. \ref{lb182}, we have $\Cl_{\ovl\Omega}(V\cap \ovl\Omega)= \ovl{V\cap\ovl\Omega}\cap\ovl\Omega$, which contains $\ovl{V\cap\Omega}$ since both $\ovl{V\cap\ovl\Omega}$ and $\ovl\Omega$ do. So $x\in U\subset\ovl U\subset W$ where $U=V\cap\Omega$. Since $\Omega$ is precompact, $U$ is precompact.
\end{comment}



\begin{df}\label{lb334}
Let $X$ be LCH. Let $Y$ be a metric space. Let $(f_\alpha)_{\alpha\in I}$ be a net in $Y^X$. Let $f\in Y^X$.  We say that $(f_\alpha)$  \textbf{converges locally uniformly} \index{00@Locally uniform convergence} to $f$ if the following equivalent conditions are satisfied:
\begin{enumerate}[label=(\arabic*)]
\item For each $x\in X$, there exists $U\in\Nbh_X(x)$ such that $(f_\alpha|_U)$ converges uniformly to $f|_U$.
\item For each precompact open subset $W\subset X$, the net $(f_\alpha|_W)$ converges uniformly to $f|_W$.
\end{enumerate}
\end{df}

\begin{prob}
Prove that in Def. \ref{lb334}, conditions (1) and (2) are equivalent. 
\end{prob}


\begin{exe}
Let $X$ be LCH. Let $\mc V$ be a normed vector space. Let $(f_\alpha)_{\alpha\in I}$ be a net in $C(X,\mc V)$ converging pointwise to $f:X\rightarrow 
\mc V$. Assume that the net $(f_\alpha)$ converges locally uniformly on $X$ (clearly to $f$). Prove that $f$ is continuous. 
\end{exe}






An example of locally uniform convergence was given in Thm. \ref{lb112}.



\subsubsection{Countability in topological spaces}



\begin{prob}
Let $(X,\mc T)$ be a second countable LCH space. Prove that the topology $\mc T$ has a countable basis $\mc B$ whose members are all precompact.
\end{prob}

\begin{proof}[Hint]
Use Lindel\"of property for open subsets of $X$.
\end{proof}

\begin{sprob}\label{lb255}
Let $X$ be a Lindel\"of metric space. Prove that $X$ is separable. 
\end{sprob}


\begin{prob}\label{lb305}
Let $(X_n)_{n\in\Zbb_+}$ be a sequence of topological spaces. Equip $S=\prod_{n\in\Zbb_+}X_n$ with the product topology. 
\begin{enumerate}
\item Prove that $S$ is second countable if each $X_n$ is second countable.
\item Prove that $S$ is separable if each $X_n$ is separable
\end{enumerate}
\end{prob}


Recall from Pb. \ref{lb221} the basic facts about connected components.

\begin{prob}\label{lb679}
Let $X$ be a locally connected topological space. Prove that if $X$ is second countable, then $X$ has countably many connected components. Use this result to show that every open subset of $\Rbb$ is a countable disjoint union of open intervals.
\end{prob}



\subsubsection{The problem of embedding}


\begin{df}\label{lb327}
Let $\scr F$ be a set of functions $X\rightarrow Y$. We say that $\scr F$ \textbf{separates points} \index{00@Separates points} of $X$, if for every distinct $x_1,x_2\in X$ there exists $f\in\scr F$ such that $f(x_1)\neq f(x_2)$. 
\end{df}

\begin{prob}\label{lb259}
Let $X$ be a nonempty compact metric space.
\begin{enumerate}
\item Prove that there is a sequence of continuous functions $(f_n)_{n\in\Zbb_+}$ from $X$ to $[0,1]$ separating points of $X$. 
\item Prove that 
\begin{gather*}
\Phi:X\rightarrow [0,1]^{\Zbb_+} \qquad x\mapsto (f_1(x),f_2(x),\dots)
\end{gather*}
gives a homeomorphism $\Phi:X\rightarrow\Phi(X)$ where $\Phi(X)$ is a closed subspace of $[0,1]^{\Zbb_+}$ (equipped with the subspace topology).
\end{enumerate}
\end{prob}

The topological space $[0,1]^{\Zbb_+}$, equipped with the product topology, is called the \textbf{Hilbert cube}. \index{00@Hilbert cube}




\begin{proof}[Hint]
Part 1: Choose an infinite countable basis $\mc B=(U_1,U_2,\dots)$ for the topology of $X$ where each $U_n$ is nonempty. (Why can you do so?) Use Urysohn functions (Rem. \ref{lb257}) to construct $f_n:X\rightarrow [0,1]$ such that $f^{-1}(0)=X\setminus U_n$.

Part 2: Notice Pb. \ref{lb258}.
\end{proof}


\begin{thm}\label{lb261}
Let $X$ be a topological space. The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is a compact metrizable space.
\item $X$ is homeomorphic to a closed subset of the Hilbert cube $[0,1]^{\Zbb_+}$.
\end{enumerate}
\end{thm}

\begin{proof}
(1)$\Rightarrow$(2): By Pb. \ref{lb259}. (2)$\Rightarrow$(1): By Cor. \ref{lb260}, $[0,1]^{\Zbb_+}$ is metrizable. By countable Tychonoff theorem (Thm. \ref{lb89} or Pb. \ref{lb241}), $[0,1]^{\Zbb_+}$ is compact. So its closed subsets are compact by Cor. \ref{lb234}.
\end{proof}



\begin{sexe}
Let $N\in\Zbb_+$. Prove that the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is a compact Hausdorff space. Moreover, there exist $f_1,\dots,f_N\in C(X,\Rbb)$ separating points of $X$.
\item $X$ is homeomorphic to a bounded closed subset of $\Rbb^N$.
\end{enumerate}
\end{sexe}


\newpage

\section{The injection \pmb{$\Psi:C(X,C(Y,\mc V))\rightarrow C(X\times Y,\mc V)$}}\label{lb352}





In this chapter, unless otherwise stated,  $X,Y$ are topological spaces, all normed vectors spaces are over $\Fbb\in\{\Rbb,\Cbb\}$, and $\mc V$ is a normed vector space.

\begin{rem}
Let $\Phi:\mc V\rightarrow\mc W$ be a linear map of normed vector spaces. Recall that $\Phi$ is an isometry (in the category of metric spaces) iff $\Vert \Phi(u)-\Phi(v)\Vert=\Vert u-v\Vert$ for all $u,v\in\mc V$. By linearity, we have
\begin{align}
\Phi\text{ is an isometry}\qquad \Longleftrightarrow\qquad \Vert\Phi(v)\Vert=\Vert v\Vert~~\text{for all } v\in\mc V
\end{align}
\end{rem}


\begin{df}\label{lb302}
A linear map $\Phi:\mc V\rightarrow\mc W$ of normed vector spaces over $\Fbb$ is a called an \textbf{isomorphism of normed vector spaces} \index{00@Isomorphism of normed vector spaces/Banach spaces} if $\Phi$ is an isometric isomorphism. If $\Phi$ is an isomorphism, and if one of $\mc V,\mc W$ is complete, then the other one is also complete. In this case, we call $\Phi$ an \textbf{isomorphism of Banach spaces}.
\end{df}


Recall that if $X$ is compact, then the norm on $C(X,\mc V)$ is assumed to be the $l^\infty$-norm (cf. Conv. \ref{lb88}).


\subsection{$\Psi$ is bijective when $Y$ is compact}



\begin{thm}\label{lb274}
Equip $C(Y,\mc V)$ with the uniform convergence topology (cf. Exp. \ref{lb272}). Then there is a well-defined injective linear map
\begin{gather}\label{eq77}
\begin{gathered}
\Psi:C(X,C(Y,\mc V))\rightarrow C(X\times Y,\mc V)\\[0.5ex]
\Psi(F)(x,y)=F(x)(y)
\end{gathered}
\end{gather}
(for each $F\in C(X,C(Y,\mc V))$ ). Moreover, the following are true:
\begin{enumerate}[label=(\alph*)]
\item If $Y$ is compact, then $\Psi$ is a linear isomorphism of vector spaces.
\item If $X,Y$ are compact, then $\Psi$ is an isomorphism of normed vector spaces.
\end{enumerate}
\end{thm}



\begin{proof}
Step 1. Write $\mc W=C(Y,\mc V)$ for simplicity. To prove that \eqref{eq77} is a well-defined map, we need to prove that for each $F\in C(X,\mc W)$, the map $f=\Psi(F):X\times Y\rightarrow \mc V$ sending $(x,y)$ to $f(x,y)=F(x)(y)$ is continuous.

The continuity of $F:X\rightarrow\mc W$ means that if $(x_\alpha)_{\alpha\in I}$ is a net in $X$ converging to $x$, then $F(x_\alpha)$ converges to $F(x)$, i.e., 
\begin{subequations}
\begin{align}
\lim_{\alpha\in I}A_\alpha=0
\end{align}
where each $A_\alpha\in\ovl\Rbb_{\geq0}$ is 
\begin{align}
A_\alpha= \sup_{y\in Y}\Vert f(x_\alpha,y)-f(x,y)\Vert
\end{align}
\end{subequations}
Now, suppose that $(x_\alpha,y_\alpha)_{\alpha\in I}$ is a net in $X\times Y$ converging to $(x,y)$. Then
\begin{align*}
&\Vert f(x_\alpha,y_\alpha)-f(x,y)\Vert\leq \Vert f(x_\alpha,y_\alpha)-f(x,y_\alpha)\Vert +\Vert f(x,y_\alpha)-f(x,y)\Vert\\
\leq& A_\alpha+\Vert f(x,y_\alpha)-f(x,y)\Vert
\end{align*}
Since $F(x):y\in Y\mapsto f(x,y)\in\mc V$ is continuous, we have $\lim_\alpha \Vert f(x,y_\alpha)-f(x,y)\Vert=0$. Therefore, by Squeeze theorem, we have
\begin{align}
\lim_{\alpha\in I}\Vert f(x_\alpha,y_\alpha)-f(x,y)\Vert=0
\end{align}
Thus, $f$ is continuous at every $(x,y)$.\\[-1ex]

Step 2. Clearly $\Psi$ is linear and injective. Assume that $Y$ is compact. Then the surjectivity of $\Psi$ follows from Exp. \ref{lb225}. This proves (a). Assume that $X$ is also compact. Choose any $F\in C(X,\mc W)$ and write $f=\Psi(F)$.  Then
\begin{align*}
\sup_{x\in X}\sup_{y\in Y}~\Vert f(x,y)\Vert=\sup_{x\in X,y\in Y}\Vert f(x,y)\Vert
\end{align*}
by the easy Lem. \ref{lb273}. This proves that $\Phi$ is an isometry, and hence proves (b).
\end{proof}

\begin{lm}\label{lb273}
Let $g:A\times B\rightarrow\ovl\Rbb$ be a function where $A,B$ are sets. Then
\begin{align*}
\sup_{a\in A}\sup_{b\in B}g(a,b)=\sup_{(a,b)\in A\times B}g(a,b)
\end{align*}
\end{lm}

\begin{proof}
Write $\lambda_a=\sup_{b\in B}g(a,b)$ and $\rho=\sup_{(a,b)\in A\times B}g(a,b)$. Then, clearly $\lambda_a\leq\rho$ for each $a$. So $\sup_a\lambda_a\leq\rho$. For each $a,b$ we have $g(a,b)\leq \lambda_a$, and hence $g(a,b)\leq \sup_a\lambda_a$. Taking $\sup$ over $a,b$ yields $\rho\leq\sup_a\lambda_a$.
\end{proof}





\begin{rem}
Thanks to Thm. \ref{lb274}, we can reduce many problems about multi-variable functions to problems about single-variable functions. Here is an example we will study in the future: If $I=[a,b],J=[c,d]$ are compact intervals and $F\in C(I\times J,\Rbb)$, then with the help of Thm. \ref{lb274}, the Fubini's theorem
\begin{align*}
\int_a^b \int_c^d F(x,y)dxdy=\int_c^d\int_a^b F(x,y)dxdy
\end{align*}
for Riemann integrals follows directly from the easy general fact
\begin{align*}
\int_a^b\Lambda\circ f(x)dx=\Lambda\Big(\int_a^b f(x)dx \Big)
\end{align*}
where $f:[a,b]\rightarrow\mc W$ is a continuous map to a real Banach space $\mc W$, and $\Lambda:\mc W\rightarrow\Rbb$ is a continuous linear map.

Thm. \ref{lb274} can be used the other way round: We will prove in the future that every Banach space $\mc V$ is isomorphic to a closed linear subspace of $C(Y,\Fbb)$ for some compact Hausdorff space $Y$. Thus, a problem about continuous maps $X\rightarrow \mc V$ (where $\mc V$ is an abstract Banach space) can be reduced to a problem about continuous scalar-valued functions $X\times Y\rightarrow\Fbb$. \hfill\qedsymbol
\end{rem}


In the following sections, we give a surprising application of Thm. \ref{lb274}: We show that uniform-convergence and equicontinuity, two closely related but different notions, can be understood in the same context.

\subsection{Equicontinuity}

Let $I$ be a set not necessarily preordered or directed.

\begin{df}\label{lb514}
Assume that $Y$ is a metric space. A family of functions $(f_\alpha)_{\alpha\in I}$ from $X$ to $Y$ is called \textbf{equicontinuous at} $x\in X$ \index{00@Equicontinuous at a point} if the following equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item The function
\begin{gather}
X\rightarrow Y^{I}\qquad x\mapsto (f_\alpha(x))_{\alpha\in I}
\end{gather}
is continuous at $x$, where $Y^{I}$ is equipped with the uniform convergence topology (Exp. \ref{lb272}).
\item For every $\eps>0$, there exists $U\in\Nbh_X(x)$ such that for every $p\in U$ we have
\begin{align*}
\sup_{\alpha\in I}d_Y(f_\alpha(p),f_\alpha(x))<\eps
\end{align*}
\end{enumerate}
Clearly, if $(f_\alpha)_{\alpha\in I}$ is equicontinuous at $x$, then $f_\alpha:X\rightarrow Y$ is continuous at $x$ for every $\alpha\in I$. We say that $(f_\alpha)_{\alpha\in I}$ is \textbf{(pointwise) equicontinuous} \index{00@Equicontinuous, pointwise} if it is equicontinuous at every point of $X$.
\end{df}



\begin{proof}[Proof of equivalence]
This is immediate if we choose the uniform convergence metric on $Y^I$ to be
\begin{align*}
d((y_\alpha),(y_\alpha'))=\min\Big\{\sup_{\alpha\in I}d_Y(y_\alpha,y'_\alpha),1\Big\}
\end{align*}
and use (the base version of) Def. \ref{lb188}-(2).
\end{proof}

\begin{rem}
Warning: The above definition of equicontinuity is weaker than the one in Rudin's book \cite[Ch. 7]{Rud-P}, which will be called \textbf{uniform equicontinuity} in this course (cf. Def. \ref{lb316}).
\end{rem}



\begin{eg}\label{lb309}
Assume that $X$ and $Y$ are metric spaces. Fix $C\geq 0$. Then
\begin{align*}
\{f\in Y^X:f\text{ has Lipschitz constant }C\}
\end{align*}
is an equicontinuous family of functions $X\rightarrow Y$.

In the future, we will see that if $f:[a,b]\rightarrow\Rbb$ (where $[a,b]\subset\Rbb$) is differentiable and satisfies $|f'(x)|\leq C$ for all $x\in[a,b]$, then $f$ has Lipschitz constant $C$. (For example, if we assume moreover that $f'$ is continuous, then for each $a\leq x<y\leq b$ we have $|f(y)-f(x)|=|\int_x^y f'|\leq \int_x^y|f'|\leq C(y-x)$.) Therefore, all such functions form an equicontinuous family of functions.  \hfill\qedsymbol
\end{eg}


Equicontinuity is important for several reasons. First, equicontinuity is closely related to compactness, both under the uniform convergence topology and under the pointwise convergence topology. This is hinted at in Rem. \ref{lb304}, and will be explored in more detail in a future chapter. Second, equicontinuity and uniform convergence are symmetric notions:

\begin{rem}
Note that if $x\in \ovl{X\setminus x}$, a map $\varphi:X\rightarrow Y$ is continuous at $x$ iff $\lim_{p\rightarrow x}f(p)=f(x)$, where $\lim_{p\rightarrow x}f(p)$ is the limit of a net (cf. Rem. \ref{lb275}). Now, assume that $Y$ is a metric space. Then we see that a family $(f_\alpha)_{\alpha\in I}$ in $Y^X$ satisfies that
\begin{align}
(f_\alpha)_{\alpha\in I}\text{ is equicontinuous at }x\qquad\Longleftrightarrow\qquad\lim_{p\rightarrow x}\sup_{\alpha\in I}d(f_\alpha(p),f_\alpha(x))=0
\end{align} 
If we compare this with
\begin{align}
f_\alpha\rightrightarrows f \qquad\Longleftrightarrow\qquad \lim_{\alpha\in I}\sup_{x\in X}d(f_\alpha(x),f(x))=0
\end{align}
(if $I$ is a directed set and $f\in Y^X$), we see that equicontinuity and uniform convergence are ``symmetric about the diagonal line of the Cartesian product $I\times X$": Equicontinuity is a uniform convergence over the index set $I$, and the uniform convergence $f_\alpha\rightrightarrows f$ is uniform over $X$.
\end{rem}



The symmetry of equicontinuity and uniform convergence will be further studied in Sec. \ref{lb306}. (Indeed, we will see that it is better to view ``uniform convergence + continuity" and ``pointwise convergence + equicontinuity" as symmetric conditions.) As an application, we will see that equicontinuity is equivalent to uniform convergence for any sequence $(f_n)$ of pointwise convergent continuous functions on a compact topological space. (See Cor. \ref{lb284}.) Thus, in this case, one can prove the uniform convergence of $(f_n)$ by proving for instance that it converges pointwise and has a uniform Lipschitz constant.







\begin{comment}

By \eqref{eq78}, the uniform topology of $Y^I$ has a base generated by $\{V_{(y_\alpha)}^\eps:(y_\alpha)\in Y^I,\eps>0\}$ where
\begin{align*}
V_{(y_\alpha)}^\eps=\Big\{(y'_\alpha)\in Y^I:\sup_{\alpha\in I}d(y'_\alpha,y_\alpha)<\eps\Big\}
\end{align*}
So the equivalence of (1) and (2) follows immediately from the base version of Def. \ref{lb188}-(2).
\end{comment}


\subsection{Uniform convergence and equicontinuity: two faces of $\Psi$}\label{lb306}



\subsubsection{Main results}\label{lb282}

In this subsection, we fix a directed set $I$, and let
\begin{align*}
I^*=I\cup\{\infty_I\}
\end{align*}
where $\infty_I$ is a new symbol not in $I$. Write $\infty_I$ as $\infty$ for simplicity. Equip $I^*$ with the standard topology as in Def. \ref{lb175}. Recall that this topology has basis
\begin{align*}
\mc B=\big\{\{\alpha\}:\alpha\in I \big\}\cup \big\{I^*_{\geq\alpha}:\alpha\in I \big\}
\end{align*}
Clearly $I\times X$ is dense in $I^*\times X$. Equip $C(X,\mc V)$ and $C(I^*,\mc V)$ with the uniform convergence topologies.

Throughout this subsection, we fix a net $(f_\alpha)_{\alpha\in I}$ in $\mc V^X$ and an element $f_\infty\in\mc V^X$. Define
\begin{align}
F:I^*\times X\rightarrow\mc V\qquad F(\mu,x)=f_\mu(x)  \label{eq88}
\end{align} 
The meaning of the title of this section is illustrated by the following theorem.





\begin{thm}\label{lb280}
We have (a)$\Leftrightarrow$(b) and (1)$\Leftrightarrow$(2)  where:
\begin{itemize}
\item[(a)] $(f_\alpha)_{\alpha\in I}$ converges uniformly to $f_\infty$, and $f_\alpha:X\rightarrow\mc V$ is continuous for each $\alpha\in I$.
\item[(b)] $F$ gives rise to a continuous map $I^*\rightarrow C(X,\mc V)$
\item[(1)] $(f_\alpha)_{\alpha\in I}$ is equicontinuous and converges pointwise to $f_\infty$.
\item[(2)] $F$ gives rise to a continuous map $X\rightarrow C(I^*,\mc V)$.
\end{itemize}
\end{thm}



%% Record #11 2023/10/25 three lectures  27



\begin{proof}
Assume (a). Then we have $f_\infty\in C(X,\mc V)$ due to Thm. \ref{lb279}. So $F$ gives rise to a map $I^*\rightarrow C(X,\mc V)$. Since $f_\alpha\rightrightarrows f_\infty$, we see that $F$ is continuous by Pb. \ref{lb278}-2. This proves (b).

Assume (b), then the fact that the map $I^*\rightarrow C(X,\mc V)$ has range in $C(X,\mc V)$ means precisely that each $f_\alpha$ and $f_\infty$ are continuous. The continuity of the map $I^*\rightarrow C(X,\mc V)$ at $\infty$ means $f_\alpha\rightrightarrows f_\infty$. This proves (a).


(1)$\Rightarrow$(2): Assume (1). The equicontinuity of $(f_\alpha)$ is equivalent to that
\begin{align}
x\in X\mapsto (f_\alpha(x))_{\alpha\in I}\in \mc V^I  \label{eq84}
\end{align}
is continuous where $\mc V^I$ is equipped with the uniform convergence topology. Equivalently, for each $x\in X$ and $\eps>0$ there is $U\in\Nbh_X(x)$ such that for all $p\in U$ and all $\alpha\in I$ we have
\begin{align}
\Vert f_\alpha(p)-f_\alpha(x)\Vert\leq\eps \label{eq83}
\end{align} 
Since $(f_\alpha)_{\alpha\in I}$ converges pointwise to $f_\infty$, by applying $\lim_{\alpha\in I}$ to \eqref{eq83}, we see that $\Vert f_\mu(p)-f_\mu(x)\Vert\leq\eps$ for all $p\in U$ and $\mu\in I^*$. So
\begin{align}\label{eq86}
x\in X\mapsto (f_\mu(x))_{\mu\in I^*}\in \mc V^{I^*}
\end{align}
is continuous, where $\mc V^{I^*}$ is given the uniform convergence metric. By Pb. \ref{lb278}-2, the pointwise convergence of $(f_\alpha)_{\alpha\in I}$ to $f_\infty$ is equivalent to the continuity of
\begin{align}\label{eq85}
\mu\in I^*\rightarrow f_\mu(x)\in\mc V
\end{align}
for each $x\in X$. So the map \eqref{eq86} has range inside $C(I^*,\mc V)$.

(2)$\Rightarrow$(1): Assume (2). The continuity of $X\rightarrow C(I^*,\mc V)$ implies that of \eqref{eq84}. So $(f_\alpha)_{\alpha\in I}$ is equicontinuous. Its pointwise convergence to $f$ is due to the continuity of \eqref{eq85} for each $x$, which is clearly true by (2).
\end{proof}


\begin{rem}\label{lb340}
Conditions (a) and (1) in Thm. \ref{lb280} are symmetric. Condition (a) says roughly that $F$ converges uniformly under the limit over $I$, and converges pointwise under the limit over $X$. Condition (1) says roughly that $F$ converges uniformly under the limit over $X$, and pointwise under the limit over $I$. The next theorem clarifies the relationship between these two symmetric conditions. We will see a similar condition in Thm. \ref{lb289}.
\end{rem}



\begin{thm}\label{lb277}
Consider the following statements:
\begin{enumerate}
\item[(1)] The function $F:I^*\times X\rightarrow\mc V$ is continuous.
\item[(2)] $(f_\alpha)_{\alpha\in I}$ converges uniformly to $f_\infty$, and $f_\alpha:X\rightarrow\mc V$ is continuous for each $\alpha\in I$.
\item[(3)] $(f_\alpha)_{\alpha\in I}$ is equicontinuous and converges pointwise to $f_\infty$.
\end{enumerate}
Then we have
\begin{gather*}
(2)\Longrightarrow(1)\Longleftarrow (3)\\
(2)\Longleftrightarrow(1)\qquad \text{if $X$ is compact}\\
(1)\Longleftrightarrow (3) \qquad \text{if }I=\Nbb
\end{gather*}
where $\Nbb$ is equipped with the usual order.
\end{thm}


A more explicit description of condition (1) will be given in Prop. \ref{lb281}.



\begin{proof}
This follows immediately from Thm. \ref{lb280}, Thm. \ref{lb274}, and the fact that $I^*$ is compact if $I=\Nbb$. 
\end{proof}

\begin{rem}
From the above proof, we see that the equivalence (1)$\Leftrightarrow$(3) holds in the more general case that $I^*$ is compact. See Pb. \ref{lb283} for an equivalent description of the compactness of $I^*$.
\end{rem}


\begin{eg}
Define $f_n:(0,1)\rightarrow\Rbb$ by $f_n(x)=x^n$. Then $(f_n)_{n\in\Zbb_+}$ is equicontinuous (by Exp. \ref{lb309}) and pointwise convergent, but not uniformly convergent. Accordingly, $(0,1)$ is not compact. 
\end{eg}

\begin{eg}
Let $I=\Zbb_+\times\Zbb_+$, equipped with the product preorder: $(k_1,n_1)\leq (k_2,n_2)$ means $k_1\leq k_2$ and $n_1\leq n_2$. Consider $(f_{k,n})_{(k,n)\in I}$, where $f_{k,n}:[0,1]\rightarrow\Rbb$ is defined by $\dps f_{k,n}(x)=\frac{x^n}{k}$. Then $\lim_{(k,n)\in I}f_{k,n}$ converges uniformly to $0$ by squeeze theorem and $0\leq f_{k,n}\leq k^{-1}$. Thus, condition (1) of Thm. \ref{lb277} is satisfied. However, this net of functions is not equicontinuous at $x=1$. Moreover, for every $(k_0,n_0)\in I$, the net of functions $(f_{k,n})_{(k,n)\in I_{\geq(k_0,n_0)}}$ is not equicontinuous at $1$.
\end{eg}

\begin{proof}
Choose any $x\in[0,1)$. Then (recalling Lem. \ref{lb273})
\begin{align*}
\sup_{k\geq k_0}\sup_{n\geq n_0}|f_{k,n}(1)-f_{k,n}(x)|=\sup_{k\geq k_0}\sup_{n\geq n_0}\frac{1-x^n}{k}=\sup_{k\geq k_0}\frac 1k=\frac 1{k_0}
\end{align*}
\end{proof}




In the following, let me give a more explicit description of condition (1) of Thm. \ref{lb277} in terms of $(f_\alpha)$ and $f_\infty$. 


\begin{pp}\label{lb281}
Assume that $f_\alpha:X\rightarrow\mc V$ is continuous for each $\alpha\in I$. Then part (1) of Thm. \ref{lb277} is equivalent to both (1') and (1''), where
\begin{itemize}
\item[(1')] For each $x\in X$, we have 
\begin{align}\label{eq87}
\lim_{
\begin{subarray}{c}
(\alpha,p)\in I\times X\\
(\alpha,p)\rightarrow (\infty,x)
\end{subarray}
}
f_\alpha(p)=f_\infty(x)
\end{align}
\item[(1'')] For each $x\in X$ and $\eps>0$, there exist $\beta\in I$ and $U\in\Nbh_X(x)$ such that 
\begin{align}\label{eq79}
\Vert f_\alpha(p)-f_\infty(x)\Vert<\eps\qquad (\text{for all } \alpha\in I_{\geq\beta}\text{ and } p\in U)
\end{align}
\end{itemize}
\end{pp}

Note that one can make sense of the LHS of \eqref{eq87} because $I\times X$ is clearly dense in $I^*\times X$. %Also, if $(f_\alpha)$ and $f_\infty$ satisfy (1) or (2), we say that $(f_\alpha)$ \textbf{converges quasi-uniformly} to $f_\infty$. \index{00@Quasi-uniform convergence} If $(f_\alpha)$ and $f_\infty$ satisfy (1) or (2) for a given $x$ (instead of for all $x$), we say that $(f_\alpha)$ \textbf{converges quasi-uniformly} to $f_\infty$ at $x$.

\begin{proof}
The equivalence of (1') and (1'') is clear by Def. \ref{lb197}. The continuity of $f_\alpha$ for all $\alpha\in I$ means precisely that $F|_{I\times X}$ is continuous. Therefore, (1') is equivalent to part (1) of Thm. \ref{lb277}, thanks to the following Thm. \ref{lb276}.
\end{proof}


See Exp. \ref{lb401} for an elementary example satisfying (1') of Prop. \ref{lb281} (or equivalently, (1) of Thm. \ref{lb277}), but not satisfying (2) or (3) of Thm. \ref{lb277}. (See also Pb. \ref{lb285} and Thm. \ref{lb286} for related facts.)






\subsubsection{Proving continuity using limits}



\begin{thm}\label{lb276}
Let $\varphi:X\rightarrow Y$ be a map of topological spaces where $Y$ is metrizable. Let $A$ be a dense subset of $X$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\varphi:X\rightarrow Y$ is continuous.
\item The restriction $\varphi|_A:A\rightarrow Y$ is continuous. Moreover, for each $x\in X\setminus A$, the restriction $\varphi|_{A\cup\{x\}}$ is continuous at $x$, namely (cf. Def. \ref{lb197}) ,
\begin{align}
\lim_{
\begin{subarray}{c}
p\in A\\
p\rightarrow x
\end{subarray}
}\varphi(p)=\varphi(x)
\end{align}
\end{enumerate}
\end{thm}




\begin{proof}
Clearly (2) is equivalent to
\begin{align}\label{eq81}
\varphi|_{A\cup\{x\}}\text{ is continuous at $x$ for all $x\in X$}
\end{align}
So (1)$\Rightarrow$(2). Assume (2) and that $Y$ is a metric space. Choose any $x\in X$. We want to show that $\varphi:X\rightarrow Y$ is continuous at $x$. Choose any $\eps>0$. Recall that an open subset of $A$ is precisely the intersection of $A$ and an open subset of $X$. Thus, by \eqref{eq81}, there is $U\in\Nbh_X(x)$ such that for all $p\in A\cap U$ we have
\begin{align}
d(\varphi(p),\varphi(x))\leq \eps/2\label{eq82}
\end{align}


It remains to prove \eqref{eq82} for all $p\in U$. Choose any $p\in U$. By Lem. \ref{lb342}, $A\cap U$ is dense in $U$.  Thus, there is a net $(p_\alpha)$ in $A\cap U$ converging to $p$. In particular, for each $\alpha$ we have
\begin{align*}
d(\varphi(p_\alpha),\varphi(x))\leq \eps/2
\end{align*}
Since $\varphi|_{A\cup\{p\}}$ is continuous (by \eqref{eq81}), we have $\lim_\alpha\varphi(p_\alpha)=\varphi(p)$. Thus, applying $\lim_\alpha$ to the above inequality proves \eqref{eq82} for $p\in U$.
\end{proof}


\begin{lm}\label{lb342}
Suppose that $A$ is a dense subset of $X$. Let $U$ be an open subset of $A$. Then $A\cap U$ is dense in $U$.
\end{lm}

This lemma is clearly false if $U$ is not assumed to be open: simply take $U=X\setminus A$.

\begin{proof}[First proof]
Choose any $x\in U$. We want to find a net in $A\cap U$ converging to $x$. Since $A$ is dense in $X$, there is a net $(x_\alpha)_{\alpha\in J}$ in $X$ converging to $x$. Since $U$ is a neighborhood of $x$, $(x_\alpha)$ is eventually in $U$, say $x_\alpha\in U$ whenever $\alpha\geq\beta$. Then $(x_\alpha)_{\alpha\in J_{\geq \beta}}$ is a net in $A\cap U$ converging to $x$.
\end{proof}

\begin{proof}[Second proof]
We want to show that every nonempty open subset of $U$ intersects $A\cap U$. But an open subset of $U$ is precisely an open subset of $X$ contained inside $U$ (Exe. \ref{lb341}). So this set (when nonempty) must intersect $A$ because $A$ is dense in $X$.
\end{proof}



\begin{sexe}
In Thm. \ref{lb276}, weaken the metrizability of $Y$ to the condition that $Y$ is regular. (See the definition below.) Prove the conclusion of Thm. \ref{lb276}.
\end{sexe}

\begin{sdf}\label{lb504}
A topological space $Y$ is called \textbf{regular} \index{00@Regular topological space} if for every $y\in Y$ and every $U\in\Nbh_Y(y)$ there exists $V\in\Nbh_Y(y)$ such that $\Cl_Y(V)\subset U$.
\end{sdf}









\subsubsection{Immediate consequences of Thm. \ref{lb277}}

The following result is parallel to Thm. \ref{lb279} for uniformly convergent nets of continuous functions.

\begin{co}\label{lb303}
Let $(f_\alpha)_{\alpha\in I}$ be an equicontinuous net of functions $X\rightarrow\mc V$. Assume that $(f_\alpha)$ converges pointwise to $f:X\rightarrow\mc V$. Then $f$ is continuous.
\end{co}

\begin{proof}
Write $f_\infty=f$ and define $F$ by \eqref{eq88}. Then $F$ is continuous by (3)$\Rightarrow$(1) of Thm. \ref{lb277}. So $f=f_\infty$ is continuous, since $f$ is the composition of $F$ with the inclusion map $x\in X\mapsto (\infty,x)\in I^*\times X$.
\end{proof}






\begin{rem}\label{lb304}
In the future, we will study the general Tychonoff theorem, which says for example that if $(f_\alpha)_{\alpha\in I}$ is a net of functions $X\rightarrow\Rbb^N$ which is pointwise bounded, i.e. $\sup_{\alpha\in I}\Vert f(x)\Vert<+\infty$ for all $x\in X$, then $(f_\alpha)$ has a subnet converging poinwise. However, if we assume moreover that each $f_\alpha$ is continuous, we cannot conclude in general that $(f_\alpha)$ has a subnet converging pointwise to a \textit{continuous} function. But we can make such a conclusion when $(f_\alpha)$ is equicontinuous, thanks to Cor. \ref{lb303}. Therefore, Cor. \ref{lb303} tells us that equicontinuity is useful for studying the problems of compactness of families of continuous functions (under the pointwise convergence topology). Cf. Thm. \ref{lb509}.
\end{rem}


In fact, we have a slightly stronger version of Cor. \ref{lb303}:

\begin{co}\label{lb310}
Let $(f_\alpha)_{\alpha\in I}$ be a net of functions $X\rightarrow\mc V$ equicontinuous at $x$. Assume that $(f_\alpha)$ converges pointwise to $f:X\rightarrow\mc V$. Then $f$ is continuous at $x$.
\end{co}

\begin{proof}
Define $X_x$ to be the same as $X$ as a set, but has a different topology: the one generated by the basis
\begin{align*}
\mc B_x=\Nbh_X(x)\cup\big\{\{p\}:p\neq x\big\}
\end{align*}
(cf. Exe. \ref{lb308}). Define $g_\alpha:X_x\rightarrow\mc V$ and $g:X_x\rightarrow\mc V$ to be the same as $f_\alpha$ and $f$. Then $(g_\alpha)$ is a net of equicontinuous functions converging pointwise to $g$. Therefore, by Cor. \ref{lb303}, $g$ is continuous. So $f$ is continuous at $x$. 
\end{proof}



\begin{rem}
By a similar argument, we can generalize Thm. \ref{lb279} to the following form: Let $(f_\alpha)$ be a net of functions $X\rightarrow \mc V$ converging uniformly to $f:X\rightarrow\mc V$. Suppose that each $f_\alpha$ is continuous at $x$. Then $f$ is continuous at $x$.
\end{rem}






\begin{eg}\label{lb311}
In this example, we pretend to know derivatives. Let $(f_n)$ be a sequence of functions $\Rbb_{\geq0}\rightarrow\Rbb$ defined by $f_n(x)=x^{1/n}$. (We understand $0^{\frac 1n}=0$.) Find all $x\in\Rbb_{\geq 0}$ at which $(f_n)$ is equicontinuous.
\end{eg}

\begin{proof}
We prove that $\Rbb_{>0}$ is the set of all points at which $(f_n)$ is equicontinuous. First, assume $x>0$. Choose $0<a<1<b$ such that $a<x<b$. Then, on $[a,b]$, $f_n'(x)=\frac 1n x^{\frac 1n-1}$ is bounded by $C=\max\{a^{-1},b\}$. So $(f_n|_{[a,b]})$ has Lipschitz constant $C$ by Exp. \ref{lb309}. So $(f_n)$ is equicontinuous at $x$. 

Note that $(f_n)$ converges pointwise to $f$ where $f(x)=1$ if $x>0$ and $f(0)=0$. But $f$ is not continuous at $0$. So $(f_n)$ is not equicontinuous at $0$ due to Cor. \ref{lb310}.
\end{proof}

One can also prove that $(f_n)$ is equicontinuous on $(0,1)\cup(1,+\infty)$ without using derivatives: See Thm. \ref{lb286}-1.





\begin{co}\label{lb284}
Let $(f_\alpha)_{\alpha\in I}$ be a net in $C(X,\mc V)$ converging pointwise to $f\in\mc V^X$. Consider the following statements:
\begin{enumerate}[label=(\arabic*)]
\item $(f_\alpha)_{\alpha\in I}$ converges uniformly to $f$.
\item $(f_\alpha)_{\alpha\in I}$ is equicontinuous.
\end{enumerate}
Then the following are true.
\begin{enumerate}
\item If $(f_\alpha)_{\alpha\in I}$ is a sequence $(f_n)_{n\in\Zbb_+}$, then (1)$\Rightarrow$(2).
\item If $X$ is compact, then (2)$\Rightarrow$(1).
\end{enumerate}
\end{co}


\begin{proof}
Immediate from Thm. \ref{lb277}.
\end{proof}









\subsection{Uniform convergence and double limits}



In this section, we use Thm. \ref{lb277} to study the problem of when the order of double limits can be exchanged. We first give a useful criterion on uniform convergence.


\begin{pp}\label{lb288}
Assume that $\mc V$ is a Banach space. Let $(f_\alpha)_{\alpha\in I}$ be a net in $C(X,\mc V)$. Assume that $(f_\alpha)$ converges uniformly on a dense subset $E$ of $X$. Then $(f_\alpha)$ converges uniformly on $X$ to some $f\in C(X,\mc V)$.
\end{pp}

The completeness of $\mc V$ is important here.

\begin{proof}
Since $E$ is dense, by \eqref{eq80} (applied to the function $|f_\alpha-f_\beta|$), we have
\begin{align*}
\sup_{x\in X}\Vert f_\alpha(x)-f_\beta(x)\Vert=\sup_{x\in E}\Vert f_\alpha(x)-f_\beta(x)\Vert
\end{align*}
where the RHS converges to $0$ under $\lim_{\alpha,\beta}$. Thus, $(f_\alpha)$ is a Cauchy net in $\mc V^X$ where $\mc V^X$ is equipped with the uniform convergence metric as in Exp. \ref{lb272}. So $(f_\alpha)_{\alpha\in I}$ converges uniformly on $X$ to some $f:X\rightarrow\mc V$ by Thm. \ref{lb339}. By Thm. \ref{lb279}, $f$ is continuous.
\end{proof}




\begin{thm}[\textbf{Moore-Osgood theorem}]\label{lb289}  \index{00@Moore-Osgood theorem}
Let $(f_{\alpha,\beta})_{(\alpha,\beta)\in I\times J}$ be a net in a Banach space $\mc V$ with index set $I\times J$ where $I,J$ are directed sets. Assume the following conditions:
\begin{enumerate}[label=(\arabic*)]
\item For each $\alpha\in I$, there exists $f_{\alpha,\infty}\in\mc V$ such that 
\begin{align}
\lim_{\beta\in J} f_{\alpha,\beta}=f_{\alpha,\infty}
\end{align}
\item For each $\beta\in J$, there exists $f_{\infty,\beta}\in\mc V$ such that
\begin{align}
\lim_{\alpha\in I}\sup_{\beta\in J}\Vert f_{\alpha,\beta}-f_{\infty,\beta}\Vert=0
\end{align}
\end{enumerate}
Then the following are true:
\begin{enumerate}
\item The following limits exist and are equal:
\begin{align}
\lim_{(\alpha,\beta)\in I\times J}f_{\alpha,\beta}=\lim_{\alpha\in I}f_{\alpha,\infty}=\lim_{\beta\in J}f_{\infty,\beta}
\end{align}
\item If $I=\Nbb$ where $\Nbb$ is equipped with the usual order, then
\begin{align}\label{eq89}
\lim_{\beta\in J}\sup_{\alpha\in I}\Vert f_{\alpha,\beta}-f_{\alpha,\infty}\Vert=0
\end{align}
\end{enumerate}
\end{thm}

Conditions (1) and (2) in Thm. \ref{lb289}, which say that the limit of $f_{\alpha,\beta}$ is pointwise over one index and uniform in another index, should remind you of conditions (2) and (3) in Thm. \ref{lb277} (cf. Rem. \ref{lb340}). In fact, we shall use Thm. \ref{lb277} to understand and prove the Moore-Osgood theorem.


\begin{proof}
Part 1: By Thm. \ref{lb122}, it suffices to prove that $\lim_{\alpha,\beta}f_{\alpha,\beta}$ converges. Define topological spaces $I^*=I\cup\{\infty_I\}$ and $J^*=J\cup\{\infty_J\}$ as in Subsec. \ref{lb282}. Define 
\begin{gather*}
g_\alpha:J^*\rightarrow\mc V\qquad g_\alpha(\nu)=f_{\alpha,\nu}
\end{gather*}
where $g_\alpha(\infty_J)=f_{\alpha,\infty}$. By (1), for each $\alpha\in I$, the function $g_\alpha$ is continuous at $\infty_J$, and hence $g_\alpha\in C(J^*,\mc V)$. By (2), $(g_\alpha)_{\alpha\in I}$ converges uniformly on $J$. Since $J$ is a dense subset of $J^*$, by Prop. \ref{lb288}, $(g_\alpha)_{\alpha\in I}$ converges uniformly on $J^*$ to some $g_{\infty_I}:J^*\rightarrow\mc V$. Thus, by Thm. \ref{lb277}, the function
\begin{align*}
F:I^*\times J^*\rightarrow\mc V\qquad F(\mu,\nu)=g_\mu(\nu)
\end{align*}
is continuous. Its continuity at $(\infty_I,\infty_J)$ implies that $\dps\lim_{\alpha\in I,\beta\in J}f_{\alpha,\beta}=\dps\lim_{\alpha\in I,\beta\in J} F(\alpha,\beta)$ converges to $F(\infty_I,\infty_J)$.

Part 2: By Thm. \ref{lb277}, $(g_\alpha)_{\alpha\in I}$ is an equicontinuous family of functions $J^*\rightarrow\mc V$. Its equicontinuity at $\infty_J$ means precisely \eqref{eq89}.
\end{proof}


\begin{rem}
Whenever you see a theorem stated in very plain language but proved using a huge machinery, you should always ask yourself if a direct proof is possible. A huge machinery or fancy language is not always necessary for the proof, but often helps to understand the nature of the problem.

Now, since Thm. \ref{lb289} is stated without using the language of topological spaces and continuous maps, it is desirable to have a direct and elementary proof. It could be done by directly translating the above proof (and the proof of the results cited in that proof) into the pure language of nets. However, we prefer to give a simpler proof which is related to, but is not a direct translation of, the above topological proof.   \hfill\qedsymbol
\end{rem}





\begin{proof}[\textbf{A direct proof of Thm. \ref{lb289}}]
Part 1: By Thm. \ref{lb122}, it suffices to prove that $\lim_{\alpha,\beta}f_{\alpha,\beta}$ converges.  Since $V$ is complete, it suffices to prove the Cauchy condition:
\begin{itemize}
\item[(i)] For each $\eps>0$, there exists $(\alpha,\beta)\in I\times J$ such that for all $(\mu,\nu)\in I_{\geq\alpha}\times J_{\geq\beta}$, we have $\Vert f_{\alpha,\beta}-f_{\mu,\nu}\Vert<\eps$.
\end{itemize}
Choose $\eps>0$. By condition (2) of Thm. \ref{lb289}, $(f_{\alpha,\beta})$, as a net of functions $J\rightarrow \mc V$ with index set $I$, converges uniformly. Thus, by the Cauchy condition for the uniform convergence metric as in Exp. \ref{lb272} (which is available due to Thm. \ref{lb339}), we have:
\begin{itemize}
\item[(ii)] There exists $\alpha\in I$ such that for all $\mu\geq\alpha$, we have $\sup_{\nu\in J}\Vert f_{\alpha,\nu}-f_{\mu,\nu}\Vert<\eps/2$.
\end{itemize}
Fix $\alpha$ as above. By condition (1), the limit $\lim_{\beta\in J} f_{\alpha,\beta}$ exists. Thus:
\begin{itemize}
\item[(iii)] There exists $\beta\in J$ such that for all $\nu\geq\beta$ we have $\Vert f_{\alpha,\beta}-f_{\alpha,\nu}\Vert<\eps/2$.
\end{itemize}
Combining (ii) and (iii) and using triangle inequality, we see that for each $\mu\geq\alpha$ and $\nu\geq\beta$,
\begin{align*}
\Vert f_{\alpha,\beta}-f_{\mu,\nu}\Vert\leq \Vert f_{\alpha,\beta}-f_{\alpha,\nu}\Vert+\Vert f_{\alpha,\nu}-f_{\mu,\nu}\Vert<\eps
\end{align*}
This proves (i).\\[-1ex]

Part 2: Assume $I=\Nbb$. Since $(f_{\alpha,\beta})$ is a Cauchy net, for each $\eps>0$ there exist $\alpha_0\in I,\beta_0\in J$ such that for all $\alpha\geq\alpha_0$ and  $\beta,\nu\geq\beta_0$ we have $\Vert f_{\alpha,\beta}-f_{\alpha,\nu}\Vert<\eps$. Applying $\lim_\nu$ gives
\begin{align*}
\Vert f_{\alpha,\beta}-f_{\alpha,\infty}\Vert\leq\eps\qquad(\forall\alpha\geq\alpha_0,\beta\geq\beta_0)
\end{align*}
Since $I=\Nbb$, there are finitely many $\alpha$ not $\geq\alpha_0$.  For any such $\alpha$, by condition (1) of Thm. \ref{lb289}, there exists $\beta_\alpha\in J$ such that for all $\beta\geq\beta_{\alpha}$, we have $\Vert f_{\alpha,\beta}-f_{\alpha,\infty}\Vert\leq\eps$. Choose $\wtd\beta$  greater than or equal to $\beta_0$ and all these (finitely many) $\beta_\alpha$. Thus, we have $\Vert f_{\alpha,\beta}-f_{\alpha,\infty}\Vert\leq\eps$ for all $\alpha\in I$ and all $\beta\geq\wtd\beta$. This proves \eqref{eq89}.
\end{proof}


\begin{rem}
The main theme of this chapter is the study of the relationship between the convergence of $\lim_{\alpha,\beta}f_{\alpha,\beta}$ and the uniform convergence of $\lim_\alpha f_{\alpha,\beta}$ and $\lim_\beta f_{\alpha,\beta}$. The main results of this chapter (i.e. Thm. \ref{lb274}, Thm. \ref{lb277} (together with Prop. \ref{lb281}), and Moore-Osgood Thm. \ref{lb289}) can be summarized as follows:
\begin{enumerate}
\item[(1)] If one of $\lim_\alpha f_{\alpha,\beta}$ and $\lim_\beta f_{\alpha,\beta}$ converges uniformly and the other one converges pointwise, then $\lim_{\alpha,\beta}f_{\alpha,\beta}$ converges. (Consequently, by Thm. \ref{lb122}, we have $\lim_\alpha\lim_\beta f_{\alpha,\beta}=\lim_\beta\lim_\alpha f_{\alpha,\beta}=\lim_{\alpha,\beta}f_{\alpha,\beta}$.)
\item[(2)] If $\lim_\alpha f_{\alpha,\beta}$, $\lim_\beta f_{\alpha,\beta}$, and $\lim_{\alpha,\beta}f_{\alpha,\beta}$ all converge pointwise, and if ``there is a compactness on $\beta$", then $\lim_\alpha f_{\alpha,\beta}$ converges uniformly over all $\beta$.
\end{enumerate}
The detailed statements of (1) and (2) are given in Thm. \ref{lb277} ((together with Prop. \ref{lb281})), or equivalently, in Thm. \ref{lb274}. (See also Rem. \ref{lb340}.) Although Thm. \ref{lb274} and Thm. \ref{lb277} look very different, they are actually telling the same story. (We have proved Thm. \ref{lb277} from Thm. \ref{lb274}. But it is not hard to see that Thm. \ref{lb274} also implies Thm. \ref{lb277}.) The Moore-Osgood theorem is only about part (1), but not about part (2). (Or, more accurately, the second part of Moore-Osgood corresponds to a very weak version of (2).)
\end{rem}











%% Record #12 2023/10/30 two lectures  29





















\subsection{Problems and supplementary materials}


Recall from the beginning of this chapter that $X$ is a topological space and $\mc V$ is a normed vector space over $\Fbb\in\{\Rbb,\Cbb\}$.

\begin{prob}\label{lb283}
Let $I$ be a directed set. Let $I^*=I\cup\{\infty\}$, equipped with the standard topology as in Subsec. \ref{lb282}. Prove that the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $I^*$ is compact.
\item For each $\alpha\in I$, the complement of $I^*_{\geq\alpha}=\{\beta\in I^*:\beta\geq\alpha\}$ is a finite set.
\end{enumerate}
\end{prob}


\begin{proof}[Note]
Prop. \ref{lb249} can make your proof shorter.
\end{proof}


\begin{prob}
Let $(f_n)$ be a sequence of functions $\Rbb_{\geq0}\rightarrow\Rbb$ defined by $f(x)=x^{\frac 1n}$ (as in Exp. \ref{lb311}). Give a direct proof that $(f_n)$ is not equicontinuous at $0$ using the definition of equicontinuity. Do not use Cor. \ref{lb310}.
\end{prob}


\begin{prob}
Give a direct proof of Cor. \ref{lb303} without using Thm. \ref{lb274} (and its consequences) or using Thm. \ref{lb289}.
\end{prob}

\begin{prob}
Give a direct proof of Cor. \ref{lb284} without using Thm. \ref{lb274} (and its consequences) or using Thm. \ref{lb289}.
\end{prob}

\begin{sprob}\label{lb285}
Let $(f_\alpha)_{\alpha\in I}$ be a net in $C(X,\Rbb)$. Assume that $(f_\alpha)_{\alpha\in I}$ is increasing, i.e., $f_\alpha\leq f_\beta$ whenever $\alpha\leq\beta$. Assume that $(f_\alpha)_{\alpha\in I}$ converges pointwise to $f\in C(X,\Rbb)$. Prove that for every $x\in X$,
\begin{align}
\lim_{
\begin{subarray}{c}
\alpha\in I\\
p\rightarrow x
\end{subarray}
}
f_\alpha(p)=f(x)
\end{align}
in the sense of Prop. \ref{lb281}. Namely, prove that for every $x\in X$ and $\eps>0$ there exist $\beta\in I$ and $U\in\Nbh_X(x)$ such that $|f_\alpha(p)-f(x)|<\eps$ for all $\alpha\geq\beta$ and all $p\in U$.
\end{sprob}

\begin{proof}[Note]
Let $g_\alpha=f-f_\alpha$. Then $(g_\alpha)_{\alpha\in I}$ is a decreasing net of continuous functions converging pointwise to $0$. It suffices to prove the easier statement that $\dps\lim_{\alpha\in I,p\rightarrow x}g_\alpha(p)=0$ for every $x\in X$. (Why is this sufficient?)
\end{proof}

\begin{sthm}\label{lb286}
Let $(f_\alpha)_{\alpha\in I}$ be a net in $C(X,\Rbb)$. Assume that $(f_\alpha)_{\alpha\in I}$ is increasing and converges pointwise to $f\in C(X,\Rbb)$. The following statements are true.
\begin{enumerate}
\item If $(f_\alpha)_{\alpha\in I}$ is a sequence $(f_n)_{n\in\Zbb_+}$, then $(f_n)_{n\in\Zbb_+}$ is equicontinuous.
\item (\textbf{Dini's theorem}) \index{00@Dini's theorem} If $X$ is compact, then $(f_\alpha)_{\alpha\in I}$ converges uniformly to $f$.
\end{enumerate}
\end{sthm}


\begin{proof}
By Pb. \ref{lb285}, $(f_\alpha)_{\alpha\in I}$ and $f_\infty=f$ satisfy (1') of Prop. \ref{lb281}. Therefore, the two statements follow directly from Thm. \ref{lb277}.
\end{proof}


\begin{eg}\label{lb401}
Let $f_n:(0,1)\rightarrow \Rbb$ be $f_n(x)=x^n$, where $n\in\Zbb$. Then $(f_n)_{n\in\Zbb}$ is a decreasing net of continuous functions converging pointwise to $0$. Here, $\Zbb$ is given the usual order ``$\leq$". By Pb. \ref{lb285}, $(f_n)$ satisfies
\begin{align*}
\lim_{(n,p)\rightarrow(+\infty,x)
}f_n(p)=f(x)
\end{align*}
for all $x\in (0,1)$. (So it satisfies condition (1) of Thm. \ref{lb277}.) However, $(f_n)_{n\in\Zbb}$ is neither equicontinuous (since $\sup_{n\in\Zbb}|f_n(p)-f_n(x)|=+\infty$ whenever $0<p<x<1$) nor converging uniformly to $0$ (since $\sup_{x\in (0,1)}|f_n(x)|=1$ if $n>0$). Accordingly, $(0,1)$ is not compact, and $I^*=I\cup\{\infty_I\}$ is not compact if $I=\Zbb$. 

However, if we replace $\Zbb$ by $\Zbb_+$, then $(f_n)_{n\in\Zbb_+}$ is equicontinuous by Exp. \ref{lb309} (applied to any compact subinterval of $(0,1)$), or by Thm. \ref{lb286}. But $(f_n)_{n\in\Zbb_+}$ is still not uniformly convergent. 
 \hfill\qedsymbol
\end{eg}


\begin{prob}\label{lb453}
Let $X_1,X_2,\dots$ be a sequence of nonempty topological spaces. Let $S=\prod_{n\in\Nbb_+}X_n$, equipped with the product topology. Let $f: S \rightarrow \mathbb{R}$ be continuous. Fix $\left(p_n\right)_{n \in \mathbb{Z}_{+}} \in S$. For each $n \in \mathbb{Z}_{+}$, define
\begin{gather*}
\varphi_n : S \rightarrow S \\
\left(x_1, x_2, \ldots, x_{n-1}, x_n, x_{n+1}, \ldots\right)  \mapsto\left(x_1, x_2, \ldots, x_{n-1}, p_n, p_{n+1}, \ldots\right)
\end{gather*}
Prove for every $x_\blt=(x_n)_{n\in\Zbb_+}$ that
\begin{align*}
\lim_{
\begin{subarray}{c}
(n,y_\blt)\in \Zbb_+\times S\\
(n,y_\blt)\rightarrow(\infty,x_\blt)
\end{subarray}
}f\circ\varphi_n(y_\blt)=f(x_\blt)
\end{align*}
in the sense of Prop. \ref{lb281}. Conclude that $(f\circ\varphi_n)_{n\in\Zbb_+}$ is equicontinuous, and that if each $X_n$ is compact then $(f\circ\varphi_n)$ converges uniformly to $f$. (Recall that in this case $S$ is compact by the countable Tychonoff theorem, cf. Pb. \ref{lb241}.)
\end{prob}










\begin{comment}


Let $g_n:[0,1]\rightarrow\Rbb$ be $g_n(x)=n^{-1}x^n$ if $n\neq0$. Equip $\Zbb^\times=\Zbb\setminus\{0\}$ with the usual order $\leq$. Then $(g_n)_{n\in\Zbb^\times}$ is a net of continuous functions converging pointwise to $0$, and is decreasing over $n$ when  $n\in\Zbb_+$.  Thus, by Pb. \ref{lb285}, for every $x\in[0,1]$, we have $\dps\lim_{(n,p)\rightarrow(+\infty,x)
}g_n(p)=g(x)$. Clearly $(g_n)_{n\in\Zbb^\times}$ converges uniformly to $0$. However, it is not hard to check that this net of functions is not equicontinuous. (The bad thing happens when $n<0$.) 


These examples indicate why condition (1) of Thm. \ref{lb277} or (1') of Prop. \ref{lb281} is more natural than ``equicontinuity + pointwise convergence": Condition (1') of Prop. \ref{lb281} can be viewed as a continuity condition which is not ``equi" over all $\alpha$ in the index set $I$, but is ``equi" for sufficiently large $\alpha$.
\end{comment}

















\newpage


\section{Extending continuous functions to the closures}



\subsection{Introduction}

Let $\wtd X,Y$ be topological spaces, let $X\subset \wtd X$, and let $f:X\rightarrow Y$ be a continuous map. The \textbf{extension problem} asks whether $f$ can be extended to a continuous map $\wtd f:\wtd X\rightarrow Y$. ``Extended" means that $\wtd f|_X=f$.  Since we can try to first extend $f$ from $X$ to its closure $\Cl_{\wtd X}(X)$, and then from $\Cl_{\wtd X}(X)$ to $\wtd X$, the extension problem can naturally be divided into two cases: (1) $X$ is dense in $\wtd X$. (2) $X$ is closed in $\wtd X$.

In this chapter, we study the first case. (The second case will be discussed in Sec. \ref{lb464}.) Assume that $X$ is dense in $\wtd X$. Then by Prop. \ref{lb196}, we know that $f$ can have at most one extension if $Y$ is Hausdorff. So there is essentially no uniqueness issue. 


The study of extension problem in this case has a long history. As we have seen in Subsec. \ref{lb290}, the limits of functions can be understood in this light: If $x\in\wtd X\setminus X$, then $f$ can be extended to a continuous function on $X\cup\{x\}$ iff $\lim_{p\rightarrow x}f(p)$ exists. Of course, this is simply a rephrasing of the definition of $\lim_{p\rightarrow x}f(p)$. But the idea of ``extending $f$ to $\wtd X$ by first extending $f$ to a slightly larger set with one extra point $\{x\}$" is helpful and can sometimes simplify proofs.

Indeed, recall that in the proof of Prop. \ref{lb281} we used Thm. \ref{lb276}, which tells us that if $\lim_{p\rightarrow x}f(p)$ converges for all $x\in\wtd X\setminus X$, then $f$ can be extended (necessarily uniquely) to a continuous $\wtd f:\wtd X\rightarrow Y$ where $Y$ is assumed metrizable. Thus, \uline{the extensibility of $f$ to $\wtd X$ can be checked pointwise}. Thm. \ref{lb276} is our first important general result on the extension problem. Let me state Thm. \ref{lb276} in the following equivalent way, which is more convenient for the  study of extension problems.
\begin{co}\label{lb307}
Let $f:X\rightarrow Y$ be a continuous map of topological spaces where $Y$ is metrizable and $X$ is a dense subspace of a topological space $\wtd X$. The following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item There exists a continuous map $\wtd f:\wtd X\rightarrow Y$ such that $\wtd f|_X= f$.
\item For each $x\in\wtd X\setminus X$, the limit $\lim_{p\rightarrow x}f(p)$ exists.
\end{enumerate}
\end{co}

\begin{proof}
If (1) is true, then $\wtd f|_{X\cup\{x\}}$ is continuous whenever $x\in\wtd X\setminus X$. This proves (2). Conversely, assume (2). Extend $ f$ to a map $\wtd f:\wtd X\rightarrow Y$ by setting $\wtd f(x)=\lim_{p\rightarrow x}f(p)$ if $x\in\wtd X\setminus X$. Then $\wtd f$ is continuous by Thm. \ref{lb276}.
\end{proof}







This chapter will focus on another useful method for extending continuous functions in the setting of metric spaces. A main result (cf. Cor. \ref{lb298}) is that if $\wtd X,Y$ are metric spaces and $Y$ is complete, then a sufficient condition for the extensibility of $f$ onto $\wtd X$ is that $f$ is uniformly continuous. Moreover, uniform continuity is also a necessary condition if $\wtd X$ is compact. (Recall that compact metric spaces are complete, cf. Thm. \ref{lb79}.) It is worth mentioning that the extensibility of $f$ is a purely topological question, whereas the uniform continuity of $f$ depends on the metric on $\wtd X$.



\subsection{Uniform continuity}


Fix metric spaces $\wtd X,Y$ and a dense subset $X\subset\wtd X$. 

\subsubsection{Basics}


We first give some examples of $f\in C(X,Y)$ that cannot be extended to a continuous function on $\wtd X$. Since uniform continuity is a sufficient condition for the extensibility (when $Y$ is complete), these examples are not uniformly continuous. Recall our convention that subsets of $\Rbb$ or $\Rbb^N$ are equipped with the Euclidean metrics.

\begin{eg}
Assume $X=(0,+\infty)$, $\wtd X=[0,+\infty)$, $Y=\Rbb$, and $f:X\rightarrow Y$ is defined by $f(x)=1/x$. Then $\lim_{x\rightarrow 0}f(x)$ is $+\infty$ in $\ovl\Rbb$, and hence does not converge in $Y$. So $f$ cannot be extended onto $\wtd X$.
\end{eg}

\begin{eg}
$X=(0,1]$, $\wtd X=[0,1]$, $Y=\Rbb$, $f:X\rightarrow Y$ is defined by $f(x)=\sin(1/x)$. Then $\lim_{n\rightarrow\infty}f(x_n)$ equals $0$ if the sequence $(x_n)$ in $X$ converging to $0$ is defined by $x_n=1/2n\pi$, and equals $1$ if $x_n=1/(2n+\frac 12)\pi$. Thus, by Rem. \ref{lb202}, $\lim_{x\rightarrow0}f(x)$ does not exist in $Y$. So $f$ cannot be extended onto $\wtd X$.
\end{eg}


\begin{df}\label{lb291}
A map $f:X\rightarrow Y$ is called \textbf{uniformly continuous} \index{00@Uniformly continuous} if the following equivalent conditions are satisfied:
\begin{enumerate}[label=(\arabic*)]
\item For every $\eps>0$ there exists $\delta>0$ such that for all $x,x'\in X$ we have
\begin{subequations}
\begin{align}\label{eq91}
d(x,x')<\delta\qquad\Longrightarrow\qquad d(f(x),f(x'))<\eps
\end{align}
\item For every nets $(x_\alpha)_{\alpha\in I},(x'_\alpha)_{\alpha\in I}$ in $X$ (with the same index set $I$) we have
\begin{align}\label{eq92}
\lim_{\alpha\in I} d(x_\alpha,x'_\alpha)=0\qquad\Longrightarrow\qquad\lim_{\alpha\in I} d(f(x_\alpha),f(x'_\alpha))=0
\end{align}
\item For every sequences $(x_n)_{n\in\Zbb_+},(x_n')_{n\in\Zbb_+}$ in $X$ we have
\begin{align}\label{eq93}
\lim_{n\rightarrow\infty} d(x_n,x'_n)=0\qquad\Longrightarrow\qquad\lim_{n\rightarrow\infty} d(f(x_n),f(x'_n))=0
\end{align}
\end{subequations}
\end{enumerate}
\end{df}

Uniformly continuous maps are clearly continuous. Def. \ref{lb291}-(2) says that uniformly continuous functions are those sending Cauchy-equivalent nets (cf. Def. \ref{lb155}) to Cauchy-equivalent nets. 


\begin{proof}[Proof of equivalence]
(1)$\Rightarrow$(2): Assume (1). Choose nets $(x_\alpha)_{\alpha\in I},(x'_\alpha)_{\alpha\in I}$ in $X$. Choose any $\eps>0$. Choose $\delta>0$ such that \eqref{eq91} holds. If $d(x_\alpha,x'_\alpha)\rightarrow0$, then $d(x_\alpha,x'_\alpha)<\delta$ for sufficiently large $\alpha$. So $d(f(x_\alpha),f(x'_\alpha))<\eps$ for sufficiently large $\alpha$. This proves (2).

(2)$\Rightarrow$(3): Obvious.

$\neg$(1) $\Rightarrow$ $\neg$(3): Assume that (1) is false. Then there exists $\eps>0$ such that for all $\delta>0$  there exist $x,x'\in X$ with $d(x,x')<\delta$ such that $d(f(x),f(x'))\geq\eps$. Thus, for each $n\in\Zbb_+$, there exist $x_n,x_n'\in X$ such that $d(x_n,x_n')<1/n$ and $d(f(x_n),f(x_n'))\geq\eps$. The sequences $(x_n),(x_n')$ imply that \eqref{eq93} is false.
\end{proof}




\begin{co}\label{lb292}
Assume that $f:X\rightarrow Y$ is uniformly continuous. Let $(x_\alpha)_{\alpha\in I}$ be a Cauchy net in $X$. Then $(f(x_\alpha))_{\alpha\in I}$ is a Cauchy net in $Y$.
\end{co}

\begin{proof}
Apply Def. \ref{lb291}-(2) to the nets $(x_\alpha)_{(\alpha,\beta)\in I^2}$ and $(x_\beta)_{(\alpha,\beta)\in I^2}$ of $X$.
\end{proof}








\subsubsection{Extensibility of uniformly continuous functions}



The following theorem can be viewed as the uniform continuity version of Prop. \ref{lb288}. In particular, both results assume the completeness of the codomain.

\begin{thm}\label{lb297}
Let $f:X\rightarrow Y$ be uniformly continuous, and assume that $Y$ is complete. Then there exists a (necessarily unique) uniformly continuous $\wtd f:\wtd X\rightarrow Y$ satisfying $\wtd f|_X=f$.
\end{thm}


\begin{proof}
Choose any $x\in\wtd X$. Since $X$ is dense in $\wtd X$, we can choose a sequence $(x_n)$ in $X$ converging to $x$ in $\wtd X$. In particular, $(x_n)$ is a Cauchy sequence. Therefore, by Cor. \ref{lb292}, $(f(x_n))$ is a Cauchy sequence in $Y$, which converges to some point $\wtd f(x)\in Y$ by the completeness of $Y$. If $x\in X$, we assume that $(x_n)$ is the constant sequence $x$. This shows that $\wtd f(x)=f(x)$ if $x\in X$.

We have constructed a function $\wtd f:\wtd X\rightarrow Y$ satisfying $\wtd f|_X=f$. Let us prove that $\wtd f$ is uniformly continuous. Choose any $\eps>0$. Since $f$ is uniformly continuous, there is $\delta>0$ such that for all $p,q\in X$ we have
\begin{align*}
d(p,q)<2\delta\qquad\Longrightarrow\qquad d(f(p),f(q))<\eps/2
\end{align*}
Choose any $x,x'\in \wtd X$ satisfying $d(x,x')<\delta$. By our construction of $\wtd f$, there are sequences $(x_n)$ in $X$ converging to $x$ and $(x_n')$ in $X$ converging to $x'$ such that $f(x_n)\rightarrow \wtd f(x)$ and $f(x_n')\rightarrow\wtd f(x')$. Thus, there exist $N\in\Zbb_+$ such that for all $n\geq N$ we have
\begin{align*}
d(x,x_n)<\frac\delta 2\qquad d(x',x_n')<\frac\delta 2\qquad d(\wtd f(x),f(x_n))<\frac\eps 4\qquad d(\wtd f(x'),f(x'_n))<\frac\eps 4
\end{align*}
Choose $n=N$. Then by triangle inequality, we have $d(x_n,x_n')<2\delta$, and hence $d(f(x_n),f(x_n'))<\eps/2$. So $d(\wtd f(x),\wtd f(x'))<\eps$ by triangle inequality again.
\end{proof}






We now study the other direction. The following theorem implies that if a continuous $f:X\rightarrow Y$ can be extended to a continuous $\wtd f:\wtd X\rightarrow Y$, and if $\wtd X$ is compact, then $f$ is uniformly continuous.


\begin{thm}\label{lb294}
Suppose that $f:X\rightarrow Y$ is continuous and $X$ is compact. Then $f$ is uniformly continuous.
\end{thm}

In the same spirit as in Sec. \ref{lb293}, we give two proofs for this theorem, one using sequences and the other using open covers.

\begin{proof}[First proof]
Assume that $f$ is not uniformly continuous. By Def. \ref{lb291}-(3), there exist sequences $(x_n)$ and $(x_n')$ in $X$ such that $\lim_{n\rightarrow\infty}d(x_n,x_n')=0$, and that $d(f(x_n),f(x_n'))\nrightarrow0$. The latter means that there is $\eps>0$ such that $d(f(x_n),f(x_n'))$ is frequently $\geq\eps$. Thus, by passing to a subsequence, we may assume that $d(f(x_n),f(x_n'))\geq\eps$ for all $n$. Since $X\times X$ is sequentially compact, by replacing $(x_n,x_n')$ with a convergent subsequence, we assume that $x_n\rightarrow x$ and $x_n'\rightarrow x'$ where $x,x'\in X$. Since $d(x_n,x_n')\rightarrow0$, we must have $x=x'$. By the continuity of $f$, $f(x_n)$ and $f(x_n')$ converge to $f(x)=f(x')$, contradicting the fact that $d(f(x_n),f(x_n'))\geq\eps$ for all $n$. 
\end{proof}


To prove Thm. \ref{lb294} using open covers, we prove a more general result instead. The following theorem is useful for proving properties of the form ``there exists $\delta>0$ such that for all $x,x'\in X$ satisfying $d(x,x')<\delta$, we have ...".


\begin{thm}[\textbf{Lebesgue number lemma}]\index{00@Lebesgue number lemma}  \label{lb295}
Assume that the metric space $X$ is compact. Let $\fk U\subset 2^X$ be an open cover of $X$. Then there exists $\delta>0$ satisfying the following conditions:
\begin{itemize}
\item For every $x\in X$ there exists $U\in\fk U$ such that $B_X(x,\delta)\subset U$.
\end{itemize}
\end{thm}

The number $\delta$ in Thm. \ref{lb295} is called a \textbf{Lebesgue number of $\fk U$}. \index{00@Lebesgue number} In the following proof, we follow the local-to-global strategy as in Sec. \ref{lb293}. 

\begin{proof}
Choose any $p\in X$. Then there is $U\in\fk U$ containing $p$. So there is $\delta_p>0$ such that $B(p,2\delta_p)\subset U$. Therefore, there exists $V_p\in\Nbh_X(p)$ such that for every $x$ in $V_p$ we have $B(x,\delta_p)\subset U$. (Simply take $V_p=B(p,\delta_p)$.) This solves the problem locally: for each $x\in V_p$, the ball $B(x,\delta_p)$ is a subset of some member of $\fk U$.

Since $X=\bigcup_{p\in X}V_p$ and since $X$ is compact, there is a finite subset $E\subset X$ such that $X=\bigcup_{p\in E}V_p$. Take $\delta=\min\{\delta_p:p\in E\}$. For each $x\in X$, choose $p\in E$ such that $x\in V_p$. Then $B(x,\delta_p)$ is a subset of some member of $\fk U$ by the last paragraph. So the same is true for $B(x,\delta)$.
\end{proof}


Of course, similar to the examples studied in Sec. \ref{lb293}, Thm. \ref{lb295} can also be proved by contradiction and by using sequential compactness. See Pb. \ref{lb296}.



\begin{proof}[\textbf{Second proof of Thm. \ref{lb294}}]
Let us verify Def. \ref{lb291}-(1). Choose any $\eps>0$. For each $x\in X$, the set $U_x=f^{-1}(B_Y(f(x),\eps/2))$ is a neighborhood of $x$ by Prop. \ref{lb191}. So $\{U_x:x\in X\}$ is an open cover of $X$. Let $\delta$ be a Lebesgue number of $\fk U$. Choose any $x,y\in X$ satisfying $d(x,y)<\delta$.  Then $B_X(x,\delta)\subset U_{z}$ for some $z\in X$. So $x,y\in B_X(x,\delta)$ and hence $x,y\in U_z$. Therefore,
\begin{align*}
d(f(x),f(y))\leq d(f(x),f(z))+d(f(z),f(y))<\eps/2+\eps/2=\eps
\end{align*}
\end{proof}



\begin{co}\label{lb298}
Choose $f\in C(X,Y)$. Consider the following statements:
\begin{enumerate}[label=(\arabic*)]
\item $f$ is uniformly continuous.
\item There exists $\wtd f\in C(\wtd X,Y)$ such that $\wtd f|_X=f$.
\end{enumerate}
Then (1)$\Rightarrow$(2) if $Y$ is complete, and (2)$\Rightarrow$(1) if $\wtd X$ is compact.
\end{co}

\begin{proof}
Immediate from Thm. \ref{lb297} and Thm. \ref{lb294}.
\end{proof}


\begin{eg}
Let $D=B_\Cbb(0,1)=\{z\in\Cbb:|z|<1\}$ and $\Sbb^1=\{z\in\Cbb:|z|=1\}$. Let $f:D\rightarrow Y$ be a continuous function where $Y$ is a complete metric space. Then $f$ is uniformly continuous iff $\lim_{w\rightarrow z}f(w)$  exists for every $z\in \Sbb^1$.
\end{eg}

\begin{proof}
By Cor. \ref{lb307}, the limit $\lim_{w\rightarrow z}f(w)$  exists for every $z\in \Sbb^1$  iff $f$ can be extended to a continuous function $\wtd f:\ovl D=D\cup \Sbb^1\rightarrow Y$.  By Cor. \ref{lb298}, this is equivalent to that $f$ is uniformly continuous (because $\ovl D$ is compact and $Y$ is compete).
\end{proof}



\subsubsection{Uniform equicontinuity}

Although the notion of uniform equicontinuity will rarely be used in our notes, it is used in many textbooks.  So let me give a brief account of uniform equicontinuity. 



\begin{df}\label{lb316}
Let $(f_\alpha)_{\alpha\in I}$ be a family of functions $X\rightarrow Y$. (Here, the index set $I$ is not necessarily directed.) Define a metric $d$ on $Y^I$ in a similar way as \eqref{eq78}, namely, if $\mbf y,\mbf y'\in Y^I$ then
\begin{align*}
d(\mbf y,\mbf y')=\min\Big\{1,\sup_{\alpha\in I} d_Y(\mbf y(\alpha),\mbf  y'(\alpha))\Big\}
\end{align*}
We say that $(f_\alpha)_{\alpha\in I}$ is \textbf{uniformly equicontinuous} \index{00@Uniformly equicontinuous} if the map
\begin{gather}\label{eq106}
X\rightarrow Y^I\qquad x\mapsto (f_\alpha(x))_{\alpha\in I}
\end{gather}
is uniformly continuous with respect to the metric $d$. Clearly, this is equivalent to saying that:
\begin{itemize}
\item For every $\eps>0$ there exists $\delta>0$ such that for every $x,x'\in X$ satisfying $d_(x,x')<\delta$, we have
\begin{align*}
\sup_{\alpha\in I}d_Y(f_\alpha(x),f_\alpha(x'))<\eps
\end{align*}
\end{itemize}
\end{df}

Uniformly equicontinuous families of functions are equicontinuous, because uniformly continuous functions are continuous. Conversely, we have:

\begin{pp}
Assume that $X$ is compact and $(f_\alpha)_{\alpha\in I}$ is a family of functions $X\rightarrow Y$. Then $(f_\alpha)_{\alpha\in I}$ is equicontinuous iff it is uniformly equicontinuous.
\end{pp}

\begin{proof}
``$\Leftarrow$" is obvious, as mentioned above. ``$\Rightarrow$" follows immediately by applying Thm. \ref{lb294} to the continuous map \eqref{eq106}.
\end{proof}






\subsection{Completion of metric spaces}



Fix a metric space $X$ in this section. We are going to apply uniform continuity to the study of completions of metric spaces. Roughly speaking, a completion of $X$ is a complete metric space $\wht X$ containing $X$ as a dense subspace. However, completions are not unique, but are unique ``up to equivalence". So we want to show that two completions $\wht X,\wtd X$ of the same metric space $X$ are equivalent. However,  it is confusing to view $X$ as a subset of $\wtd X$ and $\wtd X$ simultaneously. A better approach is to consider (automatically injective) isometries $\varphi:X\rightarrow\wht X,\psi:X\rightarrow\wtd X$, and to show that $\varphi$ and $\psi$ are equivalent using the language of commutative diagrams (cf. Sec. \ref{lb299}).


\begin{df}
A \textbf{completion} \index{00@Completion of metric space} of the metric space $X$ is an isometry $\varphi:X\rightarrow\wht X$ where $\wht X$ is a complete metric space, and $\varphi(X)$ is dense in $\wht X$. We sometimes just say that $\wht X$ is a completion of $X$.
\end{df}


Thus, if $A$ is a dense subset of a complete metric space $B$, then the inclusion map $A\hookrightarrow B$ is a completion. Therefore, $\Rbb$ is a completion of $\Qbb$, and $[0,1]$ is a completion of $(0,1)$, $[0,1)$, $[0,1]\cap\Qbb$.



\begin{eg}
Let $A$ be a dense subset of a metric space $X$. Suppose that $\varphi:X\rightarrow\wht X$ is a completion of $X$. Then $\varphi|_A:A\rightarrow \wht X$ is clearly a completion of $A$. 
\end{eg}

\begin{eg}
Let $X$ be a subset of a complete metric space $Y$. Then $X\hookrightarrow\Cl_Y(X)$ is a completion of $X$ because every closed subset of $Y$ is complete (cf. Prop. \ref{lb86}), and hence $\Cl_Y(X)$ is complete. For example, $\{(x,y)\in\Rbb^2:x\geq0\}$ is a completion of both $A=\{(x,y)\in\Rbb^2:x>0\}$ and $B=A\cap\Qbb^2$.
\end{eg}



We want to prove that every metric space $X$ has a completion $\wht X$. First, we need a lemma, which can be viewed as analogous to Pb. \ref{lb287}.

\begin{lm}\label{lb300}
Suppose that $X$ is a dense subspace of a metric space $\wht X$. Suppose that every Cauchy sequence in $X$ converges to an element of $\wht X$. Then $\wht X$ is complete.
\end{lm}

\begin{proof}
Let $(x_n)$ be a Cauchy sequence in $\wht X$. Since $X$ is dense, there exists $x_n'\in X$ such that $d(x_n,x_n')<1/n$. So $(x_n')$ is Cauchy-equivalent to $(x_n)$. Thus $(x_n')$ is a Cauchy sequence by Exe. \ref{lb128}. By assumption, $(x_n')$ converges to some $x\in X$. So $(x_n')$ also converges to $x$ by Exe. \ref{lb128}.
\end{proof}



\begin{thm}\label{lb301}
Every metric space $X$ has a completion $\varphi:X\rightarrow\wht X$. Moreover, any completion $\psi:X\rightarrow\wtd X$ is \textbf{equivalent} (also called \textbf{isomorphic}) to $\varphi$ in the sense that there is an isometric isomorphism of metric spaces $\Phi:\wht X\rightarrow\wtd X$ such that the following diagram commutes:
\begin{equation}\label{eq94}
\begin{tikzcd}[column sep=small]
                     & X \arrow[ld,"\varphi"'] \arrow[rd,"\psi"] &   \\
\wht X \arrow[rr, "\Phi","\simeq"'] &                         & \wtd X
\end{tikzcd}
\end{equation}
\end{thm}


Recall that the commutativity of \eqref{eq94} means that $\psi=\Phi\circ\varphi$.

\begin{proof}[\textbf{Proof of existence}]
The construction of $\varphi:X\rightarrow\wht X$ is similar to the construction of $\Rbb$ from $\Qbb$ in Ch. \ref{lb167}. Let $\scr C$ be the set of Cauchy sequences in $X$. Let $\wht X=\scr C/\sim$ be the quotient set (cf. Def. \ref{lb157}) where $\sim$ is the Cauchy-equivalence relation: $(x_n)\sim(y_n)$ iff $\lim_{n\rightarrow\infty}d(x_n,y_n)=0$. We let $[x_n]_{n\in\Zbb_+}$ or simply let $[x_n]$ denote the equivalence class of $(x_n)$ in $\wht X$. The map $\varphi$ is defined by
\begin{align*}
\varphi:X\rightarrow\wht X\qquad x\mapsto [x]_{n\in\Zbb_+}
\end{align*}
where $[x]_{n\in\Zbb_+}$ is the equivalence class of the constant sequence $(x,x,\dots)$.\\[-0.5ex]


Step 1: Let us define a metric $d_{\wht X}$ on $\wht X$. Note that if $(x_n),(y_n)\in \scr C$, then by triangle inequality,
\begin{align*}
|d(x_m,y_m)-d(x_n,y_n)|\leq d(x_m,x_n)+d(y_m,y_n)
\end{align*}
where the RHS converges to $0$ as $m,n\rightarrow+\infty$. Therefore, the LHS also converges to $0$. This shows that $(d(x_n,y_n))_{n\in\Zbb_+}$ is a Cauchy sequence in $\Rbb_{\geq0}$, and hence converges. Therefore, we define
\begin{gather*}
d_{\wht X}:\wht X\times \wht X\rightarrow\Rbb_{\geq0}\\
d_{\wht X}([x_n],[y_n])=\lim_{n\rightarrow\infty}d(x_n,y_n)
\end{gather*}
This is well-defined: If $[x_n]=[x_n']$, and $[y_n]=[y_n']$, then 
\begin{align*}
|d(x_n,y_n)-d(x_n',y_n')|\leq d(x_n,x_n')+d(y_n,y_n')
\end{align*}
which converge to $0$ as $n\rightarrow\infty$. So $(d(x_n',y_n'))_{n\in\Zbb_+}$ and $(d(x_n,y_n))_{n\in\Zbb_+}$ are Cauchy-equivalent, and hence converge to the same number.

Clearly $d_{\wht X}([x_n],[y_n])=0$ iff $(x_n)\sim (y_n)$ iff $[x_n]=[y_n]$. And clearly $d_{\wht X}([x_n],[y_n])=d_{\wht X}([y_n],[x_n])$. If $[x_n],[y_n],[z_n]$ are in $\scr C$, applying $\lim_{n\rightarrow\infty}$ to
\begin{align*}
d(x_n,z_n)\leq d(x_n,y_n)+d(y_n,z_n)
\end{align*}
yields $d_{\wht X}([x_n],[z_n])\leq d_{\wht X}([x_n],[y_n])+d_{\wht X}([y_n],[z_n])$. So $d_{\wht X}$ is a metric.\\[-0.5ex]

Step 2. The map $\varphi:X\rightarrow\wht X$ is clearly an isometry. Let us show that it has dense range. Choose any $[x_n]_{n\in\Zbb_+}\in\wht X$. We shall show that $\varphi(x_k)=[x_k,x_k,\dots]$ approaches $[x_n]_{n\in\Zbb_+}$ as $k\rightarrow\infty$. 

For each $k$, we have
\begin{align}
d_{\wht X}(\varphi(x_k),[x_n]_{n\in\Zbb_+})=\lim_{n\rightarrow\infty}d(x_k,x_n)\label{eq96}
\end{align}
where the RHS converges because $d_{\wht X}$ is defined. Since $(x_n)_{n\in\Zbb_+}$ is a Cauchy sequence in $X$, we have
\begin{align}
\lim_{k,n\rightarrow\infty} d(x_k,x_n)=0   \label{eq95}
\end{align}
Therefore, by \eqref{eq95} and the convergence of the RHS of \eqref{eq96}, we can use  Thm. \ref{lb122} to conclude that
\begin{align*}
\lim_{k\rightarrow\infty}d_{\wht X}(\varphi(x_k),[x_n]_{n\in\Zbb_+})=\lim_{k\rightarrow\infty}\lim_{n\rightarrow\infty}d(x_k,x_n)=\lim_{k,n\rightarrow\infty}d(x_k,x_n)=0
\end{align*}


Step 3. It remains to prove that $\wht X$ is complete. By Lem. \ref{lb300} (applied to $\varphi(X)\subset\wht X$) and the fact that $\varphi$ is an isometry, it suffices to prove that for every Cauchy sequence $(x_k)_{k\in\Zbb_+}$ in $X$, the sequence $(\varphi(x_k))_{k\in\Zbb_+}$ converges in $\wht X$. But this is true: we have shown in Step 2 that $(\varphi(x_k))_{k\in\Zbb_+}$ converges to $[x_n]_{n\in\Zbb_+}$.
\end{proof}

\begin{proof}[\textbf{Proof of equivalence}]
Suppose that $\psi:X\rightarrow\wtd X$ is another completion. The map
\begin{gather*}
\Phi:\varphi(X)\rightarrow \psi(X)\qquad \varphi(x)\mapsto\psi(x)
\end{gather*}
is well-defined since $\varphi$ is injective. Moreover, $\Phi$ is an isometry since $\varphi$ and $\psi$ are isometries. In particular, $\Phi$ is uniformly continuous. Therefore, by Cor. \ref{lb298}, $\Phi$ can be extended to a uniformly continuous map $\Phi:\wht X\rightarrow\wtd X$. Clearly $\psi=\Phi\circ\varphi$. The continuous map
\begin{gather*}
\wht X\times\wht X\rightarrow \Rbb\\
(p,q)\mapsto d_{\wtd X}(\Phi(p),\Phi(q))-d_{\wht X}(p,q)
\end{gather*}
is zero on the dense subset $\varphi(X)\times \varphi(X)$ of its domain. Therefore it is constantly zero by Prop. \ref{lb196}. This proves that $\Phi$ is an isometry.

It remains to prove that $\Phi$ is surjective. Since $\wht X$ is complete and $\Phi$ restricts to an isometric isomorphism of metric spaces $\wht X\rightarrow\Phi(\wht X)$, $\Phi(\wht X)$ is a complete metric subspace of $\wht X$. Thus, by Prop. \ref{lb86}, $\Phi(\wht X)$ is a closed subset of $\wtd X$. But $\Phi(\wht X)$ is dense in $\wtd X$ since it contains $\psi(X)$. Therefore $\Phi(\wht X)=\wtd X$.
\end{proof}

The proof of Thm. \ref{lb301} is complete.





\subsection{Why did Hausdorff believe in completion?}\label{lb549}


Thm. \ref{lb301}, the existence and uniqueness of completion of an arbitrary metric space, was proved by Hausdorff in his 1914 work introducing Hausdorff topological spaces \cite[Sec. 8.8, p.315]{Hau14}. The construction of completion using equivalence classes of Cauchy sequences is quite abstract: Although it mimics  Cantor's construction of real numbers (cf. Ch. \ref{lb167}), its main application is not in the realm of finite-dimensional geometric objects, but in the world of function spaces. But what is the practical significance of the equivalence classes of Cauchy sequences of functions? In concrete analysis problems about functions, these objects are much more difficult to deal with than functions themselves.


Strangely enough, the only nontrivial examples we have now is $\Qbb\hookrightarrow\Rbb$. Besides this, we do not yet have  any exciting new metric spaces arising from completion. For example: we know that $[0,1]$ is the completion of $(0,1)$ under the Euclidean metric, and that $C([0,1],\Rbb)$ is the completion of the set of real polynomials $\Rbb[x]$ under the norm $\sup_{x\in[0,1]}|f(x)|$. (The density of the set of polynomials in $C([0,1],\Rbb)$ is due to Weierstrass.) But $[0,1]$ and $C([0,1],\Rbb)$ are examples we are already familiar with. 


Mathematicians do not generalize just for the sake of generalization. They want to solve problems by generalizing old concepts to a broader context. Moreover, mathematicians do not randomly choose a way of generalization and then build a huge theory. Instead, they develop a theory only in the direction that has already proved useful in solving explicit problems. Thus, Hausdorff proved Thm. \ref{lb301} because he was already convinced of the importance of abstract completion by certain powerful examples. For the moment, we are not ready to study these examples rigorously. (We will do this in the next semester.) But I want to give an informal introduction to one of these examples which historically has paved the way for many important ideas in analysis.


\subsubsection{The Fourier series method in integral equations}\label{lb370}




In the years of 1900-1907, many important progress has been made in integral equations, which originated from the study of Dirichlet problems (finding solutions of harmonic equation $\Delta \varphi(x,y)=0$ with given boundary condition, where $\Delta=(\partial_x)^2+(\partial_y)^2$). For example, one asks if there is $\lambda\in\Rbb$ and a  single-variable complex valued function $f:[0,2\pi]\rightarrow\Cbb$ satisfying the eigenvalue problem $Tf=\lambda f$ where
\begin{align*}
(Tf)(x)=\int_{0}^{2\pi}K(x,y)f(y)dx
\end{align*}
and $K:[0,2\pi]^2\rightarrow\Rbb$ is given. (Cf. also Sec. \ref{lb55}.)

Hilbert studied this problem using Fourier series. In the modern language, the theory of Fourier series claims that $C([0,2\pi],\Cbb)$, under the $L^2$-norm
\begin{align}\label{eq100}
\Vert f\Vert_{L^2}=\sqrt{\int_0^{2\pi}|f(x)|^2\cdot \frac{dx}{2\pi} }
\end{align}
has a completion
\begin{gather}\label{eq98}
\tcboxmath{\begin{gathered}
\Phi:C([0,2\pi],\Cbb)\rightarrow l^2(\Zbb,\Cbb)\\
\Phi(f)=\wht f
\end{gathered}}
\end{gather}
where $l^2(\Zbb,\Cbb)$ is the space of all $g:\Zbb\rightarrow\Cbb$ satisfying $\sqrt{\sum_{n\in\Zbb} |g(n)|^2}<+\infty$, and 
\begin{gather*}
\wht f:\Zbb\rightarrow\Cbb\qquad 
\wht f(n)=\int_0^{2\pi} f(x)e^{-\im nx}\cdot \frac{dx}{2\pi}
\end{gather*}
gives the \textbf{Fourier coefficients} of $f$. See Cor. \ref{lb603}.

Hilbert studied the eigenvalue problem $Tf=\lambda f$ by transforming it into a linear algebra problem on $l^2(\Zbb,\Cbb)$ (so that $T$ becomes an $\infty\times\infty$ discrete matrix $\wht T$), finding the possible eigenvectors $\wht f$ and eigenvalues $\lambda$ satisfying
\begin{align*}
\wht T\wht f=\lambda\wht f
\end{align*}
and returning to the original problems by finding the function $f$ whose Fourier coefficents are $\wht f$. It is easy to return: if $f\in C([0,2\pi],\Cbb)$, then one gets $f$ from $\wht f$ by the formula
\begin{align}
f(x)=\sum_{n\in\Zbb} \wht f(n)e^{\im nx} \label{eq97}
\end{align} 
where the RHS is called the \textbf{Fourier series} of $f$.

Here comes the crucial point: the range of $\Phi$, namely $\{\wht f:f\in C([0,1],\Cbb)\}$, is not the whole space $l^2(\Zbb,\Cbb)$ but only its dense subspace. This remains true if we enlarge $C([0,1],\Cbb)$ to the space of Riemann integrable functions $\scr R([0,1],\Cbb)$. However, the eigenvectors of $\wht T$ found by Hilbert were only known to be elements of $l^2(\Zbb,\Cbb)$. If we use \eqref{eq97} to find the eigenvectors of the original $T$, namely, if for an arbitrary $g\in l^2(\Zbb,\Cbb)$ we write 
\begin{align}
\Psi(g)=\sum_{n\in\Zbb} g(n)e^{\im nx}  \label{eq99}
\end{align}
then $\Psi(g)$ is not necessarily continuous or even Riemann integrable. It seems that, in the light of the proof of Thm. \ref{lb301}, the only way to make sense of \eqref{eq99} is as follows:
\begin{itemize}
\item One views $\Psi(g)$ as the Cauchy-equivalence class of the Cauchy sequence $(s_n)_{n\in\Zbb_+}$ in $C([0,2\pi],\Cbb)$ under the $L^2$-norm, where $s_n(x)=\sum_{k=-n}^n g(k)e^{\im kx}$.
\end{itemize}
But how can we understand $\Psi(g)$ as an actual function on $[0,2\pi]$ solving the eigenvalue problem $T\Psi(g)=\lambda\Psi(g)$? 


Therefore, \eqref{eq98} and \eqref{eq99} are the very first example of abstract completion, and also one of the most important examples motivating Hausdorff's study of completion in general. %The completion \eqref{eq98} is abstract, in the sense that an element $g$ in the completion $l^2(\Zbb,\Cbb)$ of $C([0,2\pi],\Cbb)$ gives an ``abstract function" $\Psi(g)$ as described by \eqref{eq99}. $\Psi(g)$ is abstract, because it is not necessarily continuous or even Riemann integrable. 


\subsubsection{Riesz-Fischer theorem}


Hilbert established these results by  1906, the same year Fr\'echet defined metric spaces. The story was finished by Riesz and Fischer, who proved in 1907 that $\Psi(g)$ can actually be represented by a \textbf{Lebesgue measurable} $f:[0,2\pi]\rightarrow\Cbb$ satisfying $\Vert f\Vert_{L^2}<+\infty$, where the norm $\Vert f\Vert_{L^2}$ is defined by \eqref{eq100} using Lebesgue integral instead of Riemann integral. Let $L^2([0,2\pi],\Cbb)$ denote the space of all such functions, and view $C([0,2\pi],\Cbb)$ as its subspace. Then using the language of Thm. \ref{lb301}, we have a commutative diagram
\begin{equation}\label{eq101}
\begin{tikzcd}[column sep=tiny]
                     & C([0,2\pi],\Cbb) \arrow[ld,"\Phi"'] \arrow[rd,hook,"\iota"] &   \\
l^2(\Zbb,\Cbb) \arrow[rr,"\simeq"',"\Psi"] &                         & L^2([0,2\pi],\Cbb)
\end{tikzcd}
\end{equation}
where $\iota$ is the inclusion map. Thus:
\begin{itemize}
\item The abstract completion $\Phi:C([0,2\pi],\Cbb)\rightarrow l^2(\Zbb,\Cbb)$ is equivalent to the concrete completion $C([0,2\pi],\Cbb)\subset L^2([0,2\pi],\Cbb)$, where ``concrete" means that it is function-theoretic.
\end{itemize}
This equivalence of abstract and concrete completions, connecting Hilbert's algebraic  approach to integral equations and Lebesgue's function-theoretic approach to measure theory, is the original form of \textbf{Riesz-Fischer theorem}  \index{00@Riesz-Fischer theorem} (proved in 1907),\footnote{More precisely, the original form of Riesz-Fischer theorem says that the $\Psi$ in \eqref{eq101} (whose explicit description is as in \eqref{eq99}) is an isometric isomorphism. As a consequence, the space $L^2$ is complete, because it is fairly easy to show that $l^2$ is complete. However, most modern analysis textbooks present Riesz-Fischer theorem in the following simple  form: ``$L^2$ is a complete metric space".} one of the most significant theorems in early 20th century.


Thanks to Riesz-Fischer theorem, people were convinced that abstract completions of function spaces could deepen one's understanding of analysis in much the same way that Lebesgue's measure theory broadened one's understanding of functions by harmonizing with Hilbert's $l^2$ theory. So what is the hesitation in trying to prove an abstract theorem like Thm. \ref{lb301}?

Hausdorff proved the existence and uniqueness of completion in general, but it was Riesz-Fischer theorem that proved the value of abstract completion.



\subsection{Completion of normed vector spaces}

Fix a normed vector space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$.

\begin{df}\label{lb597}
A \textbf{completion of the normed vector space $V$} \index{00@Completion of normed vector spaces} (or a \textbf{Banach space completion} of $V$) \index{00@Banach space completion} is a \textit{linear} isomertry $\varphi:V\rightarrow\wht{V}$ such that $\wht{V}$ is a Banach space, and $\varphi(V)$ is dense in $\wht V$.
\end{df}

Thus, if $\varphi:V\rightarrow\wht V$ is a completion, then $V$ is isomorphic to $\varphi(V)$ as normed vector spaces, and hence can be viewed as equivalently a dense normed subspace of $\wht V$.



\begin{thm}\label{lb312}
Every normed vector space $V$ has a completion $\varphi:V\rightarrow\wht V$. Morevoer, every completion $\psi:V\rightarrow\wtd V$ is \textbf{isomorphic} to $\varphi$ in the sense that there is an isomorphism of Banach spaces (cf. Def. \ref{lb302}) $\Phi:\wht V\rightarrow\wtd V$ such that the following diagram commutes:
\begin{equation}
\begin{tikzcd}[column sep=small]
                     & V \arrow[ld,"\varphi"'] \arrow[rd,"\psi"] &   \\
\wht V \arrow[rr, "\Phi","\simeq"'] &                         & \wtd V
\end{tikzcd}
\end{equation}
\end{thm}

%% Record #13 2023/11/1 three lectures  32

\begin{proof}[\textbf{Proof of existence}]
By Thm. \ref{lb301}, we have a completion $\varphi:V\rightarrow\wht V$ in the context of metric spaces. So $\wht V$ is a completion of metric space with metric $d_{\wht V}$, and $\varphi$ is an isometry of metric spaces with dense range. We need to show that $\wht V$ is a complete normed vector space, and that $\varphi$ is linear with dense range. Indeed, we shall identify $V$ with $\varphi(\wht V)$ via $\varphi$ so that $V$ is the metric subspace of $\wht V$. We shall extend the structure of normed vector space from $V$ to $\wht V$. In the following, the convergence in $\wht V$ and the continuity of the maps about $\wht V$ are understood using the metric $d_{\wht V}$. 

The map
\begin{gather*}
+:V\times V\rightarrow V\qquad (u,v)\mapsto u+v
\end{gather*}
is Lipschitz continuous, and hence uniformly continuous. Therefore, by Cor. \ref{lb298}, it can be extended (uniquely) to a continuous map $+:\wht V\times\wht V\rightarrow\wht V$. Similarly, if $\lambda\in\Fbb$, the Lipschitz continuous map
\begin{gather*}
V\rightarrow V\qquad v\mapsto \lambda\cdot v
\end{gather*}
can be extended to a continuous map $\wht V\rightarrow\wht V$. Thus, we have defined the addition $+$ and the scalar multiplication $\cdot$ for $\wht V$. Similarly, the map
\begin{align*}
\Vert \cdot\Vert:V\rightarrow\Rbb_{\geq0}  \qquad v\mapsto\Vert v\Vert
\end{align*}
is uniformly continuous and hence can be extended to a map $\wht V\rightarrow\Rbb_{\geq0}$.


We want to show that the above addition, scalar multiplication, and norm function make $\wht V$ a normed vector space.  For example, suppose that $\lambda\in\Fbb$. We want to prove that $\lambda(u+v)=\lambda u+\lambda v$ for all $u,v\in\wht V$, and we know that this is true when $u,v\in V$. Indeed, since $V\times V$ is dense in $\wht V\times\wht V$, and since the following two continuous maps
\begin{gather*}
(u,v)\in\wht V\times\wht V\mapsto \lambda(u+v)\in\wht V\\
(u,v)\in\wht V\times\wht V\mapsto \lambda u+\lambda v\in\wht V
\end{gather*}
are equal on $V\times V$, these two maps are the same by Prop. \ref{lb196}. The same argument proves that $\wht V$ is a vector space.


Since the following continuous map
\begin{gather*}
\varphi:(u,v)\in\wht V\times\wht V\mapsto \Vert u\Vert+\Vert v\Vert-\Vert u+v\Vert\in\Rbb
\end{gather*}
satisfies $\varphi(V\times V)\subset\Rbb_{\geq0}$, by $\varphi(\wht V\times \wht V)\subset\ovl{\varphi(V\times V)}$ (due to Prop. \ref{lb196} again), we conclude that $\varphi(\wht V\times\wht V)\subset \Rbb_{\geq0}$. So $\Vert u+v\Vert\leq \Vert u\Vert+\Vert v\Vert$ for all $u,v\in\wht V$. A similar argument shows $\Vert\lambda v\Vert=|\lambda|\cdot\Vert v\Vert$. %Finally, suppose that $v\in\wht V$ satisfies $\Vert v\Vert =0$. Pick a sequence $(u_n)$ in $V$ satisfying $d_{\wht V}(u_n,v)\rightarrow 0$. Since $\Vert\cdot\Vert:\wht V\rightarrow\Rbb$ is continuous, 
%\begin{align*}
%\lim_{n\rightarrow\infty}d_V(u_n,0)=\lim_{n\rightarrow\infty}\Vert u_{n}\Vert=\big\Vert \lim_{n\rightarrow\infty}u_n\big\Vert=\Vert v\Vert=0
%\end{align*}
%Therefore $(u_n)$ converges to $0$ in $V$. This proves $v=0$. So $\wht V$ is normed.

Since the metric on $V$ is induced by the norm of $V$, the map
\begin{gather*}
\wht V\times\wht V\rightarrow\Rbb\qquad (u,v)\mapsto d_{\wht V}(u,v)-\Vert u-v\Vert
\end{gather*}
is zero on the dense subset $V\times V$ of $\wht V\times\wht V$. Since this map is continuous, it is constantly zero. In particular, if $v\in\wht V$ satisfies $\Vert v\Vert=0$, then $d_{\wht V}(v,0)=0$, and hence $v=0$. So $\Vert\cdot\Vert$ is a norm on $\wht V$, and the complete metric $d_{\wht V}$ on $\wht V$ (arising from the metric-space-completion of $V$) is defined by this norm. So this norm is complete. Therefore, $\wht V$ is a Banach space. Since $V$ is dense in $\wht V$ under $d_{\wht V}$, $V$ is dense in $\wht V$ under the norm of $\wht V$.
\end{proof}


\begin{proof}[\textbf{Proof of uniqueness}]
By Thm. \ref{lb301}, there is a unique isometric isomorphism of metric spaces $\Phi:\wht V\rightarrow\wtd V$ such that $\psi=\Phi\circ\varphi$. Since $\varphi$ and $\psi$ are linear injections, $\Phi$ is a linear isomorphism when restricted to $\varphi(V)\rightarrow\psi(V)$. Since $\Phi$ is continuous and $\varphi(V)$ is dense in $\wht V$, we conclude that $\Phi$ is linear thanks to the following property.
\end{proof}


\begin{pp}\label{lb315}
Let $T:V\rightarrow W$ be a continuous map of normed vector spaces. Assume that $V_0$ is a dense linear subspace of $V$. Assume that $T|_{V_0}:V_0\rightarrow W$ is linear. Then $T$ is linear.
\end{pp}


\begin{proof}
This is same as the proof of the existence part of Thm. \ref{lb312}. Choose any $\alpha,\beta\in\Fbb$. Then the following continuous map
\begin{gather*}
(u,v)\in V\times V\mapsto T(\alpha u+\beta v)-(\alpha T(u)+\beta T(v))
\end{gather*}
is zero on the dense subset $V_0\times V_0$. So it is zero on $V\times V$.
\end{proof}


\begin{exe}
Let $V$ and $W$ be normed vector spaces over $\Fbb$, where $\Fbb=\Rbb$ (resp. $\Fbb=\Cbb$). Let $T:V\rightarrow W$ be a continuous map. Assume that $V_0$ is a dense $\Kbb$-linear subspace of $V$, where $\Kbb=\Qbb$ (resp. $\Kbb=\Qbb+\im\Qbb$). Assume that the restriction $T|_{V_0}:V_0\rightarrow W$ is $\Kbb$-linear. Prove that $T:V\rightarrow W$ is $\Fbb$-linear.
\end{exe}


\begin{rem}\label{lb538}
So far in this course, we have proved a lot of results about functions whose codomains are normed vector spaces. Some results assume that these spaces are Banach (i.e. complete), some do not. Thanks to Thm. \ref{lb312}, we can assume that all these results are stated only for Banach spaces, and then check whether they also hold for normed vector spaces in general (which is not difficult). This will make us easier to remember theorems.

For example, suppose that we know that Thm. \ref{lb279} holds only for Banach spaces: Namely, suppose we know that for any Banach space $V$ and topological space $X$, if $(f_\alpha)$ is a net in $C(X,V)$ converging uniformly to $f:X\rightarrow V$, then $f$ is continuous. Then we know that this result also holds when $V$ is a normed vector space. To see this, consider the completion $V\subset\wht V$. Then $(f_\alpha)$ is a net of continuous functions $X\rightarrow\wht V$ converging uniformly to some $f:X\rightarrow \wht V$ (satisfying $f(X)\subset V$). Then $f:X\rightarrow\wht V$ is continuous. Hence $f:X\rightarrow V$ is continuous.

Consider Prop. \ref{lb288} as another example. It tells us that if $V$ is a Banach space and $(f_\alpha)$ is a net in $C(X,V)$ converging uniformly on a dense subset $E\subset X$, then $(f_\alpha)$ converges uniformly on $X$. Now assume that $V$ is only a normed vector space, and take completion $V\subset\wht V$. Then $(f_\alpha)$ is a net in $C(X,\wht V)$ converges uniformly on $E$ to a function $E\rightarrow V$. Thus, by Prop. \ref{lb288}, it converges uniformly to a function $f:X\rightarrow\wht V$. Although we know $f(E)\subset V$ by assumption, we do not know whether $f(X)\subset V$. So we cannot prove the normed vector space version of Prop. \ref{lb288}.  \hfill\qedsymbol 
\end{rem}





\subsection{Bounded linear maps}\label{lb620}

In this section, we consider normed vector spaces over a given field $\Fbb\in\{\Rbb,\Cbb\}$.

We have seen that uniform continuity is crucial to the construction of addition and scalar multiplication in Thm. \ref{lb312}. Indeed, uniform continuity is ubiquitous in the world of normed vector spaces: We shall see that every continuous linear map of normed vector spaces is uniformly continuous. 


\begin{df}
Let $T:V\rightarrow W$ be a linear map of normed vector spaces. The \textbf{operator norm} $\Vert T\Vert$ \index{00@Operator norm} is defined to be
\begin{align}
\Vert T\Vert\xlongequal{\mathrm{def}} \sup_{v\in\ovl B_V(0,1)}\Vert Tv\Vert  \label{eq120}
\end{align}
\end{df}

\begin{rem}\label{lb372}
$\Vert T\Vert$ is the smallest number in $\ovl\Rbb_{\geq0}$ satisfying
\begin{align}
\Vert Tv\Vert\leq \Vert T\Vert\cdot\Vert v\Vert\qquad (\forall v\in V)  \label{eq105}
\end{align}
\end{rem}


\begin{proof}
\eqref{eq105} is clearly true when $v=0$. Assume $v\neq0$. Since $v/\Vert v\Vert\in\ovl B_V(0,1)$, we have $\Vert T(v/\Vert v\Vert)\Vert\leq \Vert T\Vert$. This proves \eqref{eq105}.

Now suppose that $C\in\ovl\Rbb_{\geq0}$ satisfies that $\Vert Tv\Vert\leq C\Vert v\Vert$ for all $v$. Then for each $v\in\ovl B_V(0,1)$ we have $\Vert Tv\Vert\leq C$. So $\Vert T\Vert\leq C$.
\end{proof}



\begin{pp}\label{lb313}
Let $T:V\rightarrow W$ be a linear map of normed vector spaces. Then the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $T$ is Lipschitz continuous.
\item $T$ is continuous.
\item $T$ is continuous at $0$.
\item $\Vert T\Vert<+\infty$.
\end{enumerate}
Moreover, if one of these conditions holds, then $T$ has Lipschitz constant $\Vert T\Vert$.
\end{pp}

\begin{proof}
Clearly (1)$\Rightarrow$(2) and (2)$\Rightarrow$(3). Suppose (3) is true. Note that $T0=0$. So there is $\delta>0$ such that $Tv\in \ovl B_W(0,1)$ for all $v\in \ovl B_V(0,\delta)$. Namely, for all $v\in V$ satisfying $\Vert v\Vert\leq \delta$ we have $\Vert Tv\Vert=\Vert Tv-T0\Vert\leq 1$. Thus, if $\Vert v\Vert\leq 1$, then $\Vert \delta v\Vert\leq\delta$. So
\begin{align*}
\Vert Tv\Vert=\delta^{-1}\Vert T(\delta v)\Vert\leq \delta^{-1}
\end{align*}
This proves $\Vert T\Vert\leq\delta^{-1}$. So (4) is proved.

Assume (4). By Rem. \ref{lb372}, for each $u,v\in V$ we have
\begin{align*}
\Vert Tu-Tv\Vert=\Vert T(u-v)\Vert\leq \Vert T\Vert\cdot \Vert u-v\Vert
\end{align*}
This proves that $T$ has Lipschitz constant $\Vert T\Vert$.
\end{proof}





Due to Prop. \ref{lb313}-(4), we make the following definition:
\begin{df}
Let $V,W$ be normed vector spaces over $\Fbb$. We call $T:V\rightarrow W$ to be a \textbf{bounded linear map} \index{00@Bounded linear map} if $T$ is a continuous linear map. We write \index{LVW@$\fk L(V,W),\fk L(V)$}
\begin{align}
\fk L(V,W)=\{\text{bounded linear maps }V\rightarrow W\}\qquad \fk L(V)=\fk L(V,V)
\end{align}
\end{df}

Thus, the word ``bounded" means that the linear map $T$ is bounded on $\ovl B_V(0,1)$, but not that $T$ is bounded on $V$.

\begin{pp}\label{lb314}
$\fk L(V,W)$ is a linear subspace of $W^V$, and the operator norm $\Vert \cdot\Vert$ is a norm on $\fk L(V,W)$.
\end{pp}

\begin{proof}
By Rem. \ref{lb372}, for each linear $S,T:V\rightarrow W$ and $\lambda\in\Fbb$, and for each $v\in V$ we have
\begin{gather*}
\Vert (S+T)v\Vert\leq \Vert S v\Vert+\Vert Tv\Vert\leq(\Vert S\Vert+\Vert T\Vert)\Vert v\Vert\\
\Vert \lambda Tv\Vert=|\lambda|\cdot\Vert Tv\Vert\leq |\lambda|\cdot \Vert T\Vert \cdot\Vert v\Vert
\end{gather*}
Thus, by Rem. \ref{lb372} again, we have
\begin{align}
\Vert S+T\Vert\leq \Vert S\Vert+\Vert T\Vert\qquad \Vert\lambda T\Vert\leq |\lambda|\cdot \Vert T\Vert
\end{align}
The proposition now follows easily from the above inequalities. (Notice Rem. \ref{lb367})
\end{proof}


Since Lipschitz continuous functions are uniformly continuous, we have:
\begin{pp}\label{lb500}
Let $V_0$ be a dense linear subspace of a normed vector space $V$. Let $W$ be a Banach space. Let $T_0:V_0\rightarrow W$ be a bounded linear map. Then there is a unique bounded linear map $T:V\rightarrow W$ such that $T|_{V_0}=T_0$.
\end{pp}

\begin{proof}
Uniqueness is clear from the density of $V_0$. By Prop. \ref{lb313}, $T_0$ is uniformly continuous. Therefore, by Cor. \ref{lb298}, $T_0$ can be extended to a continuous map $T:V\rightarrow W$, which is linear by Prop. \ref{lb315}.
\end{proof}











\subsection{Problems and supplementary material}

\begin{prob}
Give a direct proof of Thm. \ref{lb294} using open covers instead of using subsequences. Do not use Lebesgue numbers.
\end{prob}


The following Pb. \ref{lb296} gives another proof that sequentially compact metric spaces are compact. Therefore, do not use this fact in your solution of Pb. \ref{lb296}.

\begin{sprob}\label{lb296}
Let $X$ be a sequentially compact metric space. Let $\fk U\subset 2^X$ be an open cover of $X$.
\begin{enumerate}
\item Prove that $\fk U$ has a Lebesgue number. Namely, prove that there exists $\delta>0$ such that for every $x\in X$ there is $U\in\fk U$ satisfying $B_X(x,\delta)\subset U$.
\item In our proof that $X$ is separable (cf. Thm. \ref{lb252}), we showed that for every $\delta>0$ there exists a finite set $E\subset X$ such that $d(x,E)<\delta$ for all $x\in X$. Use this fact and Part 1 to prove that $\fk U$ has a finite subcover.
\end{enumerate}
\end{sprob}





\begin{df}
Two norms $\Vert\cdot\Vert_1$ and $\Vert\cdot\Vert_2$ on a vector space $\Vbb$ over $\Rbb$ or $\Cbb$ are called \textbf{equivalent} \index{00@Equivalent norms} if there exist $\alpha,\beta>0$ such that for all $v\in V$ we have
\begin{align*}
\Vert v\Vert_1\leq \alpha \Vert v\Vert_2\qquad \Vert v\Vert_2\leq \beta \Vert v\Vert_1
\end{align*}
Clearly, two equivalent norms induce equivalent metrics, and hence induce the same topology.
\end{df}


\begin{prob}\label{lb559}
Let $\Fbb\in\{\Rbb,\Cbb\}$. Let $\Vert\cdot\Vert$ be the Euclidean norm on $\Fbb^n$. Let $\nu:\Fbb^n\rightarrow\Rbb_{\geq 0}$ be a norm on $\Fbb^n$.
\begin{enumerate}
\item Prove that there exists $\alpha>0$ such that $\nu(\mbf x)\leq \alpha\Vert\mbf x\Vert$ for all $\mbf x\in\Fbb^n$. In particular, show that $\nu$ is continuous (under the Euclidean topology).
\item Let $\beta=\inf\{\nu(\mbf x):\mbf x\in\Fbb^n,\Vert \mbf x\Vert\leq 1\}$. Prove that $\beta>0$. Prove that $\Vert \mbf x\Vert\leq\beta^{-1}\cdot\nu(\mbf x)$ for all $\mbf x\in\Fbb^n$.
\end{enumerate}
\end{prob}


The above problem proves

\begin{thm}\label{lb363}
Let $\Fbb\in\{\Rbb,\Cbb\}$. Then any norm on $\Fbb^n$ is equivalent to the Euclidean norm. In particular, the operator norm on $\Fbb^{m\times n}$ (if we view an $m\times n$ matrix as an element of $\fk L(\Fbb^n,\Fbb^m)$) is equivalent to the Euclidean norm.
\end{thm}













\newpage







\section{Derivatives}

\subsection{Basic properties of derivatives}\label{lb575}


Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Let $\Omega$ be a nonempty open subset of $\Cbb$. The following assumption will be often considered:




\begin{df}
Let $f:[a,b]\rightarrow V$ (where  $-\infty<a<b<+\infty$)  be a map with variable $t$, and let $x\in[a,b]$. The \textbf{derivative} \index{00@Derivative} of $f$ at $x$ is
\begin{align*}
f'(x)\equiv \frac{df}{dt}(x)\xlongequal{\mathrm{def}}\lim_{
\begin{subarray}{c}
t\in[a,b]\setminus\{x\}\\
t\rightarrow x
\end{subarray}
}
\frac{f(t)-f(x)}{t-x}
=\lim_{
\begin{subarray}{c}
h\in [a-x,b-x]\setminus\{0\}\\
h\rightarrow 0
\end{subarray}
}
\frac{f(x+h)-f(x)}h
\end{align*}
provided that the limits converge. In other words (cf. Def. \ref{lb197}),
\begin{itemize}
\item $f'(x)$ converges to $v\in V$ iff for every $\eps>0$ there exists $\delta>0$ such that for every $t\in[a,b]$ satisfying $0<|t-x|<\delta$ we have
\begin{align*}
\Big\Vert \frac{f(t)-f(x)}{t-x}-v  \Big\Vert<\eps
\end{align*}
\end{itemize}


If $f'(x)$ exists for some $x$, we say that $f$ is \textbf{differentiable} \index{00@Differentiable} at $x$. If $f'(x)$ exists for  every $x\in [a,b]$, we say that $f$ is a \textbf{differentiable function} and view $f'$ as a function $[a,b]\rightarrow V$. 

Derivatives on intervals $[a,b),(a,b],(a,b)$ are understood in a similar way. \hfill\qedsymbol
\end{df}


\begin{comment}
To simplify subscripts, we write
\begin{align}\label{eq107}
f'(x)=\left\{
\begin{array}{ll}
\dps\lim_{t\rightarrow x}\frac{f(t)-f(x)}{t-x}&\text{ if }a<x<b\\[2ex]
\dps\lim_{t\rightarrow x^+}\frac{f(t)-f(x)}{t-x}&\text{ if }x=a\\[2ex]
\dps\lim_{t\rightarrow x^-}\frac{f(t)-f(x)}{t-x}&\text{ if }x=b
\end{array}
\right.
\end{align}
for the obvious reason. (Recall Def. \ref{lb200}.)

\begin{rem}
Note that in the three cases of \eqref{eq107}, $f'(x)$ can also equals
\begin{align*}
\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}h\qquad \lim_{h\rightarrow 0^+}\frac{f(x+h)-f(x)}h\qquad \lim_{h\rightarrow 0^-}\frac{f(x+h)-f(x)}h
\end{align*}
\end{rem}

\end{comment}






\begin{df}
Let $E$ be a subset of $\Rbb^n$. Let $f:E\rightarrow V$ be a function with variables $t_1,\dots,t_n$, and $\mbf x=(x_1,\dots,x_n)\in E$. Let $1\leq i\leq n$. Suppose that there are $a,b$ satisfying  $-\infty<a<x_i<b<+\infty$ such that $(x_1,\dots,x_{i-1},t,x_{i+1},\dots,x_n)$ belongs to $E$ for all $t\in[a,b]$. The derivative of the function $t_i\mapsto f((x_1,\dots,x_{i-1},t_i,x_{i+1},\dots,x_n)$ at $t=x_i$ is denoted by
\begin{align*}
\frac{\partial f}{\partial t_i}(\mbf x)\equiv\partial_i f(\mbf x)
\end{align*} 
and called the \textbf{partial derivative} \index{00@Partial derivative} of $f$ at $\mbf x$ with respect to the variable $t_i$.
\end{df}



\begin{df}
If $V$ is over $\Cbb$, and if $f:\Omega\rightarrow V$ and $z\in\Omega$, we define the \textbf{derivative} of $f$ at $z$ to be
\begin{align*}
f'(z)=\lim_{
\begin{subarray}{c}
w\in\Omega\setminus\{z\}\\
w\rightarrow z
\end{subarray}
}\frac{f(w)-f(z)}{w-z}
\end{align*}
provided that the RHS exists, and simply write it as
\begin{align*}
\lim_{w\rightarrow z}\frac{f(w)-f(z)}{w-z}=\lim_{\zeta\rightarrow0}\frac{f(z+\zeta)-f(z)}\zeta
\end{align*}
\end{df}

\begin{cv}\label{lb332}
Unless otherwise stated, when talking about derivatives of a function defined on an interval $I$, we always assume that $I$ is inside $\Rbb$ and has at least two points.  When talking about derivatives of a function $f:\Omega\rightarrow V$, we always assume that $V$ is over $\Cbb$.
\end{cv}



\begin{df}
Given a function $f:E\rightarrow V$ where $E$ is an interval in $\Rbb$ with at least two points or $E=\Omega$, if $n\in\Nbb$ and $x\in E$, we define the \pmb{$n$}\textbf{-th derivative} $f^{(n)}(x)$ \index{fn@$f^{(n)}$}  inductively by $f^{(0)}=f$ and $f^{(n)}(x)=(f^{(n-1)})'(x)$ if $f^{(n-1)}$ exists on some neighborhood of $x$ with respect to $E$. $f'',f''',f'''',\dots$ mean $f^{(2)},f^{(3)},f^{(4)},\dots$.

The $n$-th partial derivative on the $i$-th variable is written as $\partial_i^nf$. \hfill\qedsymbol
\end{df}


It is desirable to use sequences or nets to study derivatives. For that purpose, the following lemma is useful:

\begin{lm}\label{lb322}
Let $E$ be  an interval in $\Rbb$ with at least two elements, or let $E=\Omega$. Let $z\in E$. Let $f:E\rightarrow V$. Let $v\in V$. 
The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item We have $f'(z)=v$.
\item For any sequence $(z_n)_{n\in\Zbb_+}$ in $E\setminus\{z\}$ converging to $z$, we have $\dps\lim_{n\rightarrow\infty} \frac{f(z_n)-f(z)}{z_n-z}=v$.
\item For any net $(z_\alpha)_{\alpha\in\mc I}$ in $E$ converging to $z$, we have $\dps\lim_{\alpha\in\mc I}\varphi(z_\alpha)=v$, where $\varphi:E\rightarrow V$ is defined by
\begin{align}\label{eq110}
\varphi(w)=\left\{
\begin{array}{ll}
\dps\frac{f(w)-f(z)}{w-z}&\text{ if }w\neq z\\[2ex]
v&\text{ if }w=z
\end{array}
\right.
\end{align}
\end{enumerate}
\end{lm}

\begin{proof}
The equivalence of (1)$\Leftrightarrow$(2) is due to Def. \ref{lb197}-(3m). Also, by Def. \ref{lb197}-(1), that $f'(z)=v$ is equivalent to that the function $\varphi$ in \eqref{eq110} is continuous at $z$. So it is equivalent (3) by Def. \ref{lb188}-(1).
\end{proof}






\begin{pp}\label{lb319}
Let $E$ be an interval in $\Rbb$ or $E=\Omega$. If $f:E\rightarrow V$ is differentiable at $z\in E$, then $f$ is continuous at $z$.
\end{pp}


\begin{proof}
We consider the case $f:\Omega\rightarrow V$; the other case is similar. Choose any sequence $(z_n)$ in $\Omega\setminus\{z\}$ converging to $z$. By Lem. \ref{lb322}, we have $\lim_n \frac{f(z_n)-f(z)}{z_n-z}=v$. Since $\lim_n(z_n-z)=0$, by the continuity of scalar multiplication (Prop. \ref{lb82}),  we have
\begin{align*}
\lim_{n\rightarrow\infty}f(z_n)-f(z)=0\cdot v=0
\end{align*}
Thus, by Def. \ref{lb197}-(3m), we obtain $\lim_{w\neq z,w\rightarrow z}f(w)=f(z)$, which means by Def. \ref{lb197}-(1) that $f$ is continuous at $z$.
\end{proof}


\begin{pp}\label{lb320}
Let $E$ be an interval in $\Rbb$ or $E=\Omega$, and $x\in E$. Suppose that $f,g$ are functions $E\rightarrow V$ such that $f'(x)$ and $g'(x)$ exist. Then $(f+g)'(x)$ exists, and
\begin{subequations}
\begin{align}
(f+g)'(x)=f'(x)+g'(x)
\end{align}
If $\lambda$ is a function $E\rightarrow\Fbb$ such that $\lambda'(x)$ exists, then $(\lambda f)'(x)$ exists and satisfies the \textbf{Leibniz rule} \index{00@Leibniz rule}
\begin{align}
(\lambda f)'(x)=\lambda'(x)f(x)+\lambda(x)f'(x)  \label{eq111}
\end{align}
Assume moreover that $\lambda$ does not have value $0$. Then
\begin{align}
\Big(\frac 1\lambda\cdot f\Big)'(x)=\frac{-\lambda'(x)f(x)+\lambda(x)f'(x)}{\lambda(x)^2}
\end{align}
\end{subequations}
\end{pp}


\begin{proof}
The first formula is easy. To compute the second one, we choose any sequence $(x_n)$ in $[a,b]\setminus \{x\}$ or $\Omega\setminus\{x\}$ converging to $x$. Then
\begin{align*}
\frac{\lambda(x_n)f(x_n)-\lambda(x)f(x)}{x_n-x}=\frac{(\lambda(x_n)-\lambda(x))}{x_n-x}\cdot f(x_n)+\lambda(x)\frac{(f(x_n)-f(x))}{x_n-x}
\end{align*}
which, by Lem. \ref{lb322} and Prop. \ref{lb82} and the continuity of $f$ at $x$ (Prop. \ref{lb319}), converges to the RHS of \eqref{eq111}. This proves \eqref{eq111}, thanks to Lem. \ref{lb322}.

The third formula will follow from the second one if we can prove that $1/\lambda$ has derivative $-\frac{\lambda'(x)}{\lambda(x)^2}$ at $x$. This is not hard: Choose any sequence $x_n\rightarrow x$ but $x_n\neq x$. Then
\begin{align*}
\Big(\frac 1{\lambda(x_n)}-\frac 1{\lambda(x)}\Big)\Big/ (x_n-x)=-\frac{\lambda(x_n)-\lambda(x)}{x_n-x}\cdot \frac 1{\lambda(x_n)\lambda(x)}
\end{align*}
converges to $-\lambda'(x)\cdot \frac 1{\lambda(x)^2}$ as $n\rightarrow \infty$. Here, we have used the continuity of $\lambda$ at $x$ and Prop. \ref{lb82} again.
\end{proof}


%% Record #14 2023/11/6 two lectures  34


\begin{eg}
The derivative of a constant function is $0$. Thus, by Leibniz rule, if $\lambda$ is a scalar, and if $f'(z)$ exists, then $(\lambda f)'(z)=\lambda\cdot f'(z)$.
\end{eg}

\begin{eg}\label{lb323}
The identity map $f:z\in\Cbb\mapsto z\in\Cbb$ has derivative $\lim_{w\rightarrow z}\frac{w-z}{w-z}=1$. Thus, by induction and Prop. \ref{lb320}, we have $(z^n)'=nz^{n-1}$ if $n\in\Zbb_+$. If $-n\in\Zbb_+$, then when $z\neq 0$ we have
\begin{align*}
(z^n)'=(1/z^{-n})'=-\frac{(z^{-n})'}{z^{-2n}}=-\frac{-nz^{-n-1}}{z^{-2n}}=nz^{n-1}
\end{align*}
We conclude that $(z^n)'=nz^{n-1}$ whenever $n\in\Nbb$, or whenever $n=-1,-2,\dots$ and $z\neq0$. The same conclusion holds for the real variable function $x^n$.
\end{eg}


\begin{eg}
Let $f:z\in\Cbb\mapsto\ovl z\in\Cbb$. We claim that for every $z\in\Cbb$, the limit
\begin{align}
f'(z)=\lim_{w\rightarrow z}\frac{\ovl w-\ovl z}{w-z}=\lim_{h\rightarrow 0}\ovl h/h
\end{align}
does not exist with the help of Rem. \ref{lb202}: Take $h_n=1/n$. Then $h_n\rightarrow 0$ and $\ovl {h_n}/h_n=1\rightarrow 1$ as $n\rightarrow\infty$. Take $h_n=\im/n$. Then $h_n\rightarrow 0$ and $\ovl {h_n}/h_n=-\im/\im=-1\rightarrow-1$ as $n\rightarrow\infty$. So $f'(z)$ does not exist.
\end{eg}



\begin{thm}[\textbf{Chain rule}] \index{00@Chain rule}\label{lb331}
Let $\Omega,\Gamma$ be nonempty open subsets of $\Cbb$. Assume that $f:\Omega\rightarrow\Gamma$ is differentiable at $z\in\Omega$, and that $g:\Gamma\rightarrow V$ is differentiable at $f(z)$. Then $g\circ f$ is differentiable at $z$, and
\begin{align}
(g\circ f)'(z)=g'(f(z))\cdot f'(z)\label{eq125}
\end{align}
The same conclusion holds if $\Omega$ is replaced by an interval in $\Rbb$, or if both $\Omega$ and $\Gamma$ are replaced by intervals in $\Rbb$.
\end{thm}

Recall Conv. \ref{lb332} for the assumption on the field $\Fbb$.


\begin{comment}
The most natural idea is to  compute the limit of
\begin{align*}
\frac{g\circ f(w)-g\circ f(z)}{w-z}=\frac{f(w)-f(z)}{w-z}\cdot\frac{g\circ f(w)-g\circ f(z)}{f(w)-f(z)}
\end{align*}
as $w\rightarrow z$ (understood as limits of nets as indicated in Rem. \ref{lb318}). However, the RHS of this expression does not make sense when $f(w)=f(z)$, and this bad situation may happen for many $w\in\Omega\setminus\{z\}$. We overcome this difficulty by changing the language of the proof slightly, without changing the key idea too much.
\end{comment}



\begin{proof}
Define a function $A:\Gamma\rightarrow\Cbb$ by
\begin{gather}\label{eq231}
A(\zeta)=\left\{
\begin{array}{ll}
\dps\frac{g(\zeta)-g\circ f(z)}{\zeta-f(z)}&\text{ if }\zeta\neq f(z)\\[2ex]
\dps g'(f(z))&\text{ if }\zeta=f(z)
\end{array}
\right.
\end{gather}
Choose any sequence $(z_n)$ in $\Omega\setminus\{z\}$ converging to $z$. Then
\begin{align*}
\frac{g\circ f(z_n)-g\circ f(z)}{z_n-z}=A(f(z_n))\cdot\frac{f(z_n)-f(z)}{z_n-z}
\end{align*}
By Prop. \ref{lb319}, $f$ is continuous at $z$. So $\lim_n f(z_n)=f(z)$. Thus, by Lem. \ref{lb322}-(3), the above expression converges to $g'(f(z))f'(z)$ as $n\rightarrow\infty$.
\end{proof}



\begin{pp}\label{lb337}
Let $\Omega,\Gamma$ be nonempty open subsets of $\Cbb$. Let $f:\Omega\rightarrow\Gamma$ be a bijection. Let $z\in\Omega$. Suppose that $f'(z)$ exists and $f'(z)\neq 0$. Suppose also that $f^{-1}:\Gamma\rightarrow\Omega$ is continuous at $f(z)$. Then $f^{-1}$ is differentiable at $f(z)$, and
\begin{align}
(f^{-1})'(f(z))=\frac 1{f'(z)}
\end{align}
The same conclusion holds if $\Omega$ and $\Gamma$ are replaced by intervals of $\Rbb$.
\end{pp}


\begin{proof}
Choose any sequence $(w_n)$ in $\Gamma\setminus\{f(z)\}$ converging to $f(z)$. Then, as $n\rightarrow\infty$, we have $f^{-1}(w_n)\rightarrow f^{-1}(f(z))=z$ since $f^{-1}$ is continuous at $f(z)$, and hence
\begin{align}
\frac{f^{-1}(w_n)-f^{-1}(f(z))}{w_n-f(z)}=\frac{f^{-1}(w_n)-z}{f(f^{-1}(w_n))-f(z)}
\end{align}
converges to $1/f'(z)$ by Lem. \ref{lb322} and the continuity of the map $\zeta\in\Cbb^{\times}\mapsto 1/\zeta\in\Cbb$. This proves $(f^{-1})'(f(z))=1/f'(z)$, thanks to Lem. \ref{lb322}.
\end{proof}








\begin{comment}

The Leibniz rule \eqref{eq111} can be generalized to higher derivatives. 

\end{comment}


\subsection{Rolle's and Lagrange's mean value theorems (MVT)} \index{00@MVT=mean value theorem}

Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Assume $-\infty<a<b<+\infty$.

\subsubsection{MVTs}



\begin{df}
Let $f:X\rightarrow \Rbb$ where $X$ is a topological space. We say that $f$ has a \textbf{local maximum} (resp. \textbf{local minimum}) \index{00@Local maximum/minimum} at $x\in X$, if there exists $U\in\Nbh_X(x)$ such that $f|_U$ attains its maximum (resp. minimum) at $x$. The word ``\textbf{local extremum}" \index{00@Local extremum} refers to either local maximum or local minimum.
\end{df}


You must know that derivatives can be used to find the monotonicity of real-valued real variable functions. Here is the precise statement:

\begin{pp}\label{lb324}
Assume that $f:[a,b]\rightarrow\Rbb$ is differentiable at $x\in[a,b]$, and $f'(x)>0$. Then there exists $\delta>0$ such that for any $y\in (x-\delta,x+\delta)\cap[a,b]$, we have
\begin{gather}
y>x\Rightarrow f(y)>f(x)\qquad y<x\Rightarrow f(y)<f(x)   \label{eq112}
\end{gather}
\end{pp}
We leave it to the readers the find the analogous statement for the case $f'(x)<0$.

\begin{proof}
Let $A=f'(x)>0$. Then there exists $\delta>0$ such that for all $y\in (x-\delta,x+\delta)\cap[a,b]$ not equal to $x$, we have $\dps\Big|\frac{f(y)-f(x)}{y-x}-A \Big|<\frac A2$, and hence $\dps\frac{f(y)-f(x)}{y-x}>\frac A2$. This proves \eqref{eq112}.
\end{proof}


\begin{co}\label{lb325}
Assume that $f:[a,b]\rightarrow\Rbb$ has a local extremum at $x\in (a,b)$. Assume that $f'(x)$ exists. Then $f'(x)=0$.
\end{co}
\begin{proof}
If $f'(x)$ is a non-zero number, then either $f'(x)>0$ or $(-f)'(x)>0$. In either case, Prop. \ref{lb324} indicates that $f$ cannot have a local extremum at $x$.
\end{proof}




From Prop. \ref{lb324}, it is clear that if $f'>0$ on $(a,b)$, then $f$ is strictly increasing. However, to prove that if $f'\geq0$ then $f$ is increasing, we need more preparation.


\begin{lm}[\textbf{Rolle's MVT}] \index{00@Rolle's MVT}
Suppose that $f\in C([a,b],\Rbb)$ is differentiable on $(a,b)$. Suppose moreover that $f(a)=f(b)$. Then there exists $x\in (a,b)$ such that $f'(x)=0$.
\end{lm}


\begin{proof}
If $f$ is constant than $f'=0$. Suppose that $f$ is not constant. Then there is $x\in(a,b)$ at which $f$ attains its maximum (if $f(t)>f(a)$ for some $t\in(a,b)$) or minimum (if $f(t)<f(b)$ for some $t\in(a,b)$). So $f'(x)=0$ by Cor. \ref{lb325}.
\end{proof}


\begin{eg}
Rolle's MVT does not hold for vector-valued functions. Especially, it does hot hold for functions to $\Cbb\simeq\Rbb^2$. Consider $f:[0,2\pi]\rightarrow\Cbb$ defined by $f(t)=e^{\im t}=\cos t+\im\sin t$. We assume that $\sin'=\cos$ and $\cos'=-\sin$ are proved. Then $f(0)=f(2\pi)=1$, whereas $f'(t)=-\sin t+\im\cos t$ is never zero.
\end{eg}





\begin{thm}[\textbf{Lagrange's MVT}] \index{00@Lagrange's MVT}\label{lb345}
Suppose that $f\in C([a,b],\Rbb)$ is differentiable on $(a,b)$. Then there is $x\in(a,b)$ such that
\begin{align}\label{eq113}
f'(x)=\frac{f(b)-f(a)}{b-a}
\end{align}
\end{thm}



\begin{proof}
The case $f(a)=f(b)$ is just Rolle's MVT. When $f(a)\neq f(b)$, we can ``shift the function $f$ vertically" so that its two end points have the same height. Technically, we consider $g(x)=f(x)-kx$ where $k=\frac{f(b)-f(a)}{b-a}$ is the slope of the interval from $(a,f(a))$ to $(b,f(b))$. Then $g(a)=g(b)$. By Rolle's MVT, there is $x\in(a,b)$ such that $g'(x)=0$, i.e. $f'(x)-k=0$.
\end{proof}


\subsubsection{Applications of MVTs}








\begin{co}\label{lb330}
Assume that $f\in C([a,b],\Rbb)$ is differentiable on $(a,b)$. Then
\begin{align}
f'\geq0\text{ on }(a,b)\qquad\Longleftrightarrow\qquad f\text{ is increasing on }[a,b] \label{eq114}
\end{align}
Moreover, if $f'>0$ on $(a,b)$, then $f$ is strictly increasing on $[a,b]$.
\end{co}


\begin{proof}
If $f'(x)<0$ for some $x\in (a,b)$, then Prop. \ref{lb324} implies that $x$ has a neighborhood on which $f$ is not increasing. Conversely, suppose that $f'(x)\geq 0$ for all $x\in (a,b)$. Choose $x,y\in[a,b]$ satisfying $a\leq x<y\leq b$. Then by Lagrange's MVT, there is $z\in(x,y)$ such that $f(y)-f(x)=f'(z)(y-x)\geq0$. So $f$ is increasing on $[a,b]$. We have finished proving \eqref{eq114}.

Suppose that $f'>0$ on $(a,b)$. Then by Prop. \ref{lb324}, $f$ is strictly increasing on $(a,b)$. If $a<x<b$, then there is $\eps>0$ such that $f(a+1/n)\leq f(x)-\eps$ for sufficiently large $n$. Since $f$ is continuous at $a$, by taking $\lim_{n\rightarrow\infty}$ we get $f(a)\leq f(x)-\eps$ and hence $f(a)<f(x)$. A similar argument proves $f(b)>f(x)$. So $f$ is strictly increasing on $[a,b]$.
\end{proof}

\begin{co}\label{lb424}
Suppose that $f\in C([a,b],\Rbb)$ is differentiable on $(a,b)$, and $f'(x)\neq 0$ for all $x\in(a,b)$. Then $f$ is strictly monotonic (i.e. strictly increasing or strictly decreasing). In particular, by Cor. \ref{lb330}, either $f'\geq0$ on $(a,b)$ or $f'\leq 0$ on $(a,b)$.
\end{co}

\begin{proof}
Rolle's MVT implies $f(b)-f(a)\neq 0$, and similarly, $f(y)-f(x)\neq 0$ whenever $a\leq x<y\leq b$. Therefore $f$ is injective. The strict monotonicity of $f$ follows from the following general fact:
\end{proof}

\begin{pp}\label{lb347}
Let $-\infty\leq a< b\leq +\infty$, and let $f:[a,b]\rightarrow\ovl\Rbb$ be a continuous injective function. Then $f$ is strictly monotonic.
\end{pp}


\begin{proof}
Choose a strictly increasing homeophism  $\varphi:[0,1]\rightarrow[a,b]$. It suffices to prove that $g=f\circ\varphi:[0,1]\rightarrow\ovl\Rbb$ is strictly monotonic. Assume for simplicity that $g(0)\leq g(1)$. Let us show that $g$ is increasing. Then the injectivity implies that $g$ is strictly increasing.

We claim that for every $x\in(0,1)$ we have $g(0)\leq g(x)\leq g(1)$. Suppose the claim is true. Then for every $0< x< y< 1$ we have $g(0)\leq g(y)\leq g(1)$. Applying the claim to the interval $[0,y]$ shows $g(0)\leq g(x)\leq g(y)$. So $f$ is increasing.

Let us prove the claim. Suppose the claim is not true. Then there is $x\in (0,1)$ such that either $g(0)\leq g(1)<g(x)$ or $g(x)< g(0)\leq g(1)$. In the first case, by intermediate value theorem (applied to $f|_{[0,x]}$), there is $p\in [0,x]$ such that $g(p)=g(1)$. So $g$ is not injective since $p<1$. This is impossible. Similarly, the second case is also impossible.
\end{proof}





Besides monotonicity, the uniqueness of antiderivatives is another classical application of MVT.

\begin{df}
An \textbf{antiderivative} \index{00@Antiderivative} of a function $f:E\rightarrow V$ is a differentiable function $g:E\rightarrow V$ satisfying $g'=f$ on $E$.
\end{df}

The renowned fact that any two antiderivatives $g_1,g_2$ of $f:[a,b]\rightarrow V$ differ by a constant is immediate from the following fact (applied to $g_1-g_2$):

\begin{co}\label{lb326}
Suppose that $f\in C([a,b],V)$ is differentiable on $(a,b)$ and satisfies $f'=0$ on $(a,b)$. Then $f$ is a constant function.
\end{co}

\begin{proof}[Proof for $V=\Fbb^N$]
Since $\Cbb^N\simeq\Rbb^{2N}$, it suffices to prove the case $V=\Rbb^N$. Since a sequence in $\Rbb^N$ converges to a vector iff each component of the sequence converges to the corresponding component of the vector, it suffices to prove the case $f:[a,b]\rightarrow\Rbb$.  

Choose any $x\in(a,b]$. By Lagrange's MVT, there exists $y\in(a,x)$ such that $\dps 0=f'(y)=\frac{f(x)-f(a)}{x-a}$. This proves $f(x)=f(a)$ for all $x\in(a,b]$.
\end{proof}


Now we discuss the general case. 

\begin{rem}\label{lb560}
When $V$ is not necessarily finite-dimensional, the method of reducing Cor. \ref{lb326} to the case of scalar-valued functions is quite subtle: How should we understand the ``components" of an element $v$ in the Banach space $V$? In fact, in the general case, we should view \index{zz@$\bk{\varphi,v}\equiv\bk{v,\varphi}:=\varphi(v)$}
\begin{align}
\bk{\varphi,v}\equiv\bk{v,\varphi}\xlongequal{\mathrm{def}}\varphi(v)
\end{align}
as a component of $v$, where $\varphi$ is an element inside the \textbf{dual (Banach) space} \index{00@Dual (Banach) space} of $V$, defined by \index{V@$V^*$}
\begin{align}
V^*=\fk L(V,\Fbb)
\end{align}
An element $\varphi$ in $V^*$ is called a \textbf{bounded linear functional} on $V$. \index{00@Bounded linear functional} (In general, a \textbf{linear functional} \index{00@Linear functional} on a vectors space $W$ over a field $\Kbb$ is simply a linear map $W\rightarrow\Kbb$.)
\end{rem}


The vector space $V^*$, equipped with the operator norm, is indeed a Banach space. (This is why we call $V^*$ the dual \textit{Banach} space.) For the moment we do not need this fact. And we will discuss this topic in a later chapter.



\begin{rem}\label{lb393}
In the future, we will prove that $V^*$ separates points of $V$. (Recall Def. \ref{lb327}.) By linearity, this is equivalent to saying that for any $v\in V$ we have
\begin{align}\label{eq115}
v=0\qquad\Longleftrightarrow\qquad\bk{\varphi,v}=0\text{ for all }\varphi\in V^*
\end{align}
In fact, in the future we will prove the famous \textbf{Hahn-Banach extension theorem}, which implies (cf. Cor. \ref{lb502}) that if $v\in V$ then
\begin{align}
\exists \varphi\in V^*\setminus\{0\}\qquad\text{such that}\qquad\bk{\varphi,v}=\Vert\varphi\Vert\cdot\Vert v\Vert \label{eq116}
\end{align}
where $\Vert\varphi\Vert$ is the operator norm of $\varphi$. (Note that, by Rem. \ref{lb372}, we have $|\bk{\varphi,v}|\leq\Vert\varphi\Vert\cdot\Vert v\Vert$ for all $v\in V,\varphi\in V^*$. It is nontrivial that ``$\leq$" can be ``$=$" for some $\varphi\neq0$.) 
\end{rem}




\begin{comment}
From \eqref{eq118}, it is clear that for any $v\in V,\varphi\in V^*$ we have
\begin{align}
|\bk{\varphi,v}|\leq\Vert\varphi\Vert\cdot\Vert v\Vert \label{eq119}
\end{align}
Thus, the inequality ``$\geq$" trivially holds in \eqref{eq116}. The ``$\leq$" in \eqref{eq116} is nontrivial. 

\begin{rem}\label{lb328}
If we equip the \textbf{double dual space} \index{00@Double dual space $V^{**}$} $V^{**}=(V^*)^*$ \index{V@$V^{**}=(V^*)^*$} with the operator norm, then for each $\fk v\in V^{**}$, by \eqref{eq120} we have
\begin{align*}
\Vert\fk v\Vert=\sup_{\varphi\in \ovl B_{V^*}(0,1)}|\bk{\fk v,\varphi}|
\end{align*}
From this relation, it is clear that \eqref{eq116} holds if and only if the canonical linear map 
\begin{gather}
V\rightarrow V^{**}\qquad v\mapsto \bk{\cdot ,v}  \label{eq117}
\end{gather} 
(where $\bk{\cdot,v}$ sends each $\varphi\in V^*$ to $\bk{\varphi,v}=\varphi(v)$) is an isometry.
\end{rem}

\end{comment}







\begin{proof}[\textbf{Proof of Cor. \ref{lb326} assuming Hahn-Banach}]
Let us prove that $f(x)=f(a)$ for any $x\in[a,b]$. Since $V^*$ separates points of $V$, it suffices to choose an arbitrary $\varphi\in V^*$ and prove that $\varphi\circ f(x)=\varphi\circ f(a)$. Indeed,  since $\varphi$ is continuous, for each sequence $(x_n)$ in $[a,b]\setminus\{x\}$ converging to $x$, in view of Lem. \ref{lb322} we have
\begin{align*}
\lim_{n\rightarrow\infty}\frac{\varphi\circ f(x_n)-\varphi\circ f(x)}{x_n-x}=\varphi\Big(\lim_{n\rightarrow\infty}\frac{f(x_n)-f(x)}{x_n-x}\Big)=\varphi(f'(x))=\varphi(0)=0
\end{align*}
if $a<x<b$. Thus $(\varphi\circ f)'=0$ on $(a,b)$. Therefore, by the finite-dimensional version of Cor. \ref{lb326}, we have that $\varphi\circ f$ is constant on $[a,b]$.
\end{proof}




Hahn-Banach theorem is extremely useful for reducing a problem about vector-valued functions to one about scalar-valued functions. In this course, we will also use Hahn-Banach to prove another fun fact: every Banach space over $\Fbb$ is isormorphic to a closed linear subspace of $C(X,\Fbb)$ where $X$ is a compact Hausdorff space. However, in the following section we would like to give an elementary proof of Cor. \ref{lb326} without using Hahn-Banach. 



\subsection{Finite-increment theorem}

Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Assume $-\infty<a<b<+\infty$.

Cor. \ref{lb326} follows immediately from the following theorem by taking $g=0$.


\begin{thm}[]\label{lb329}
Suppose that $f\in C([a,b],V)$ and $g\in C([a,b],\Rbb)$ are differentiable on $(a,b)$. Assume that for every $x\in(a,b)$ we have $\Vert f'(x)\Vert\leq g'(x)$. Then
\begin{align}
\Vert f(b)-f(a)\Vert\leq g(b)-g(a)  \label{eq124}
\end{align}
\end{thm}





\begin{proof}
By continuity, it suffices to prove $\Vert f(\beta)-f(\alpha)\Vert\leq g(\beta)-g(\alpha)$ for all $\alpha,\beta$ satisfying $a<\alpha<\beta<b$. Therefore, by replacing $[a,b]$ by $[\alpha,\beta]$, it suffices to assume that $f,g$ are differentiable on $[a,b]$.\\[-1ex]

Step 1. For each $\eps>0$, consider the condition on $x\in[a,b]$:
\begin{align}
\Vert f(x)-f(a)\Vert\leq g(x)-g(a)+\eps(x-a)\label{eq121}
\end{align}
The set
\begin{align*}
E_\eps=\big\{x\in[a,b]:x\text{ satisfies \eqref{eq121}}\big\}
\end{align*}
is nonempty because $a\in E_\eps$. One checks easily that $E_\eps$ is a closed subset of $[a,b]$: This is because, for example, $E_\eps$ is the inverse image of the closed subset $(-\infty,0]$ of $\Rbb$ under the map
\begin{gather}
[a,b]\rightarrow\Rbb\qquad x\mapsto \Vert f(x)-f(a)\Vert- g(x)+g(a)-\eps(x-a)  \label{eq122}
\end{gather}
We now fix $x=\sup E_\eps$. Then $x\in E_\eps$ because $E_\eps$ is closed. We shall prove that $x=b$. Then the fact that $b\in E_\eps$ for all $\eps>0$ proves \eqref{eq124}.\\[-1ex]


Step 2. Suppose that $x\neq b$. Then $a\leq x<b$. We shall prove that there exists $y\in(x,b)$ such that
\begin{align}
\Vert f(y)-f(x)\Vert\leq g(y)-g(x)+\eps(y-x) \label{eq123}
\end{align}
Add this inequality to \eqref{eq121} (which holds because $x\in E_\eps$). Then by triangle inequality, we obtain $y\in E_\eps$, contradicting $x=\sup E_\eps$.


Let us prove the existence of such $y$. For each $y\in (x,b)$, define $v(y)\in V,\lambda(y)\in\Rbb$ such that
\begin{gather*}
f(y)-f(x)=f'(x)(y-x)+v(y)(y-x)\\
g(y)-g(x)=g'(x)(y-x)+\lambda(y)(y-x)
\end{gather*}
The definition of $f'(x)$ and $g'(x)$ implies that $v(y)\rightarrow0$ and $\lambda(y)\rightarrow0$ as $y\rightarrow x$. Therefore, there exists $y\in(x,b)$ such that $\Vert v(u)\Vert<\eps/2$ and $|\lambda(y)|<\eps/2$. Thus
\begin{align*}
&\Vert f(y)-f(x)\Vert-(g(y)-g(x))\nonumber\\
\leq&(\Vert f'(x)\Vert-g'(x)) (y-x)+(\Vert v(y)\Vert-\lambda(y))\cdot (y-x)\nonumber\\
\leq&0\cdot(y-x)+\big(\frac\eps2+\frac\eps2\big)\cdot(y-x)=\eps(y-x)
\end{align*}
This proves \eqref{eq123}.
\end{proof}


\begin{rem}
If we apply Thm. \ref{lb329} to the special case that $f=0$, we see that if $g\in C([a,b],\Rbb)$ satisfies $g'\geq0$ on $(a,b)$, then $g$ is increasing. This gives another proof of \eqref{eq114} besides the one via Lagrange's MVT.
\end{rem}


An important special case of Thm. \ref{lb329} is:

\begin{co}[\textbf{Finite-increment theorem}] \index{00@Finite-increment theorem}\label{lb333}
Suppose that $f\in C([a,b],V)$ is differentiable on $(a,b)$, and that there exists $M\in\Rbb_{\geq0}$ such that $\Vert f'(x)\Vert\leq M$ for all $x\in(a,b)$. Then
\begin{align}
\Vert f(b)-f(a)\Vert\leq M|b-a|
\end{align}
\end{co}

\begin{proof}
Choose $g(x)=Mx$ in Thm. \ref{lb329}.
\end{proof}

It is fairly easy to prove finite-increment theorem for complex-variable functions.

\begin{df}\label{lb364}
A subset $E$ of a real vector space is called \textbf{convex}, \index{00@Convex set} if for every $x,y\in E$, the interval \index{00@Interval $[x,y]$ in a real vector space}
\begin{align}
[x,y]=\{tx+(1-t)y:t\in[0,1]\}
\end{align}
is a subset of $E$.
\end{df}

When talking about convex subsets of $\Cbb$, we view $\Cbb$ as $\Rbb^2$. Then, for example, all open disks in $\Cbb$ are convex.

\begin{co}[\textbf{Finite-increment theorem}]\index{00@Finite-increment theorem}\label{lb335}
Assume $\Fbb=\Cbb$. Let $\Omega$ be a nonempty open convex subset of $\Cbb$. Let $f:\Omega\rightarrow V$ be differentiable on $\Omega$. Choose any $z_1,z_2\in\Omega$. Suppose that there exists $M\in\Rbb_{\geq0}$ such that $\Vert f'(z)\Vert\leq M$ for all $z$ in the interval $[z_1,z_2]$. Then 
\begin{align}
\Vert f(z_2)-f(z_1)\Vert\leq M|z_2-z_1|
\end{align}
\end{co}

\begin{proof}
Define $\gamma:[0,1]\rightarrow \Cbb$ by $\gamma(t)=(1-t)z_1+tz_2$. Then $\gamma([0,1])\subset\Omega$ because $\Omega$ is convex. By chain rule (Thm. \ref{lb331}), we have
\begin{align*}
(f\circ\gamma)'(t)=f'(\gamma(t))\cdot \gamma'(t)=(z_2-z_1)f'(\gamma(t))
\end{align*}
whose norm is bounded by $M|z_2-z_1|$. Applying Cor. \ref{lb333} to $f\circ\gamma$ finishes the proof.
\end{proof}





\begin{eg}
Let $X$ be an interval in $\Rbb$, or let $X=\Omega$ where $\Omega$ is convex. Let $\scr A$ be the set of differentiable functions $f:\Omega\rightarrow V$ satisfying $\Vert f'\Vert_{l^\infty}\leq M$. Then elements of $\scr A$ have Lipschitz constant $M$ by Finite-increment theorems. So $\scr A$ is equicontinuous.
\end{eg}







\subsection{Commutativity of derivatives and limits}

Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Let $\Omega$ be a nonempty open subset of $\Cbb$. Recall Conv. \ref{lb332}. Recall Def. \ref{lb334} for the meaning of locally uniform convergence.


\subsubsection{Main theorem}




\begin{thm}\label{lb336}
Let $X$ be $\Omega$ or an interval in $\Rbb$. Let $(f_\alpha)_{\alpha\in I}$ be a net of differentiable functions $X\rightarrow V$. Suppose that the following are true:
\begin{enumerate}[label=(\alph*)]
\item The net $(f_\alpha)_{\alpha\in I}$ converges pointwise to some $f:X\rightarrow V$.
\item The net $(f_\alpha')_{\alpha\in I}$ converges uniformly to some $g:X\rightarrow V$. 
\end{enumerate}
Then $f$ is differentiable on $X$, and $f'=g$.
\end{thm}


\begin{proof}
We prove the case $X=\Omega$. The other case is similar. Choose any $z\in \Omega$. By shrinking $\Omega$, we assume that $\Omega$ is an open disk centered at $z$. We know $\lim_{w\rightarrow z}\frac{f_\alpha(w)-f_\alpha(z)}{w-z}$ converges to $f_\alpha'(z)$ for each $\alpha$. Therefore, if we can show that $\frac{f_\alpha(w)-f_\alpha(z)}{w-z}$ converges uniformly (over all $w\in \Omega\setminus\{z\}$) under $\lim_\alpha$, then it must converge uniformly to $\frac{f(w)-f(z)}{w-z}$ since it converges pointwise to $\frac{f(w)-f(z)}{w-z}$ by (a). Then, by Moore-Osgood Thm. \ref{lb289}, we have
\begin{align*}
\lim_{w\rightarrow z}\lim_\alpha\frac{f_\alpha(w)-f_\alpha(z)}{w-z}=\lim_\alpha\lim_{w\rightarrow z}\frac{f_\alpha(w)-f_\alpha(z)}{w-z}=\lim_\alpha f_\alpha'(z)=g(z)
\end{align*}
finishing the proof.

To prove the uniform convergence, by the Cauchy condition on $V^{\Omega\setminus\{z\}}$ (equipped with a complete uniform convergence metric as in Exp. \ref{lb272}), it suffices to prove that
\begin{align}\label{eq126}
\sup_{w\in\Omega\setminus\{z\}}\Big\Vert \frac{f_\alpha(w)-f_\alpha(z)}{w-z}-\frac{f_\beta(w)-f_\beta(z)}{w-z}\Big\Vert
\end{align}
converges to $0$ under $\lim_{\alpha,\beta\in I}$. Applying Cor. \ref{lb335} to the function $f_\alpha-f_\beta$, we see
\begin{align}
&\Vert f_\alpha(w)-f_\beta(w)-f_\alpha(z)+f_\beta(z)\Vert\leq |w-z|\cdot\sup_{\zeta\in[z,w]} \Vert f_\alpha'(\zeta)-f_\beta'(\zeta)\Vert\nonumber\\
\leq&|w-z|\cdot \Vert f_\alpha'-f_\beta'\Vert_{l^\infty(\Omega,V)}\label{eq127}
\end{align}
Thus, we have 
\begin{align*}
\eqref{eq126}\leq \Vert f_\alpha'-f_\beta'\Vert_{l^\infty(\Omega,V)}
\end{align*}
where the RHS converges to $0$ under $\lim_{\alpha,\beta}$ due to (b). This finishes the proof.
\end{proof}


We didn't assume the uniform convergence of $(f_\alpha)$ in Thm. \ref{lb336} because it is often redundant:

\begin{lm}\label{lb343}
Assume that either $X$ is a bounded interval in $\Rbb$, or $X=\Omega$ where $\Omega$ is assumed to be bounded and convex. Then under the assumptions in Thm. \ref{lb336}, the net $(f_\alpha)_{\alpha\in I}$ converges uniformly to $f$. 
\end{lm}




\begin{proof}
We already know that $(f_\alpha)$ converges pointwise to $f$. In fact, we shall only use the fact that $\lim_\alpha f_\alpha(z)=f(z)$  for some $z\in X$. Let $z$ be such a point. Motivated by the proof of Thm. \ref{lb336}, let us prove  the Cauchy condition that
\begin{align*}
\lim_{\alpha,\beta\in I}\sup_{w\in\Omega}\Vert f_\alpha(w)-f_\beta(w)-f_\alpha(z)+f_\beta(z)\Vert=0
\end{align*}
Then $(f_\alpha-f_\alpha(z))$ converges uniformly, and hence $(f_\alpha)$ converges uniformly. Indeed, the Cauchy condition follows easily from \eqref{eq127}, in which $\sup_{w\in\Omega}|w-z|$ is a finite number because $X$ is bounded.
\end{proof}


Thus, whether or not $\Omega$ satisfies boundedness and convexity, the net $(f_\alpha)$ must converge locally uniformly to $f$. Knowing the locally uniform convergence is often enough for applications. And here is another proof of this fact:

\begin{proof}[\textbf{Another proof} that $(f_\alpha)$ converges locally uniformly to $f$]
We consider the case $X=\Omega$; the other case is similar. For each $z\in X$, choose a convex precompact $U\in\Nbh_X(x)$. (Namely, $\Cl_X(U)$ is compact.) By (b) of Thm. \ref{lb336}, there is $\mu\in I$ such that $\sup_{x\in\ovl U}\Vert f_\alpha'(x)-f_\mu'(x)\Vert\leq 1$ for all $\alpha\geq\mu$. Let $h_\alpha=f_\alpha-f_\mu$, and replace $I$ by $I_{\geq\mu}$. By finite-increment Thm. \ref{lb335}, for each $x,y\in U$ we have
\begin{align*}
\Vert h_\alpha(x)-h_\alpha(y)\Vert\leq \Vert x-y\Vert
\end{align*}
i.e., $(h_\alpha|_U)_{\alpha\in I}$ has uniform Lipschitz constant $1$, and hence is an equicontinuous set of functions. Choose a closed ball $B$ centered at $z$ such that $B\subset U$. Then, since $(h_\alpha|_B)_{\alpha\in I}$ is equicontinuous and converges pointwise to $f-f_\mu$ (on $B$), by Cor. \ref{lb284}, $(f_\alpha-f_\mu)_{\alpha\in I}$ converges uniformly on $B$ to $f-f_\mu$. Thus $f_\alpha$ converges uniformly on $B$ to $f$.
\end{proof}



\subsubsection{An interpretation of Thm. \ref{lb336} in terms of Banach spaces}


We give a more concise way of understanding the two conditions in Thm. \ref{lb336}.


\begin{co}\label{lb344}
Let $X$ be $\Omega$ or an interval in $\Rbb$. Define \index{l1@$l^{1,\infty}$}
\begin{align*}
l^{1,\infty}(X,V)=\{f\in V^X: f\text{ is differentiable on }X\text{ and }\Vert f\Vert_{l^{1,\infty}}<+\infty\}
\end{align*}
where $\Vert \cdot\Vert_{l^{1,\infty}}=\Vert\cdot\Vert_{1,\infty}$ is defined by 
\begin{align*}
\Vert f\Vert_{1,\infty}=\Vert f\Vert_{l^\infty}+\Vert f'\Vert_{l^\infty}=\sup_{x\in X}\Vert f(x)\Vert+\sup_{x\in X}\Vert f'(x)\Vert
\end{align*}
Then $l^{1,\infty}(X,V)$ is a Banach space with norm $\Vert\cdot\Vert_{1,\infty}.$
\end{co}


\begin{proof}
It is a routine check that $\Vert f\Vert_{l^{1,\infty}}$ defines a norm on ${l^{1,\infty}}(X,V)$. We now prove that ${l^{1,\infty}}(X,V)$ is complete. Let $(f_n)$ be a Cauchy sequence in ${l^{1,\infty}}(X,V)$. So $(f_n)$ and $(f_n')$ are Cauchy sequences in $l^\infty$, converging uniformly to $f,g\in V^X$ respectively. By Lem. \ref{lb336}, $f$ is differentiable, and $f'=g$. (In particular, $\Vert f'\Vert_\infty<+\infty$.) So $f_n'\rightrightarrows f'$. Thus
\begin{align*}
\Vert f_n-f\Vert_{1,\infty}=\Vert f_n-f\Vert_\infty+\Vert f_n'-f'\Vert_\infty\rightarrow 0
\end{align*} 
\end{proof}


\begin{rem}
Thm. \ref{lb336} and Cor. \ref{lb344} are almost equivalent. We have proved Cor. \ref{lb344} using Thm. \ref{lb336}. But we can also prove a slightly weaker version of Thm. \ref{lb336} using Cor. \ref{lb344} as follows: In the setting of Thm. \ref{lb336}, assume that each $f_\alpha:X\rightarrow V$ is differentiable, and that 
\begin{align}\label{eq128}
f_\alpha\rightrightarrows f\qquad f_\alpha'\rightrightarrows g
\end{align}
where $f,g\in l^\infty(X,V)$. Then  Cor. \ref{lb344} implies that $f$ is differentiable and $f'=g$. 
\end{rem}


\begin{proof}[$\star$ Proof]
By \eqref{eq128}, there is $\beta\in I$ such that $\Vert f_\alpha-f\Vert_\infty<+\infty$ and $\Vert f_\alpha'-g\Vert<+\infty$ for all $\alpha\geq\beta$. Thus, by replacing $I$ with $I_\beta$, we assume that $f_\alpha$ and $f_\alpha'$ are bounded on $X$. So $f_\alpha\in l^{1,\infty}(X,V)$. 

By \eqref{eq128}, both $(f_\alpha)$ and $(f'_\alpha)$ converge in $l^\infty(X,V)$. So they are Cauchy nets under the $l^\infty$-norm. So $(f_\alpha)$ is a Cauchy net in $l^{1,\infty}(X,V)$. By Cor. \ref{lb344}, $(f_\alpha)$ converges to $\wtd f\in l^{1,\infty}(X,V)$ under the $l^{1,\infty}$ norm. In particular, $f_\alpha\rightrightarrows \wtd f$ and $f_\alpha'\rightrightarrows \wtd f'$. Since, by assumption, we have $f_\alpha\rightrightarrows f$ and $f_\alpha'\rightrightarrows g$, we therefore get $f=\wtd f$ and $g=\wtd f'$.
\end{proof}



It is not surprising that Thm. \ref{lb336} can be rephrased in terms of the completeness of a normed vector space. After all, our proof of Thm. \ref{lb336} uses Cauchy nets in an essential way. In the future, we will use the completeness of $l^{1,\infty}$ to understand the commutativity of derivatives and other limit processes. 








\subsection{Derivatives of power series}


This section is a continuation of the previous one. We shall use Thm. \ref{lb336} to compute derivatives of power series.



\subsubsection{General result}


\begin{co}\label{lb338}
Assume that $V$ is over $\Cbb$. Let $f(z)=\sum_{n=0}^\infty v_nz^n$ be a power series in $V$. Assume that its radius of convergence is $R>0$. Then $f$ is differentiable on $B_\Cbb(0,R)$ and
\begin{align*}
f'(z)=\sum_{n=0}^\infty nv_nz^{n-1} 
\end{align*}
\end{co}

Note that since $\lim_n\sqrt[n]{n}=1$, $(\sqrt[n]{\Vert v_n\Vert})_{n\in\Zbb_+}$ and $(\sqrt[n]{n\Vert v_n\Vert})_{n\in\Zbb_+}$ have the same cluster points. Therefore
\begin{align}
\limsup_{n\rightarrow\infty}\sqrt[n]{\Vert v_n\Vert}=\limsup_{n\rightarrow\infty}\sqrt[n]{n\Vert v_n\Vert}
\end{align}
So $\sum v_nz^n$ and $\sum_{n=0}^\infty nv_nz^{n-1}$ have the same radius of convergence.

\begin{proof}[First proof]
For each $n\in\Nbb_+$, let $g_n(z)=v_0+v_1z+\cdots+v_nz^n$. Then by Thm. \ref{lb112}, for each $0<r<R$, the sequences $(g_n)$ (resp. $(g_n')$) converges uniformly to $f$ (resp. converges uniformly to $h(z)=\sum_0^\infty nv_nz^{n-1}$) on $B_\Cbb(0,r)$. Therefore, by Thm. \ref{lb336}, we have $f'(z)=h(z)$ for all $z\in B_\Cbb(0,r)$, and hence all $z\in B_\Cbb(0,R)$ since $r$ is arbitrary.
\end{proof}

\begin{proof}[Second proof]
Choose any $0<r<R$, and let $X_r=B_\Cbb(0,r)$. Consider $\sum v_n z^n$ as a series in the Banach space $l^{1,\infty}(X_r,\Cbb)$ (cf. Cor. \ref{lb344}). The norm of each term is $\Vert v_n\Vert+(n+1)\Vert v_{n+1}\Vert$. So the radius of convergence is $R$. Thus  $\sum v_n z^n$ converges in $l^{1,\infty}(X_r,\Cbb)$ to some $g\in l^{1,\infty}(X_r,\Cbb)$. This means that, on $X_r$, $\sum v_nz^n$ converges uniformly to $g$ (and hence $f=g$ on $X_r$) and $\sum nv_nz^{n-1}$ converges uniformly  to $g'$. So $f'(z)=g'(z)=\sum nv_nz^{n-1}$ for all $z\in X_r$.
\end{proof}



\subsubsection{Examples}\label{lb353}

\begin{eg}
By Cor. \ref{lb338}, the function $\exp:\Cbb\rightarrow\Cbb$ is differentiable, and
\begin{align*}
\frac d{dz}e^z=e^z
\end{align*}
Thus, if $\gamma:[a,b]\rightarrow\Cbb$ is differentiable, then by chain rule, $\exp\circ\gamma$ is differentiable on $[a,b]$, and
\begin{align*}
\frac d{dt}e^{\gamma(t)}=e^{\gamma(t)}\cdot \frac d{dt}\gamma(t)
\end{align*}
For example:
\begin{align*}
(e^{\alpha t})'=\alpha e^{\alpha t}\qquad (e^{t^2+\im t})'=(2t+\im)e^{t^2+\im t}
\end{align*}
\end{eg}


\begin{eg}
Define functions $\sin:\Cbb\rightarrow\Cbb$ and $\cos:\Cbb\rightarrow\Cbb$ \index{sincos@$\sin,\cos$} by
\begin{gather*}
\cos z=\frac {e^{\im z}+e^{-\im z}}2\qquad \sin z=\frac{e^{\im z}-e^{-\im z}}{2\im}
\end{gather*}
It follows from $e^ze^w=e^{z+w}$ that
\begin{align}
\cos(z+w)=\cos z\cos w-\sin z\sin w\qquad \sin(z+w)=\sin z\cos w+\cos z\sin w  \label{eq138}
\end{align}
By chain rule, we have $(e^{\alpha z})'=\alpha e^{\alpha z}$. So
\begin{gather*}
\sin'z=\cos z\qquad \cos' z=-\sin z
\end{gather*}
\end{eg}


%% Record #15 2023/11/8 three lectures  37

\begin{rem}
From $e^z=\sum_{n\in\Nbb}z^n/n!$, it is clear that the complex conjugate of $e^z$ is 
\begin{align*}
\ovl{e^z}=\ovl{\sum_{n=0}^\infty\frac {z^n}{n!}}=\sum_{n=0}^\infty\frac {\ovl z^n}{n!}=e^{\ovl z}
\end{align*}
Here, we have exchanged the order of conjugate and infinite sum, because the function $z\in\Cbb\mapsto \ovl z$ is continuous. Thus, if $t\in\Rbb$, then
\begin{align*}
\ovl{e^{\im t}}=e^{-\im t}\qquad |e^{\im t}|^2=\ovl{e^{\im t}} e^{\im t}=e^{-\im t}e^{\im t}=1
\end{align*}
It follows that
\begin{gather*}
\cos t=\Real(e^{\im t})\qquad\sin t=\Imag(e^{\im t})\\
(\cos t)^2+(\sin t)^2=(\Real(e^{\im t}))^2+(\Imag(e^{\im t}))^2=|e^{\im t}|^2=1
\end{gather*}
It also follows from $|e^{\im t}|=1$ that if $a,b\in\Rbb$ then, by $e^{a+b\im}=e^ae^{b\im}$, we have
\begin{align*}
e^{a+b\im}=e^a\qquad\in\Rbb_{>0}
\end{align*}
\end{rem}





\begin{eg}
By Prop. \ref{lb337}, the function $\log:\Rbb_{>0}\rightarrow\Rbb$ is differentiable, and
\begin{align*}
(\log x)'=\frac 1x
\end{align*}
\end{eg}

\begin{eg}
We have 
\begin{align*}
\lim_{t\rightarrow 0}\frac{\log(1+t)}t=(\log x)'|_{x=1}=1
\end{align*}
and hence $\lim_{x\rightarrow +\infty}x\log(1+1/x)=1$ by Def. \ref{lb197}-(3) (since $\lim_{x\rightarrow+\infty}1/x=0$ as a net limit). Taking exponential, and using the continuity of $\exp$ at $1$, we get 
\begin{align*}
\lim_{x\rightarrow+\infty}\Big(1+\frac 1x\Big)^x=e
\end{align*} 
\end{eg}




\begin{eg}
If $a>0$ and $z\in\Cbb$, recalling $a^z=e^{z\log a}$, we use chain rule to find
\begin{align*}
\frac d{dz}a^z=a^z\log a
\end{align*}
Similarly, if $\alpha\in\Cbb$ and $x>0$, then the chain rule gives the derivative of $x^\alpha$:
\begin{align*}
\frac d{dx}x^\alpha=\alpha\cdot x^{\alpha-1}
\end{align*}
\end{eg}



\begin{eg}
Compute $\dps\sum_{n=0}^\infty \frac {n^2}{3^n}$
\end{eg}


\begin{proof}
The series $\dps f(z)=\sum_{n=0}^\infty \frac{z^n}{3^n}$ has radius of convergence $3$. When $|z|<3$, it converges absolutely to $(1-\frac z3)^{-1}=3(3-z)^{-1}$. So, by Cor. \ref{lb338}, when $|z|<3$ we have
\begin{gather*}
    \sum_{n=0}^\infty \frac{nz^{n-1}}{3^n}=f'(z)=3(3-z)^{-2}\\
    \sum_{n=0}^\infty \frac{n(n-1)z^{n-2}}{3^n}=f''(z)=6(3-z)^{-3}
\end{gather*}
So the value of the original series equals
\begin{align*}
f''(1)+f'(1)=\frac 32
\end{align*}
\end{proof}


\subsubsection{A proof of (generalized) Leibniz rule}



We end this section by giving a fun proof of Leibniz rule for higher derivatives. For simplicity, we consider only scalar-valued functions.



\begin{pp}[\textbf{Leibniz rule}] \index{00@Leibniz rule}  \label{lb348}
Let $X$ be either a nonempty interval in $\Rbb$ (resp. a nonempty open subset of $\Cbb$). Let $f,g$ be functions from $X$ to $\Rbb$ (resp. to $\Cbb$). Let $z\in X$. Suppose that $f^{(n)}(z)$ and $g^{(n)}(z)$ exist. Then
\begin{align}
(fg)^{(n)}(z)=\sum_{j=0}^n {n\choose j}f^{(n-j)}(z)g^{(j)}(z)
\end{align}
\end{pp}




The above Leibniz rule is usually proved using the formula
\begin{align}
{n+1\choose j}={n\choose j-1}+{n\choose j}
\end{align}
where $n\in\Nbb$ and $1\leq j\leq n$. In the following, we give a proof without using this formula. We need the fact that the function
\begin{align}
\Cbb^n\rightarrow C(\Rbb,\Cbb)\qquad  (a_0,\dots,a_{n-1})\mapsto p(x)=\sum_{j=0}^{n-1}a_jx^j  \label{eq133}
\end{align}
is injective: this is because $j!\cdot a_j=f^{(j)}(0)$ by Exp. \ref{lb323}.



\begin{proof}[\textbf{Proof of Prop. \ref{lb348}}]
By induction on $n$ and by the classical Leibniz rule (Prop. \ref{lb320}), we have
\begin{align}
(fg)^{(n)}(z)=\sum_{j=0}^n C_{n,j}\cdot f^{(n-j)}(z)g^{(j)}(z) \label{eq132}
\end{align}
where each $C_{n,j}$ is an integer independent of $f$ and $g$. (In particular, $C_{n,j}$ is independent of whether the variables are real of complex.) Thus, to determine the value of $C_{n,j}$, we can use some special functions. 

We consider $f(x)=e^{s x}$ and $g(x)=e^{t x}$ where $s,t\in\Rbb$. Recall that we have proved that $(e^{\alpha t})'=\alpha e^{\alpha t}$ using chain rule and the derivative formula for exponentials. So \eqref{eq132} reads
\begin{align*}
(s+t)^n\cdot e^{(s+t)x}=\sum_{j=0}^n C_{n,j}\cdot s^{n-j}t^{j}\cdot e^{(s+t)x}
\end{align*}
Taking $x=0$ gives
\begin{align}
(s+t)^n=\sum_{j=0}^n C_{n,j}\cdot s^{n-j}t^j  \label{eq135}
\end{align}
Comparing this with the binomial formula \eqref{eq60}, and noticing the injectivity of  \eqref{eq133} (first applied to \eqref{eq135} for each fixed $t$, where \eqref{eq135} is viewed as a polynomial of $s$; then applied to each coefficient before $s^{n-j}$, which is a polynomial of $t$), we immediately get $C_{n,j}={n\choose j}$.
\end{proof}




\begin{comment}
\begin{rem}
By induction on $k$, one easily proves
\begin{align}
(z_1+\cdots+z_k)^n=\sum_{j_1+\cdots+j_k=n}{n\choose j_1,\dots,j_k}z_1^{j_1}\cdots z_k^{j_k}
\end{align}
where
\begin{align}
{n\choose j_1,\dots,j_k}=\frac{n!}{j_1!\cdots j_k!}
\end{align}
is the \textbf{multinomial coefficient}. Then, the same method as above proves
\begin{align}
(f_1\cdots f_k)^{(n)}=\sum_{j_1+\cdots+j_k=n}{n\choose j_1,\dots,j_k} f_1^{(j_1)}\cdots f_k^{(j_k)}
\end{align}
\end{rem}
\end{comment}

















\subsection{Problems and supplementary material}


Let $\Omega$ be nonempty open subset of $\Cbb$. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$. Assume that $\Fbb=\Cbb$ if we take derivatives with respect to complex variables.


\begin{df}\label{lb354}
Let $X$ be either $\Omega$ or an interval in $\Rbb$. Define \index{Cn@$C^n(X,V)$} \index{C@$C^\infty$}
\begin{gather*}
C^n(X,V)=\{f\in V^X:f,f',\dots,f^{(n)}\text{ exist and are continuous}\}\qquad (\text{if }n\in\Nbb)\\
C^\infty(X,V)=\bigcap_{n\in\Nbb}C^n(X,V)
\end{gather*}
If $X\subset\Rbb$, elements in $C^\infty(X,V)$ are called \textbf{smooth functions}. \index{00@Smooth functions}
\end{df}


\begin{prob}\label{lb355}
Let $X$ be either $\Omega$ or an interval in $\Rbb$. For each $n\in\Nbb$, define \index{ln@$l^{n,\infty}$}
\begin{align*}
l^{n,\infty}(X,V)=\{f\in V^X:f,f',\dots,f^{(n)}\text{ exist, and }\Vert f\Vert_{n,\infty}<+\infty\}
\end{align*}
where $\Vert f\Vert_{n,\infty}=\Vert f\Vert_{l^{n,\infty}}$ is defined by
\begin{align*}
\Vert f\Vert_{n,\infty}=\Vert f\Vert_\infty+\Vert f'\Vert_\infty+\cdots+\Vert f^{(n)}\Vert_\infty
\end{align*}
(In particular, we understand $l^{0,\infty}$ as $l^\infty$.) Clearly $\Vert\cdot\Vert_{n,\infty}$ is a norm. We have proved that $l^{n,\infty}(X,V)$ is complete when $n=0,1$. 
\begin{enumerate}
\item Prove by induction on $n$ that $l^{n,\infty}(X,V)$ is complete for every $n$.
\item Prove that for each $n\in\Nbb$, $C^n(X,V)\cap l^{n,\infty}(X,V)$ is a closed subset of $l^{n,\infty}(X,V)$ (and hence, is a Banach space by Prop. \ref{lb86}).  Prove that if $X$ is compact then $C^n(X,V)\subset l^{n,\infty}(X,V)$.
\end{enumerate}
\end{prob}







\begin{cv}
Unless otherwise stated, when $X$ is compact, we always choose $l^{n,\infty}$ to be the norm on $C^n(X,V)$.
\end{cv}








\newpage




\section{More on derivatives}

\subsection{Cauchy's MVT and L'H\^opital's rule}


The goal of this section is to prove L'H\^opital's rule. For that purpose, we first need to prove Cauchy's MVT.



\subsubsection{Main theorems}




\begin{thm}[\textbf{Cauchy's MVT}] \index{00@Cauchy's MVT}
Let $-\infty<a<b<+\infty$. Let $f,g\in C([a,b],\Rbb)$ be differentiable on $(a,b)$. Then there exists $x\in(a,b)$ such that
\begin{align*}
    f'(x) (g(b)-g(a))=g'(x)(f(b)-f(a))
\end{align*}
\end{thm}

In particular, if $g'\neq0$ on $(a,b)$, then $g$ is injective (Cor. \ref{lb424}), and we can write the above formula as
\begin{align*}
\frac{f'(x)}{g'(x)}=\frac{f(b)-f(a)}{g(b)-g(a)}
\end{align*}


\begin{proof}
Cauchy's MVT specializes to Lagrange's MVT if we set $g(x)=x$. Moreover, in the proof of Lagrange's MVT (Thm. \ref{lb345}), we applied Rolle's MVT to the function $f(x)-kx$ where $k$ is the slope of a line segment. Motivated by this observation, we consider the function $\psi(x)=f(x)-kg(x)$. If one wants $\psi(a)=\psi(b)$, one then solves that $k=\frac{f(b)-f(a)}{g(b)-g(a)}$. But we would rather consider $(g(b)-g(a))\psi$ in order to avoid the issue that the denominator is possibly zero. So we set
\begin{align*}
h(x)=(g(b)-g(a))f(x)-(f(b)-f(a))g(x)
\end{align*}
Clearly $h(a)=h(b)$. By Rolle's MVT, there exists $x\in(a,b)$ such that
\begin{align*}
0=h'(x)=(g(b)-g(a))f'(x)-(f(b)-f(a))g'(x)
\end{align*}
\end{proof}








\begin{thm}[\textbf{L'H\^opital's rule}]  \index{00@L'H\^opital's rule}
Let $-\infty\leq a<b\leq+\infty$. Let $f,g\in C((a,b),\Rbb)$ be differentiable on $(a,b)$. Assume that $g'$ is nowhere zero on $(a,b)$. (So $g$ is strictly monotonic, cf. Prop. \ref{lb347}.) Assume 
\begin{align}
\lim_{x\rightarrow a}\frac{f'(x)}{g'(x)}=A\qquad \text{exists in }\ovl\Rbb    \label{eq129}
\end{align}
Assume that one of the following two cases are satisfied:
\begin{gather*}
\lim_{x\rightarrow a}f(x)=\lim_{x\rightarrow a}g(x)=0  \tag{Case $\frac 00$}\\
\lim_{x\rightarrow a}|g(x)|=+\infty \tag{Case $\frac *\infty$}
\end{gather*}
Then we have $\dps\lim_{x\rightarrow a}\frac{f(x)}{g(x)}=A$. The same conclusion holds if ``$x\rightarrow a$" is replaced by ``$x\rightarrow b$".
\end{thm}


\begin{rem}
Since $g$ is strictly monotonic, there is at most one $x\in(a,b)$ such that $g(x)=0$. So $\lim_{x\rightarrow a}f(x)/g(x)$ means the limit over $x\in (a,b)\setminus g^{-1}(0)$. Alternatively, one can assign an arbitrary value to $f(x)/g(x)$ when $g(x)=0$, and understand $\lim_{x\rightarrow a}$ as a limit over $x\in (a,b)$.
\end{rem}


\begin{eg}
Compute $\dps\lim_{x\rightarrow+\infty}\frac{x^n}{e^x}$
\end{eg}

\begin{proof}
By L'H\^opital's rule in the case $\frac *\infty$, we have
\begin{align*}
\lim_{x\rightarrow+\infty}\frac{x^n}{e^x}=\lim_{x\rightarrow+\infty}\frac{nx^{n-1}}{e^x}=\lim_{x\rightarrow+\infty}\frac{n(n-1)x^{n-2}}{e^x}=\cdots=\lim_{x\rightarrow+\infty}\frac{n!}{e^x}=0
\end{align*}
where the convergence of the limit is derived from right to left.
\end{proof}




\subsubsection{Proof of L'H\^opital's rule}\label{lb349}




We divide the proof of L'H\^opital's rule into several steps. Also, we only treat the case $x\rightarrow a$, since the other case is similar. In the following, $(a,b)$ means an interval, but not an element in the Cartesian product (which will be written as $a\times b$).


\begin{proof}[\textbf{Step 1}]
We let $\dps\frac{f(x)-f(y)}{g(x)-g(y)}$ take value $\dps\frac{f'(x)}{g'(x)}$ if $x=y$. In this step, we prove
\begin{align}\label{eq130}
\lim_{x\times y\rightarrow a\times a}~\frac{f(x)-f(y)}{g(x)-g(y)}=A
\end{align}
where $x\times y$ is defined on $(a,b)^2=(a,b)\times (a,b)$. In view of Def. \ref{lb197}-(3m), we pick any sequence $x_n\times y_n$ in $(a,b)^2$. By Cauchy's MVT, there is $\xi_n \in [x_n,y_n]$ (if $x_n\leq y_n$) or $\xi_n\in[y_n,x_n]$ (if $x_n\geq y_n$) such that
\begin{align*}
f'(\xi_n)(g(x_n)-g(y_n))=g'(\xi_n)(f(x_n)-f(y_n)).
\end{align*} 
So we have
\begin{align*}
    \frac{f'(\xi_n)}{g'(\xi_n)}=\frac{f(x_n)-f(y_n)}{g(x_n)-g(y_n)} 
\end{align*}
Since $\lim_n x_n=\lim_n y_n=a$, we clearly have $\lim_n \xi_n=a$.  Therefore, by \eqref{eq129} (this is the only place where we use \eqref{eq129}), we have
\begin{align*}
\lim_{n\rightarrow\infty} \frac{f(x_n)-f(y_n)}{g(x_n)-g(y_n)}=\lim_{n\rightarrow\infty}\frac{f'(\xi_n)}{g'(\xi_n)}=A
\end{align*}
This proves \eqref{eq130}.
\end{proof}





\begin{proof}[\textbf{Step 2}]
It follows from Def. \ref{lb197}-(3) that if $(x_n)$ and $(y_k)$ are sequences in $(a,b)$ converging to $a$, then
\begin{align}
\lim_{n,k\rightarrow\infty} \frac{f(x_n)-f(y_k)}{g(x_n)-g(y_k)}=A \label{eq131}
\end{align}  
In other words, we apply Def. \ref{lb197}-(3) to the net $(x_n\times y_k)_{n\times k\in\Zbb_+^2}$ which replaces the sequence $(x_n\times y_n)_{n\in\Zbb_+}$ in Step 1. We shall use \eqref{eq131} to prove the two cases of L'H\^opital's rule.


Let us prove the case $\frac 00$. This is the easier case. Choose any sequence $(x_n)$ in $(a,b)$ converging to $a$. We want to prove that $\lim_n f(x_n)/g(x_n)=A$. So we choose any sequence $(y_k)$ in $(a,b)$ converging to $a$, and we know that the double limit \eqref{eq131} exists. Moreover, if $n$ is fixed, then $\lim_k f(y_k)=\lim_k g(y_k)=0$ by assumption. Thus
\begin{align}
\lim_{k\rightarrow\infty}\frac{f(x_n)-f(y_k)}{g(x_n)-g(y_k)}=\frac{f(x_n)}{g(x_n)} \label{eq134}
\end{align}
Thus, by Thm. \ref{lb122}, when $n\rightarrow\infty$, the RHS of the above equation converges to \eqref{eq131}. This finishes the proof for the case $\frac 00$.
\end{proof}



\begin{proof}[\textbf{Step 3}]
Finally, we address the (more difficult) case $\frac *\infty$. Assume $\lim_{x\rightarrow a}|g(x)|=+\infty$. Again, we choose a sequence $(x_n)$ in $(a,b)$ converging to $a$. To prove $f(x_n)/g(x_n)\rightarrow A$, one may want to pick any sequence $(y_k)$ in $(a,b)$, and compute the limit on the LHS of \eqref{eq134}. Unfortunately, in this case, we do not know whether this limit converges or not: As one can compute, it is equal to $\lim_{k\rightarrow\infty}f(y_k)/g(y_k)$, whose convergence is part of the result we need to prove!

It is not hard to address this issue: Since $\ovl\Rbb$ is (sequentially) compact, by Thm. \ref{lb74}, it suffices to prove that any cluster point $B\in\ovl\Rbb$ of $(f(x_n)/g(x_n))_{n\in\Zbb_+}$ is equal to $A$. Thus, we let $(y_k)$ be any subsequence of $(x_n)$ such that $(f(y_k)/g(y_k))_{k\in\Zbb_+}$ converges to $B$. Let us prove $A=B$ using the same method as in Step 2. We compute that
\begin{align}
\frac{f(x_n)-f(y_k)}{g(x_n)-g(y_k)}=\frac{g(y_k)}{g(x_n)-g(y_k)}\cdot\frac{f(x_n)-f(y_k)}{g(y_k)}  \label{eq136}
\end{align}
Since $\lim_k |g(y_k)|=+\infty$, we have $\lim_{k\rightarrow\infty}C/g(y_k)\rightarrow 0$ for any $C\in\Rbb$ independent of $k$. Therefore, as $k\rightarrow\infty$, the first factor on the RHS of \eqref{eq136} converges to $-1$, and the second factor converges to $-B$. It follows that
\begin{align*}
\lim_{n\rightarrow\infty}\lim_{k\rightarrow\infty}\frac{f(x_n)-f(y_k)}{g(x_n)-g(y_k)}=\lim_{n\rightarrow\infty}B=B
\end{align*}
Therefore $A=B$ by Thm. \ref{lb122}.
\end{proof}




\begin{comment}

\begin{rem}
In this course, I have often encouraged the reader to give an elementary proof of a theorem that has been proved in fancy language or in a language different from the common one. However, I shall not make the same suggestion this time.  Many textbook proofs of case $\frac *\infty$ of L'H\^opital's rule use elementary techniques of estimation. However, these proofs seem difficult to digest because they (implicitly) involve approximations with respect to two variables, but they do not use the correct language to discuss such approximations: the language of double limits. Without the correct language, math will become a mess.
\end{rem}
\end{comment}


The proof of L`H\^opital's rule is now complete.


\begin{rem}
The above proof can be easily translated into a language without double limits. We consider the case of $\frac *\infty$ and assume for example that $-\infty<A<+\infty$ and $a\in\Rbb$, and sketch the proof as follows. 

By \eqref{eq129} and Cauchy' MVT, for every $\eps>0$ there is $\delta>0$ such that for all $a<x,y<a+\delta$ (where $x\neq y$) we have
\begin{align*}
A-\eps\leq\frac{f(x)-f(y)}{g(x)-g(y)}\leq A+\eps
\end{align*}
Choose any sequence $(x_n)$ in $(a,b)$ converging to $a$. Let $B$ be any cluster point of $(f(x_n)/g(x_n))_{n\in\Zbb_+}$ in $\ovl\Rbb$. We need to prove that $A=B$. Let $(y_k)_{k\in\Zbb_+}$ be any subsequence of $(x_n)$ such that $\lim_k f(y_k)/g(y_k)=B$. Then there is $N\in\Nbb$ such that for all $n\geq N,k\geq N$ satisfying $x_n\neq y_k$, we have
\begin{align*}
A-\eps\leq\frac{f(x_n)-f(y_k)}{g(x_n)-g(y_k)}\leq A+\eps
\end{align*}
For each $n\geq N$, apply $\lim_{k\rightarrow\infty}$ to the above inequality, and notice $\lim_k |g(y_k)|=+\infty$. Then we get $A-\eps\leq B\leq A+\eps$, finishing the proof. \hfill\qedsymbol
\end{rem}










\subsection{Trigonometric functions and $\pi$}\label{lb397}



In this section, we prove that $\sin$, $\cos$, and $\pi$ satisfy the properties we learned in high schools. Some of them have already been proved in Subsec. \ref{lb353}. We leave the proof of the basic properties of the other trigonometric functions to the reader.

Let $x$ be a real variable. Recall that $\sin,\cos:\Rbb\rightarrow\Rbb$ are determined by the fact that $e^{\im x}=\cos x+\im\sin x$. In particular, that $|e^{\im x}|=1$ implies $(\cos x)^2+(\sin x)^2=1$. We have proved that
\begin{align*}
\sin'=\cos\qquad \cos'=-\sin
\end{align*}
Since $e^{\im\cdot 0}=1$, we have
\begin{align*}
(\sin x)'|_{x=0}=\cos 0=1\qquad (\cos x)'|_{x=0}=-\sin 0=0
\end{align*}
In particular, since $\sin'=\cos$ is strictly positive on a neighborhood of $0$, by Cor. \ref{lb330}, $\sin$ is strictly increasing on that neighborhood.




We shall define $\frac \pi 2$ to be the smallest positive zero $\cos$. However, we must first prove the existence of this number:


\begin{lm}
There exists $x\geq 0$ such that $\cos x=0$.
\end{lm}


\begin{proof}
Suppose this is not true. Then by $\cos 0=1$ and intermediate value theorem, we have $\cos x>0$ for all $x\geq 0$. In other words, $\sin'>0$ on $\Rbb_{\geq 0}$. Thus, by Cor. \ref{lb330}, $\sin$ is strictly increasing on $\Rbb_{\geq0}$. Therefore $A=\lim_{x\rightarrow+\infty}\sin x$ exists in $\ovl\Rbb$. Since $\sin 0=0$, we must have $A\in\ovl\Rbb_{>0}$. By L'H\^opital's rule in case $\frac *\infty$, we have
\begin{align*}
\lim_{x\rightarrow+\infty} \frac{\cos x}x=\lim_{x\rightarrow+\infty}-\sin x=-A<0
\end{align*}
contradicting the fact that $\cos x>0$ if $x\geq 0$.
\end{proof}



\begin{df}
We define the number $\pi$ to be \index{zz@$\pi$, the number pi}
\begin{align*}
\pi=2\cdot\inf\big(\Rbb_{\geq 0}\cap\cos^{-1}(0)\big)=2\cdot\inf\{x\in\Rbb_{\geq 0}:\cos x=0\}
\end{align*}
Note that $\big(\Rbb_{\geq 0}\cap\cos^{-1}(0)\big)$ is a closed subset of $\Rbb$. So its infinimum belongs to this set. Therefore, $\frac \pi 2$ is the smallest $x\geq0$ satisfying $\cos(x/2)=0$.
\end{df}




\begin{pp}
We have
\begin{align}\label{eq137}
\sin 0=0\qquad\sin\frac\pi 2=1\qquad\cos 0=1\qquad\cos\frac\pi2=0
\end{align}
On $(0,\pi/2)$, $\sin$ and $\cos$ are strictly positive. On $[0,\pi/2]$, $\sin$ is strictly increasing, and $\cos$ is strictly decreasing.
\end{pp}

\begin{proof}
All the formulas in \eqref{eq137}, except $\sin(\pi/2)=1$, has been proved. Since $(\sin\frac\pi 2)^2=1-(\cos\frac\pi2)^2$, we have $\sin\frac\pi2=\pm1$.

By the definition of $\pi$, we know that $\sin'=\cos$ is $>0$ on $[0,\frac\pi2)$. So $\sin$ is strictly increasing on $[0,\frac\pi2]$. Thus, $\sin0=0$ implies that $\sin>0$ on $(0,\frac\pi 2]$. In particular, $\sin\frac \pi2=1$. Since $\cos'=-\sin$ is $<0$ on $(0,\frac\pi2)$, $\cos$ is strictly decreasing on $[0,\frac\pi2]$.
\end{proof}


\begin{pp}
We have $\sin(-x)=-\sin x$ and $\cos(-x)=\cos x$.
\end{pp}


\begin{proof}
Immediate from $e^{-\im x}=1/e^{\im x}$ and the definitions of $\sin$ and $\cos$.
\end{proof}


From what we have proved, we know that the graph of $\sin$ and $\cos$ on $[-\frac\pi2,\frac\pi2]$ looks as follows.
\begin{align*}
\vcenter{\hbox{{
			\includegraphics[height=2.2cm]{fig2.png}}}}
\end{align*}

\begin{pp}
We have
\begin{gather*}
\sin x=\cos(x-\frac\pi2)=\cos(\frac\pi2-x)\qquad \cos(x)=\sin(\frac\pi 2-x)
\end{gather*}
\end{pp}

\begin{proof}
Immediate from \eqref{eq138} and \eqref{eq137}.
\end{proof}
Thus, the graph of $\sin$ is the rightward translation of that of $\cos$ by $\frac\pi 2$. Therefore, the graph of $\sin$ on $[-\frac\pi2,0]$ is translated to the graph of $\cos$ on $[-\pi,-\frac\pi2]$. That $\cos(x)=\cos(-x)$ gives us the graph of $\cos$ on $[\frac\pi2,\pi]$. Thus, we know the graph of $\cos$ on $[-\pi,\pi]$. 



\begin{thm}[\textbf{Euler's formula}] \index{00@Euler's formula}
We have $e^{\im\pi}=-1$, and hence $e^{2\im\pi}=e^{\im\pi}e^{\im\pi}=1$.
\end{thm}


\begin{proof}
We have $e^{\im\pi/2}=\cos(\frac\pi2)+\im\sin(\frac\pi2)=\im$. Hence $e^{\im\pi}=\im^2=-1$.
\end{proof}

\begin{pp}
We have $\sin x=\sin(x+2\pi)$ and $\cos x=\cos(x+2\pi)$
\end{pp}


\begin{proof}
Immediate from \eqref{eq138} and that $1=e^{2\im\pi}=\cos(2\pi)+\im\sin(2\pi)$.
\end{proof}




Thus, $\cos$ and $\sin$ have period $2\pi$. This completes the graphs of $\cos x$ and $\sin x=\cos(x-\frac\pi2)$ on $\Rbb$.

The fact that $2\pi$ is the length of the unit circle will be proved in Exp. \ref{lb398}.



\subsection{Taylor's theorems}


Assume throughout this section that $-\infty<a<b<+\infty$ and $V$ is a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.


In this section, we generalize MVTs and finite-increment theorem to higher derivatives. These generalizations are all under the name ``Taylor theorem". Recall Def. \ref{lb354} and Pb. \ref{lb355} for the meaning of $l^{n,\infty},C^n,C^\infty$. We first discuss the generalization of finite-increment theorem, which can be applied to vector-valued functions.





 
\subsubsection{Taylor's theorems for vector-valued functions}



\begin{df}\label{lb574}
Let $X$ be a normed vector space. Let $A\subset X$. Let $a\in\Cl_X(A)\setminus A$ (or more generally, assume $a\in\Cl_X(A\setminus \{a\})$). Let $f:A\rightarrow V$. Let $r\in\Rbb_{\geq0}$.
\begin{itemize}
\item We write $\dps f(x)=o(\Vert x-a\Vert^r)$ if $\dps\lim_{x\rightarrow a}\frac{f(x)}{\Vert x-a\Vert^r}=0$.
\item We write $\dps f(x)=O(\Vert x-a\Vert^r)$ if $\dps\limsup_{x\rightarrow a}\frac{\Vert f(x)\Vert}{\Vert x-a\Vert^r}<+\infty$ where $\limsup$ is the limit superior of a net (cf. Rem. \ref{lb269} and Pb. \ref{lb346}). In other words, there exists $U\in\Nbh_X(a)$ such that $\sup_{x\in A\cap U}\Vert f(x)\Vert/\Vert x-a\Vert^r<+\infty$.
\end{itemize}
When $r=0$, we simply write $o(1)$ and $O(1)$. The two symbols $o,O$ are called \textbf{Landau symbols}. \index{00@Landau symbols $o,O$}
\end{df}


In this section, we will frequently use the formula
\begin{align}
S_n(x)=\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k
\end{align}
whenever the RHS makes sense.


\begin{thm}[\textbf{Taylor's theorem, Peano form}]\index{00@Taylor's theorem, Peano form}\label{lb360}
Let $f:[a,b]\rightarrow V$ and $n\in\Zbb_+$. Assume that $f',f'',\dots,f^{(n)}$ exist at $a$ (resp. at $b$). Then for each $x\in (a,b)$ we have
\begin{subequations}
\begin{gather}
f(x)=\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k+o((x-a)^n)\label{eq142}\\
\text{resp.}\nonumber\\
f(x)=\sum_{k=0}^n\frac{f^{(k)}(b)}{k!}(x-b)^k+o((b-x)^n) \label{eq143}
\end{gather}
\end{subequations}
\end{thm}


Expressions of the form \eqref{eq142} are called \textbf{Taylor expansions} \index{00@Taylor expansion} of $f$ with \textbf{center} $a$.

\begin{rem}
In order for $f^{(n)}(a)$ to exist, it is assumed that $f^{(n-1)}$ exists on a neighborhood of $a$.
\end{rem}


\begin{proof}[Proof of Peano-form]
Since the two cases are similar, we only treat the case at $a$. Define \textbf{remainder} \index{00@Remainder of Taylor expansion} $g(x)=f(x)-S_n(x)$. Then
\begin{align}
g(a)=g'(a)=\cdots=g^{(n)}(a)=0  \label{eq140}
\end{align}
It suffices to prove Taylor's theorem for $g$, i.e.
\begin{align}
g(x)=o((x-a)^n)
\end{align}
We prove this by induction. The case $n=1$ is obvious from the definition of derivatives, which says
\begin{align}
\frac{g(x)-g(a)}{x-a}-g'(a)=o(1)
\end{align}
and hence $g(x)=(x-a)o(1)=o(x-a)$. 

Now assume $n\geq 2$. Assume that Peano form has been proved for case $n-1$. Applying this result to $g'$. We then get $g'(x)=o((x-a)^{n-1})$. Since $g''(a)$ exists,  $g'$ exists on $[a,c]$ where $a<c<b$. In particular, $g$ is continuous on $[a,c]$. Therefore, by finite-increment Thm. \ref{lb333}, if $x\in(a,c)$, then
\begin{align*}
\Vert g(x)\Vert\leq (x-a)\cdot\sup_{a<t<x}\Vert g'(t)\Vert
\end{align*}
If we can prove $\sup_{a\leq t\leq x}\Vert g'(t)\Vert=o((x-a)^{n-1})$, then we immediately have $g(x)=o((x-a)^n)$. Thus, the proof of Peano form is finished by the next lemma.
\end{proof}

\begin{lm}
Assume that $f:[a,b]\rightarrow V$ satisfies $f(x)=o((x-a)^r)$. Define
\begin{align*}
\wtd f(x)=\sup_{a<t<x}\Vert f(t)\Vert
\end{align*}
Then $\wtd f(x)=o((x-a)^r)$.
\end{lm}


\begin{proof}
Choose any $\eps>0$. Since $f(x)=o((x-a)^r)$, we know that there is $c\in (a,b)$ such that for all $a<x<c$ we have $\Vert f(x)\Vert\leq \eps |x-a|^r$. Thus
\begin{align*}
|\wtd f(x)|\leq\sup_{a<t<x}\eps |t-a|^r =\eps|x-a|^r
\end{align*}
\end{proof}


Among all the versions of Taylor's theorem discussed in this section, the following one is most useful. (Another useful version, the integral form of Taylor's theorem, will be proved in Thm. \ref{lb425}.) Note that if we assume $f\in C^{(n+1)}([a,b],V)$ in Thm. \ref{lb360}, then Thm. \ref{lb359} immediately implies Thm. \ref{lb360}.  



\begin{thm}[\textbf{Higher order finite-increment theorem}]\index{00@Higher order finite-increment theorem}\label{lb359}
Let $n\in\Nbb$ and $f\in C^n([a,b],V)$. Assume that $f^{(n+1)}$ exists everywhere on $(a,b)$. Then, for every $x\in(a,b]$ resp. $x\in[a,b)$, we have respectively
\begin{subequations}
\begin{gather}
\Big\Vert f(x)-\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k \Big\Vert \leq \frac{(x-a)^{n+1}}{(n+1)!}\cdot \sup_{a<t<x}\Vert f^{(n+1)}(t)\Vert  \label{eq139}\\
\Big\Vert f(x)-\sum_{k=0}^n\frac{f^{(k)}(b)}{k!}(x-b)^k \Big\Vert \leq \frac{(b-x)^{n+1}}{(n+1)!}\cdot \sup_{x<t<b}\Vert f^{(n+1)}(t)\Vert
\end{gather}
\end{subequations} 
\end{thm}





\begin{proof}
We only prove the first formula: applying the first formula to $\wtd f(x)=f(-x)$ implies the second one. We prove \eqref{eq139} by induction on $n$. Moreover, we shall prove \eqref{eq139} for the case $x=b$. The general case follows by restricting $f$ to $[a,x]$.  When $n=0$, \eqref{eq139} is the content of (classical) finite-increment Thm. \ref{lb333}. Assume case $n-1$ has been proved ($n\in\Zbb_+$). In case $n$, take $g(x)=f(x)-S_n(x)$. Then \eqref{eq140} is true. Let
\begin{align*}
M=\sup_{a<t<b}\Vert f^{(n+1)}(t)\Vert =\sup_{a<t<b}\Vert g^{(n+1)}(t)\Vert
\end{align*}
By case $n-1$, for each $a\leq x\leq b$ we have
\begin{align*}
\Vert g'(x)\Vert \leq\frac{(x-a)^n}{n!}\cdot M=h'(x)\qquad\text{where }~h(x)=\frac{M(x-a)^{n+1}}{(n+1)!} 
\end{align*}
Thus, by Thm. \ref{lb329}, we have
\begin{align*}
\Vert g(b)-g(a)\Vert\leq h(b)-h(a)=\frac{M(b-a)^{n+1}}{(n+1)!} 
\end{align*}
This proves \eqref{eq139} for $g$, and hence for $f$.
\end{proof}



\subsubsection{Taylor's theorem for real-valued functions}



\begin{thm}[\textbf{Taylor's theorem, Lagrange form}]\index{00@Taylor's theorem, Lagrange form}\label{lb395}
Let $n\in\Nbb$ and $f\in C^n([a,b],\Rbb)$. Assume that $f^{(n+1)}$ exists everywhere on $(a,b)$. Then for every $x\in(a,b]$ resp. $x\in[a,b)$, there exists $\xi\in(a,x)$ resp. $\eta\in (x,b)$ such that, respectively,
\begin{subequations}
\begin{gather}
f(x)=\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k+\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}\label{eq141}\\
f(x)=\sum_{k=0}^n\frac{f^{(k)}(b)}{k!}(x-b)^k+\frac{f^{(n+1)}(\eta)}{(n+1)!}(x-b)^{n+1}
\end{gather}
\end{subequations}
\end{thm}



\begin{proof}
We only prove \eqref{eq141}. Applying \eqref{eq141} to $\wtd f(x)=f(-x)$ (defined on $[-b,-a]$) implies the second formula. Again, it suffices to prove \eqref{eq141} for $g(x)=f(x)-S_n(x)$, which satisfies \eqref{eq140}. Thus, we want to find $\xi\in(a,x)$ satisfying
\begin{align*}
g(x)=\frac{g^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}
\end{align*}
This can be proved by applying Cauchy's MVT repeatedly:
\begin{align*}
&\frac{g(x)}{(x-a)^{n+1}}\xlongequal{{\color{ForestGreen}\exists x_1\in (a,x)}}\frac{g'(x_1)}{(n+1)(x_1-a)^n}\xlongequal{{\color{ForestGreen}\exists x_2\in (a,x_1)}}\frac{g''(x_2)}{(n+1)n(x_2-a)^{n-1}}\\
=&\cdots \xlongequal{{\color{ForestGreen}\exists x_n\in (a,x_{n-1})}}\frac{g^{(n)}(x_n)}{(n+1)!(x_n-a)}\xlongequal{{\color{ForestGreen}\exists \xi\in (a,x_n)}}\frac{g^{(n+1)}(\xi)}{(n+1)!}
\end{align*}
\end{proof}




We will mainly use Thm. \ref{lb359} instead of the Lagrange form, since the latter does not apply directly to vector valued functions. However, Thm. \ref{lb359} can be derived from the Lagrange form and Hahn-Banach theorem. We have used the fact that $V^*$ separates points of $V$ to prove that a function $[a,b]\rightarrow V$ is constant if its derivative exists and is constantly zero. (See Cor. \ref{lb326}.) However, to prove Thm. \ref{lb359}, we need the stronger fact that for every $v\in V$ there exists a nonzero $\varphi\in V^*$ such that $\bk{\varphi,v}=\Vert\varphi\Vert\cdot\Vert v\Vert$. (See Rem. \ref{lb393}.) By scaling $\varphi$, we can assume that $\Vert\varphi\Vert=1$ and $\bk{\varphi,v}=\Vert v\Vert$.

We now give the second proof of Thm. \ref{lb359}:


%% Record #16 2023/11/15 three lectures  40


\begin{proof}[\textbf{Proof of Thm. \ref{lb359} using Lagrange form and Hahn-Banach}]
The Lagrange form clearly implies Thm. \ref{lb359} in the special case that $V=\Rbb$. Now consider the general case, and view $V$ as a real Banach space if it was originally over $\Cbb$. Take $g(x)=f(x)-S_n(x)$. We need to prove $\Vert g(x)\Vert\leq \frac{(x-a)^{n+1}}{(n+1)!}M$ where $M=\sup_{a<t<x}\Vert g^{(n+1)}(t)\Vert$.

By Hahn-Banach (Rem. \ref{lb393}), there exists $\varphi\in V^*$ with $\Vert\varphi\Vert=1$ such that $\varphi\circ g(x)=\Vert g(x)\Vert$. Applying the one dimensional special case to $\varphi\circ g$, we have
\begin{align}
\Vert g(x)\Vert=\varphi\circ g(x)\leq \frac{(x-a)^{n+1}}{(n+1)!}\cdot\sup_{a<t<x}\big|(\varphi\circ g)^{(n+1)}(t)\big|  \label{eq179}
\end{align}
provided that $\varphi\circ g\in C^n([a,b],V)$ and that $(\varphi\circ g)^{(n+1)}$ exists on $(a,b)$.

By the continuity of $\varphi$, we have
\begin{align}\label{eq180}
(\varphi\circ g)'(t)=\lim_{s\rightarrow t}\frac{\varphi\circ g(s)-\varphi\circ g(t)}{s-t}=\varphi\Big(\lim_{s\rightarrow t}\frac{g(s)-g(t)}{s-t}\Big)=\varphi(g'(t))
\end{align}
Applying this formula repeatedly, we see that $\varphi\circ g\in C^n([a,b],V)$, that $(\varphi\circ g)^{(n+1)}$ exists on $(a,b)$, and that $(\varphi\circ g)^{(n+1)}=\varphi\circ g^{(n+1)}$. By Rem. \ref{lb372}, we have
\begin{align*}
\sup_{a<t<x}\big|(\varphi\circ g)^{(n+1)}(t)\big|= \sup_{a<t<x}\big|\bk{\varphi,g^{(n+1)}(t)}\big|\leq \sup_{a<t<x}\Vert g^{(n+1)}(t)\Vert=M
\end{align*}
Combining this result with \eqref{eq179} finishes the proof.
\end{proof}


When $V$ is $\Rbb^N$ equipped with the Euclidean norm, the Hahn-Banach theorem (in the form of Rem. \ref{lb393}) is very easy: Define the linear map $\varphi:\Rbb^N\rightarrow \Rbb$ sending each $v\in \Rbb^N$ to its dot product with $g(x)/\Vert g(x)\Vert$. Then $\varphi$ satisfies $\Vert\varphi\Vert=1$ and $\varphi\circ g(x)=\Vert g(x)\Vert$.  So there is nothing mysterious in the above proof. (You are encouraged to compare this proof with the one of \cite[Thm. 5.19]{Rud-P}.)






\subsection{Functions approximated by their Taylor series}


Taylor's theorems do not imply that a smooth function $f$ on an interval of $\Rbb$ can be approximated by its \textbf{Taylor series} \index{00@Taylor series} (with \textbf{center} $a$):
\begin{align}
\sum_{k=0}^\infty \frac{f^{(k)}(a)}{k!}(x-a)^k
\end{align}
The following is a typical example:

\begin{eg}\label{lb358}
Define $f:\Rbb\rightarrow\Rbb$ by
\begin{align*}
f(x)=\left\{
\begin{array}{ll}
\dps \exp\big(-\frac 1{x^2}\big)& (\text{if }x>0)\\[1ex]
0&(\text{if }x\leq 0)
\end{array}
\right.
\end{align*}
Then $f^{(n)}(0)=0$ for all $n\in\Nbb$. So the Taylor series of $f$ at $0$ is $0$, which cannot approximate $f(x)$ when $x>0$.
\end{eg}


On the contrary, if $f$ is defined on an open subset of $\Cbb$, and if $f'$ exists everywhere on its domain, then $f^{(n)}$ exists for all $n$, and $f$ can be approximated locally uniformly by its Taylor series. This is a deep result in complex analysis. In fact, a thorough understanding of power series is impossible without the help of complex analysis.

In the following, we show that some important real variable functions can be approximated by their Taylor series. Actually, the proof can be simplified by complex analysis, since these examples are the restriction of some differentiable complex variable functions (aka \textbf{holomorphic functions}) to the real line.


\begin{eg}\label{lb480}
Consider the Taylor expansion of $\log:\Rbb_{>0}\rightarrow\Rbb$ at $x=1$. By induction on $n\in\Zbb_+$, one computes that
\begin{align*}
\log^{(n)}(x)=(-1)^{n-1}\frac{(n-1)!}{x^n}
\end{align*}
Therefore, its Taylor expansion in order $n$ is
\begin{align*}
\sum_{k=1}^n \frac{(-1)^{k-1}}{k} (x-1)^k+R_{n+1}(x)
\end{align*}
where $R_{n+1}(x)$ is the remainder. To show that $\log x$ is approximated uniformly (on certain domain) by its Taylor series, one need to show that $R_{n+1}$ converges uniformly to $0$ on that domain. 

We would like to prove that for every $0<r<1$ we have $R_{n+1}\rightrightarrows 0$ on $[1-r,1+r]$. This would imply that series on the RHS of the following formula (whose radius of convergence is $1$) converges uniformly to $f$ on $[1-r,1+r]$:
\begin{align}
\log(x)=\sum_{k=1}^n \frac{(-1)^{k-1}}{k} (x-1)^k
\end{align}
However, using the Taylor's theorems proved in the previous section, one can prove the uniform convergence only when $0<r\leq1/2$. The general case of $0<r<1$ should be proved by another method.   \hfill\qedsymbol
\end{eg}


\begin{proof}[\textbf{Proof for the case $0<r\leq \frac 12$}] In fact, we shall prove the uniform convergence on $[1-r,2]$. 
By Thm. \ref{lb359}, for all $x\in[1-r,1+r]$ we have
\begin{align*}
|R_{n+1}(x)|\leq \frac{|x-1|^{n+1}}{(n+1)!}\cdot\sup_{
\begin{subarray}{c}
1\leq t\leq x\\
\text{or }x\leq t\leq 1
\end{subarray}
}
\big|\log^{(n+1)}(t)\big|=\frac{|x-1|^{n+1}}{n+1}\cdot\sup_{
\begin{subarray}{c}
1\leq t\leq x\\
\text{or }x\leq t\leq 1
\end{subarray}
}\frac 1{t^{n+1}}
\end{align*}
where the $\sup$ is over $1\leq t\leq x$ or $x\leq t\leq 1$, depending on whether $1\leq x\leq 1+r$ or $1-r\leq x\leq 1$.

If $1\leq x\leq 1+r$, then $1\leq t\leq x$ implies $1/t^{n+1}\leq 1$. So
\begin{align*}
|R_{n+1}(x)|\leq\frac{(x-1)^{n+1}}{n+1}\leq\frac{r^{n+1}}{n+1} 
\end{align*}
where the RHS converges to $0$ as $n\rightarrow\infty$ whenever $0<r\leq 1$. In particular, we get the renowned formula
\begin{align}
\log 2=\sum_{n=1}^\infty \frac{(-1)^{n-1}}n
\end{align}

If $1-r\leq x\leq 1$, then $x\leq t\leq 1$ implies $1/t^{n+1}\leq 1/x^{n+1}$. Thus
\begin{align*}
|R_{n+1}(x)|\leq\frac 1{n+1}\Big(\frac 1x-1 \Big)^{n+1}\leq \frac 1{n+1}\Big(\frac 1{1-r}-1 \Big)^{n+1}=\frac 1{n+1}\Big(\frac r{1-r}\Big)^{n+1}
\end{align*}
If $0<r\leq 1/2$, then $0<r/(1-r)\leq 1$, and hence the RHS above converges to $0$ as $n\rightarrow\infty$. This proves that $R_{n+1}\rightrightarrows0$ on $[1-r,1+r]$ when $0<r\leq1/2$.
\end{proof}



\begin{proof}[\textbf{Proof for the case $0<r<1$}]
Assume $0<r<1$. For the convenience of discussion, we prove instead that 
\begin{align}
\sum_{k=1}^\infty \frac{(-1)^{k-1}}{k} x^k  \label{eq144}
\end{align}
the Taylor series of $\log(1+x)$ at $0$, converges uniformly to $\log(1+x)$ on $[-r,r]$

The radius of convergence of the series \eqref{eq144} is $1$. Thus, by Thm. \ref{lb112}, \eqref{eq144} converges locally uniformly on $(-1,1)$ to some function $f:(-1,1)\rightarrow\Rbb$. By Cor. \ref{lb338}, $f$ is differentiable on $(-1,1)$, and the series
\begin{align}
\sum_{k=0}^\infty (-1)^kx^k   \label{eq145}
\end{align}
converges locally uniformly on $(-1,1)$ to $f'$. But we clearly have
\begin{align}
\eqref{eq145}=\frac{1}{1-(-x)}=\frac 1{1+x}
\end{align}
Thus $f'(x)=\frac d{dz}(\log(1+x))$. Now, \eqref{eq144} clearly implies that $f(0)=0=\log(1+0)$. Therefore, by Cor. \ref{lb326}, we obtain $f(x)=\log(1+x)$ on $(-1,1)$. In other words, \eqref{eq144} converges locally uniformly on $(-1,1)$ (and hence uniformly on $[-r,r]$) to $\log(1+x)$.
\end{proof}



\begin{eg}
Let $\alpha\in\Cbb$. Then
\begin{align}
\sum_{k=0}^\infty{\alpha\choose k}x^k   \label{eq146}
\end{align}
the Taylor series of $(1+x)^\alpha=\exp(\alpha\log(1+x))$ at $x=0$, converges locally uniformly on $(-1,1)$ to $(1+x)^\alpha$.
\end{eg}



\begin{proof}
Using Prop. \ref{lb109}, one easily checks that the radius of convergence of \eqref{eq146} is $1$. So Thm. \ref{lb112} implies that \eqref{eq146} converges locally uniformly on $(-1,1)$ to some $f:(-1,1)\rightarrow\Cbb$. Let
\begin{align*}
g(x)=(1+x)^\alpha
\end{align*}
Our goal is to prove $f=g$. 

It is clear that $(1+x)g'(x)=\alpha g(x)$. By Cor. \ref{lb338},  the series
\begin{align}
\sum_{k=0}^\infty k{\alpha\choose k}x^{k-1}=\sum_{k=0}^\infty (k+1){\alpha\choose k+1}x^k
\end{align}
converges locally uniformly on $(-1,1)$ to $f'$. Thus, by Cor. \ref{lb362}, we have
\begin{align*}
&(1+x)f'(x)=\sum_{k=0}^\infty \bigg( k{\alpha\choose k}+(k+1){\alpha\choose k+1}\bigg)x^k\\
=&\sum_{k=0}^\infty \alpha{\alpha\choose k}x^k=\alpha f(x)
\end{align*}
Therefore, $f$ satisfies the same differential equation as $g$, namely $(1+x)f'=\alpha f$. Moreover, we clearly have $f(0)=1=g(0)$. So we have $f=g$ on $(-1,1)$ if we apply the next lemma to $f-g$.
\end{proof}


\begin{lm}\label{lb553}
Let $I$ be an interval of $\Rbb$ (with at least two points) such that $0\in I$. Let $\Fbb\in\{\Rbb,\Cbb\}$. Let $\varphi\in C(I,\Fbb^{n\times n})$. Assume that $f\in C^1(I,\Fbb^n)$ satisfies
\begin{align}
f'(x)=\varphi(x)\cdot f(x)\qquad f(0)=0
\end{align} 
Then for all $x\in I$ we have $f(x)=0$.
\end{lm}

\begin{proof}
We prove this lemma for the case that $I$ is an open interval. The proof for the other types of intervals is similar. Also, we equip $\Fbb^{n\times n}$ with the operator norm by viewing $n\times n$ matrices as elements of $\fk L(\Fbb^n)$. (Recall that the operator norm is equivalent to the Eucliden norm, see Thm. \ref{lb363}.) Let
\begin{align*}
\Omega=\{x\in I:f(x)=0\}=f^{-1}(0)
\end{align*}
which is a closed subset of $I$ (since the inverse image under a continuous map of a closed set is closed). Clearly $\Omega$ is nonempty since $0\in\Omega$. If we can prove that $\Omega$ is open, then we have $\Omega=I$ because $I$ is connected. This will finish the proof.

So let us prove that any $p\in\Omega$ is an interior point of $\Omega$ with respect to $I$. Choose $r>0$ such that $[p-r,p+r]\subset I$. Let
\begin{align*}
C=\sup_{-r\leq x\leq r}\Vert\varphi(x)\Vert
\end{align*}
which is a finite number by extreme value theorem. Let $\delta=\min\{\frac 1{2C},r\}$ which is $>0$. It suffices to prove that
\begin{align*}
M=\sup_{x\in[p-\delta,p+\delta]}\Vert f(x)\Vert
\end{align*}
is zero. (Note that $M<+\infty$ again by EVT.) Then we will have $[p-\delta,p+\delta]\subset\Omega$, finishing the proof. 

For each $x\in[p-\delta,p+\delta]$, by Rem. \ref{lb372} for operator norms, we have
\begin{align*}
\Vert f'(x)\Vert=\Vert\varphi(x)f(x)\Vert\leq\Vert\varphi(x)\Vert\cdot \Vert f(x)\Vert\leq CM
\end{align*}
Thus, by finite-increment theorem, we have for all $x\in[p-\delta,p+\delta]$ that
\begin{align*}
\Vert f(x)\Vert=\Vert f(x)-f(p)\Vert\leq CM|x-p|\leq CM\delta\leq \frac M2
\end{align*}
Applying $\sup_{x\in[p-\delta,p+\delta]}$ to $\Vert f(x)\Vert$, we get $M\leq M/2$. So $M=0$.
\end{proof}

\begin{rem}
The above Lemma can also be proved in a similar way as Thm. \ref{lb329}: Let $x\in I$. Assume WLOG that $x>0$. Let $E=f^{-1}(0)\cap[0,x]$, which is a closed subset of $[0,x]$. So $t=\sup E$ is in $[0,x]$. To show $f(x)=0$, one just need to prove $t=x$. If not, then $t<x$. Then, as in the above proof, one can find $\delta>0$ such that $\varphi(s)=0$ whenever $s\in[t,t+\delta]$, impossible.
\end{rem}






\subsection{Convex functions}






In this section, we fix an interval $I\subset\Rbb$ containing at least two points.


\begin{df}
Let $\mbf x=(x_1,x_2)$, $\mbf y=(y_1,y_2)$, $\mbf z=(z_1,z_2)$ be three points of $\Rbb^2$ satisfying $x_1<y_1<z_1$. We say that the ordered triple $(\mbf x,\mbf y,\mbf z)$ is a \textbf{convex triple} \index{00@Convex triple} if  the following equivalent conditions are satisfied:
\begin{enumerate}[label=(\arabic*)]
\item $\mbf y$ is below or on the interval $[\mbf x,\mbf z]$. In other words, we have $y_2\leq tx_2+(1-t)z_2$ if $t\in[0,1]$ is such that $y_1= tx_1+(1-t)z_1$.
\item We have $\dps \frac{y_2-x_2}{y_1-x_1}\leq \frac{z_2-y_2}{z_1-y_1}$.
\item We have $\dps \frac{y_2-x_2}{y_1-x_1}\leq \frac{z_2-x_2}{z_1-x_1}$.
\item We have $\dps \frac{z_2-x_2}{z_1-x_1}\leq\frac{z_2-y_2}{z_1-y_1}$.
\end{enumerate}
\end{df}

\begin{proof}[Proof of equivalence]
The equivalence of these statements is clear from the picture, and is not hard to check rigorously using inequalities. We leave the details to the reader.
\end{proof}


Recall the definition of convex subsets in real vector spaces (Def. \ref{lb364}).

\begin{df}
A function $f:I\rightarrow\Rbb$ is called \textbf{convex} if the following equivalent conditions are true:
\begin{enumerate}
\item[(1)] The set
\begin{align}
D_f=\{(x,y)\in\Rbb^2:x\in I,y\geq f(x)\}\label{eq147}
\end{align}
is a convex subset of $\Rbb^2$. 
\item[(1)] For any three points $x<y<z$ of $I$, the points $(x,f(x)),(y,f(y)),(z,f(z))$ form a convex triple.
\end{enumerate}
\end{df}

\begin{proof}[Proof of equivalence]
Again, the equivalence is clear from the picture. The details are left to the readers.
\end{proof}



The convexity of differentiable functions is easy to characterize:

\begin{thm}\label{lb366}
Let $f:I\rightarrow\Rbb$ be differentiable. Then $f$ is convex iff $f'$ is increasing. In particular, if $f''$ exists on $I$, then $f$ is convex iff $f''\geq0$ on $I$.
\end{thm}

\begin{proof}
Assume that $f'$ is increasing. Choose $x<y<z$ in $I$. By Lagrange's MVT, there exist $\xi\in(x,y)$ and $\eta\in(y,z)$ such that the slope of the interval $[(x,f(x)),(y,f(y))]$ equals $f'(\xi)$, and that the slope of $[(y,f(y)),(z,f(z))]$ equals $f'(\eta)$. Since $f'(\xi)\leq f'(\eta)$, the points $(x,f(x)),(y,f(y)),(z,f(z))$ form a convex triple. This proves that $f$ is convex.

Now assume that $f$ is convex. Choose any $x<y$ in $I$. We need to prove that $f'(x)\leq f'(y)$. Choose any $\eps>0$. Then there exist $x_1$ and $y_1$ such that $x<x_1<y_1<y$, that $f'(x)-\eps$ is $\leq$ the slope $k_1$ of $[(x,f(x)),(x_1,f(x_1))]$, and that $f'(y)+\eps$ is $\geq$ the slope $k_2$ of $[(y_1,f(y_1)),(y,f(y))]$. Since $f$ is convex, $k_1$ is $\leq$ the slope $k'$ of $[(x_1,f(x_1)),(y_1,f(y_1))]$, and $k'\leq k_2$. Therefore $k_1\leq k_2$. Thus $f'(x)-\eps\leq f'(y)+\eps$. Since $\eps>0$ is arbitrary, we get $f'(x)\leq f'(y)$.

When $f''$ exists, the equivalence between $f''\geq0$ and the increasing monotonicity of $f'$ is due to Cor. \ref{lb330}.
\end{proof}











The most important general property about convex functions is:

\begin{thm}[\textbf{Jensen's inequality}]  \index{00@Jensen's inequality}
Let $f:I\rightarrow\Rbb$ be a convex function. Let $n\in\Zbb_+$ and $x_1,\dots,x_n\in I$. Choose $t_1,\dots,t_n\in[0,1]$ such that $t_1+\cdots+t_n=1$. Then
\begin{align}
f(t_1x_1+\cdots+t_nx_n)\leq t_1f(x_1)+\cdots+t_nf(x_n)
\end{align}
\end{thm}

\begin{proof}
The points $(x_1,f(x_1)),\dots,(x_n,f(x_n))$ are in $D_f=\eqref{eq147}$. Therefore, by Lem. \ref{lb365}, the point
\begin{align*}
t_1(x_1,f(x_1))+\cdots+t_n(x_n,f(x_n))=(t_1x_1+\cdots+t_nx_n,t_1f(x_1)+\cdots+t_nf(x_n))
\end{align*}
is in the convex set $D_f$. 
\end{proof}



\begin{lm}\label{lb365}
Let $V$ be a real vector space. Let $E$ be a convex subset of $V$. Let $n\in\Zbb_+$ and $p_1,\dots,p_n\in E$. Any \textbf{convex combination} of $p_1,\dots,p_n$ \index{00@Convex combination} (i.e., any point of the form $t_1p_1+\cdots+t_np_n$ where $t_1,\dots,t_n\in[0,1]$ and $t_1+\cdots+t_n=1$) is inside $E$.
\end{lm}

The geometric meaning of this lemma is clear: If $V=\Rbb^2$ (which is the main case we are interested in), then $t_1p_1+\cdots+t_np_n$ stands for an arbitrary point inside the polygon with vertices $p_1,\dots,p_n$. So this point is clearly inside $E$.


\begin{proof}
We prove by induction on $n$. The case $n=1$ is obvious. Suppose case $n-1$ has been proved $n\geq 2$. Consider case $n$. It is trivial when $t_1+\cdots+t_{n-1}=0$. So let us assume $t_1+\cdots+t_{n-1}>0$. By case $n-1$, the point $q=\lambda_1 p_1+\cdots+\lambda_{n-1}p_{n-1}$ is in $E$ where
\begin{align*}
\lambda_i=\frac{t_i}{t_1+\cdots+t_{n-1}}
\end{align*}
Since $E$ is convex, the point $(t_1+\cdots+t_{n-1})q+t_np_n$ (which equals $t_1p_1+\cdots+t_np_n$) is in $E$.
\end{proof}


\begin{eg}
Since $(-\log x)''=x^{-2}$ is positive on $\Rbb_{>0}$, by Thm. \ref{lb366}, $-\log$ is a convex function on $\Rbb_{>0}$. Therefore, if $0<\lambda_1,\dots,\lambda_n\leq 1$ and $\lambda_1+\cdots+\lambda_n=1$, then by Jensen's inequality, for each $x_1,\dots,x_n>0$ we have $-\log(\sum_i\lambda_ix_i)\leq -\lambda_i\log x_i$. Taking exponentials, we get
\begin{align*}
x_1^{\lambda_1}\cdots x_n^{\lambda_n}\leq \lambda_1x_1+\cdots+\lambda_n x_n
\end{align*}
for all $x_1,\dots,x_n>0$, and hence for all $x_1,\dots,x_n\geq0$. In particular, we get the \textbf{inequality of arithmetic and geometric means}
\begin{align}
(x_1\cdots x_n)^{\frac 1n}\leq \frac{x_1+\cdots+x_n}{n}
\end{align}
and the \textbf{Young's inequality} \index{00@Young's inequality}
\begin{align}
x^{\frac 1p}y^{\frac 1q}\leq \frac xp+\frac yq
\end{align}
if $p,q>1$ and $\frac 1p+\frac 1q=1$.
\end{eg}


\begin{df}
Let $p\in[1,+\infty]$. We say that $q\in[1,+\infty]$ is the \textbf{H\"older conjugate} \index{00@H\"older conjugate} of $p$ if $\frac 1p+\frac 1q=1$.
\end{df}



%% Record #17 2023/11/20 two lectures  42



\subsection{H\"older's and Minkowski's inequalities; $l^p$ spaces}



In this section, we use Young's inequality to prove two inequalities that are of vital importance to the development of modern analysis:





\begin{thm}
Let $x_1,\dots,x_n,y_1,\dots,y_n\in\Rbb_{\geq0}$. Let $p,q\in(1,+\infty)$ satisfy $\frac 1p+\frac 1q=1$. Then the following inequalities (called respectively \textbf{H\"older's inequality} \index{00@H\"older's inequality} and \index{00@Minkowski's inequality} \textbf{Minkowski's inequality}) are true:
\begin{subequations}
\begin{gather}
x_1y_1+\cdots+x_ny_n\leq\sqrt[p]{x_1^p+\cdots+x_n^p}\cdot\sqrt[q]{y_1^q+\cdots+y_n^q}\\
\sqrt[p]{(x_1+y_1)^p+\cdots+(x_n+y_n)^p}\leq  \sqrt[p]{x_1^p+\cdots+x_n^p}+\sqrt[p]{x_1^p+\cdots+x_n^p}
\end{gather}
\end{subequations}
\end{thm}


In the special case that $p=q=2$, H\"older's inequality is called \textbf{Cauchy-Schwarz inequality}

\begin{proof}[$\star\star$ Proof]
Assume WLOG that $x_i>0$ and $y_j>0$ for some $i,j$. Young's inequality implies that for each $i$,
\begin{align*}
\frac{x_i}{\sqrt[p]{\sum_k x_k^p}}\cdot \frac{y_i}{\sqrt[q]{\sum_k y_k^q}}\leq \frac{x_i^p}{p\sum_k x_k^p}+\frac{y_i^q}{q\sum_k y_k^q}
\end{align*}
Taking sum over $i$ gives
\begin{align*}
\sum_i\frac{x_i}{\sqrt[p]{\sum_k x_k^p}}\cdot \frac{y_i}{\sqrt[q]{\sum_k y_k^q}}\leq\frac 1p+\frac 1q=1
\end{align*}
This proves H\"older's inequality.

Notice that $pq=p+q$. Let $z_i=x_i+y_i$. By H\"older's inequality, we have
\begin{align*}
x_1z_1^{p-1}+\cdots+x_nz_n^{p-1}\leq \Big(\sum_k x_k^p\Big)^{\frac 1p}\cdot\Big(\sum_k z_k^{(p-1)q}\Big)^{\frac 1q}=\Big(\sum_k x_k^p\Big)^{\frac 1p}\cdot\Big(\sum_k z_k^p\Big)^{\frac 1q}
\end{align*}
and similarly
\begin{align*}
y_1z_1^{p-1}+\cdots+y_nz_n^{p-1}\leq\Big(\sum_k y_k^p\Big)^{\frac 1p}\cdot\Big(\sum_k z_k^p\Big)^{\frac 1q}
\end{align*}
Adding up these two inequalities, we get
\begin{align*}
\sum_k z_k^p\leq \bigg(\Big(\sum_k x_k^p\Big)^{\frac 1p}+\Big(\sum_k y_k^p\Big)^{\frac 1p}\bigg)\cdot\Big(\sum_k z_k^p\Big)^{\frac 1q}
\end{align*}
Dividing both sides by $\Big(\sum_k z_k^p\Big)^{\frac 1q}$ proves Minkowski's inequality.
\end{proof}



Minkowski's inequality is often used in the following way:


\begin{thm}\label{lb596}
Let $X$ be a set, and let $V$ be a normed vector space over $\Fbb\in\{\Rbb,\Cbb\}$. Let $1\leq p<+\infty$. For each $f\in V^X$, define the \pmb{$l^p$}\textbf{-norm} \index{lp@$l^p(X,V)$ and $l^p$ norm}
\begin{align}
\Vert f\Vert_p\equiv\Vert f\Vert_{l^p}=\Big(\sum_{x\in X}\Vert f(x)\Vert^p\Big)^{\frac 1p}
\end{align}
Then for each $f,g\in V^X$ and $\lambda\in\Fbb$ we have
\begin{align}\label{eq148}
\Vert f+g\Vert_p\leq \Vert f\Vert_p+\Vert g\Vert_p\qquad \Vert \lambda f\Vert_p =|\lambda|\cdot \Vert f\Vert_p
\end{align}
In particular, $\Vert\cdot\Vert_p$ is a norm on the \pmb{$l^p$}\textbf{-space}
\begin{align}
l^p(X,V)=\{f\in V^X:\Vert f\Vert_{l^p}<+\infty\}
\end{align}
If $V$ is Banach space, then so is $l^p(X,V)$.
\end{thm}

Recall $0\cdot(\pm\infty)=0$.

\begin{proof}
Choose any $A\in\fin(2^X)$. By Minkowski's and triangle inequality, we have
\begin{align*}
&\sum_A |f+g|^p\leq\sum_A (|f|+|g|)^p\leq\Big(\Big(\sum_A |f|^p\Big)^{\frac 1p}+\Big(\sum_B |f|^p\Big)^{\frac 1p} \Big)^p\\
\leq&\Big(\Big(\sum_X |f|^p\Big)^{\frac 1p}+\Big(\sum_X |f|^p\Big)^{\frac 1p} \Big)^p=(\Vert f\Vert_p+\Vert g\Vert_p)^p
\end{align*}
Taking $\lim_{A\in\fin(2^X)}$ gives $\Vert f+g\Vert_p^p\leq (\Vert f\Vert_p+\Vert g\Vert_p)^p$, proving the first inequality of \eqref{eq148}. The second of \eqref{eq148} clearly holds by taking $\lim_{A\in\fin(2^X)}$ of
\begin{align*}
\sum_A |\lambda f|^p=|\lambda|\sum_A |f|^p
\end{align*}
and noting that the multiplication map $t\in\ovl\Rbb_{\geq 0}\mapsto |\lambda|t\in\ovl\Rbb_{\geq0}$ is continuous.

Suppose that $V$ is complete. Let $(f_n)$ be a Cauchy sequence in $l^p(X,V)$. Then for each $x\in X$, $(f_n(x))$ is a Cauchy sequence in $V$, converging to some element $f(x)\in V$. By Cauchyness, for each $\eps>0$ there is $N\in\Zbb_+$ such that for all $n,k\geq N$ we have $\Vert f_n-f_k\Vert_p\leq\eps$, and hence
\begin{align*}
\sum_{x\in A} \Vert f_n(x)-f_k(x)\Vert^p\leq\eps^p
\end{align*}
for all $A\in\fin(2^X)$. Taking $\lim_k$ gives $\sum_A |f_n-f|^p\leq\eps^p$. Taking $\lim_{A\in\fin(2^X)}$ gives $\Vert f_n-f\Vert_p\leq\eps$ for all $n\in\geq N$. (In particular, $\Vert f\Vert_p\leq \Vert f_N\Vert+\eps<+\infty$, and hence $f\in l^p(X,V)$.) This proves that $(f_n)$ converges to $f$ under the $l^p$-norm. So $l^p(X,V)$ is complete.
\end{proof}



\begin{comment}
To interpret H\"older's inequality, we notice the following fact:
\begin{rem}\label{lb368}
Let $V,W$ be normed vector spaces over $\Fbb\in\{\Rbb,\Cbb\}$. Let $T:V\rightarrow W$ be a linear map. Let $C\in\ovl\Rbb_{\geq 0}$. Let $\Vert T\Vert\in\ovl\Rbb_{\geq0}$ be the operator norm of $T$. Then 
\begin{align}
\Vert T\Vert\leq C\qquad\Longleftrightarrow\qquad \Vert T\xi\Vert\leq C\Vert\xi\Vert~~(\forall\xi\in V)
\end{align} 
Indeed, ``$\Rightarrow$" follows from Rem. \ref{lb372}, and ``$\Leftrightarrow$" follows by taking arbitrary $\xi\in \ovl B_V(0,1)$. Alternatively, it follows by dividing both sides of $\Vert T\xi\Vert\leq C\Vert\xi\Vert$ by $\Vert\xi\Vert$ (when $\Vert\xi\Vert>0$) and noticing the easy fact
\begin{align}
\Vert T\Vert=\sup_{\xi\in V,\Vert \xi\Vert=1}\Vert T\xi\Vert \label{eq149}
\end{align}
It is also obvious from \eqref{eq149} that if $C<+\infty$, then
\begin{align}
\Vert T\Vert\geq C\qquad\Longleftrightarrow\qquad \forall\eps>0,~\exists \xi\in V\setminus\{0\}~\text{such that }\Vert T\xi\Vert\geq(C-\eps)\Vert\xi\Vert
\end{align}
To see this, we again divide both sides of $\Vert T\xi\Vert\geq(C-\eps)\Vert\xi\Vert$ by $\Vert\xi\Vert$.
\end{rem}
\end{comment}

\begin{thm}\label{lb369}
Let $X$ be a set. Let $\Fbb\in\{\Rbb,\Cbb\}$. Let $p,q\in[1,+\infty]$ such that $\frac 1p+\frac 1q=1$. Then there is a linear isometry
\begin{subequations}
\begin{gather}\label{eq150}
\Psi: l^p(X,\Fbb)\rightarrow l^q(X,\Fbb)^*\qquad f\mapsto\Psi(f)
\end{gather}
where for each $g\in l^q(X,\Fbb)$, the value of $\Psi(f)$ at $g$ is
\begin{align}
\bk{\Psi(f),g}=\sum_{x\in X}f(x)g(x)  \label{eq151}
\end{align}
\end{subequations}
where the RHS converges. 
\end{thm}


In fact, we will see in Thm. \ref{lb527} that if $q<+\infty$ (and hence $p>1$), $\Psi$ is surjective, and hence is an isomorphism of normed vector spaces. (This is not a difficult fact. So you can prove it yourself.)


\begin{proof}
We treat the case $1<p,q<+\infty$, and leave the case that $p\in\{1,+\infty\}$ to the readers. Assume $f\in l^p$ and $g\in l^q$, then for each $A\in\fin(2^X)$ we have by H\"older's inequality that
\begin{align*}
\Big\Vert\sum_A fg\Big\Vert\leq \sum_A |fg|\leq \Big(\sum_A|f|^p \Big)^{\frac 1p}\Big(\sum_A|g|^q \Big)^{\frac 1q}\leq \Vert f\Vert_p\cdot\Vert g\Vert_q
\end{align*}
Applying $\lim_{A\in\fin(2^X)}$ to the second term above implies that $\sum_X |fg|<+\infty$. So $\sum_Xfg$ converges absolutely, and hence converges. So \eqref{eq151} makes sense, and hence $\Psi$ is a well-defined linear map. Applying $\lim_{A\in\fin(2^X)}$ to the first term above implies that
\begin{align*}
|\bk{\Psi(f),g}|\leq \Vert f\Vert_p\cdot\Vert g\Vert_q
\end{align*}
Thus, by Rem. \ref{lb372}, we obtain $\Vert\Psi(f)\Vert\leq\Vert f\Vert_p$.

To show $\Vert\Psi(f)\Vert=\Vert f\Vert_p$, assume WLOG that $f\neq 0$, and define $g:X\rightarrow\Fbb$ to be $g=\frac{\ovl f}{|f|}\cdot |f|^{p-1}$, where $g(x)$ is understood as $0$ if $f(x)=0$. Then
\begin{align*}
\bk{\Psi(f),g}=\sum_X |f|^p=\Vert f\Vert_p^p
\end{align*}
and $\Vert g\Vert_q^q=\sum_X |f|^{pq-q}=\sum_X|f|^p=\Vert f\Vert_p^p$. So
\begin{align*}
\Vert f\Vert_p\cdot\Vert g\Vert_q=\Vert f\Vert_p^{1+p/q}=\Vert f\Vert_p^p=\bk{\Psi(f),g}
\end{align*}
This proves $\Vert\Psi(f)\Vert=\Vert f\Vert_p$ by Rem. \ref{lb372}. So $\Psi$ is a linear isometry.
\end{proof}



I do not want to deviate too much from the main purpose of this section, which is to show some important applications of convex functions. I will therefore stop my discussion about $l^p$ spaces, and continue this topic in the future. The crucial role of $l^p$ spaces and their continuous versions (namely $L^p$ spaces) in modern analysis is a long story. The study of these objects constitutes a major part of the second half of this course. Let me mention just one point: the compactness of $\ovl B_{l^2(\Zbb,\Cbb)}(0,1)$ under the pointwise convergence topology (by viewing it as a subspace of $\Cbb^\Zbb$) was the most important reason for Hilbert and Schmidt to study the Hilbert space $l^2(\Zbb,\Cbb)$, and it was the crucial property that allowed them to fully solve the eigenvalue problem in integral equations (cf. Subsec. \ref{lb370}).












\subsection{Problems and supplementary material}


Let $-\infty<a<b<+\infty$. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.


\begin{prob}
Let $f$ be the function in Exp. \ref{lb358}. Prove that $f^{(n)}(0)=0$ for all $n\in\Nbb$.
\end{prob}




\begin{prob}\label{lb361}
Let $n\in\Nbb$. Let $f:[a,b]\rightarrow V$ such that $f,f',\dots,f^{(n)}$ exist everywhere on $[a,b]$. Use the higher order finite-increment theorem to prove that
\begin{align}
\Big\Vert f(x)-\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k\Big\Vert\leq \frac{(x-a)^{n}}{n!}\cdot \sup_{a<t<x}\Vert f^{(n)}(t)-f^{(n)}(a)\Vert  \label{eq174}
\end{align}
\end{prob}

\begin{rem}\label{lb404}
Setting $n=1$ and dividing both sides by $x-a$, we obtain an especially useful formula for any differentiable $f:[a,b]\rightarrow V$ as follows.
\begin{align}
\Big\Vert \frac{f(x)-f(a)}{x-a}-f'(a)\Big\Vert\leq \sup_{a<t<x}
\big\Vert f'(t)-f'(a)\big\Vert  \label{eq169}
\end{align}
\end{rem}


\begin{prob}
Use Thm. \ref{lb336} to prove the following theorem. (Formula \eqref{eq169} might be helpful.)
\end{prob}



\begin{thm}\label{lb406}
Let $I=[a,b]$ and $J=[c,d]$ be intervals in $\Rbb$ with at least two points. Let $f:I\times J\rightarrow V$ be a function such that $\partial_1f,\partial_2f,\partial_2\partial_1f$ exist on $I\times J$, and that $\partial_2\partial_1f$ is continuous. Then $\partial_1\partial_2f$ exists on $I\times J$ and equals $\partial_2\partial_1f$.
\end{thm}




\begin{sprob}
Prove that, for $x \in(-1,1]$,
\begin{align}\label{eq198}
\arctan x=\sum_{k=0}^{\infty}(-1)^k \frac{x^{2 k+1}}{2 k+1}=x-\frac{x^3}{3}+\frac{x^5}{5}-\frac{x^7}{7}+-\cdots,
\end{align}
and hence (\textbf{Leibniz formula})
\begin{align*}
\frac{\pi}{4}=\sum_{k=0}^{\infty} \frac{(-1)^k}{2 k+1}=1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+-\cdots
\end{align*}
\end{sprob}


\begin{proof}[Hint]
Use the method in the second proof of Exp. \ref{lb480} to prove \eqref{eq198} for $x\in(-1,1)$. To prove \eqref{eq198} for $x=1$, show that the series on the RHS of \eqref{eq198} converges uniformly on $[0,1]$ to a continuous function using Dirichlet's test for uniform convergence (Thm. \ref{lb481}).
\end{proof}







\begin{sprob}\label{lb357}
Let $n\in\Nbb$. Find the radius of convergence of
\begin{align}
f(z)=\sum_{k=n}^\infty {k\choose n}z^k
\end{align}
Find the explicit formula of $f(z)$.
\end{sprob}



\begin{prob}
Prove the following higher order finite-increment theorem for complex variables. (For simplicity, it suffices prove the case that $z_0=0$.)
\end{prob}

\begin{thm}[\textbf{Higher order finite-increment theorem}]\index{00@Higher order finite-increment theorem}  \label{lb356}
Let $V$ be a Banach space over $\Cbb$. Let $R>0$ and $z_0\in\Cbb$. Let $f:\Omega\rightarrow V$ where
\begin{align*}
\Omega=B_\Cbb(z_0,R)=\{z\in\Cbb:|z-z_0|<R\}
\end{align*}
Assume that $f',f'',\dots,f^{(n+1)}$ exist everywhere on $\Omega$. Then for every $z\in\Omega$ we have
\begin{align}
\Big\Vert f(z)-\sum_{k=0}^n\frac{f^{(k)}(z_0)}{k!}(z-z_0)^k \Big\Vert \leq\frac{|z-z_0|^{n+1}}{(n+1)!}\cdot \sup_{\xi\in[z_0,z]} \Vert f^{(n+1)}(\xi)\Vert
\end{align}
where $[z_0,z]=\{tz+(1-t)z_0:0\leq t\leq 1\}$.
\end{thm}



\begin{sprob}
Let $V$ be a Banach space over $\Cbb$. Assume that the power series $f(z)=\sum_{k=0}^\infty v_kz^k$ (where $v_k\in V$) has radius of convergence $R>0$. Choose any $z_0\in B_\Cbb(0,R)$. Prove that there exists a neighborhood $\Omega\in\Nbh_\Cbb(z_0)$ contained inside $B_\Cbb(0,R)$, such that the Taylor series of $f$ at $z_0$ converges uniformly on $\Omega$ to $f$. Namely, prove that the series $g(z)$ converges locally uniformly to $f$ on $\Omega$ where
\begin{align}
g(z)=\sum_{k=0}^\infty\frac{f^{(k)}(z_0)}{k!}(z-z_0)^k
\end{align}
\end{sprob}

\begin{proof}[Hint]
Use Thm. \ref{lb356}.  Pb.  \ref{lb357} might also be helpful.
\end{proof}




\begin{sprob}
Let $W$ be a real vector space. Let $A$ be a nonempty subset of $W$. Define the \textbf{convex hull} \index{00@Convec hull} of $A$ to be the set of all convex combinations (recall Lem. \ref{lb365}) of elements of $A$, i.e.
\begin{align}
\mathrm{Cvh}(A)=\big\{\text{convex combinations of }v_1,\dots,v_n:n\in\Zbb_+\text{ and } v_1,\dots,v_n\in A\big\}
\end{align}
Prove that $\mathrm{Cvh}(A)$ is the smallest convex set containing $A$. In other words, prove that $\mathrm{Cvh}(A)$ is a convex set containing $A$, and that $\mathrm{Cvh}(A)\subset B$ if $B$ is a convex subset of $W$ containing $A$.
\end{sprob}



\begin{prob}
Prove Thm. \ref{lb369} for the case $p=1$ and $p=+\infty$.
\end{prob}












\newpage



\section{Riemann integrals}



\subsection{Introduction: the origin of integral theory in Fourier series}\label{lb550}



\subsubsection{Antiderivatives VS. approximation by areas of rectangles}

Modern people can easily appreciate the importance of giving a rigorous definition to the integral $\int_a^bf(x)dx$ where $f$ is a general (say, continuous) function. And you may already know that this integral is defined by taking the limit of Riemann sums, which are ``areas of some rectangles". This idea sounds so natural to you, because you know that by the time Calculus was invented, people knew very well that  $\int_a^b f$ means the area of the region between the graph of $f$ and the $x$-axis, and that this area can be approximated by the areas of some rectangles or triangles. So why did Riemann integral not appear until 19th century, more than a hundred years after Newton's and Leibniz's invention of calculus? And why was the inadequacy of defining integrals by means of antiderivatives not recognized until 19th century?


Beyond the superficial reason that 19th century is the century in which people began to pay attention to the foundations of calculus, there is a deeper reason: the study of \textbf{partial differential equations (PDE)} and the introduction of \textbf{Fourier series} caused a drastic change in the general view of what functions are. This change of view motivated people to search for a general definition of integrals, in particular one that does not use antiderivatives.

Before the systematic study of PDEs (i.e., before 19th century), functions only mean analytic functions, which means that they can be approximated by their Taylor series. But if $f$ is such a function, then $f(x)=\sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!}(x-a)^n$. So the values of $f$ are all determined by $f(a),f'(a),f''(a),\dots$, and hence \uwave{are determined by $f|_I$ where $I$ is an arbitrary nonempty open interval}. (Technically, such functions are called \textbf{(real) analytic functions}.) Therefore, periodic functions (except trigonometric functions) were not accepted, because they are not determined by their values on a small interval. (A function with expression $f(x)=x$ on $[0,1]$ should also have the same expression on $\Rbb$. So it must not be the periodic function $f(x)=x-n$ where $x\in(n,n+1]$.) The function Exp. \ref{lb358} was not accepted, because it was not determined by $f(0),f'(0),f''(0),\dots$.

Therefore, in these old days, people did not worry about the rigorous definition of integrals, because they can simply \uline{define integrals using antiderivatives} instead of using the areas of rectangles to approximate the integrals, which is practically less convenient than finding the antiderivatives. In particular, one understood
\begin{align*}
\int_a^b \sum a_n (x-c)^ndx\xlongequal{\mathrm{def}}F(b)-F(a)\qquad\text{where}\qquad F(x)=\sum \frac{a_n}{n+1}(x-c)^{n+1}
\end{align*}

\subsubsection{PDEs and periodic functions}

The wonderful dream was shattered when people started working on PDEs. The simplest such example is \textbf{wave equation}
\begin{align}
\partial_x^2f(x,t)-\partial_t^2f(x,t)=0
\end{align} 
This equation describes the vibration of a string: at time $t$, the shape of the string is the graph of the function $x\in[a,b]\mapsto f(x,t)\in\Rbb$. (Here, we assume that the two end points of the string are $(a,0)$ and $(b,0)$.)

D'Alambert solved the wave equation in the following way. Using the trick
\begin{align*}
\partial_x^2-\partial_t^2=4\partial_u\partial_v\qquad\text{where}\qquad u=x+t,v=x-t
\end{align*}
it is not hard to find the general solution of the wave equation:
\begin{align}
f(x,t)=\frac 12\big(g(x+t)+g(x-t) \big)+\frac 12\int_{x-t}^{x+t}h(s)ds
\end{align}
where $g(x)=f(x,0)$ and $h(x)=\partial_t f(x,0)$. In particular, if we assume that $h=0$ (i.e., at time $t=0$ the string is held in place and does not vibrate), then the solution is
\begin{align}
f(x,t)=\frac 12\big(g(x+t)+g(x-t) \big)
\end{align}

Here comes the trouble. If we assume that the two end points of the string are always pinned at $(0,0)$ and $(1,0)$ respectively, then we have $f(0,t)=f(1,t)=0$ for all $t\in\Rbb$. Translating this condition to $g$, we get $g(t)=-g(-t)=-g(2-t)$ and hence $g$ is a function with period $2$, totally unacceptable! Worse still, the derivatives of periodic functions might have points of discontinuities.

\subsubsection{Fourier series}


The next important progress was made by Fourier in the study of \textbf{heat equation}
\begin{align}
\partial_t f(x,t)-\partial_x^2 f(x,t)=0
\end{align}
where $x$ is defined on a closed interval (say $[-\pi,\pi]$) representing a thin rod, $f(x,t)$ is the temperature   of the point $x$ of this rod at time $t$. Fourier solved the problem by \textbf{separation of variables}: He first assumed $f(x,t)=u(x)v(t)$. Then the heat equation implies $u''(x)v(t)=u(x)v'(t)$, and hence
\begin{align}
\frac{u''(x)}{u(x)}=\frac{v'(t)}{v(t)}  \label{eq152}
\end{align}
The LHS is independent of $t$, and the RHS is independ of $x$. So \eqref{eq152} should be a constant $-\lambda$. The solution is then $u(x)=e^{\im\sqrt\lambda x}$ and $v(t)=e^{-\lambda t}$. If one assumes that the temperature at the two end points $-\pi,\pi$ are equal, then $\sqrt\lambda$ must be an integer $n\in\Zbb$. So $f(x,t)=e^{\im nx-n^2t}$. Taking infinite linear combinations, Fourier found the general solution
\begin{align}
f(x,t)=\sum_{n=-\infty}^{+\infty} a_n e^{\im nx-n^2t}  \label{eq153}
\end{align}
The initial temperature $g(x)=f(x,0)$ is a \textbf{Fourier series}
\begin{align}
g(x)=\sum_{n=-\infty}^{+\infty} a_n e^{\im nx} \label{eq156}
\end{align}
and, in particular, a function with period $2\pi$ (since each $e^{\im nx}$ is so). 

In order to use \eqref{eq153} to determine the solution $f(x,t)$ when the initial condition $f(x,t)=g(x)$ is given, one must first find the values of these \textbf{Fourier coefficients} $a_n$ in terms of $g$. In fact, since 
\begin{align}
\frac 1{2\pi}\int_{-\pi}^\pi e^{\im kx}\cdot e^{-\im nx}dx=\delta_{k,n}
\end{align}
(recall \eqref{eq154}), it is not hard to guess the formula
\begin{align}
\tcboxmath{a_n=\frac 1{2\pi}\int_{-\pi}^\pi g(x)e^{-\im nx}dx}  \label{eq155}
\end{align}
since this formula is true when $g(x)=e^{\im kx}$.



\subsubsection{From Riemann integrals to Lebesgue integrals}\label{lb598}

Here comes the question that is closely related to integral theory: What is the meaning of the integral \eqref{eq155} if $g$ is no longer a real analytic function? 

To be more precise, in 19th century there was a long debate about what are (good) functions, and what functions have \textbf{Fourier expansions}, i.e., expansions of the form \eqref{eq156}. Fourier himself believed that all ``functions" can be approximated by trigonometric functions (equivalently, functions of the form $e^{\im nx}$). But Lagrange did not, partly due to his (and many people's) insistence that functions must be real analytic (cf. \cite[Sec. 28.2]{Kli}). The doubt on whether many functions have Fourier expansions is quite understandable. From the modern perspective, we know that many Lebesgue measurable functions are not approximated by their Fourier series pointwise or uniformly, but are approximated under some other norms (e.g. the $L^2$-norm $\Vert f\Vert_{L^2}=\sqrt{\int_{-\pi}^\pi |f|^2}$). Indeed, there are many continuous functions $[-\pi,\pi]\rightarrow\Rbb$ whose Fourier series diverge on a dense subset of $[-\pi,\pi]$ (cf. \cite[Thm. 5.12]{Rud-R}). Therefore, Fourier's pioneering view that all ``reasonable" functions can be approximated by Fourier series is incorrect unless we define what ``approximation" means in a new and appropriate way.

In order to understand which functions have Fourier series expansions (in whatever sense), the first step is to understand for which function $g$ the integral \eqref{eq155} makes sense. Therefore, \uline{the history of extending the class of integrable functions from continuous functions to Riemann integrable functions and finally to Lebesgue integrable functions is also part of the history of understanding which functions can have Fourier expansions and which functions are ``reasonable".}

%By ``all functions" (or more precisely, all ``reasonable" functions), Fourier also had in mind periodic functions that are not necessarily differentiable on their domains. (By contrast, power series are almost always differentiable on their domains, see Cor. \ref{lb338}.) From modern perspective, we know that many non-differentiable Lebesgue measurable functions can be approximated (in a suitable sense) by their Fourier series. Thus, Fourier's intuition that many functions admitting Fourier expansions are not differentiable is correct. Therefore, the integral \eqref{eq155} cannot be understood in terms of antiderivatives.

%The pursuit of a rigorous foundation of analysis was deeply impacted by these confusions. For example, Cauchy first gave a rigorous definition of continuous functions (though not quite rigorous in today's standard since he did not use the language of $\eps-\delta$). It was also Cauchy who first defined integrals for general continuous functions. In other words, Cauchy made an important progress in understanding what functions can have Fourier expansions by showing that the integral \eqref{eq155} makes sense whenever $g$ is continuous. 




The goal of this chapter is to learn Riemann integrals. The construction of Riemann integrals is much easier than Lebesgue theory. But compared to the latter, Riemann integrals have some serious drawbacks. 

For example, suppose that a sequence of functions $(f_n)$ on $[-\pi,\pi]$ converges to $f$ in some sense. It is natural to ask whether the Fourier coefficients of $(f_n)$ converge to those of $f$. (A typical example is $f_n(x)=\sum_{k=-n}^na_k e^{\im kx}$. Then this question asks whether the $n$-th Fourier coefficient of the series $\sum_{k=-\infty}^{+\infty}a_k e^{\im kx}$ is $a_n$.) In view of \eqref{eq155}, this problem is reduced to the problem of showing $\int f=\lim_n\int f_n$. If $(f_n)$ is a sequence of Riemann integrable functions converging uniformly to $f$, then $f$ is Riemann integrable, and $\int f=\lim \int f_n$. (See Cor. \ref{lb380}.) However, if $(f_n)$ only converges pointwise to $f$, then $f$ is not necessarily Riemann integrable. Even if $f$ is Riemann integrable, one does not have a useful criterion for $\int f=\lim \int f_n$ in the framework of Riemann integrals. 

But uniform convergence often does not hold in application, and especially in Fourier theory and PDEs. For example, let $f$ be the function on $\Rbb$ with period $2\pi$ defined by $f(x)=x$ if $-\pi<x<\pi$ and $f(\pi)=0$. Then the Fourier series
\begin{align}
\sum_{n=1}^\infty (-1)^{n-1}\frac 2n\sin(nx) \label{eq194}
\end{align}
converges pointwise to $f$. (See also Pb. \ref{lb394}.) It does not converge uniformly to $f$, because the uniform limit of a sequence of continuous functions is continuous, but $f$ is not continuous.



Lebesgue's integral theory will provide a more satisfying answer to the above problem. We will learn it in the second semester.




\subsection{Riemann integrability and oscillation}

In this section, we fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$, and let $I=[a,b]$ where $-\infty<a<b<+\infty$. We understand $|I|$ as $b-a$. \index{I@$\vert I\vert=b-a$ if $I=[b-a]$}


\subsubsection{Riemann integrals}\label{lb384}


\begin{df}
A \textbf{partition} \index{00@Partition of an interval} of the interval $I=[a,b]$ is defined to be an element of the form
\begin{align}
\sigma=\{a_0,a_1,\dots,a_n\in I:a_0=a<a_1<a_2<\cdots<a_n=b,n\in\Zbb_+\}
\end{align}
Equivalently, this partition can be written as
\begin{align}
I=I_1\cup I_2\cup\cdots\cup I_n\qquad I_j=[a_{j-1},a_j]
\end{align}
If $\sigma,\sigma'\in \fin(2^I)$ are partitions of $I$, we say that $\sigma'$ is a \textbf{refinement} \index{00@Refinement of a partition} \index{00@Finer partition} of $\sigma$ (or that $\sigma'$ is \textbf{finer than} $\sigma$), if $\sigma\subset\sigma'$. In this case, we also write  \index{zz@$\sigma\prec\sigma'$} 
\begin{align*}
\sigma\prec\sigma'
\end{align*}
We define $\mc P(I)$ or simply $\mc P$ to be\index{PI@$\mc P(I)$, the set of partitions of $I$}
\begin{align*}
\mc P(I)=\{\text{partitions of $I$}\}
\end{align*}
\end{df}



\begin{rem}
If $\sigma,\sigma'\in\mc P(I)$, then clearly $\sigma\cup\sigma'\in\mc P(I)$ and $\sigma,\sigma'\prec \sigma\cup\sigma$. Therefore, $\prec$ is a partial order on $\mc P(I)$. We call $\sigma\cup\sigma'$ the \textbf{common refinement} \index{00@Common refinement} of $\sigma$ and $\sigma'$.
\end{rem}


\begin{df}
A \textbf{tagged partition} \index{00@Tagged partition} of $I$ is an ordered pair
\begin{align*}
(\sigma,\xi_\blt)=\big(\{a_0=a<a_1<\cdots<a_n=b\},(\xi_1,\dots,\xi_n) \big)
\end{align*}
where $\sigma\in\mc P(I)$ and $\xi_j\in I_j=[a_{j-1},a_j]$ for all $1\leq j\leq n$. The set\index{QI@$\mc Q(I)$, the directed set of tagged partitions}
\begin{align*}
\mc Q(I)=\{\text{tagged partitions of $I$}\}
\end{align*}
equipped with the preorder $\prec$ defined by
\begin{align}
(\sigma,\xi_\blt)\prec(\sigma',\xi_\blt')\qquad\Longleftrightarrow\qquad \sigma\subset\sigma'
\end{align}
is a directed set.
\end{df}


\begin{df}
Let $f:I\rightarrow V$. For each $(\sigma,\xi_\blt)\in\mc Q(I)$, define the \textbf{Riemann sum}\index{00@Riemann sum} \index{Sf@$S(f,\sigma,\xi_\blt)$}
\begin{align*}
S(f,\sigma,\xi_\blt)=\sum_{j\geq 1}f(\xi_j)(a_j-a_{j-1})
\end{align*}
The \textbf{Riemann integral} \index{00@Riemann integral} is defined to be the limit of the net $(S(f,\sigma,\xi_\blt))_{(\sigma,\xi_\blt)\in\mc Q(I)}$:
\begin{align*}
\int_a^b f\equiv\int_a^b f(x)dx=\lim_{(\sigma,\xi_\blt)\in\mc Q(I)}S(f,\sigma,\xi_\blt)
\end{align*}
When the RHS exists, we way that $f$ is \textbf{Riemann integrable} \index{00@Riemann integrable} on $I$. We let \index{R@$\scr R([a,b],V)$}
\begin{align*}
\scr R(I,V)=\{\text{Riemann integrable }f\in V^I\}
\end{align*}
\end{df}


\begin{cv}\label{lb381}
There are several equivalent ways to write the integrals:
\begin{align*}
\int_a^b f=-\int_b^a f=\int_{[a,b]}f
\end{align*}
if $a<b$. Also, if $a=b$, the above terms are understood to be $0$,  and all functions on $[a,b]$ are considered Riemann integrable.
\end{cv}


\begin{rem}
It is clear that $\int_a^b f=v\in V$ iff for every $\eps>0$ there exists $\sigma_0\in \mc P(I)$ such that for any partition $\sigma=\{a_0<\cdots<a_n\}$ finer than $\sigma_0$, and for any $\xi_j\in[a_{j-1},a_j]$ (for all $1\leq j\leq n$) we have
\begin{align*}
\Big\Vert v-\sum_{j\geq 1}f(\xi_j)(a_j-a_{j-1})  \Big\Vert<\eps
\end{align*}
There is not need to tag $\sigma_0$.
\end{rem}




\subsubsection{Riemann integrability and strong Riemann integrability}

In this subsection, we give a useful criterion for Riemann integrability.


\begin{df}\label{lb416}
Let $A$ be a nonempty subset of a metric space $Y$. The \textbf{diameter} of $A$ is defined to be \index{diam@$\diam(A)$}
\begin{align*}
\diam(A)=\sup_{x,y\in A}d(x,y)
\end{align*}
If $f:X\rightarrow Y$ is a map where $X$ is a set, and if $E\subset X$, the \textbf{oscillation} \index{00@Oscillation on a subset} of $f$ on $E$ is defined to be $\diam f(E)$.
\end{df}


The following lemma allows us to control the difference of Riemann sums by means of the oscillation.

\begin{lm}\label{lb371}
Let $f:I=[a,b]\rightarrow V$ and $M=\diam(f(I))$. Then for each $(\sigma,\xi_\blt),(\sigma',\xi_\blt')\in\mc Q(I)$ we have
\begin{align}
\Vert S(f,\sigma,\xi_\blt)-S(f,\sigma',\xi_\blt')\Vert\leq M(b-a)
\end{align}
\end{lm}



\begin{proof}
Write $\sigma\cup\sigma'=\{a_0=a<a_1<\cdots<a_n=b\}$. Then there exist $\gamma_1,\dots,\gamma_n\in\{\xi_1,\xi_2,\dots\}$  such that
\begin{align*}
S(f,\sigma,\xi_\blt)=\sum_{i=1}^n f(\gamma_i)(a_i-a_{i-1})
\end{align*}
(Here is how to choose $\gamma_i$: Let $k$ be the unique number such that the $k$-th subinterval of $\sigma$ contains $[a_{i-1},a_i]$. Then let $\gamma_i=\xi_k$.)
Similarly, there exist $\gamma_1',\dots,\gamma_n'\in\{\xi_1',\xi_2',\dots\}$ such that
\begin{align*}
S(f,\sigma',\xi_\blt')=\sum_{i=1}^n f(\gamma_i')(a_i-a_{i-1})
\end{align*}
Then
\begin{align*}
&\Vert S(f,\sigma,\xi_\blt)-S(f,\sigma',\xi_\blt')\Vert\leq\sum_{i=1}^n\Vert f(\gamma_i)-f(\gamma_i')\Vert\cdot(a_i-a_{i-1})\\
\leq& \sum_{i=1}^n M(a_i-a_{i-1})\leq M(b-a)
\end{align*}
\end{proof}



\begin{df}
Let $f:I\rightarrow V$. For each $\sigma=\{a_0<\cdots<a_n\}\in\mc P(I)$, write $I_i=[a_{i-1},a_i]$ and define the \textbf{oscillation of $f$ on $\sigma$} \index{00@Oscillation on a partition} to be
\begin{align}
\omega(f,\sigma)=\sum_{j=1}^n \diam(f(I_j))\cdot (a_j-a_{j-1})
\end{align}
\end{df}


\begin{exe}
Show that if $\sigma'\supset\sigma$, then $\omega(f,\sigma')\leq \omega(f,\sigma)$. Therefore, $(\omega(f,\sigma))_{\sigma\in\mc P(I)}$ is a decreasing net in $\ovl\Rbb_{\geq0}$.
\end{exe}


Now, Lem. \ref{lb371} can be upgraded to the following version.

\begin{lm}\label{lb373}
Let $f:I\rightarrow V$. Let $\sigma\in\mc P(I)$. Choose $(\sigma',\xi_\blt'),(\sigma'',\xi_\blt'')\in\mc Q(I)$ such that $\sigma\subset\sigma'$ and $\sigma\subset\sigma''$. Then
\begin{align}
\Vert S(f,\sigma',\xi_\blt')-S(f,\sigma'',\xi_\blt'')\Vert\leq \omega(f,\sigma)  \label{eq157}
\end{align}
\end{lm}



\begin{proof}
Write $\sigma=\{a_0<\cdots<a_n\}$ and $I_j=[a_{j-1},a_j]$. Let $S(f,\sigma',\xi_\blt')_{I_j}$ be the \textbf{restriction of $S(f,\sigma',\xi_\blt')$ to $I_j$}. Namely, 
\begin{align}
S(f,\sigma',\xi_\blt')_{I_j}=\sum_{
\begin{subarray}{c}
\text{all $k$ such that}\\
[a_{k-1},a_k]\subset I_j
\end{subarray}
} f(\xi_k)(a_k-a_{k-1})
\end{align} 
Define $S(f,\sigma'',\xi_\blt'')_{I_j}$ in a similar way. Then, by Lem. \ref{lb371} we have
\begin{align*}
\Vert S(f,\sigma',\xi_\blt')_{I_j}-S(f,\sigma'',\xi_\blt'')_{I_j}\Vert\leq \diam(f(I_j))|I_j|
\end{align*}
Thus, \eqref{eq157} follows immediately from triangle inequality and
\begin{align*}
\sum_{j=1}^n  S(f,\sigma',\xi_\blt')_{I_j}= S(f,\sigma',\xi_\blt')\qquad \sum_{j=1}^n  S(f,\sigma'',\xi_\blt'')_{I_j}= S(f,\sigma'',\xi_\blt'')
\end{align*}
\end{proof}



\begin{df}
We say that $f:I\rightarrow V$ is \textbf{strongly Riemann integrable} \index{00@Strong Riemann integrability} if 
\begin{align*}
\inf_{\sigma\in\mc P(I)} \omega(f,\sigma)=0
\end{align*}
\end{df}

\begin{thm}\label{lb410}
Let $f:I\rightarrow V$. Consider the following statements:
\begin{enumerate}[label=(\arabic*)]
\item $f\in\scr R(I,V)$.
\item $f$ is strongly Riemann integrable.
\end{enumerate}
Then (2)$\Rightarrow$(1). If $V$ is $\Rbb^N$ or $\Cbb^N$, then (1)$\Leftrightarrow$(2).
\end{thm}

When $V$ is infinite-dimensional, Riemann integrable functions are not necessarily strongly Riemann integrable. See Pb. \ref{lb389}.

\begin{proof}
Assume (2). Choose any $\eps>0$. Then there exists $\sigma\in\mc P(I)$ such that $\omega(f,\sigma)<\eps$. By Lem. \ref{lb373}, for every $(\sigma',\xi_\blt'),(\sigma'',\xi_\blt'')\in\mc Q(I)$ satisfying $\sigma\subset\sigma'$ and $\sigma\subset\sigma''$, we have $\Vert S(f,\sigma',\xi_\blt')-S(f,\sigma'',\xi_\blt'')\Vert<\eps$. This proves that $(S(f,\sigma,\xi_\blt))_{(\sigma,\xi_\blt)\in\mc Q(I)}$ is a Cauchy net in $V$, and hence $f\in\scr R(I,V)$.

Now assume that $V$ is $\Rbb^N$ or $\Cbb^N$. Since $\Cbb^N\simeq\Rbb^{2N}$, it suffices to consider the case $V=\Rbb^N$. Since a net in $\Rbb^N$ converges iff each component of this net converge, and since the strong Riemann integrability can be checked componentwisely, it suffices to prove (1)$\Rightarrow$(2) for the case $V=\Rbb$.

So let us assume $f\in\scr R(I,\Rbb)$. Then $(S(f,\sigma,\xi_\blt))_{(\sigma,\xi_\blt)\in\mc Q(I)}$ is a Cauchy net in $\Rbb$. Thus, there exists $\sigma=\{a_0<\cdots<a_n\}\in\mc P(I)$ such that for all $(\sigma',\xi_\blt'),(\sigma'',\xi_\blt'')\in\mc Q(I)$ satisfying $\sigma\subset\sigma'$ and $\sigma\subset\sigma''$, we have $\Vert S(f,\sigma',\xi_\blt')-S(f,\sigma'',\xi_\blt'')\Vert<\eps$. To prove (2), we only need to take $\sigma'=\sigma''=\sigma$. Thus, for any tags $\xi_\blt',\xi_\blt''$ of $\sigma$, we have
\begin{align*}
\Vert S(f,\sigma,\xi_\blt'')-S(f,\sigma,\xi_\blt')\Vert<\eps
\end{align*}
Write $I_j=[a_{j-1},a_j]$. Then
\begin{align*}
\diam(f(I_j))=\sup f(I_j)-\inf f(I_j)
\end{align*}
So  there exist $\xi_j',\xi_j''\in I_j$ such that
\begin{gather*}
f(\xi_j')\leq \inf f(I_j)+\eps\qquad f(\xi_j'')\geq\sup f(I_j)-\eps
\end{gather*}
It follows that $\dps\diam(f(I_j))-2\eps\leq f(\xi_j'')-f(\xi_j')$. Thus
\begin{align*}
&\omega(f,\sigma)=\sum_j \diam(f(I_j))\cdot|I_j|\leq\sum_j (f(\xi_j'')-f(\xi_j'))\cdot|I_j|+\sum_j2\eps\cdot|I_j|\\
=& S(f,\sigma,\xi_\blt'')-S(f,\sigma,\xi_\blt')+2(b-a)\eps<\eps+2(b-a)\eps
\end{align*}
Since $\eps>0$ is arbitrary, we have $\inf_{\sigma\in\mc P(I)} \omega(f,\sigma)=0$.
\end{proof}

\begin{eg}\label{lb377}
Every continuous function $f:I\rightarrow V$ is strongly Riemann integrable, and hence Riemann integrable.
\end{eg}


\begin{proof}
Since $I=[a,b]$ is compact, by Thm. \ref{lb294}, $f\in C(I,V)$ is uniformly continuous. Therefore, for every $\eps>0$, there exists $n\in\Zbb_+$ such that for all $x,y\in I$ satisfying $|x-y|\leq 1/n$, we have $\Vert f(x)-f(y)\Vert<\eps$. Let $\sigma=\{a_0<a_1<\cdots<a_n\}$ be the partition of $I$ such that $|I_j|=a_j-a_{j-1}=1/n$. Then $\diam(f(I_j))\leq\eps$, and hence
\begin{align*}
\omega(f,\sigma)\leq\sum_j \eps\cdot|I_j|=\eps(b-a)
\end{align*}
Since $\eps$ is arbitrary, we get $\inf_{\sigma\in\mc P(I)} \omega(f,\sigma)=0$.
\end{proof}


\begin{eg}\label{lb704}
The \textbf{Dirichlet function} $\chi_\Qbb$ is not (strongly) Riemann integrable on $I=[a,b]$, since for every $\sigma\in\mc P(I)$ we have $\omega(\chi_\Qbb,\sigma)=b-a$.
\end{eg}




\subsection{Basic properties of Riemann integrals}


Let $I=[a,b]$ be a compact interval in $\Rbb$. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$. We begin this section with the following fundamental fact, which will be used to prove Fubini's theorem, the second fundamental theorem calculus, and much more.


\begin{thm}\label{lb392}
Let $W$ be also a Banach space over $\Fbb$. Let $T\in\fk L(V,W)$. Then for every $f\in\scr R(I,V)$, we have $T\circ f\in\scr R(I,W)$ and
\begin{align}
T\Big(\int_a^b f \Big)=\int_a^b T\circ f
\end{align}
\end{thm}

In other words, we have a commutative diagram
\begin{equation}\label{eq166}
\begin{tikzcd}[column sep=large]
\scr R(I,V) \arrow[r,"T\circ"] \arrow[d,"\int_I"'] & \scr R(I,W) \arrow[d,"\int_I"] \\
V \arrow[r,"T"]           & W        
\end{tikzcd} 
\end{equation}
where the top arrow denotes the composition map $f\mapsto T\circ f$. 

\begin{proof}
By linearity, we have
\begin{align}
S(T\circ f,\sigma,\xi_\blt)=T\big(S(f,\sigma,\xi_\blt) \big)  \label{eq164}
\end{align}
Since $T$ is continuous, the limit over $(\sigma,\xi_\blt) \in \mc Q(I)$ of \eqref{eq164} is
\begin{align*}
T\Big(\lim_{(\sigma,\xi_\blt)\in\mc Q(I)}S(f,\sigma,\xi_\blt)\Big)=T\Big(\int_a^b f \Big)
\end{align*}
This proves that $\lim S(T\circ f,\sigma,\xi_\blt)$ converges to $T(\int_a^bf)$, i.e., $\int_a^b T\circ f$ exists and equals $T(\int_a^bf)$.
\end{proof}




\begin{rem}\label{lb374}
Let $f:I\rightarrow V$ and $\eps>0$. To simplify the following discussion, we say that $f$ is \textbf{$\eps$-dominated} \index{zz@$\eps$-dominated} by $\sigma\in\mc P(I)$, if for all $(\sigma',\xi_\blt'),(\sigma'',\xi_\blt'')\in\mc Q([a,b])$ satisfying $\sigma\subset\sigma',\sigma''$, we have $\Vert S(f,\sigma',\xi_\blt')-S(f,\sigma'',\xi_\blt'')\Vert<\eps$. 

Thus, by Cauchy condition, if $f$ is $\eps$-dominated by some partition for every $\eps>0$, then $f\in\scr R(I,V)$. Moreover,
\begin{align}
\Big\Vert \int_a^b f-S(f,\sigma,\xi_\blt) \Big\Vert\leq\eps
\end{align}
for every $(\sigma,\xi_\blt)\in\mc Q(I)$ such that $f$ is $\eps$-dominated by $\sigma$. \hfill\qedsymbol
\end{rem}





\subsubsection{Integral operators as bounded linear maps}


\begin{pp}\label{lb379}
Let $f\in\scr R(I,V)$ and $g\in\scr R(I,\Rbb)$. Assume that $|f|\leq g$, i.e., $\Vert f(x)\Vert\leq g(x)$ for all $x\in I$. Then $\dps \Big\Vert \int_a^b f \Big\Vert\leq\int_a^b g$.
\end{pp}


\begin{proof}
Apply $\lim_{(\sigma,\xi_\blt)\in\mc Q(I)}$ to the obvious inequality $\Vert S(f,\sigma,\xi_\blt) \Vert\leq S(g,\sigma,\xi_\blt)$.
\end{proof}


\begin{co}\label{lb417}
Assume that $f:I\rightarrow V$ is strongly Riemann integrable. Then $|f|:I\rightarrow\Rbb$ is (strongly) Riemann integrable, and $\dps\dps \Big\Vert \int_a^b f \Big\Vert\leq\int_a^b |f|$
\end{co}


\begin{proof}
By triangle inequality, for every $\sigma\in\mc P(I)$ we have $\omega(|f|,\sigma)\leq \omega(f,\sigma)$. So $|f|$ is strongly integrable. The rest of the corollary follows from Prop. \ref{lb379}.
\end{proof}




\begin{thm}\label{lb375}
$\scr R(I,V)$ is a closed linear subspace of $l^\infty(I,V)$. So $\scr R(I,V)$ is a Banach space under the $l^\infty$-norm. Moreover,  the map
\begin{align}
\int: \scr R(I,V)\rightarrow V\qquad f\mapsto\int_a^b f \label{eq159}
\end{align}
is a bounded linear map with operator norm $b-a$ if we equip $\scr R(I,V)$ with the $l^\infty$-norm.
\end{thm}

%% Record #18 2023/11/22 three lectures  45


\begin{proof}
By the basic properties of limits of nets, we know that if $f,g\in\scr R(I,V)$ and $\alpha,\beta\in\Fbb$, then $\alpha f+\beta g\in\scr R(I,V)$, and
\begin{align}
\int_a^b (\alpha f+\beta g)=\alpha\int_a^b f+\beta\int_a^b g
\end{align}
This proves that $\scr R(I,V)$ is a linear subspace of $V^I$, and that \eqref{eq159} is linear. 

Let us prove $\scr R(I,V)\subset l^\infty(I,V)$. Choose $f\in\scr R(I,V)$. Then $f$ is $1$-dominated by some $\sigma=\{a_0<\cdots<a_n\}\in\mc P(I)$. Fix any tag $\xi_\blt$ on $\sigma$. Choose any $x\in X$. Let $[a_{i-1},a_i]$ be the subinterval containing $x$. Let $\eta_\blt=(\xi_1,\dots,\xi_{i-1},x,\xi_{i+1},\dots,\xi_n)$. Then $\Vert S(f,\sigma,\eta_\blt)-S(f,\sigma,\xi_\blt)\Vert<1$ implies that $\Vert f(x)-f(\xi_i)\Vert\leq 1/(a_i-a_{i-1})$. So
\begin{align*}
\Vert f\Vert_{l^\infty}\leq \max\big\{\Vert f(\xi_i)\Vert+(a_i-a_{i-1})^{-1}:1\leq i\leq n  \big\}<+\infty
\end{align*}

Let $(f_n)$ be a sequence in $\scr R(I,V)$ converging to $f\in l^\infty(I,V)$. Choose any $\eps>0$. Then there is $n$ such that $\Vert f-f_n\Vert_\infty<\eps$. Since $f_n$ is Riemann integrable, $f_n$ is $\eps$-dominated by some $\sigma\in\mc P(I)$. By triangle inequality, $f$ is $(\eps+(b-a)\eps)$-dominated by $\sigma$. Since $\eps$ is arbitrary, we conclude that $f\in\scr R(I,V)$. This proves that $\scr R(I,V)$ is closed, and hence is Banach by Prop. \ref{lb86}.

Let us prove the claim about the operator norm. Choose any $f\in\scr R(I,V)$. Let $M=\Vert f\Vert_\infty<+\infty$. Then $|f|\leq M$. It is easy to see that $\int_a^b M=M(b-a)$. Thus, by Prop. \ref{lb379}, we have $\Vert\int_a^b f\Vert\leq M(b-a)=\Vert f\Vert_\infty\cdot(b-a)$, where ``$\leq$" becomes ``$=$" if we let $f$ be a nonzero constant function. This proves that \eqref{eq159} has operator norm $b-a$ thanks to Rem. \ref{lb372}.
\end{proof}






\begin{co}\label{lb380}
Let $(f_\alpha)_{\alpha\in\scr I}$ be a net in $\scr R(I,V)$ converging uniformly to $f\in V^I$. Then $f\in\scr R(I,V)$ and $\dps\lim_{\alpha\in\scr I}\int_a^b f_\alpha=\int_a^b f$.
\end{co}


\begin{proof}
This is immediate from Thm. \ref{lb375}, which implies that $\scr R(I,V)$ is closed in $l^\infty(I,V)$ (and hence closed in $V^I$ since $l^\infty(I,V)$ is closed in $V^I$), and that the map \eqref{eq159} is continuous.
\end{proof}



\subsubsection{Some criteria for Riemann integrability}



\begin{pp}\label{lb382}
Let $f,g:I\rightarrow V$. Suppose that $\{x\in I:f(x)\neq g(x)\}$ is a finite set. Suppose that $f\in\scr R(I,V)$. Then $g\in\scr R(I,V)$, and $\dps\int_a^bf=\int_a^b g$.
\end{pp}

\begin{proof}
By Thm. \ref{lb375}, it suffices to prove that $g-f$ is Riemann integrable and $\int_a^b(g-f)=0$. This is easy to show, since $g-f$ is zero outside finitely many points. 
\end{proof}





\begin{pp}\label{lb376}
Let $f:[a,b]\rightarrow V$. Let $c\in[a,b]$. Then $f\in\scr R([a,b],V)$ iff $f|_{[a,c]}\in\scr R([a,c],V)$ and $f|_{[c,b]}\in\scr R([c,b],V)$. Moreover, if $f\in\scr R([a,b],V)$, then
\begin{align}
\int_a^b f=\int_a^cf+\int_c^bf\label{eq158}
\end{align}
\end{pp}






\begin{proof}
This is obvious when $c=a$ or $c=b$ (recall Conv. \ref{lb381}). So we assume $a<c<b$. 

First, we assume $f\in\scr R([a,b],V)$. By Cauchy condition, for each $\eps>0$, $f$ is $\eps$-dominated by some $\sigma\in\mc P(I)$.  By enlarging $\sigma$, we assume that $c\in\sigma$. Then it is easy to see that $f|_{[a,c]}$ is $\eps$-dominated by $\sigma\cap[a,c]$, and $f|_{[c,b]}$ is $\eps$-dominated by $\sigma\cap[c,b]$. So $f|_{[a,c]}$ and $f|_{[c,b]}$ are Riemann integrable.

Now assume that $f|_{[a,c]}$ and $f|_{[c,b]}$ are Riemann integrable. Choose any $\eps>0$. Then $f|_{[a,c]}$ is $\eps$-dominated by some $\tau\in \mc P([a,c])$, and $f|_{[c,b]}$ is $\eps$-dominated by some $\varrho\in\mc P([c,b])$. Then $f$ is $2\eps$-dominated by $\sigma=\tau\cup\varrho$. This proves that $f\in\scr R(I,V)$. Let  $\alpha_\blt$ be a tag on $[a,c]$, and let $\beta_\blt$ be  a tag on $[c,b]$. Then $\xi_\blt=(\alpha_\blt,\beta_\blt)=(\alpha_1,\alpha_2,\dots,\beta_1,\beta_2,\dots)$ is a tag on $[a,b]$, and
\begin{align*}
S(f,\sigma,\xi_\blt)=S(f|_{[a,c]},\tau,\alpha_\blt)+S(f|_{[c,b]},\varrho,\beta_\blt)
\end{align*} 
By Rem. \ref{lb374}, we have
\begin{gather*}
\Big\Vert \int_a^b f-S(f,\sigma,\xi_\blt) \Big\Vert\leq 2\eps\\
\Big\Vert \int_a^c f-S(f|_{[a,c]},\tau,\alpha_\blt) \Big\Vert\leq \eps\\
\Big\Vert \int_c^b f-S(f|_{[c,b]},\varrho,\beta_\blt) \Big\Vert\leq \eps
\end{gather*}
Therefore, the difference of the LHS and the RHS of \eqref{eq158} has norm $\leq 4\eps$. This proves \eqref{eq158} since $\eps$ can be arbitrary.
\end{proof}



\begin{eg}\label{lb378}
Let $f\in l^\infty(I,V)$. Suppose that there exist $\sigma=\{a_0<a_1<\cdots<a_n\}\in\mc P(I)$ such that $\dps f|_{(a_{j-1},a_j)}:(a_{j-1},a_j)\rightarrow V$ is continuous for each $1\leq j\leq n$. Then $f\in\scr R(I,V)$.
\end{eg}

\begin{proof}
By Prop. \ref{lb376}, it suffices to prove that each $f|_{[a_{j-1},a_j]}$ is Riemann integrable. Thus, we assume WLOG that $M:=\Vert f\Vert_\infty<+\infty$, and that $f$ is continuous when restricted to $(a,b)$. Choose any $\eps>0$. Choose $\delta>0$ such that $M\delta<\eps$ and $a+\delta<b-\delta$. Let $J=[a+\delta,b-\delta]$. Then $f|_J$ is continuous, and hence Riemann integrable by Exp. \ref{lb377}. So $f|_J$ is $\eps$-dominated by some $\varrho\in\mc P(J)$. Since $\diam(f(I))\leq 2M$, by Lem. \ref{lb371}, $f|_{[a,a+\delta]}$ is $2\eps$-dominated by $\{a,a+\delta\}$, and $f|_{[b-\delta,b]}$ is $2\eps$-dominated by $\{b-\delta,b\}$. So $f$ is $5\eps$-dominated by $\sigma=\varrho\cup\{a,b\}$. Since $\eps>0$ is arbitrary, we conclude $f\in\scr R(I,V)$ by Cauchy condition.
\end{proof}

\begin{eg}
The function $f:[0,1]\rightarrow\Rbb$ defined by $f(x)=\sin(1/x)$ if $0<x\leq 1$ and $f(0)=0$ is Riemann integrable, although $f$ is not uniformly continuous on $(0,1]$.
\end{eg}

\begin{eg}\label{lb388}
Let $I_1,\dots,I_n$ be intervals inside $I$. Choose $v_1,\dots,v_n\in V$ and set $f=\sum_{j=1}^n v_j\chi_{I_j}$. Then $f\in\scr R(I,V)$. 
\end{eg}

\begin{proof}
The case of arbitrary $n$ follows from the case $n=1$ by linearity and Thm. \ref{lb375}. Assume $n=1$, and let $c=\inf I_1$ and $d=\sup I_1$. Then the restriction of $f$ to $[a,c]$ (resp. $[c,d]$ and $[d,b]$) equals a constant function except possibly at the end points of the interval. So $f|_{[a,c]},f|_{[c,d]},f|_{[d,b]}$ are Riemann integrable by Prop. \ref{lb382}. So $f$ is Riemann integrable by Prop. \ref{lb376}.
\end{proof}




\begin{eg}\label{lb383}
Let $f\in C(I,V)$. For each $n\in\Zbb_+$, choose a tag $\xi_{\blt,n}$ for the partition $\sigma=\{a,a+|I|/n,a+2|I|/n,\dots,a+(n-1)|I|/n,b\}$ of $I$. Then
\begin{align}
\lim_{n\rightarrow\infty} \frac {b-a}n\sum_{i=1}^n f(\xi_{i,n})=\int_a^bf  \label{eq161}
\end{align}
\end{eg}



\begin{proof}
Let $f_n=\sum_{i=1}^n f(\xi_{i,n})\cdot\chi_{J_{i,n}}$ where $J_{i,n}=\big(a+\frac{i-1}n|I|,a+\frac in|I|\big]$ if $i>1$, and $J_{1,n}=[a,a+|I|/n]$. Then $f_n\in\scr R(I,V)$ by Exp. \ref{lb388}. Moreover, by Prop. \ref{lb376}, the LHS of \eqref{eq161} equals $\lim_n\int_a^b f_n$.

By Thm. \ref{lb294}, $f$ is uniformly continuous. So for every $\eps>0$ there exists $N\in\Zbb_+$ such that for all $n\geq N$ and all $x,y\in \Cl_\Rbb(J_{i,n})$ we have $\Vert f(x)-f(y)\Vert<\eps$. Thus, for all $n\geq N$ we have $\Vert f-f_n\Vert_{l^\infty}<\eps$. Therefore $f_n\rightrightarrows f$. Thus, by Cor. \ref{lb380} we have $\int_a^b f_n\rightarrow\int_a^b f$. This proves \eqref{eq161}.
\end{proof}



\begin{eg}
By Thm. \ref{lb391}, we have $\int_1^2x^{-1}dx=\log 2$. Thus, by Exp. \ref{lb383}, we have $\lim_{n\rightarrow\infty}\sum_{i=1}^n\frac{2-1}{n}\cdot (1+i/n)^{-1}=\log 2$, namely
\begin{align*}
\lim_{n\rightarrow\infty}\big((n+1)^{-1}+(n+2)^{-1}+\cdots+(2n)^{-1} \big)=\log 2
\end{align*}
\end{eg}








\subsection{Integrals and derivatives}




Let $I=[a,b]$ be an interval in $\Rbb$. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.


\subsubsection{Fundamental theorems of calculus (FTC)} \index{00@FTC=fundamental theorem of calculus}


There are two versions of FTC. Roughly speaking, the first FTC says that integrals give antiderivatives. The second FTC says that antiderivatives give integrals. These two FTC are equivalent when the function $f$ to be integrated is continuous. Otherwise, there is a subtle difference (which I can never remember) between these two theorems. 


\begin{thm}[\textbf{First FTC}]\label{lb390}
Let $f\in\scr R(I,V)$. Define
\begin{gather}
F:I\rightarrow V\qquad F(x)=\int_a^x f
\end{gather}
Then $F\in C(I,V)$. If $f$ is continuous at $x$, then $F'(x)=f(x)$.
\end{thm}

In particular, if $f\in C(I,V)$, then $F'=f$. Thus, by Cor. \ref{lb326}, the antiderivatives of $f$ are precisely of the form $F(x)+v_0$ where $v_0\in V$ is viewed as a constant function. 


Recall Conv. \ref{lb381}.

\begin{proof}
Recall by Thm. \ref{lb375} that $\Vert f\Vert_\infty<+\infty$. For each $x,y\in[a,b]$ we have
\begin{align*}
\Vert F(y)-F(x)\Vert=\Big\Vert\int_a^y f-\int_a^x f \Big\Vert=\Big\Vert\int_x^y f \Big\Vert\leq \Vert f\Vert_\infty\cdot|y-x|
\end{align*}
So $F$ has Lipschitz constant $\Vert f\Vert_\infty$. Now suppose that $f$ is continuous at $x$. Then for every $\eps>0$, there exists $U\in\Nbh_I(x)$ such that $\Vert f(x)-f(y)\Vert<\eps$ for every $y\in U$. Thus, for each $y\in U\setminus\{x\}$, since $\int_x^y f(x)dt=f(x)(y-x)$, we have
\begin{align*}
&\Big\Vert \frac{F(y)-F(x)}{y-x}-f(x)\Big\Vert=|y-x|^{-1}\Big\Vert \int_x^y (f(t)-f(x))dt\Big\Vert\\
\leq& |y-x|^{-1}\int_{[x,y]}\Vert f(t)-f(x)\Vert dt\leq |y-x|^{-1}\int_{[x,y]}\eps dt=\eps
\end{align*}
This proves $F'(x)=f(x)$.
\end{proof}


\begin{thm}[Second FTC]\label{lb391}
Let $f\in\scr R(I,V)$. Assume that $F:I\rightarrow V$ is differentiable and $F'=f$. Then
\begin{align}
\int_a^bf=F\big|^b_a\xlongequal{\mathrm{def}} F(b)-F(a)  \label{eq163}
\end{align}
\end{thm}

This theorem is easy when $f\in C(I,V)$: In this case,   by Thm. \ref{lb390}, we have $F(x)=v_0+\int_a^x f$ for some $v_0\in V$. Then \eqref{eq163} follows immediately from Prop. \ref{lb376}. The proof for the general case is more difficult:

\begin{proof}[Proof assuming Hahn-Banach]
Since $\Rbb$ is a subfield of $\Cbb$, we may view $V$ as a real Banach space. We first consider the special case that $V=\Rbb$. Let $A=\int_a^bf$. Choose any $\eps>0$. Since $f\in\scr R(I,\Rbb)$, there exists $\sigma=\{a_0<\cdots<a_n\}\in\mc P(I)$ such that for every tag $\xi_\blt$ on $\sigma$, we have $|A-S(f,\sigma,\xi_\blt)|<\eps$. By Lagrange's MVT (Thm. \ref{lb345}), there exists $\xi_i\in (a_{i-1},a_i)$ such that
\begin{align*}
F(a_i)-F(a_{i-1})=f(\xi_i)(a_i-a_{i-1})
\end{align*}
Thus, we have a tag $\xi_\blt$ such that $S(f,\sigma,\xi_\blt)=F(b)-F(a)$. Hence $|A-F(b)+F(a)|<\eps$. Since $\eps$ is arbitrary, we get $A=F(b)-F(a)$.

The case $V=\Rbb^N$ can be reduced to the above special case easily. We now consider the general case that $V$ is a Banach space over $\Rbb$. Similar to \eqref{eq180}, for every $\varphi\in V^*=\fk L(V,\Rbb)$,  we have that $\varphi\circ F$ is differentiable, and that $(\varphi\circ F)'=\varphi\circ f$. Note that $\varphi\circ f\in\scr R(I,\Rbb)$ by Thm. \ref{lb392}. Apply the one-dimensional special case to $\varphi\circ f$. Then by Thm. \ref{lb392}, we have
\begin{align*}
\varphi\Big(\int_a^b f\Big)=\int_a^b \varphi\circ f=\varphi\circ F\big|_a^b=\varphi(F(b)-F(a))
\end{align*}
By Hahn-Banach theorem, $V^*$ separates points of $V$. (See Rem. \ref{lb393}.) Therefore $\int_a^b f=F(b)-F(a)$.
\end{proof}




\subsubsection{Applications of FTC: integration by parts}




\begin{pp}[\textbf{Integration by parts}]  \index{00@Integration by parts}
Let $f\in C^1(I,V)$ and $g\in C^1(I,\Fbb)$. Then
\begin{align}
\int_a^b f'g=(fg)\big|_a^b-\int_a^b fg'
\end{align}
\end{pp}


\begin{proof}
$(fg)\big|_a^b=\int_a^b(fg)'=\int_a^bf'g+\int_a^b fg'$.
\end{proof}

\begin{thm}[\textbf{Taylor's theorem, integral form}] \index{00@Taylor's theorem, integral form}  \label{lb425}
Let $n\in\Nbb$ and $f\in C^{n+1}([a,b],V)$. Then for every $x\in[a,b]$ we have
\begin{align}\label{eq165}
f(x)=\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k+\int_a^x\frac{f^{(n+1)}(t)}{n!}(x-t)^ndt
\end{align}
\end{thm}

\begin{proof}
When $n=0$, \eqref{eq165} is just FTC. We now prove \eqref{eq165} by induction. Suppose that case $n-1$ has been proved, then by integration by parts,
\begin{align*}
&f(x)-\sum_{k=0}^{n-1}\frac{f^{(k)}(a)}{k!}(x-a)^k=\int_a^x\frac{f^{(n)}(t)}{(n-1)!}(x-t)^{n-1}dt\\
=&\int_a^x -\frac{f^{(n)}(t)}{n!}\partial_t(x-t)^ndt\\
=&-\frac{f^{(n)}(t)}{n!}(x-t)^n\Big|_{t=a}^x+\int_a^x\frac{f^{(n+1)}(t)}{n!}(x-t)^ndt\\
=&\frac {f^{(n)}(a)}{n!}(x-a)^n+\int_a^x\frac{f^{(n+1)}(t)}{n!}(x-t)^ndt
\end{align*}
\end{proof}


\begin{exe}
Use the integral form of Taylor's theorem to give a quick proof higher order finite-increment Thm. \ref{lb359} under the assumption that $f\in C^{n+1}([a,b],V)$. (This assumption is slightly stronger than that in Thm. \ref{lb359}, but is enough for applications.)
\end{exe}




In the case that $V=\Rbb$, the integral form of Taylor's theorem actually implies a slightly weaker (but useful enough) version of Lagrange form. This relies on the following easy fact:


\begin{pp}[\textbf{Mean value theorem}]
Let $f,g\in C([a,b],\Rbb)$ such that $g(x)\geq 0$ for all $x\in(a,b)$. Then there exists $\xi\in [a,b]$ such that
\begin{align}
\int_a^bfg=f(\xi)\int_a^bg
\end{align}
Moreover, $\xi$ can be chosen to be in $(a,b)$ if $g(x)>0$ for all $x\in(a,b)$.
\end{pp}

\begin{proof}
Let $m=\inf f(I)$ and $M=\sup f(I)$. Then $mg\leq fg\leq Mg$, and hence $m\int_Ig\leq \int_Ifg\leq \int_IMg$. So there is $y\in [m,M]$ such that $\int_Ifg=y\int_Ig$. By extreme value theorem, we have $m,M\in f(I)$. Thus, by intermediate value property, we have $y\in f(I)$. So $y=f(\xi)$ for some $\xi\in I$.

Now assume $g(x)>0$ for all $a<x<b$. Let $\varphi(x)=\int_a^xfg$ and $\psi(x)=\int_a^xg$. By Cauchy's MVT, there exists $\xi\in(a,b)$ such that $g(\xi)(\varphi(b)-\varphi(a))=f(\xi)g(\xi)(\psi(b)-\psi(a))$. This finishes the proof.
\end{proof}



\begin{rem}
Let $f\in C^{n+1}([a,b],\Rbb)$. By the above mean value theorem, for each $x\in(a,b]$ there exists $\xi\in(a,x)$ such that
\begin{align*}
\int_a^x\frac{f^{(n+1)}(t)}{n!}(x-t)^ndt=f^{(n+1)}(\xi)\int_a^x\frac{(x-t)^n}{n!}dt=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}
\end{align*}
Thus, the integral form of Taylor's theorem implies Lagrange's form (Thm. \ref{lb395}) in the case that $f\in C^{n+1}(I,\Rbb)$.
\end{rem}


\subsubsection{Application of FTC: change of variables}



\begin{pp}[\textbf{Change of variables}]\label{lb396}
Let $\Phi\in C^1(I,\Rbb)$ such that $\Phi(I)\subset J=[c,d]$. Let $f\in C(J,V)$. Then
\begin{align}\label{eq278}
\int_{\Phi(a)}^{\Phi(b)} f=\int_a^b(f\circ\Phi)\cdot\Phi'
\end{align}
\end{pp}

\begin{proof}
Define $F:J\rightarrow V$ and $G:I\rightarrow V$ by $\dps F(y)=\int_{\Phi(a)}^y f$ and $\dps G(x)=F\circ\Phi(x)=\int_{\Phi(a)}^{\Phi(x)}f$. Then, by chain rule and FTC, $G'=(F'\circ\Phi)\cdot\Phi'=(f\circ\Phi)\cdot\Phi'$. By FTC, we have $G'=H'$ where $H:I\rightarrow V$ is defined by
\begin{align*}
H(x)=\int_a^x (f\circ\Phi)\cdot\Phi'
\end{align*}
Thus, since $G(a)=F(a)=0$, by Cor. \ref{lb326} we have $G=H$. Then $G(b)=H(b)$ finishes the proof.
\end{proof}



The change of variable formula allows us to define the length of a curve:


\begin{df}
Let $\gamma$ be a \textbf{$C^1$-curve} in $V$, i.e. $\gamma\in C^1([a,b],V)$. Its \textbf{length} \index{00@Length of a curve} is defined to be
\begin{align*}
l(\gamma)=\int_a^b\Vert \gamma'(t)\Vert dt
\end{align*}
\end{df}

\begin{pp}\label{lb477}
The length of $\gamma$ is invariant under a reparametrization. Namely, if $f:[c,d]\rightarrow[a,b]$ is a bijection and is in $C^1([c,d],\Rbb)$, then $l(\gamma)=l(\gamma\circ f)$.
\end{pp}

\begin{proof}
By Prop. \ref{lb347}, $f$ is either increasing or decreasing. We prove the case that $f$ is decreasing; the other case is similar. Then $f(c)=b$ and $f(d)=a$. By chain rule, we have
\begin{align*}
l(\gamma\circ f)=\int_c^d |(\gamma\circ f)'|=\int_c^d |(\gamma'\circ f)\cdot f'|=\int_c^d (|\gamma'|\circ f)\cdot |f'|
\end{align*}
which equals $-\int_c^d (|\gamma'|\circ f)\cdot f'=\int_d^c(|\gamma'|\circ f)\cdot f'$ because $f'\leq 0$ by Cor. \ref{lb330}. This expression equals $\int_a^b |\gamma'|=l(\gamma)$ by Prop. \ref{lb396}.
\end{proof}


\begin{eg}\label{lb398}
The upper half circle $\Sbb^1_+=\{z\in\Cbb:|z|=1,\Imag z\geq 0\}$ has a bijective $C^\infty$-parametrization $\gamma:[0,\pi]\rightarrow\Cbb,\gamma(t)=e^{\im t}$. Its length is $\pi$.
\end{eg}


\begin{proof}
We have $l(\gamma)=\int_0^\pi |\gamma'|=\int_0^\pi 1=\pi$. So it remains to prove that $\gamma$ restricts to a bijection $[0,\pi]\rightarrow \Sbb^1_+$. Clearly $\gamma([0,\pi])\subset \Sbb^1=\{z\in\Cbb:|z|=1\}$. Note that $\gamma(t)=\cos(t)+\im\sin(t)$. In Sec. \ref{lb397}, we have proved that $\sin(x)\geq0$ when $x\in[0,\pi]$. So $\gamma([0,\pi])\subset \Sbb^1_+$. 

We have also proved in Sec. \ref{lb397} that $\cos:[0,\pi/2]\rightarrow\Rbb$ is a decreasing (continuous) function such that $\cos(0)=1$ and $\cos(\pi/2)=0$, and that $\cos(x)=\cos(\pi-x)$. The last relation shows that $\cos:[0,\pi]\rightarrow\Rbb$ is decreasing, and $\cos(0)=1,\cos(\pi)=-1$. Therefore, by the intermediate value theorem, we see that $\cos$ sends $[0,\pi]$ bijectively to $[-1,1]$. Since the projection map onto the $x$-axis sends $\Sbb^1_+$ bijectively to $[-1,1]$, we conclude that $\gamma$ sends $[0,\pi]$ bijectively to $\Sbb^1_+$.
\end{proof}

\begin{rem}
You may wonder if the above argument really proves that $\pi$ is the length of $\Sbb^1_+$: Suppose that $\lambda:[a,b]\rightarrow \Cbb$ is another $C^\infty$ map restricting to a bijection $[a,b]\rightarrow \Sbb^1_+$, how can we show that $\int_0^\pi |\gamma'|=\int_a^b|\lambda'|$ ? Clearly, there is a bijection $f:[a,b]\rightarrow[0,\pi]$ such that $\lambda=\gamma\circ f$. Thus, by Prop. \ref{lb477}, the two integrals are equal if $f\in C^\infty$, or at least if $f\in C^1$. However, it seems that there is no general argument ensuring that $f\in C^1$.

In the next semester, we will learn that $f\in C^\infty$ if the two $C^\infty$-parametrizations $\lambda,\gamma$ satisfy that $\lambda'$ and $\gamma'$ are nowhere zero. (Clearly $\gamma'$ is nowhere zero if $\gamma(t)=e^{\im t}$.) Such parametrizations are called \textbf{(smooth) immersions}.  \hfill\qedsymbol
\end{rem}

%% Record #19 2023/11/27 two lectures  47








\subsection{Problems and supplementary material}


Let $I=[a,b]$ where $-\infty<a<b<+\infty$.



\begin{prob}\label{lb386}
Let $f\in l^\infty(I,\Rbb)$. Recall from Subsec. \ref{lb384} that the refinements of partitions define preoders on $\mc P(I)$ and $\mc Q(I)$ so that they are directed sets. For each $\sigma=\{a_0<a_1<\cdots<a_n\}\in\mc P(I)$, define the \textbf{upper Darboux sum} and the \textbf{lower Darboux sum} \index{00@Upper and lower Darboux sums}
\begin{gather*}
\ovl S(f,\sigma)=\sum_{i=1}M_i(a_i-a_{i-1})\qquad~~~ \underline S(f,\sigma)=\sum_{i=1}m_i(a_i-a_{i-1})\\
\text{where}\qquad M_i=\sup_{\xi\in[a_{i-1},a_i]}f(\xi)\qquad m_i=\inf_{\xi\in[a_{i-1},a_i]}f(\xi)
\end{gather*}
Prove that 
\begin{align*}
-(b-a)\Vert f\Vert_\infty\leq\underline S(f,\sigma)<\ovl S(f,\sigma)\leq (b-a)\Vert f\Vert_\infty
\end{align*}
Choose any tag $\xi_\blt$ on $\sigma$. Prove that
\begin{gather}
\ovl S(f,\sigma)=\sup_{(\sigma',\xi_\blt')\succ (\sigma,\xi_\blt)} S(f,\sigma',\xi_\blt')\qquad~~~ \underline S(f,\sigma)=\inf_{(\sigma',\xi_\blt')\succ (\sigma,\xi_\blt)} S(f,\sigma',\xi_\blt')
\end{gather}
\end{prob}


Pb. \ref{lb386} immediately implies:
\begin{thm}\label{lb444}
Let $f\in l^\infty(I,\Rbb)$. Define the \textbf{upper Darboux integral} and the \textbf{lower Darboux integral} \index{00@Upper and lower Darboux integrals} \index{00@Darboux integrals} to be
\begin{align*}
\ovl\int_a^b f=\inf_{\sigma\in\mc P(I)}\ovl S(f,\sigma)\qquad~~~\underline\int_a^b f=\sup_{\sigma\in\mc P(I)}\underline S(f,\sigma)
\end{align*}
which are elements of $[-(b-a)\Vert f\Vert_\infty,(b-a)\Vert f\Vert_\infty]$. Then we have (recalling Pb. \ref{lb346})
\begin{align*}
\ovl\int_a^b f=\limsup_{(\sigma,\xi_\blt)\in\mc Q(I)}S(f,\sigma,\xi_\blt)\\
\underline\int_a^b f=\liminf_{(\sigma,\xi_\blt)\in\mc Q(I)}S(f,\sigma,\xi_\blt)
\end{align*}
Therefore, by Cor. \ref{lb385}, we have 
\begin{align}
f\in\scr R(I,\Rbb)\qquad\Longleftrightarrow\qquad \ovl\int_a^bf=\underline\int_a^bf
\end{align}
Moreover, if $f\in\scr R(I,\Rbb)$, then $\int_a^b f=\ovl\int_a^bf=\underline\int_a^bf$.
\end{thm}



\begin{prob}\label{lb387}
Let $\mc V$ be a vector space over $\Cbb$. Since $\Rbb$ is a subfield of $\Cbb$, $\mc V$ can be viewed as a real normed vector space. Let $\Lambda:\mc V\rightarrow\Rbb$ be a $\Rbb$-linear map. Recall $\im=\sqrt{-1}$. Define a map
\begin{align}
\Lambda_\Cbb:\mc V\rightarrow\Cbb\qquad \Lambda_\Cbb(v)=\Lambda(v)-\im\Lambda(\im v) \label{eq162}
\end{align}
\begin{enumerate}
\item Prove that $\Lambda_\Cbb$ is $\Cbb$-linear.
\item Given a $\Cbb$-linear $\Phi:\mc V\rightarrow\Cbb$, we define its \textbf{real part} \index{00@Real part of a $\Cbb$-linear functional}
\begin{align}
\Real \Phi:\mc V\rightarrow\Rbb\qquad v\mapsto\Real~\big(\Phi(v)\big)
\end{align}
Then clearly $\Real(\Phi)$ is $\Rbb$-linear. Prove that $\Phi\mapsto\Real\Phi$ is a bijection from the set of $\Cbb$-linear maps $\mc V\rightarrow\Cbb$ to the set of $\Rbb$-linear maps $\mc V\rightarrow\Rbb$, and that its inverse is the map $\Lambda\mapsto\Lambda_\Cbb$ defined by \eqref{eq162}.
\item Assume that $\mc V$ is a (non-necessarily complete) normed $\Cbb$-vector space. For each $\Cbb$-linear $\Phi:\mc V\rightarrow\Cbb$, prove the following equation about operator norms:
\begin{align}
\Vert \Phi\Vert=\Vert\Real\Phi\Vert
\end{align}
(Hint: One of ``$\leq$" and ``$\geq$" is obvious. To prove the other one, for each $v\in \mc V$, find some $\theta\in\Rbb$ such that $e^{\im\theta}\Phi(v)\in\Rbb$.)
\end{enumerate}
\end{prob}


\begin{rem}
The above problem shows how to extend a real-valued integral to a complex-valued one. For example, suppose that we define real-valued Riemann integrals using Darboux integrals. Suppose we have proved that the map $f\in\scr R(I,\Rbb)\mapsto\int_a^b f$ is $\Rbb$-linear with operator norm $(b-a)$. Then, applying Pb. \ref{lb387} to the $\Rbb$-linear map
\begin{align*}
\Lambda:\scr R(I,\Cbb)\rightarrow \Rbb\qquad f\mapsto \int_a^b \Real f(t)dt
\end{align*}
gives a $\Cbb$-linear map
\begin{gather*}
\int_a^b:\scr R(I,\Cbb)\rightarrow\Cbb\\
\int_a^b f=\int_a^b\Real f+\im\int_a^b\Imag f
\end{gather*}
since $\Real(\im f)=-\Imag f$. Moreover, this linear map has operator norm $(b-a)$. This defines the complex integral operator by means of real Darboux integrals. In the next semester, we will use the same method to extend real-valued Lebesgue integrals to complex-valued ones. Pb. \ref{lb387} will also be used to prove the Hahn-Banach extension theorem.
\end{rem}


\begin{exe}
Let $u,v\in C(I,\Rbb)$. Find the $\Cbb$-linear map $C(I,\Cbb)\rightarrow\Cbb$ whose real part is $f\in C(I,\Cbb)\mapsto \int_I(u\Real f+v\Imag f)\in\Rbb$. 
\end{exe}



\begin{sprob}\label{lb389}
Let $V=l^\infty([0,1],\Rbb)$, equipped with the $l^\infty$-norm. Define 
\begin{align*}
f:[0,1]\rightarrow V\qquad f(x)=\chi_{[x,1]}
\end{align*}
Then, for every $x\neq y$ in $[0,1]$ we have $\Vert f(x)-f(y)\Vert_{l^\infty([0,1],\Rbb)}=1$. This implies $\omega(f,\sigma)=1$ for every $\sigma\in\mc P(I)$. So $f$ is not strongly Riemann integrable on any closed subintegral of $[a,b]$. 

Define $F:[0,1]\rightarrow V$ such that for each $x\in [0,1]$,
\begin{align*}
F(x):[0,1]\rightarrow\Rbb\qquad t\mapsto\min\{t,x\}
\end{align*}
Choose any $x\in[0,1]$. Prove that $f\in\scr R([0,x],V)$ and $\int_0^xf=F(x)$. (In particular, $\int_0^1f=\id_{[0,1]}$.) Prove that $F'(x)$ does not exist.  \hfill\qedsymbol
\end{sprob}

\begin{comment}
\begin{proof}[Hint]
To find $\int_0^1f$, for each $y\in\Rbb$, define $\varphi_y:V\rightarrow\Rbb$ by $\varphi_y(f)=f(y)$. Then $\varphi_y\in\fk L(V,\Rbb)$. Use $\varphi_y(\int_0^1f)=\int_0^1\varphi_y(f)$ to calculate $\int_0^1f$.
\end{proof}
\end{comment}



\begin{prob}
Let $V$ be a Banach space. Use the fundamental theorem of calculus to give another proof that $C^1(I,V)$ is complete under the $l^{1,\infty}$-norm. (Do not use Thm. \ref{lb336}.)
\end{prob}





\begin{sthm}[\textbf{Gronwall's inequality}] \index{00@Gronwall's inequality}
Let $f\in C([a,b],\Rbb_{\geq0})$. Let $\alpha\in\Rbb_{\geq0}$, and $\beta\in C([a,b],\Rbb_{\geq0})$. Assume that for each $x\in[a,b]$ we have
\begin{align}
f(x)\leq  \alpha+\int_a^x\beta(t)f(t) dt  \label{eq160}
\end{align}
Then for each $x\in[a,b]$ we have
\begin{align}
f(x)\leq \alpha\cdot  \exp\Big(\int_a^x \beta(t)dt \Big)
\end{align}
\end{sthm}
In particular, if $\beta$ is a constant, then Gronwall's inequality reads
\begin{align*}
f(x)\leq\alpha\cdot e^{\beta (t-a)}
\end{align*}

\begin{sprob}
Prove Gronwall's inequality. Hint: Let $g(x)$ be the RHS of \eqref{eq160}. Show that $\exp(-\int_a^x\beta)\cdot g(x)$ is a decreasing function.
\end{sprob}

\begin{srem}
Gronwall's inequality is often used in the following way. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$. Let $f\in C(I,V)$, $v\in V$, and $\beta\in C(I,\Rbb_{\geq0})$. Suppose that for all $x\in[a,b]$ we have
\begin{align}
\Vert f(x)-v\Vert\leq\int_a^x\beta(t)\Vert f(t)\Vert dt
\end{align}
Applying Gronwall's inequality to $|f|$ and $\alpha=\Vert v\Vert$, we see that for every $x\in[a,b]$,
\begin{align}
\Vert f(x)\Vert\leq \Vert v\Vert\cdot  \exp\Big(\int_a^x \beta(t)dt \Big)
\end{align}
\end{srem}





\begin{sprob}
Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$. Assume that $\varphi\in C(I\times V,V)$ has Lipschitz constant $L\in\Rbb_{\geq0}$ over the second variable, i.e., for each $t\in[a,b]$ and $u,v\in V$ we have
\begin{align}
\Vert\varphi(t,u)-\varphi(t,v)\Vert\leq L\Vert u-v\Vert
\end{align}
Use Gronwall's inequality  to solve the following problems.
\begin{enumerate}
\item Let $f_1,f_2:I\rightarrow V$ be differentiable and satisfying the differential equation
\begin{align*}
\dps f'_i(t)=\varphi(t,f_i(t))\qquad (\forall t\in I)
\end{align*}
with the same initial condition $f_1(a)=f_2(a)$. Prove that $f_1=f_2$ on $I$.
\item Let $X$ be a topological space. Let $f:I\times X\rightarrow V$ be a function such that $\partial_1 f$ exists everywhere, and that
\begin{align*}
\partial_1 f(t,x)=\varphi(t,f(t,x))\qquad(\forall t\in I,x\in X)
\end{align*}
Assume that the function $f(a,\cdot):X\rightarrow V$ (sending $x$ to $f(a,x)$) is continuous. Prove that $f\in C(I\times X,V)$.
\end{enumerate} 
\end{sprob}


\begin{rem}\label{lb479}
In practice, $V$ is often $\Rbb^N$, and $\varphi$ is a ``smooth function", i.e. a function whose (mixed) partial derivatives of all orders exist and are continuous. It is also common that $\varphi$ is independent of $t\in I$. (Indeed, a $t$-dependent differential equation $f'=\varphi(t,f)$ can be transformed into a $t$-independent one $(t,f)'=(1,\varphi(t,f))$.) However, sometimes $f$ is not defined on $\Rbb^N$, but on a closed subset of $\Rbb^N$, for example, on a closed ball. In this case, the uniqueness and the continuity of the solutions of differential equations can be treated by extending $\varphi$ to a smooth function on $\Rbb^N$ that is zero outside a compact set. (Then the Lipschitz continuity of this extended function will follow automatically.) 

In fact, by the \textbf{smooth Tietze extension theorem} (cf. \cite[Lem. 2.26]{Lee}), every smooth function $A\rightarrow\Rbb^k$ (where $A$ is a compact subset of a smooth real manifold $M$ (such as a Euclidean space, an $n$-dimensional sphere, etc.)) can be extended to a smooth function $M\rightarrow\Rbb^k$ vanishing outside a compact set. We will study this theorem in the second semester. (The case $A\subset\Rbb$ will be proved in Exp. \ref{lb478}.) \hfill\qedsymbol
\end{rem}
















\newpage




\section{More on Riemann integrals}


\subsection{Commutativity of integrals and other limit processes}\label{lb419}


Let $I=[a,b]$ and $J=[c,d]$ where $-\infty<a<b<+\infty$ and $-\infty<c<d<+\infty$. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.


\subsubsection{Fubini's theorem}


\begin{thm}[\textbf{Fubini's theorem for Riemann integrals}] \label{lb399} \index{00@Fubini's theorem for Riemann integrals}
Let $f\in C(I\times J,V)$. Then $\int_I\int_Jf=\int_J\int_If$. More precisely,
\begin{align}
\int_I\Big(\int_Jf(x,y)dy\Big)dx=\int_J\Big(\int_I f(x,y)dx\Big)dy
\end{align}
\end{thm}

Our strategy is to view $\int_I f(x,y)dx$ as the integral of the function $I\rightarrow C(J,V)$ sending $x$ to $f(x,\cdot)$. Then Fubini's theorem follows from the commutativity of integrals and the bounded linear map $\int_J:C(J,V)\rightarrow V$ (cf. Thm. \ref{lb392}). We first make some general discussion before giving the rigorous proof.


Let $Y$ be a compact topological space. By Thm. \ref{lb274}, we have a canonical equivalence $C(I\times Y,V)\simeq C(I,C(Y,V))$ by viewing $f\in C(I\times Y,V)$ as a map
\begin{subequations}\label{eq167}
\begin{align}
\Phi(f):I\rightarrow C(Y,V)
\end{align}
where
\begin{align}
\Phi(f)(x)=f(x,\cdot):Y\rightarrow V\qquad y\mapsto f(x,y)
\end{align}
\end{subequations}
Recall that the space of continuous functions on a compact space is equipped with the $l^\infty$-norm, and $C(Y,V)$ is complete since $V$ is complete (Cor. \ref{lb101}).

\begin{lm}\label{lb400}
The integral  $\int_I\Phi(f)$, which is an element of $C(Y,V)$, is the function
\begin{align}
\int_If(x,\cdot)dx:Y\rightarrow V\qquad y\mapsto \int_I f(x,y)dx
\end{align}
In other words, for every $y\in Y$ we have
\begin{align}
\Big(\int_I\Phi(f)\Big)(y)=\int_I f(x,y)dx
\end{align}
Consequently, the function $\int_If(x,\cdot)dx$ is continuous.
\end{lm}


It is easy to show that $\int_If(x,\cdot)dx$ is continuous without assuming that $Y$ is compact. See Exe. \ref{lb408}.


\begin{proof}
For each $y\in Y$, define linear map
\begin{align*}
\varphi_y:C(Y,V)\rightarrow V\qquad g\mapsto g(y)
\end{align*}
Then this linear map is clear bounded (with operator norm $1$). Thus, by Thm. \ref{lb392}, we have
\begin{align*}
\Big(\int_I\Phi(f)(x)dx\Big)(y)=\varphi_y\Big(\int_I\Phi(f)(x)dx\Big)=\int_I\varphi_y\big(\Phi(f)(x)\big)dx=\int_I f(x,y)dx
\end{align*}
\end{proof}



\begin{proof}[\textbf{Proof of Thm. \ref{lb399}}]
By Thm. \ref{lb375}, the integral operator $\int_J:C(J,V)\rightarrow V$ is a bounded linear map. Therefore, by Thm. \ref{lb392} (and in particular \eqref{eq166}), we have a commutative diagram
\begin{equation*}
\begin{tikzcd}[column sep=huge]
C(I,C(J,V)) \arrow[r,"\int_J\circ"] \arrow[d,"\int_I"'] & C(I,V) \arrow[d,"\int_I"] \\
C(J,V) \arrow[r,"\int_J"]           & V          
\end{tikzcd} 
\end{equation*}
where the top arrow is the map sending $\Phi(f)$ to $\int_J\circ\Phi(f)$, i.e., sending $x\mapsto f(x,\cdot)$ to $x\mapsto \int_J f(x,y)dy$. Thus, the direction ${}^\rightarrow\!\downarrow$ sends $\Phi(f)$ to $\int_I\int_Jf(x,y)dydx$. By Lem. \ref{lb400}, the left downward arrow sends $\Phi(f)$ to the function $y\in J\mapsto \int_I f(x,y)dx$. So $\downarrow_\rightarrow$ sends $\Phi(f)$ to $\int_J\int_I f(x,y)dxdy$. Thus, the commutativity of the above diagram proves Fubini's theorem.
\end{proof}


\begin{df}\label{lb487}
Let $I_1,\dots,I_N$ be compact intervals in $\Rbb$. Let $B=I_1\times\cdots\times I_N$ and $f\in C(B,V)$. We define the \textbf{Riemann integral} of $f$ to be
\begin{align*}
\int_Bf=\int_{I_1}\cdots\int_{I_N}f(x_1,\dots,x_N)dx_N\cdots dx_1
\end{align*}
Then, by Fubini's theorem, for any bijection $\sigma:\{1,\dots,N\}\rightarrow\{1,\dots,N\}$ we have
\begin{align*}
\int_Bf=\int_{I_{\sigma(1)}}\cdots\int_{I_{\sigma(N)}}f(x_1,\dots,x_N)dx_{\sigma(N)}\cdots dx_{\sigma(1)}
\end{align*}
\end{df}


\subsubsection{Commutativity of integrals and derivatives}


Recall from Cor. \ref{lb344} that $l^{1,\infty}(J,V)$ is a Banach space. So its closed linear subspace $C^1(J,V)$ (cf. Pb. \ref{lb355}) is a Banach space under the $l^{1,\infty}$-norm $\Vert g\Vert_{1,\infty}=\Vert g\Vert_\infty+\Vert g'\Vert_\infty$.

Let $f:I\times J\rightarrow V$. Consider $f$ as a map $\Psi(f):I\rightarrow V^J$ sending $x$ to
\begin{align}
\Psi(f)(x)=f(x,\cdot):J\rightarrow V\qquad  y\mapsto f(x,y)  \label{eq173}
\end{align}
Let $\partial_J$ be the (partial) derivative with respect the variable $y\in J$. Clearly
\begin{align}
\partial_J\big(\Psi(f)(x)\big)=\partial_Jf(x,\cdot)
\end{align}
The linear map of derivative
\begin{align}
\partial_J:C^1(J,V)\rightarrow C(J,V)\qquad g\mapsto g'=\partial_Jg
\end{align}
is clearly bounded (with operator norm $\leq 1$).

\begin{pp}\label{lb402}
Let $X$ be a topological space. Let $f:X\times J\rightarrow V$, and define $\Psi(f):X\rightarrow V^J$ by \eqref{eq173}. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\Psi(f)$ is an element of $C(X,C^1(J,V))$. In other words:
\begin{itemize}
\item[(1a)] For each $x\in X$ we have $\Psi(f)(x)\in C^1(J,V)$.
\item[(1b)] The function $x\in X\mapsto \Psi(f)(x)\in C^1(J,V)$ is continuous.
\end{itemize}
\item $\partial_Jf$ exists everywhere on $X\times J$. Moreover, we have $f,\partial_Jf\in C(X\times J,V)$. 
\end{enumerate}
\end{pp}



\begin{proof}
(1a) means that the functions $y\mapsto f(x,y)$ and $y\mapsto \partial_Jf(x,y)$ exist and are continuous. (1b) is equivalent to that the maps
\begin{gather*}
x\in X\mapsto\Psi(f)(x)=f(x,\cdot)\in C(J,V)\\
x\in X\mapsto \partial_J\Psi(f)(x)=\partial_Jf(x,\cdot)\in C(J,V)
\end{gather*}
are continuous (under the $l^\infty$-norm). By Thm. \ref{lb274}, this is equivalent to that $f$ and $\partial_Jf$ are continuous maps $X\times J\rightarrow V$. So (1)$\Leftrightarrow$(2).
\end{proof}

We return to the setting of $f:I\times J\rightarrow V$.

\begin{lm}\label{lb403}
The integral  $\int_I\Psi(f)$, which is an element of $C^1(J,V)$, is the function
\begin{align}
\int_If(x,\cdot)dx:J\rightarrow V\qquad y\mapsto \int_I f(x,y)dx
\end{align}
Consequently, the function $\int_If(x,\cdot)dx$ is in $C^1(J,V)$.
\end{lm}


\begin{proof}
This lemma can be proved in the same way as Lem. \ref{lb400}, using the fact that for every $y\in J$, the linear map $g\in C^1(J,V)\rightarrow g(y)$ is bounded.
\end{proof}



\begin{comment}
The inclusion map
\begin{align}
\kappa:C^1(J,V)\rightarrow C(J,V)\qquad g\mapsto g
\end{align}
is a bounded linear map (with operator norm $\leq 1$). What we really want to prove is that $\kappa(\int_I\Psi(f))$, which is an element of $C(J,V)$, is given by the function $y\mapsto \int_If(x,y)dx$. By Thm. \ref{lb392}, we have $\kappa(\int_I\Psi(f))=\int_I\kappa\circ\Psi(f)=\int_I\Phi(f)$ where $\Phi(f)$ is defined as in \eqref{eq167}, i.e. $\Phi(f)(x)=f(x,\cdot)$. By Lem. \ref{lb400}, $\int_I\Phi(f)$ is the function $y\mapsto\int_If(x,y)dx$. This finishes the proof.
\end{comment}



\begin{thm}\label{lb405}
Let $f:I\times J\rightarrow V$. Assume that $\partial_Jf$ exists everywhere on $I\times J$. Assume moreover that $f,\partial_Jf\in C(I\times J,V)$. Then for each $y\in J$, the LHS of \eqref{eq168} exists and equals the RHS of \eqref{eq168}, where
\begin{align}
\partial_J\int_I f(x,y)dx=\int_I\partial_J f(x,y)dx  \label{eq168}
\end{align}
\end{thm}


\begin{proof}[First proof]
Again, by Thm. \ref{lb392}, we have a commutative diagram
\begin{equation*}
\begin{tikzcd}[column sep=huge]
C(I,C^1(J,V)) \arrow[r,"\partial_J\circ"] \arrow[d,"\int_I"'] & C(I,C(J,V)) \arrow[d,"\int_I"] \\
C^1(J,V) \arrow[r,"\partial_J"]           & C(J,V)         
\end{tikzcd} 
\end{equation*}
By Prop. \ref{lb402}, $\Psi(f)$ is an element of $C(I,C^1(J,V))$. The map $\Psi(f):I\rightarrow C^1(J,V)$, composed with $\partial_J$, gives $x\in I\mapsto\partial_Jf(x,\cdot)$. 
%So $\partial_J\circ\Psi(f)=\Phi(\partial_Jf)$ where $\Phi(\partial_Jf)$ is defined as in \eqref{eq167}. 
By Lem. \ref{lb400}, the direction ${}^\rightarrow\!\downarrow$ sends $\Psi(f)$ to $\int_I\partial_J f(x,\cdot)dx$. By Lem. \ref{lb403}, $\int_I\Psi(f)$ equals $\int_I f(x,\cdot)$. In particular, $\int_I f(x,\cdot)$ is a $C^1$-function. So $\downarrow_\rightarrow$ sends $\Psi(f)$ to $\partial_J\int_I f(x,\cdot)dx$. This finishes the proof.
\end{proof}



\begin{proof}[Second proof]
Fix any $y\in J$. In view of Cor. \ref{lb380}, it suffices to prove that the limit of the net of functions $(\varphi_p)_{p\in J\setminus\{y\}}$ from $I$ to $V$ converges uniformly to $\partial_Jf(\cdot,y)$ under $\lim_{p\rightarrow y}$, where
\begin{align*}
\varphi_p(x)=\frac{f(x,p)-f(x,y)}{p-y}
\end{align*}
By Rem. \ref{lb404}, we have
\begin{align*}
\Vert \varphi_p(x)-\partial_Jf(x,y)\Vert\leq A(x,p):=\sup_{q\in[p,y]\cup[y,p]}\Vert \partial_Jf(x,q)-\partial_Jf(x,y)\Vert
\end{align*}
Since $\partial_Jf$ is continuous, it can be viewed as a continuous map $J\rightarrow C(I,V)$ by Thm. \ref{lb274}. Thus, for every $\eps>0$ there exists $\delta>0$ such that for every $p\in J$ satisfying $|p-y|\leq\delta$, we have $\sup_{x\in I}\Vert \partial_Jf(x,q)-\partial_Jf(x,y)\Vert\leq\eps$ for all $q\in[p,y]\cup[y,p]$, and hence $\sup_{x\in I}A(x,p)\leq\eps$. This proves that $A(\cdot,p)$ converges uniformly to $0$ (as a net of functions $I\rightarrow\Rbb$) as $p\rightarrow y$, finishing the proof.
\end{proof}




\subsubsection{Commutativity of partial derivatives}


We write $\partial_If(x,y)$ as $\partial_1f(x,y)$ and $\partial_Jf(x,y)$ as $\partial_2f(x,y)$. In the following, we use Thm. \ref{lb405} to give a new proof of a slightly weaker version of Thm. \ref{lb406} on the commutativity of $\partial_1$ and $\partial_2$. The idea is as follows. Suppose we know that $A,B$ are linear operators on a vector space such that $A$ is invertible and $A^{-1}B=BA^{-1}$. Then one deduces $A^{-1}BA=BA^{-1}A=B$ and hence $BA=AA^{-1}BA=AB$. Now, Thm. \ref{lb405} says that $\partial_J$  commutes with $\int_I$, the inverse of $\partial_I$ (in a vague sense). So one can use a similar algebraic argument to prove that $\partial_J$ commutes with $\partial_I$.

%For the reader's convenience, we reproduce the content of Thm. \ref{lb406} as follows, along with some additional conclusions (i.e. the continuity of $\partial_1f$ and $\partial_2f$).


\begin{thm}\label{lb407}
Let $f:I\times J\rightarrow V$ be a function such that $\partial_1f,\partial_2f,\partial_2\partial_1f$ exist and are continuous on $I\times J$. Then $\partial_1\partial_2f$ exists on $I\times J$ and equals $\partial_2\partial_1f$. (So $\partial_1\partial_2f$ is also continuous.)
\end{thm}


Thm. \ref{lb407} is weaker than Thm. \ref{lb406} in that we assume $\partial_1f,\partial_2f$ to be continuous. Indeed, as we shall see in the proof, the continuity of $\partial_2f$ is not used. However, in concrete examples, it is fairly easy to check the continuity of  $\partial_1f,\partial_2f$. 


\begin{proof}
%Since $\partial_2\partial_1f$ exists, the function $y\in J\mapsto\partial_1f(a,y)$ is differentiable and in particular continuous. Thus, the continuity of $\partial_1f$ follows easily from
%\begin{align*}
%\partial_1f(x,y)=\partial_1f(a,y)+\int_a^x\partial_2\partial_1f(u,y)du
%\end{align*}
%and the continuity of $\partial_2\partial_1f$. (See Exe. \ref{lb408}.) Similarly, one can prove that $\partial_2f$ is continuous if one can prove $\partial_1\partial_2f=\partial_2\partial_1f$. 
Let $\dps F(x,y)=\int_a^x\partial_2\partial_1 f(u,y)du$, which can be defined because $\partial_2\partial_1f$ is continuous. By FTC, we have $\partial_1F=\partial_2\partial_1f$. On the other hand, by Thm. \ref{lb405} and the continuity of $\partial_1f,\partial_2\partial_1f$, we have
\begin{align*}
F(x,y)=\partial_2\int_a^x\partial_1f(u,y)du=\partial_2(f(x,y)-f(a,y))
\end{align*}
Therefore $\partial_1\partial_2f$ exists and equals $\partial_1F=\partial_2\partial_1f$.
\end{proof}


\begin{exe}\label{lb408}
Let $Y$ be a topological space. Let $f\in C(I\times Y,V)$. Prove that the following function is continuous:
\begin{gather}
I\times Y\rightarrow V\qquad (x,y)\mapsto\int_a^x f(u,y)du
\end{gather}
\end{exe}



\begin{exe}
Use Fubini's Thm \ref{lb399} to prove Thm. \ref{lb405}.
\end{exe}




\subsection{Lebesgue's criterion for Riemann integrability}\label{lb522}


Fix $I=[a,b]$ where $-\infty<a<b<+\infty$. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$. The goal of this section is to prove:


\begin{thm}[\textbf{Lebesgue's criterion}] \index{00@Lebesgue's criterion for strong Riemann integrability}  \label{lb411}
Let $f:I\rightarrow V$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $f$ is strongly Riemann integrable.
\item $f$ is bounded (i.e. $\Vert f\Vert_{l^\infty}<+\infty$). Moreover, the set of discontinuities
\begin{align}
\{x\in I:f\text{ is not continuous at }x\}
\end{align}
is a null set.
\end{enumerate}
\end{thm}


\begin{df}\label{lb409}
A subset $E$ of $\Rbb$ is called a  \textbf{(Lebesgue) null set} \index{00@Null set, Lebesgue} (or a set of \textbf{(Lebesgue) measure zero}) if for every $\eps>0$ there exist countably many closed intervals $I_1,I_2,\dots$ such that $E\subset\bigcup_i I_i$, and that $\sum_i |I_i|<\eps$. Here, $|I_i|$ is the length of $I_i$.
\end{df}

The word ``countably many" can be omitted, because the sum of uncountably many strictly positive reals numbers must be $+\infty$ due to Pb. \ref{lb413}.

\begin{rem}
Covering $E$ by open intervals instead of closed ones does not change the definition of null sets. This is because any open/closed interval can be stretched by a factor of $1+\delta$ to a larger closed/open interval, where $\delta$ is any given positive number.
\end{rem}


\begin{pp}\label{lb412}
A countable union of null subsets of $\Rbb$ is a null set.
\end{pp}

\begin{proof}
If $E=E_1\cup E_2\cup\cdots$ where each $E_i$ is a null set, then for each $\eps$, $E_i$ can be covered by countably many closed intervals of total length $<2^{-i}\eps$. So $E$ can be covered by by countably many closed intervals of total length $<\eps$.
\end{proof}


\begin{eg}
Every interval with at least two points is not a null set.
\end{eg}

\begin{proof}
Since every such interval contains a closed invertal with at least two points, it suffices to prove that the latter is not null. Thus, let us prove that $I$ is not null. Suppose $I$ is covered by some open intervals. Then, since $I$ is compact, $I$ is covered by finitely many of these open intervals, say $U_1,\dots,U_n$. It is easy to see that  $\sum_{i=1}^n |U_i|\geq|I|= b-a$. (For example, let $f=\sum_i \chi_{U_i}$. Then $f\geq \chi_I$. So $\sum_i |U_i|=\sum_i\int\chi_{U_i}=\int f\geq\int_a^b 1=b-a$.)
\end{proof}



Before proving Lebesgue's criterion, let us first see some useful applications. Recall that if $V=\Rbb^N$ then Riemann integrability is equivalent to strong Riemann integrability (Thm. \ref{lb410}).

\begin{co}
Let $f\in\scr R(I,\Rbb^n)$. Let $\Omega$ be a subset of $\Rbb^n$ containing $f(I)$. Let $g\in C(\Omega,\Rbb^m)$ such that $\Vert g\Vert_\infty<+\infty$. Then $g\circ f\in\scr R(I,\Rbb^m)$
\end{co}

Note that if $\Omega$ is compact, we automatically have $\Vert g\Vert_\infty<+\infty$.

\begin{proof}
Clearly $g\circ f$ is bounded. The set of discontinuities of $g\circ f$ is null since it is a subset of the set of discontinuities of $f$, where the latter is a null set.
\end{proof}


\begin{co}
Let $f\in\scr R(I,\Fbb^{m\times n})$ and $g\in\scr R(I,\Fbb^{n\times k})$. Then $fg\in\scr R(I,\Fbb^{m\times k})$.
\end{co}

\begin{proof}
This is immediate from Lebesgue's criterion.
\end{proof}


\subsubsection{Proof of Lebesgue's criterion}


The first step of proving Lebesgue's criterion is to express the set of discontinuities by the oscillation.

\begin{df}\label{lb462}
Let $X$ be a topological space. Let $Y$ be a metric space. The \textbf{oscillation} \index{00@Oscillation at a point} \index{zz@$\omega(f,x)$} of a function $f:X\rightarrow Y$ at $x\in X$ is defined to be
\begin{align*}
\omega(f,x)=\inf_{U\in\Nbh_X(x)}\diam(f(U))
\end{align*}
\end{df}


\begin{pp}\label{lb461}
Let $X$ be a topological space, let $Y$ be a metric space, and let $f:X\rightarrow Y$. Then  $f$ is continuous at $x\in X$ iff $\omega(f,x)=0$.
\end{pp}


\begin{proof}
Assume that $f$ is continuous at $x$. Then for every $\eps>0$, there exists $U\in\Nbh(x)$ such that $d(f(p),f(x))<\eps/2$ for every $p\in X$. Then clearly $\diam(f(U))\leq\eps$. So $\omega(f,x)\leq \eps$. Since $\eps>0$ is arbitrary, we get $\omega(f,x)=0$.

Conversely, suppose $\omega(f,x)=0$. Then for every $\eps>0$ there exists $U\in\Nbh(x)$ such that $\diam(f(U))<\eps$. So for every $p\in U$ we have $d(f(p),f(x))<\eps$.
\end{proof}



\begin{proof}[\textbf{Proof of Thm. \ref{lb411}, part 1}]
Let us prove (1)$\Rightarrow$(2). Assume that $f$ is strongly Riemann integrable. Choose any $\eps>0$. Let us prove that
\begin{align}
\Omega_\eps(f)=\{x\in I:\omega(f,x)\geq\eps\}\label{eq200}
\end{align}
is a null set. Then the set of discontinuities, which is $\bigcup_{n\in\Zbb_+}\Omega_{1/n}(f)$, is a null set by Prop. \ref{lb412}.

Since $\inf_{\sigma\in\mc P(I)}\omega(f,\sigma)=0$, for every $\delta>0$, there exists $\sigma=\{a_0<\cdots<a_n\}\in\mc P(I)$ such that, with $I_i=[a_{i-1},a_i]$, we have 
\begin{align}
\sum_{i=1}^n \diam(f(I_i))\cdot |I_i|<\delta\eps\label{eq170}
\end{align}
Note that if $x\in I\setminus\sigma$ is in some $I_i$ where $\diam(f(I_i))<\eps$, then clearly $\omega(f,x)<\eps$. Thus, if $\omega(f,x)\geq \eps$ (i.e., if $x\in\Omega_\eps(f)$), then either $x\in\sigma$, or $x\in I_i$ for some $I_i$ such that $\diam(f(I_i))\geq\eps$. We conclude
\begin{align*}
\Omega_\eps(f)\subset\{a_0,\dots,a_n\}\cup\Big(\bigcup_{k\in K}I_k\Big)
\end{align*}
where $K=\{1\leq k\leq n:\diam(f(I_k))\geq\eps\}$. Clearly $\{a_0,\dots,a_n\}$ can be covered by some intervals with total length $<\delta$. But \eqref{eq170} implies that $\sum_{k\in K}\eps|I_k|<\delta\eps$ and hence $\sum_{k\in K}|I_k|<\delta$. So $\Omega_\eps(f)$ can be covered by intervals with total length $<2\delta$ for every $\delta>0$. Thus $\Omega_\eps(f)$ is a null set.
\end{proof}


To prove the other direction, we need some preparation.


\begin{lm}\label{lb414}
Let $f:X\rightarrow Y$ where $X$ is a topological space and $Y$ is a metric space. Then for every $\eps>0$, $\Omega_\eps(f)=\{x\in X:\omega(f,x)\geq\eps\}$ is a closed subset of $X$.
\end{lm}

\begin{proof}
Let us prove that each $x\in X\setminus\Omega_\eps(f)$ is an interior point. (Recall Prop. 7\ref{lb179}.) Indeed, since $\inf_{U\in\Nbh(x)}\diam(f(U))<\eps$, there exists $U\in\Nbh(x)$ such that $\diam(f(U))<\eps$. Then for each $p\in U$ we have $\inf_{V\in\Nbh(p)}\diam(f(V))<\eps$ since $U\in\Nbh(p)$. So $U\subset X\setminus\Omega_\eps(f)$.
\end{proof}



\begin{lm}\label{lb415}
Let $\eps>0$. Suppose that for every $x\in I$ we have $\omega(f,x)<\eps$. (Namely, suppose $\Omega_\eps(f)=\emptyset$.) Then there exists a partition $I=I_1\cup\cdots\cup I_n$ satisfying $\diam(f(I_i))<\eps$ for all $i$.
\end{lm}


\begin{proof}
For every $x\in I$, there exists $U_x\in\Nbh_I(x)$ such that $\diam(f(U_x))<\eps$. Choose $n\in\Zbb_+$ such that $1/n$ is a Lebesgue number of the open cover $\mc U=\{U_x:x\in I\}$ of $I$. (Recall Thm. \ref{lb295}.) Dividing $I$ into $2n$ subintervals with the same length gives the desired partition.
\end{proof}


\begin{proof}[\textbf{Proof of Thm. \ref{lb411}, part 2}]
Let us prove (2)$\Rightarrow$(1). Suppose that $M=\Vert f\Vert_\infty$ is $<+\infty$, and that the set of discontinuities is a null set. Then for each $\eps>0$, $\Omega_\eps(f)$ is a null set. By Lem. \ref{lb414} and Heine-Borel, $\Omega_\eps(f)$ is compact. Thus, for every $\delta>0$, $\Omega_\eps(f)$ can be covered by finitely many closed intervals with total length $<\delta$. Let $\Delta$ be the union of these closed intervals. Then $\Omega_\eps(f)\subset\Delta$. Clearly, both $\Delta$ and $J=I\setminus\Int(\Delta)$ can be written as disjoint unions of finitely many closed intervals, which are their connected components. And $\Delta$ has length $|\Delta|<\delta$.

Since $J\cap\Omega_\eps(f)=\emptyset$, applying Lem. \ref{lb415} to each of the components $J_1,J_2,\dots$ of $J$, we see that $J_i$ has a partition $\varrho_i$ such that the oscillation of $f$ (recall Def. \ref{lb416}) on each subinterval cut out by $\varrho_i$  is $<\eps$. Let $\sigma=\varrho_1\cup\varrho_2\cup\cdots\cup\{a,b\}$, which is a partition of $I$. Then $\sigma$ divides $I$ into subintervals $I_1,I_2,\dots$ such that either $I_j\subset J$ or $I_j\subset\Delta$.
\begin{itemize}
\item If $I_j\subset J$, then $I_j$ belongs to a subinterval cut out by one of $\varrho_1,\varrho_2,\dots$ in $J_1,J_2,\dots$. This implies $\diam(f(I_j))<\eps$ by the construction of $\varrho_1,\varrho_2,\dots$.
\item If $I_j\subset\Delta$, then $I_j$ is a component of $\Delta$. We have $\diam(f(I_j))\leq 2M$. Moreover, the total length of such $I_j$ is equal to $|\Delta|$, which is $<\delta$.
\end{itemize}
From the discussion of these two cases, we see that 
\begin{align*}
\omega(f,\sigma)=\sum_j\diam(f(I_j))\cdot|I_j|\leq \eps\cdot (b-a)+2M\cdot\delta
\end{align*}
Since $b-a$ and $M$ are fixed numbers and $\eps,\delta$ are arbitrary, we see that $\inf_{\sigma}\omega(f,\sigma)=0$. So $f$ is strongly Riemann integrable.
\end{proof}





\subsection{Improper integrals}


In this section, all intervals in $\Rbb$ are assumed to contain at least two points. Let $I$ be an interval in $\Rbb$ with $a=\inf I$ and $b=\sup I$. So $-\infty\leq a<b\leq+\infty$. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.



\begin{df}
We define \index{R@$\scr R([a,b],V)$}
\begin{align*}
\scr R(I,V)=\{f\in V^I:f|_J\in\scr R(J,V)\text{ for every compact interval }J\subset I\}
\end{align*}
Define the \textbf{improper integral} \index{00@Improper integral}
\begin{align}\label{eq171}
\int_If\equiv\int_a^bf\xlongequal{\mathrm{def}}\lim_{
\begin{subarray}{c}
u\rightarrow a\\
v\rightarrow b
\end{subarray}}
\int_u^vf=\lim_{J}\int_Jf
\end{align}
Here, the last limit is over the directed set $\{\text{compact intervals in $I$}\}$ with preorder ``$\subset$". If the above limit exists, we say that $\int_If$ \textbf{exists} or \textbf{converges}. 

When $f\in\scr R(I,\Rbb)$ and  takes values in $\Rbb_{\geq 0}$, we write $f\in\scr R(I,\Rbb_{\geq0})$. Then $\int_If$ clearly exists in $\ovl\Rbb_{\geq0}$. We write $\dps\int_If<+\infty$ if $\int_If$ converges in $\Rbb$.   \hfill\qedsymbol
\end{df}





In the case that $a$ or $b$ is in $I$, when taking the limit over $u\rightarrow a$ and $v\rightarrow b$ in \eqref{eq171}, it is immaterial whether $u,v$ can take values $a,b$ or not, due to the following easy observation:
\begin{lm}
Let $f\in\scr R(I,V)$. If $I=[a,b]$, then the meanings of $\scr R(I,V)$ and $\int_If$ are the same as before. If $I=[a,b)$ resp. $I=(a,b]$, then 
\begin{gather*}
\int_If=\lim_{v\rightarrow b}\int_a^vf\qquad\text{resp.}\qquad \int_If=\lim_{u\rightarrow a}\int_u^bf
\end{gather*}
where the convergence of the LHS is equivalent to that of the RHS. 
\end{lm}


\begin{proof}
Assume $I=[a,b]$. Then $I$ is a compact interval. The new and old meanings of $\scr R(I,V)$ are the same by Prop. \ref{lb376}. Let $f\in\scr R(I,V)$. Then by Thm. \ref{lb375}, we have $\Vert f\Vert_\infty<+\infty$. If $a\leq u<v\leq b$, then Thm. \ref{lb375} and Prop. \ref{lb376} show that
\begin{align*}
\Big\Vert \int_a^bf-\int_u^vf \Big\Vert=\Big\Vert \int_a^uf+\int_v^bf \Big\Vert\leq \Vert f\Vert_\infty\cdot((u-a)+(b-v))
\end{align*}
which converges to $0$ as $u\rightarrow a$ and $v\rightarrow b$, whether $u,v$ take values $a,b$ or not. 

Similarly, if $I=[a,b)$, then since $f$ is Riemann integrabe on the compact subinterval $J=[a,(a+b)/2]$, we have $M:=\Vert f|_J\Vert_\infty<+\infty$. This implies that for all $u,v$ such that $a\leq u<v<b$ and $u\leq (a+b)/2$,
\begin{align*}
\Big\Vert \int_a^vf-\int_u^vf \Big\Vert=\Big\Vert \int_a^uf \Big\Vert\leq M(u-a)
\end{align*}
which converges to $0$ as $u\rightarrow a$ and $v\rightarrow b$. So $(\int_a^v f)_{u,v}$ and $(\int_u^vf)_{u,v}$ are Cauchy-equivalent nets. So their convergences and values are equivalent. The case $I=(a,b]$ is similar.
\end{proof}



%% Record #20 2023/11/29 three lectures  50


\begin{rem}\label{lb418}
Let $f\in\scr R(I,V)$. The Cauchy condition for the convergence of $\int_a^bf$ is easy to describe. In view of $\int_u^v-\int_{u'}^{v'}=\int_u^{u'}+\int_v^{v'}$ (Prop. \ref{lb376}), we have:
\begin{itemize}
\item For every $\eps>0$, there exist $u_0<v_0$ in $I$ such that for all $u,u',v,v'\in I$ satisfying $a\leq u,u'<u_0$ and $v_0<v,v'\leq b$ we have
\begin{align*}
\Big\Vert\int_u^{u'}f\Big\Vert<\eps\qquad \Big\Vert \int_v^{v'}f\Big\Vert<\eps
\end{align*} 
\end{itemize}
\end{rem}


\begin{df}\label{lb421}
Let $f\in\scr R(I,V)$. We say that $\int_If$ \textbf{converges absolutely} \index{00@Absolute convergence of improper integral} (or that $f$ is \textbf{absolutely integrable on $I$}) \index{00@Absolutely (Riemann) integrable} if there exists $g\in\scr R(I,\Rbb)$ such that $|f|\leq g$ (i.e., $\Vert f(x)\Vert\leq g(x)$ for all $x\in I$, in particular $g(x)\geq0$) and that $\int_I g<+\infty$. We let \index{R1@$\scr R^1(I,V)$}
\begin{align}
\textstyle\scr R^1(I,V)=\big\{f\in\scr R(I,V):\int_If\text{ converges absolutely}\big\}
\end{align}
which is clearly a linear subspace of $V^I$. By the Cauchy condition in Rem. \ref{lb418}, it is clear that $\int_If$ converges if $\int_If$ converges absolutely. Though $\Rbb_{\geq0}$ is not a Banach space, we still write
\begin{align*}
\textstyle\scr R^1(I,\Rbb_{\geq0})=\big\{f\in\scr R(I,\Rbb_{\geq0}):\int_If\text{ converges absolutely}\big\}
\end{align*}
which is clearly the set of all $f\in\scr R(I,\Rbb_{\geq0})$ satisfying $\int_If<+\infty$.
\end{df}




The superscript $1$ of $\scr R^1$ has a similar meaning as that of $l^1$, but is different from that of $C^1$.



\begin{rem}
Assume that $f:I\rightarrow V$ is strongly integrable on each compact subinterval of $I$. (This is the case, for example, when $f\in C(I,V)$, or when $f\in\scr R(I,V)$ and $V=\Fbb^N$.) Then $|f|:x\in I\mapsto \Vert f(x)\Vert\in\Rbb_{\geq0}$ is an element of $\scr R(I,\Rbb_{\geq0})$ by Cor. \ref{lb417}. Thus, in this case,
\begin{align*}
f\in\scr R^1(I,V)\qquad\Longleftrightarrow\qquad \int_I |f|<+\infty
\end{align*}
\end{rem}


\begin{eg}
We have
\begin{align*}
\int_1^{+\infty}x^{-2}dx=\lim_{v\rightarrow+\infty}\int_1^v x^{-2}dx=\lim_{v\rightarrow+\infty}(-x^{-1})\big|_1^v=1<+\infty
\end{align*}
Therefore $\dps\int_1^{+\infty} \frac{e^{\im x}}{x^2}dx$ converges absolutely, and hence converges.
\end{eg}


The following proposition generalizes Prop. \ref{lb379}.
\begin{pp}\label{lb427}
Let $f\in\scr R^1(I,V)$ and $g\in\scr R^1(I,\Rbb_{\geq0})$ such that $|f|\leq g$. Then $\dps\Big\Vert\int_If\Big\Vert\leq\int_Ig$.
\end{pp}

\begin{proof}
Apply the limit over $u\rightarrow a,v\rightarrow b$ to $\Vert\int_u^vf\Vert\leq\int_u^vg$.
\end{proof}



The next proposition shows that improper integrals are helpful for studying series:

\begin{pp}
Let $f\in\scr R([1,+\infty),\Rbb_{\geq0})$ be decreasing. Then we have
\begin{align}
\int_1^{+\infty} f<+\infty\qquad\Longleftrightarrow\qquad\sum_{n=1}^{+\infty}f(n)<+\infty
\end{align}
\end{pp}


\begin{proof}

Since $f$ is decreasing, we clearly have $g\leq f\leq h$ where $g,h$ are defined by the series in $l^\infty([0,+\infty),\Rbb)$:
\begin{align*}
g=\sum_{n=1}^{+\infty} f(n+1)\cdot \chi_{[n,n+1)}\qquad h=\sum_{n=1}^{+\infty} f(n)\cdot \chi_{[n,n+1)}
\end{align*}
One computes easily that $\int_1^{+\infty}g=\sum_{n=2}^{+\infty}f(n)$ and $\int_1^{+\infty}h=\sum_{n=1}^{+\infty}f(n)$ in $\ovl\Rbb_{\geq0}$. Thus
\begin{align*}
\sum_{n=2}^{+\infty}f(n)\leq\int_1^{+\infty} f\leq \sum_{n=1}^{+\infty}f(n)
\end{align*}
The proposition follows easily.
\end{proof}


\begin{eg}
Let $s>0$. The function $f:[2,+\infty)\rightarrow+\infty$ defined by $\dps f(x)=\frac 1{x(\log x)^s}$ is decreasing, and
\begin{align*}
\int_2^xf=\left\{
\begin{array}{ll}
(1-x)^{-1}\big((\log x)^{1-s}-(\log 2)^{1-s} \big) &\text{ if }s\neq 1\\[1ex]
\log(\log x)-\log(\log 2)&\text{ if }s= 1
\end{array}
\right.
\end{align*}
So $\sum_{n=2}^\infty f(n)<+\infty$ iff $\int_2^{+\infty}f<+\infty$  iff $s>1$.
\end{eg}



\subsection{Commutativity of improper integrals and other limit processes}



In this section, all intervals in $\Rbb$ are assumed to contain at least two points. Let $I$ be an interval in $\Rbb$ with $a=\inf I$ and $b=\sup I$. Let $J=[c,d]$ be a compact interval. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.

The goal of this section is to generalize the main results of Sec. \ref{lb419} to improper integrals. Namely, we shall prove the commutativity of $\int_I$ with $\int_J$ and with $\partial_J$ under reasonable assumptions. There are three ways to achieve this goal: 
\begin{itemize}
\item We know that $\int_I$ is the limit of integrals over compact intervals. Therefore, the problem is reduced to that of proving that $\lim_{u\rightarrow a,v\rightarrow b}$ can be moved inside in  $\lim\int_J\int_u^v$ and in $\lim\partial_J\int_u^v$.
\item We generalize Thm. \ref{lb392} to improper integrals, and use this generalized version to prove the commutativity in a similar way as in Sec. \ref{lb419}.
\item The commutativity of $\int_I$ with $\partial_J$ can also be studied by generalizing Cor. \ref{lb380} to improper integrals, similar to the second proof of Thm. \ref{lb405}.
\end{itemize}



We will use the second approach because its proof is more conceptual and involves fewer technical calculations, thus making it easier for us to remember the conditions of the theorems to be proved. Nevertheless, we will also give the appropriate generalization of Cor. \ref{lb380}, which is helpful for future application. Recall Def. \ref{lb421} for the meaning of $\scr R^1$. 


\subsubsection{Commutativity of improper integrals and bounded linear maps}



\begin{thm}\label{lb420}
Let $W$ be also a Banach space over $\Fbb$. Let $T\in\fk L(V,W)$. Then for every $f\in\scr R^1(I,V)$ we have $T\circ f\in\scr R^1(I,W)$ and
\begin{align}
T\Big(\int_a^b f \Big)=\int_a^b T\circ f
\end{align}
\end{thm}

In other words, we have a commutative diagram
\begin{equation}\label{eq175}
\begin{tikzcd}[column sep=large]
\scr R^1(I,V) \arrow[r,"T\circ"] \arrow[d,"\int_I"'] & \scr R^1(I,W) \arrow[d,"\int_I"] \\
V \arrow[r,"T"]           & W        
\end{tikzcd} 
\end{equation}


\begin{proof}
By Thm. \ref{lb392}, $T\circ f$ is Riemann integrable on compact subintervals of $I$. Since $f\in\scr R^1$, there exists $g\in\scr R(I,\Rbb_{\geq0})$ such that $|f|\leq g$ and $\int_Ig<+\infty$. Let $M=\Vert T\Vert$ be the operator norm, which is a finite number. By Rem. \ref{lb372}, for each $x\in I$ we have $\Vert T\circ f(x)\Vert\leq M\Vert f(x)\Vert\leq Mg(x)$, and hence $|T\circ f|\leq Mg$. This proves $T\circ f\in\scr R^1(I,W)$. In particular, the integrals of $f,T\circ f$ over $I$ converge. Thus, by the continuity of $T$ and the commutativity of $T$ with $\int_u^v$ (when  $a<u<v<b$) due to Thm. \ref{lb392}, we have
\begin{align*}
T\Big(\int_If\Big)=T\Big(\lim_{
\begin{subarray}{c}
u\rightarrow a\\
v\rightarrow b
\end{subarray}}\int_u^v f\Big)=\lim_{
\begin{subarray}{c}
u\rightarrow a\\
v\rightarrow b
\end{subarray}}T\Big(\int_u^v f\Big)=\lim_{
\begin{subarray}{c}
u\rightarrow a\\
v\rightarrow b
\end{subarray}}\int_u^v T\circ f=\int_IT\circ f
\end{align*}
\end{proof}



\subsubsection{Fubini's theorem}

Recall that $J=[c,d]$ is a compact interval but $I$ is not necessarily compact.

\begin{lm}\label{lb423}
Lem. \ref{lb400} holds verbatim to the current case that $I$ is not necessarily compact.
\end{lm}


\begin{proof}
We can prove this general case in the same way as Lem. \ref{lb400}, using the commutativity of $\int_I$ and the bounded map $g\in C(Y,V)\mapsto g(y)\in V$, which is available thanks to Thm. \ref{lb420}.
\end{proof}


\begin{thm}[\textbf{Fubini's theorem for improper integrals}] \label{lb422} \index{00@Fubini's theorem for improper integrals}
Let $f\in C(I\times J,V)$. Assume that there exists $h\in\scr R^1(I,\Rbb_{\geq0})$ satisfying
\begin{align}
\Vert f(x,y)\Vert\leq h(x)\qquad(\forall x\in I,y\in J)  \label{eq177}
\end{align}
Then the functions $\int_J f(\cdot,y)dy:I\rightarrow V$ and $\int_If(x,\cdot)dx:J\rightarrow V$ are continuous, and the equation
\begin{align}
\int_I\Big(\int_Jf(x,y)dy\Big)dx=\int_J\Big(\int_I f(x,y)dx\Big)dy \label{eq176}
\end{align}
holds where the integral over $I$ on the LHS converges absolutely.
\end{thm}



\begin{proof}
Let $T:C(J,V)\rightarrow V$ be the bounded linear map $\int_J$. By Thm. \ref{lb420}, we have a commutative diagram
\begin{equation}\label{eq178}
\begin{tikzcd}[column sep=large]
\scr R^1(I,C(J,V)) \arrow[r,"T\circ"] \arrow[d,"\int_I"'] & \scr R^1(I,V) \arrow[d,"\int_I"] \\
C(J,V) \arrow[r,"T"]           & V        
\end{tikzcd} 
\end{equation}
Since $J$ is compact, by Thm. \ref{lb274}, we can view $f$ as a continuous function $\Phi(f):I\rightarrow C(J,V)$ sending $x$ to $f(x,\cdot)$. Then \eqref{eq177} says that $\Phi(f)\leq g$. Since $g\in\scr R^1$, we conclude $\Phi(f)\in\scr R^1(I,C(J,V))$. 

By Lem. \ref{lb423}, $\int_I\Phi(f)$ equals the function $\int_I f(x,\cdot)dx$. In particular, $\int_I f(x,\cdot)dx$ is continuous since $\int_I\Phi(f)\in C(J,V)$ by \eqref{eq178}. The continuity of $\int_J f(\cdot,y)dy$ is similar, or is even easier because $J$ is compact.


Clearly $T(\int_I\Phi(f))$ is $\int_J\int_I f$. By the top arrow of \eqref{eq178}, $T\circ\Phi(f)$ belongs to $\scr R^1(I,V)$. Since $T\circ\Phi(f)$ is the function sending $x$ to $\int_J f(x,\cdot)dy$, the  integral of this function over $I$ is absolutely convergent, and $\int_I T\circ\Phi(f)=\int_I\int_Jf$. Thus, the commutativity of \eqref{eq178} proves \eqref{eq176}. 
\end{proof}



The continuity of $\int_If(x,\cdot)dx$ can be generalized: see Cor. \ref{lb429}.



\subsubsection{Commutativity of improper integrals and partial derivatives}


\begin{lm}\label{lb426}
Lemma \ref{lb403} holds verbatim to the current case that $I$ is not necessarily compact.
\end{lm}

\begin{proof}
Again, this is proved in the same way as Lem. \ref{lb400} by applying Thm. \ref{lb420} to the bounded linear functional $g\in C^1(J,V)\rightarrow g(y)$ (where $y\in J$).
\end{proof}


\begin{thm}\label{lb434}
Let $f:I\times J\rightarrow V$. Assume that $\partial_Jf$ exists everywhere on $I\times J$. Assume that $f,\partial_J f\in C(I\times J,V)$. Assume moreover that there exists $h\in\scr R^1(I,\Rbb_{\geq0})$ satisfying
\begin{align}\label{eq183}
\Vert f(x,y)\Vert\leq h(x)~~\text{ and }~~\Vert\partial_Jf(x,y)\Vert\leq h(x)\qquad(\forall x\in I,y\in J)
\end{align}
Then for each $y\in J$, the LHS of \eqref{eq181} exists and equals the RHS of \eqref{eq181}, where
\begin{align}
\partial_J\int_I f(x,y)dx=\int_I\partial_J f(x,y)dx  \label{eq181}
\end{align}
\end{thm}


\begin{proof}
Let $T:C^1(J,V)\rightarrow C(J,V)$ be the bounded linear map $\partial_J$. By Thm. \ref{lb420}, we have a commutative diagram
\begin{equation}\label{eq182}
\begin{tikzcd}[column sep=large]
\scr R^1(I,C^1(J,V)) \arrow[r,"T\circ"] \arrow[d,"\int_I"'] & \scr R^1(I,C(J,V)) \arrow[d,"\int_I"] \\
C^1(J,V) \arrow[r,"T"]           & C(J,V)      
\end{tikzcd} 
\end{equation}
Define $\Psi(f):X\rightarrow V^J$ sending $x$ to $f(x,\cdot)$. By Prop. \ref{lb402}, $\Psi(f)$ belongs to $C(I,C^1(J,V))$. By \eqref{eq183}, $\Psi(f)$ belongs to $\scr R^1(I,C^1(J,V))$. As in the first proof of Thm. \ref{lb405}, one can use Lem. \ref{lb423} and  \ref{lb426} (the improper version of Lem. \ref{lb400} and \ref{lb403}) to show that  $\int_I T\circ\Psi(f)=T\int_I\Psi(f)$ (which is a consequence of the commutativity of \eqref{eq182}) implies \eqref{eq181}.
\end{proof}












\subsubsection{Commutativity of improper integrals and net limits}



\begin{thm}\label{lb428}
Let $(f_\alpha)_{\alpha\in\scr I}$ be a net in $\scr R^1(I,V)$. Let $f\in V^I$. Assume that the following conditions are true:
\begin{enumerate}[label=(\arabic*)]
\item On every compact subinterval of $I$, the net $(f_\alpha)$ converges uniformly to $f$.
\item There exists $g\in\scr R^1(I,\Rbb_{\geq0})$ such that $|f_\alpha|\leq g$ for all $\alpha\in\scr I$.
\end{enumerate}
Then $f\in\scr R^1(I,V)$, and $\dps\int_If=\lim_{\alpha\in\scr I}\int_If_\alpha$.
\end{thm}


\begin{proof}
By Cor. \ref{lb380}, $f\in\scr R(I,V)$. Since $(f_\alpha)$ converges pointwise to $f$, we clearly have $|f|\leq g$. So $f\in\scr R^1(I,V)$.  Choose any $\eps>0$. Since $\lim_{u\rightarrow a,v\rightarrow b}\int_Ig$ converges, there exists $u,v$ such that $a<u<v<b$ and
\begin{align*}
\int_a^ug+\int_v^bg=\int_Ig-\int_u^v g<\eps
\end{align*}
Thus, by Prop. \ref{lb427}, for each $\alpha$ we have $\Vert\int_a^uf_\alpha\Vert+\Vert\int_v^bf_\alpha\Vert<\eps$ and $\Vert\int_a^uf\Vert+\Vert\int_v^bf\Vert<\eps$. Since $f_\alpha$ converges uniformly to $f$ on $[u,v]$, by Cor. \ref{lb380}, we get $\lim_\alpha\Vert \int_u^vf_\alpha-\int_u^vf\Vert=0$. Thus
\begin{align*}
&\Big\Vert \int_If-\int_If_\alpha\Big\Vert=\Big\Vert \int_a^uf+\int_v^b f-\int_a^u f_\alpha-\int_v^b f_\alpha+\int_u^v(f-f_\alpha)\Big\Vert\\
\leq&2\eps+\Big\Vert\int_u^v(f-f_\alpha)\Big\Vert
\end{align*}
where the $\limsup_\alpha$ of the RHS is $2\eps$. Thus $\Vert\int_If-\int_If_\alpha\Vert$ converges to $0$ under $\limsup_\alpha$, and hence under $\lim_\alpha$.
\end{proof}

It is a good practice to prove Thm. \ref{lb434} using Thm. \ref{lb428}. (See Pb. \ref{lb430}.)



\begin{co}\label{lb429}
Let $Y$ be a topological space. Let $f\in C(I\times Y,V)$. Assume that there exists $h\in\scr R^1(I,\Rbb_{\geq0})$ such that
\begin{align}
\Vert f(x,y)\Vert\leq h(x)\qquad (\forall x\in I,y\in Y)
\end{align}
Then the map $\int_If(x,\cdot)dx$ (sending $y\in Y$ to $\int_If(x,y)dx$) is continuous.
\end{co}

\begin{proof}
By Def. \ref{lb188}-(1), we need to prove that for every net $(y_\alpha)$ in $Y$ converging to $y$, we have $\int_If(x,y)dx=\lim_\alpha\int_If(x,y_\alpha)dx$. By Thm. \ref{lb428}, it suffices to prove that $\lim_\alpha f(\cdot,y_\alpha)$ converges uniformly  on any compact subinterval $[u,v]\subset I$ to $f(\cdot,y)$. This is true because, by Thm. \ref{lb274}, the restriction of $f$ to $[u,v]\times Y$ can be viewed as an element of $C(Y,C([u,v],V))$.
\end{proof}


\begin{eg}
Compute $\dps F(t)=\int_0^{+\infty}e^{-tx}\cdot \frac{\sin x}xdx$ for all $t>0$.
\end{eg}

\begin{proof}[Solution]
We first compute $F'(t)$. Choose any $\delta>0$. Then for any $t\geq\delta$, the norms of $e^{-tx}\frac{\sin x}x$ and $\partial_t(e^{-tx}\frac{\sin x}x)=-e^{-tx}\sin x$ are both bounded by $e^{-\delta x}$, where the latter is absolutely integrable since $\int_0^{+\infty}e^{-\delta x}dx=\delta^{-1}<+\infty$. Thus, by Thm. \ref{lb434}, we have
\begin{align*}
&F'(t)=\int_0^{+\infty}-e^{-tx}\sin xdx=\int_0^{+\infty}\frac{e^{(-t+\im)x}-e^{(-t-\im)x}}{2\im}dx\\
=&\frac{t\sin x+\cos x}{1+t^2}e^{-tx}\Big|_{x=0}^{+\infty}=-\frac 1{1+t^2}
\end{align*} 
for all $t\geq\delta$ and $\delta>0$, and hence for all $t>0$. Therefore $F(t)=C-\arctan t$ for some $C\in\Rbb$. Let us determine $C$ using $C=\lim_{t\rightarrow+\infty}F(t)+\frac\pi2$. 

To compute $\lim_{t\rightarrow+\infty}F(t)$, it suffices to assume $t\geq1$. Then $|e^{-tx}\frac{\sin x}x|\leq e^{-x}$ where $\int_0^{+\infty} e^{-x}<+\infty$. Moreover, on every compact interval $[a,b]$ in $(0,+\infty)$ (where $0<a<b<+\infty$), $\lim_{t\rightarrow+\infty}e^{-tx}\frac{\sin x}x$ converges uniformly to $0$ since $|e^{-tx}\frac{\sin x}x|\leq e^{-ta}$. Thus, the assumptions in Thm. \ref{lb428} are satisfied, and hence
\begin{align*}
\lim_{t\rightarrow+\infty}F(t)=\int_0^{+\infty} \lim_{t\rightarrow +\infty}e^{-tx}\cdot \frac{\sin x}xdx=0
\end{align*}
This proves $C=\frac\pi 2$, and hence $F(t)=\frac\pi 2-\arctan t$.
\end{proof}






\subsection{Convolutions and smooth/polynomial approximation}


Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$.


\begin{df}
Let $X$ be a topological space. The \textbf{support} of a function $f\in V^X$ \index{00@@Support} \index{Supp@$\Supp(f)$} is defined to be the closure
\begin{align*}
\Supp(f)=\ovl{\{x\in X:f(x)\neq 0\}}
\end{align*}
Define $C_c(X,V)$ \index{Cc@$C_c(X,V)$} to be the set of continuous functions with \textbf{compact support}
\begin{align*}
C_c(X,V)=\{f\in C(X,V):\Supp(f)\text{ is compact} \}
\end{align*}
Unless otherwise stated, $C_c(X,V)$ is equipped with the $l^\infty$-norm.
\end{df}


\subsubsection{Convolutions and approximation of identity}


A goal of this section is to show that the elements of $C_c(\Rbb,V)$ can be approximated by smooth functions with compact supports, i.e., elements in \index{Cc@$C_c^\infty$}
\begin{align*}
C_c^\infty(\Rbb,V)=C^\infty(\Rbb,V)\cap C_c(\Rbb,V) 
\end{align*}
Indeed, we will do more. We shall prove the celebrated \textbf{Weierstrass approximation theorem}, which implies that elements in $C_c(\Rbb,V)$ can be approximated uniformly by polynomials on compact intervals. The proof we will give is also due to Weierstrass, which relies on an important construction called convolution: If $f:\Rbb\rightarrow V$ and $g:\Rbb\rightarrow\Fbb$, then their \textbf{convolution} \index{00@Convolution} is a functions $f*g:\Rbb\rightarrow V$ defined by
\begin{subequations}\label{eq188}
\begin{align}
(f*g)(x)\equiv (g*f)(x)=\int_\Rbb f(x-y)g(y)dy  \label{eq186}
\end{align}
whenever the above integral converges for every $x\in\Rbb$. Taking $y=x-t$, we get $\int_u^v f(x-y)g(y)dy=-\int_{x-u}^{x-v}f(t)g(x-t)dt=\int_{x-v}^{x-u}f(t)g(x-t)dt$. Letting $u\rightarrow-\infty,v\rightarrow+\infty$, we get
\begin{align}
(f*g)(x)=\int_\Rbb f(y)g(x-y)dy  \label{eq187}
\end{align}
\end{subequations}


The main idea of doing approximation via convolutions is as follows. Suppose that $g$ is smooth, then by \eqref{eq187}, one should expect that $(f*g)^{(n)}(x)=\int_\Rbb f(y)(\partial_x)^ng(x-y)dy=\int_\Rbb f(y)g^{(n)}(x-y)dy$ to be true. Thus $f*g$ is expected to be smooth. Moreover, if $g$ can be approximated by polynomials, for example, if $g(x)=\sum a_nx^n$ on $\Rbb$, then it is expected that $f*g$ can be approximated by $f*g_n$ where $g_n(x)=a_nx^n$, and it is easy to see that $f*g_n=a_n\int_\Rbb f(y)(x-y)^ndy$ is a polynomial of $x$.

To summarize, one advantage of convolution is that whenever $g$ has a good property, the same is in general true for $f*g$. Another key property of convolution is that $f$ can be approximated by $f*g$ in some sense. The meaning of ``approximation" will depend on the analytic property of $f$, e.g. whether $f$ is continuous, continuous with compact support, or only integrable. In this section, we will only be interested in the case that $f\in C_c(\Rbb,V)$. In this case, the integrals in \eqref{eq188} are actually over compact intervals, which makes our lives easier. 

We shall show that $f\in C_c(\Rbb,V)$ can be approximated uniformly by $f*g$. More precisely, we shall show that for every absolutely convergent $g\in C(\Rbb,\Rbb_{\geq0})$ satisfying $\int_\Rbb g=1$, if we define $g_\eps\in C(\Rbb,\Rbb)$ (where $\eps\in\Rbb_{>0}$) by
\begin{align}
g_\eps(x)=\frac 1\eps g\big(\frac x\eps\big)  \label{eq189}
\end{align}
then $(f*g_\eps)$ converges uniformly to $f$ as $\eps\rightarrow 0$. This method is extremely useful and is used widely in analysis. (The assumption that $g\geq0$ is not necessary. We assume $g\geq0$ only for simplifying discussions.)








\begin{lm}\label{lb436}
Choose $g\in C(\Rbb,\Rbb_{\geq0})$ satisfying $\int_\Rbb g=1$, and define $g_\eps$ by \eqref{eq189}. Then $\int_\Rbb g_\eps=1$ for each $\eps>0$. Moreover, for every $\delta>0$ we have
\begin{align}
\lim_{\eps\rightarrow0}\int_\delta^{+\infty} g_\eps=\lim_{\eps\rightarrow0}\int_{-\infty}^{-\delta}g_\eps=0 \label{eq190}
\end{align}
\end{lm}


\begin{proof}
By the change of variable formula we have $\int_u^v \eps^{-1}g(\eps^{-1}x)dx=\int_{u/\eps}^{v/\eps}g(y)dy$. This gives $\int_\Rbb g_\eps=1$. Another change of variable shows $\int_\delta^{+\infty}g_\eps=\int_{\delta/\eps}^{+\infty}g=\int_0^{+\infty}g-\int_0^{\delta/\eps}g$, which clearly converges to $0$ as $\eps\rightarrow0$. This proves the first half of \eqref{eq190}. The second half is similar.
\end{proof}


%% Record #21 2023/12/4 two lectures  52


\begin{pp}\label{lb437}
Let $f\in C_c(\Rbb,V)$. Choose $g\in C(\Rbb,\Rbb_{\geq0})$ satisfying $\int_\Rbb g=1$, and define $g_\eps$ by \eqref{eq189} for each $\eps>0$. Then $(f*g_\eps)$ converges uniformly on $\Rbb$ to $f$ as $\eps\rightarrow0$.
\end{pp}

In other words, the convolution operator $f\mapsto f*g_\eps$ converges pointwise to the identity map when $\eps\rightarrow 0$.

\begin{proof}
Let $M=\Vert f\Vert_\infty$, which is $<+\infty$. Since $\int_\Rbb g_\eps=1$, for each $x\in\Rbb$ we have $\int_\Rbb f(x)g_\eps(y)dy=f(x)$. Thus
\begin{align}
&\Vert (f*g_\eps)(x)-f(x)\Vert=\Big\Vert \int_\Rbb(f(x-y)-f(x))g_\eps(y)dy \Big\Vert\nonumber\\
\leq &\int_\Rbb  \Vert f(x-y)-f(x)\Vert \cdot g_\eps(y)dy \label{eq191}
\end{align}
By Thm. \ref{lb294} and the compactness of $\Supp(f)$, $f$ is uniformly continuous on $\Rbb$. Therefore, for every $e>0$, there exists $0<\delta<1$ such that $\Vert f(x-y)-f(x)\Vert\leq e$ for all $x\in\Rbb$ and $y\in(-\delta,\delta)$. Thus, if we let $J_\delta=\Rbb\setminus(-\delta,\delta)$, then
\begin{align*}
&\eqref{eq191}\leq \int_{J_\delta}\Vert f(x-y)-f(x)\Vert \cdot g_\eps(y)dy+e\cdot \int_{-\delta}^\delta g_\eps(y)dy\\
\leq& 2M\cdot \int_{J_\delta} g_\eps(y)dy+e 
\end{align*}
which converges to $e$ under $\limsup_{\eps\rightarrow0}$ by Lem. \ref{lb436}. Since $e$ is arbitrary, we conclude that $\lim_{\eps\rightarrow0}\eqref{eq191}=0$.
\end{proof}

\begin{rem}
The uniform continuity of a function $f:\Rbb\rightarrow V$ is equivalent to the continuity of $F:\Rbb\rightarrow C(\Rbb,V)$ defined by $F(t)(x)=f(x-t)$. Note that the continuity of $F$ follows from Thm. \ref{lb274}. This means that Prop. \ref{lb437} can also be proved by Thm. \ref{lb274}.
\end{rem}




\begin{rem}
In analysis, there are two especially important classes of  $g\in C(\Rbb,\Rbb_{\geq0})$ satisfying $\int_\Rbb g=1$. The first class consists of $g\in C_c(\Rbb,\Rbb_{\geq0})$ satisfying $\int_\Rbb g=1$. Although functions with compact supports are often easy to use, they cannot be approximated by their Taylor series on $\Rbb$. (See Exp. \ref{lb358} for a related example.) Instead, we should consider another type of function:
\end{rem}


\subsubsection{Polynomial approximation}



\begin{eg}
Define the \textbf{Gauss function} $\dps g(x)=\frac 1{\sqrt\pi}e^{-x^2}$. \index{00@Gauss function} Then $g\in C(\Rbb,\Rbb_{\geq0})$ and $\int_\Rbb g=1$. Thus, $g$ satisfies the assumptions in Prop. \ref{lb437}. 
\end{eg}

Although $g$ does not have compact support, it is a real analytic function. So we can use $f*g_\eps$ to prove Weierstrass approximation theorem.

\begin{proof}
It is easy to see that $\int_1^{+\infty}1/x^2dx<+\infty$. Thus, since $e^{x^2}\geq x^2$, we get $\int_\Rbb e^{-x^2}dx<+\infty$.

It is not so easy to show that 
\begin{align} \label{eq172}
\int_{-\infty}^{+\infty}e^{-x^2}dx=\sqrt\pi
\end{align}
This integral is called  \textbf{Gauss integral}. \index{00@Gauss integral} In the next semester, we will use the change of variable formula for double integrals to prove \eqref{eq172}.
A more elementary (but also more complicated) proof is given in Pb. \ref{lb435}. For the purpose of this section, knowing $\int_\Rbb e^{-x^2}dx<+\infty$ is enough, since we can define $g$ to be $e^{-x^2}$ divided by its integral on $\Rbb$.
\end{proof}



We now show that if $g$ is the Gauss function, then $f*g_\eps$ can be approximated uniformly by polynomials on every compact interval. 

\begin{lm}\label{lb438}
Let $h(x)=\sum_{n=0}^\infty a_nx^n$ have radius of convergence $+\infty$, where $a_n\in\Fbb$ for each $a_n$. Let $h_n(x)=\sum_{j=0}^n a_jx^j$ be the partial sum. Let $f\in C_c(\Rbb,V)$. Then $\lim_{n\rightarrow\infty}f*h_n$ converges uniformly on compact intervals to $f*h$, and each $f*h_n$ is in \index{Vx@$V[x]$}
\begin{align}
V[x]=\{v_0+v_1x+\cdots+v_kx^k:k\in\Nbb,v_0,v_1,\dots,v_k\in V\}
\end{align}
\end{lm}

Note that we do not need the assumption $\int_\Rbb |h|<+\infty$ in this Lemma. So letting $h(x)=e^x$ is also OK. Clearly, if $g$ is the Gauss functions, then $g_\eps$ satisfies the assumptions on $h$. This implies that if $f\in C_c(\Rbb,V)$, then $f*g_\eps$ can be approximated uniformly on compact intervals by  polynomials with coefficients in $V$. Combining this fact with Prop. \ref{lb437}, we see that $f$ can be approximated uniformly by polynomials on compact intervals.

\begin{proof}
Note that $(f*h)(x)=\int_\Rbb h(x-y)f(y)dy$ is well-defined since it is equals $\int_{-a}^a h(x-y)f(y)dy$ if $a>0$ and if $[-a,a]$ contains $\Supp(f)$. (So this is a usual Riemann integral.) Similarly, $(f*h_n)(x)=\int_{-a}^a h_n(x-y)f(y)dy$. Since $h_n$ is a polynomial, it is obvious that $f*h_n$ is also a polynomial.

We now show that $f*h_n$ converges uniformly to $f*h$ on $[-b,b]$ for any $b>0$. Let $c=a+b$. Then 
\begin{align*}
&\sup_{x\in[-b,b]}\Vert (f*h)(x)-(f*h_n)(x)\Vert\leq\sup_{x\in[-b,b]}\int_{-a}^a |h(x-y)-h_n(x-y)|\cdot\Vert f(y)\Vert dy\\
\leq&A_n\int_{-a}^a\Vert f(y)\Vert dy
\end{align*}
where $A_n=\sup_{t\in[-c,c]}\Vert h(t)-h_n(t)\Vert$. By Thm. \ref{lb112}, $h_n$ converges uniformly on $[-c,c]$ to $h$. So $\lim_{n\rightarrow\infty}A_n=0$.
\end{proof}


\begin{thm}[\textbf{Weierstrass approximation theorem}]\index{00@Weierstrass approximation theorem}  \label{lb441}
Let $I=[a,b]$ where $-\infty<a<b<+\infty$. Then $V[x]$ is dense in $C(I,V)$ under the $l^\infty$-norm.
\end{thm}

\begin{proof}
Choose any $f\in C(I,V)$. Then $f$ can be extended to a continuous function $\Rbb\rightarrow V$ with compact support: For example, we let $f(x)=(x-a+1)f(a)$ if $x\in[a-1,a]$, let $f(x)=(b+1-x)f(b)$ if $b\in[b,b+1]$, and let $f(x)=0$ if $x<a-1$ or $x>b+1$. Let $g$ be the Gauss function. For every $e>0$, by Prop. \ref{lb437}, there exists $\eps>0$ such that $\Vert f-f*g_\eps\Vert_{l^\infty(\Rbb,V)}<e/2$. By Lem. \ref{lb438}, there exists a polynomial $p\in V[x]$ such that $\Vert f*g_\eps-p\Vert_{l^\infty(I,V)}<e/2$. So $\Vert f-p\Vert_{l^\infty(I,V)}<e$.
\end{proof}


\begin{co}\label{lb486}
Let $I=[a,b]$ where $-\infty<a<b<+\infty$. Then $C([0,1],\Rbb)$ is separable.
\end{co}

\begin{proof}
$\Qbb[x]$, the set of polynomials with coefficients in $\Qbb$, is countable. By Thm. \ref{lb441}, $\Qbb[x]$ is dense in $C([0,1],\Rbb)$.
\end{proof}

The Weierstrass approximation theorem will be generalized to \textbf{Stone-Weierstrass theorem} (cf. Thm. \ref{lb442}). Accordingly, Cor. \ref{lb486}  will be substantially generalized as an application of (the proof of) Stone-Weierstrass theorem: we will show that $C(X,\Rbb)$ is separable if $X$ is a compact metric space. (See Thm. \ref{lb482}) 



\subsubsection{Smooth approximation}

By the Weierstrass approximation theorem, we know that any $f\in C_c(\Rbb,V)$ can be approximated uniformly by polynomials on compact intervals. However, unless $f=0$, $f$ cannot be approximated by polynomials uniformly on $\Rbb$. (If $p,q\in V[x]$ are different, then $\Vert p-q\Vert_{l^\infty(X,V)}=+\infty$. So any Cauchy sequence in $V[x]$ under the ${l^\infty(X,V)}$-norm is eventually constant. So its limit is a polynomial, which does not have compact support unless when it is zero.) Nevertheless, we shall show that $f$ can be approximated by smooth compactly supported functions uniformly on $\Rbb$. This task is not difficult: one simply pick a nonzero $g\in C_c^\infty(\Rbb,\Rbb)$ satisfying $g\geq0$. Dividing $g$ by $g/\int_\Rbb g$, we may assume that $\int_\Rbb g=1$. Then it can be shown that $f*g_\eps\in C_c^\infty(\Rbb,V)$. By Prop. \ref{lb437}, $\lim_{\eps\rightarrow0}f*g_\eps$ converges uniformly on $\Rbb$ to $f$. This finishes the proof. However, we must first prove the existence of such $g$:



\begin{pp}\label{lb440}
Let $0<a<b$. Then there exists $g\in C_c^\infty(\Rbb,\Rbb)$ such that $g(\Rbb)=[0,1]$, that $g^{-1}(1)=[-a,a]$, and that $g^{-1}(0)=(-\infty,-b]\cup[b,+\infty)$.
\end{pp}



\begin{proof}
Let $\alpha:\Rbb\rightarrow\Rbb_{\geq0}$ be the smooth function in Exp. \ref{lb358}. Then $\alpha$ is increasing and $\alpha^{-1}(0)=(-\infty,0]$.  Define $\beta:\Rbb\rightarrow\Rbb_{\geq0}$ by $\beta(x)=\alpha(x+b)\alpha(-x+b)$. Then $\beta^{-1}(0)=(-\infty,-b]\cup[b,+\infty)$. In particular, $\beta\in C_c^\infty(\Rbb,\Rbb)$. Define $\gamma:\Rbb\rightarrow\Rbb_{\geq0}$ by $\gamma(x)=\alpha(x-a)+\alpha(-x-a)$. Then $\gamma^{-1}(0)=[-a,a]$. Note that $\beta(x)+\gamma(x)>0$ for all $x\in\Rbb$, since $\beta^{-1}(0)\cap\gamma^{-1}(0)=\emptyset$. 
\begin{align*}
\vcenter{\hbox{{
			\includegraphics[height=1.3cm]{fig3.png}}}}
\end{align*}
Then $g=\beta/(\beta+\gamma)$ is a desired function.
\end{proof}


\begin{co}\label{lb446}
Let $I$ be an interval in $\Rbb$ with $a=\inf I$ and $b=\sup I$ satisfying $-\infty\leq a<b\leq+\infty$. Then $C_c^\infty(I,V)$ is dense in $C_c(I,V)$ under the $l^\infty$-norm.
\end{co}

In the following, we only prove the case that $I=(a,b)$. The other cases can be reduced to this case: For example, the case $[a,b)$ is implied by the case $(a-1,b)$.

\begin{proof}[First proof]
Let $f\in C_c(\Rbb,V)$ supported in $[u,v]$ where $a<u<v<b$. By Prop. \ref{lb440}, there exists a nonzero smooth $g\in C_c(\Rbb,\Rbb_{\geq0})$ supported in $[-1,1]$ such that $\int_\Rbb g=1$. Since $g_\eps$ is supported in $[-\eps,\eps]$, it is easy to see that $f*g_\eps$ is supported in $[u-\eps,v+\eps]$. By Lem. \ref{lb400}, $f*g_\eps$ is continuous . Moreover, one can check that $f*g_\eps$ is smooth (cf. Pb. \ref{lb439}). So $f*g_\eps\in C_c^\infty(\Rbb,V)$, and $f*g$ is supported in $I$ when $\eps<\min\{u-a,b-v\}$. By Prop. \ref{lb437}, $\lim_{\eps\rightarrow 0}f*g_\eps$ converges uniformly on $\Rbb$ to $f$.
\end{proof}



\begin{proof}[Second proof]
Let $f\in C_c(\Rbb,V)$ supported in $[u,v]$ where $a<u<v<b$. Choose any $e>0$. Choose any positive $\eps<\min\{u-a,b-v\}$. By Weierstrass approximation Thm. \ref{lb441}, there exists $p\in V[x]$ such that $\Vert f(x)- p(x)\Vert\leq e$ for all $x\in[u-\eps,v+\eps]$. In particular, since $f=0$ on $J=[u-\eps,u]\cup[v,v+\eps]$, we have $\Vert p(x)\Vert\leq e$ for all $x\in J$. 

By Prop. \ref{lb440}, there exists $h\in C_c^\infty(\Rbb,\Rbb)$ such that $h(\Rbb)=[0,1]$, that $h^{-1}(1)=[u,v]$, and that $h^{-1}(0)=(-\infty,u-\eps]\cup[v+\eps,+\infty)$. Then $hp\in C_c^\infty(\Rbb,V)$ is supported in $[u-\eps,v+\eps]$ and hence in $(-a,a)$. One checks easily that $\Vert f(x)-h(x)p(x)\Vert\leq e$ for all $x\in(a,b)$.
\end{proof}


\begin{rem}
It should be noted that if $I$ is an open interval, then $C_c^\infty(I,V)$ and $C_c(I,V)$ are naturally subspaces of $C_c(\Rbb,V)$. This is not true when $I$ is a closed or an half-open-half-closed interval. Therefore, Cor. \ref{lb446} implies that any $f\in C(\Rbb,V)$ supported in an open interval $I$ can be uniformly approximated by smooth functions $\Rbb\rightarrow V$ supported in $I$.
\end{rem}


\begin{rem}
The above observation can be generalized: Let $X$ be an LCH space. Assume that $\Omega$ is a nonempty \textit{open} subset of $X$. Recall that $\Omega$ is LCH by Prop. \ref{lb245}. Then an element of $C_c(\Omega,V)$ is equivalently an element of $C_c(X,V)$ supported in $U$. The former gives the latter by ``extension by zero"; the latter gives the former by restriction to $\Omega$. We will say more about this in Sec. \ref{lb464}.
\end{rem}





\subsection{$L^1$-approximation; Riemann-Lebesgue lemma}


A non-continuous integrable function cannot be approximated uniformly by smooth functions. However, it can be approximated by the latter under the $L^1$-norm. To avoid distraction, in this section we consider $\Cbb$-valued functions. Fix $I\subset\Rbb$ to be an interval containing at least two points. Then on $\scr R^1(I,\Cbb)$ one can define the \textbf{$L^1$-seminorm} \index{L1@$L^1$-seminorm} to be
\begin{align}
\Vert f\Vert_{L^1}\equiv\Vert f\Vert_{L^1(I,\Cbb)}=\int_I |f|
\end{align}
It is easy to check that this is a \textbf{seminorm}, i.e., it satisfies the definition of a norm, except the assumption that $\Vert f\Vert_{L^1}=0$ implies $f=0$.

\begin{pp}\label{lb445}
Let $f\in\scr R^1(I,\Cbb)$. Choose any $\eps>0$. Then there exists $g\in C_c^\infty(\Rbb,\Cbb)$ supported in $I$ such that $\Vert f-g\Vert_{L^1}<\eps$. And there exists a \textbf{step function} $h$ supported in $I$ (i.e., a linear combination of functions of the form $\chi_E$ where $E$ is a compact interval in $I$) such that $\Vert f-h\Vert_{L^1}<\eps$.
\end{pp}

Note that if $E$ is a bounded interval, then $\chi_E$ is clearly a linear combination of characteristic functions over compact intervals (including the single point sets). Thus, in the above definition of step functions, one can just assume that $E$ is a bounded interval whose closure is in $I$.

The following proof shows that Prop. \ref{lb445} can be easily generalized to the case that $f:I\rightarrow V$ is strongly integrable on compact subintervals of $I$ and $\int_I|f|<+\infty$. ($V$ is a Banach space.)

\begin{proof}
Let $a=\inf I,b=\sup I$. Since $\int_I|f|=\lim_{u\rightarrow a,v\rightarrow b} \int_a^b|f|$, there exist $u,v$ such that $a<u<v<b$ and that $\int_a^u|f|+\int_v^b|f|<\eps/2$. We claim that there exists a step function $h$ supported in $J=[u,v]$ such that $\int_J |f-h|<\eps/2$. Then $\int_I|f-h|<\eps$, finishing the proof that $f$ can be $L^1$-approximated by step functions supported in $I$.

Since $f|_J$ is strongly Riemann integrable, there exists $\sigma=\{a_0<\cdots<a_n\}\in\mc P(J)$ such that $\omega(f,\sigma)<\frac\eps 2$. Let $J_i=[a_{i-1},a_i]$. Pick any $\lambda_i\in f(J_i)$. Then $|f-\lambda_i\cdot\chi_{J_i}|\leq\diam(f(J_i))$ on $J_i$, and hence $\int_{J_i}|f-\lambda_i\cdot\chi_{J_i}|\leq \diam(f(J_i))\cdot|J_i|$. Let $h=\sum_{i=1}^n\lambda_i\cdot\chi_{J_i}$. It follows that
\begin{align*}
\Big|\int_J f-\int_Jh  \Big|\leq\sum_{i=1}^n\int_{J_i}|f-\lambda_i\cdot\chi_{J_i}|\leq\sum_{i=1}^n\diam(f(J_i))\cdot|J_i|=\omega(f,\sigma)<\frac\eps 2
\end{align*}
finishing the proof.

Finally, we show that $f$ can be $L^1$-approximated by smooth functions supported in $I$. It suffices to prove that any step function supported in $I$ is so. By linearity and triangle inequality, it suffices to prove that $\chi_E$ is so, if $E$ is a nonempty compact interval in $I$. For each $\eps>0$, it is easy to construct a piecewise linear (and hence continuous) function $g_0:\Rbb\rightarrow\Rbb$ supported in $(a,b)=\Int(I)$ such that $\int_I|\chi_E-g_0|<\eps$. Choose a bounded open interval $(c,d)\subset (a,b)$ containing $\Supp(g_0)$. By Cor. \ref{lb446}, there exists $g\in C_c^\infty(\Rbb,\Rbb)$ supported in $(c,d)$ such that $|g(x)-g_0(x)|<\eps/(d-c)$ for all $x\in(c,d)$. It follows that
\begin{align*}
\int_I|g-g_0|=\int_c^d |g-g_0|\leq\eps
\end{align*}
Thus $\int_I|\chi_e-g|<2\eps$, finishing the proof.
\end{proof}




We give an application of Prop. \ref{lb445} by proving Riemann-Lebesgue lemma. The RL lemma plays a fundamental role in Fourier analysis, see for instance Pb. \ref{lb447} and Cor. \ref{lb448}. 


\begin{thm}[\textbf{Riemann-Lebesgue lemma}] \index{00@Riemann-Lebesgue lemma}
Let $f\in\scr R^1(\Rbb,\Cbb)$. Then
\begin{align*}
\lim_{t\rightarrow+\infty}\int_\Rbb f(x)e^{\im tx}dx=\lim_{t\rightarrow-\infty}\int_\Rbb f(x)e^{\im tx}dx=0
\end{align*}
\end{thm}

The idea of the proof is the following: We $L^1$-approximate $f$ by some compactly supported step function $g\in C_c(\Rbb,\Cbb)$. Then it suffices to prove the RL lemma for $g$. By linearity, it suffices to assume that $g=\chi_{[a,b]}$. This special case can be proved easily. 

\begin{proof}
By Prop. \ref{lb445}, for every $\eps>0$ there exists a compactly supported step function $g:\Rbb\rightarrow\Cbb$ such that  $\int_\Rbb|f-g|<\eps$. So $|\int_\Rbb f(x)e^{\im tx}-g(x)e^{\im tx}dx|<\eps$ by Prop. \ref{lb427}. Thus
\begin{align*}
\limsup_{t\rightarrow\pm\infty} \Big| \int_\Rbb f(x)e^{\im tx}dx\Big|\leq \limsup_{t\rightarrow\pm\infty} \Big|\int_\Rbb g(x)e^{\im tx}dx\Big|+\eps
\end{align*}
Since $\eps$ is arbitrary, it suffices to prove that $\lim_{t\rightarrow\pm\infty}\int_\Rbb g(x)e^{\im tx}dx=0$. Since $g$ is a linear combination of functions of the form $\chi_{[a,b]}$ where $-\infty<a<b<+\infty$, it suffices to prove the RL lemma for this function:
\begin{align*}
\int_\Rbb\chi_{[a,b]}\cdot e^{\im tx}dx=\int_a^b e^{\im tx}dx=\frac{e^{\im nb}-e^{\im na}}{\im t}
\end{align*}
converges to $0$ as $t\rightarrow\pm\infty$.
\end{proof}

\begin{comment}
\begin{rem}
The proof of Prop. \ref{lb445} also shows that $f\in\scr R^1(\Rbb,\Cbb)$ can be approximated by linear combinations of functions of the form $\chi_J$ (where $J$ is an interval) under the $L^1$-norm. Thus, the Riemann-Lebesgue lemma can also be proved by showing that $\lim_{t\rightarrow\pm\infty}\int_\Rbb \chi_J(x)e^{\im tx}dx=0$. For such a proof one does not need SW theorem. However, in the general measure theory, it is often more convenient to approximate an integrable function by continuous compactly supported ones. For example, if a function is defined on a manifold (e.g., a sphere, or a more complicated geometric object), it is not easy to approximate such a function by a linear combination of $\chi_E$ where $E$ is a set as simple as an interval. 
\end{rem}
\end{comment}















\subsection{Problems and supplementary material}



Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.

\begin{prob}
Solve Exe. \ref{lb408}.
\end{prob}

\begin{prob}\label{lb430}
Prove Thm. \ref{lb434} using Thm. \ref{lb428}.
\end{prob}


\begin{sprob}\label{lb435}
Define $\dps f(x)=\Big(\int_0^x e^{-t^2}dt\Big)^2$ and $\dps g(x)=\int_0^1\frac{e^{-x^2(t^2+1)}}{t^2+1}dt$
\begin{enumerate}
\item Show that $f'+g'=0$, and conclude that $f+g=\frac\pi 4$.
\item Use part 1 to prove
\begin{align}
\int_{-\infty}^{+\infty}e^{-t^2}dt=\sqrt\pi 
\end{align}
\end{enumerate}
\end{sprob}


\begin{prob}\label{lb439}
Let $f:\Rbb\rightarrow V$ be strongly integrable on compact intervals, and assume $\int_\Rbb |f|<+\infty$. Let $g\in C^1(\Rbb,\Rbb)$. Assume that $\Vert g\Vert_\infty,\Vert g'\Vert_\infty<+\infty$. (This is automatic when $g$ has compact support.) Use Thm. \ref{lb428} to prove that $f*g$ is well-defined, that $f*g\in C^1(\Rbb,\Rbb)$, and that $(f*g)'=f*g'$.
\end{prob}

\begin{proof}[Note]
Since $f$ is not assumed to be continuous, you cannot use Thm. \ref{lb434} \textit{directly} to compute $(f*g)'$ and to show $f*g\in C^1$. You have two options: (1) Use Thm. \ref{lb428}. (2) Use Prop. \ref{lb445} to  $L^1$-approximate $f$ by a continuous compactly-supported function. Then apply Thm. \ref{lb434} (together with Cor. \ref{lb429}) to that continuous function. Whichever method you use, I suggest you think about how you can use the other method to solve the problem.
\end{proof}


\begin{rem}
In particular, if $g\in C_c^\infty(\Rbb,\Rbb)$, then the above problem shows that $f*g\in C^\infty(\Rbb,\Rbb)$ and $(f*g)^{(n)}=f*g^{(n)}$ for all $n\in\Nbb$.
\end{rem}



\begin{sprob}
The translation of each $f\in\scr R^1(\Rbb,\Cbb)$ by $t\in\Rbb$ is defined to be
\begin{align*}
f_t:\scr R^1(\Rbb,\Cbb)\rightarrow\scr R^1(\Rbb,\Cbb)\qquad f_t(x)=f(x-t)
\end{align*}
Prove that the translation map is continuous under the $L^1$-seminorm, in the sense that $\lim_{t\rightarrow0}\Vert f-f_t\lVert_{L^1}=0$. Namely, prove that
\begin{align*}
\lim_{t\rightarrow 0}\int_\Rbb |f-f_t|=0
\end{align*}
\end{sprob}

\begin{proof}[Hint]
Use Prop. \ref{lb445} to $L^1$-approximate $f$ by compactly supported continuous functions or step functions. (Both types of functions will work.)
\end{proof}


\begin{df}
Let $f\in\scr R([-\pi,\pi],\Cbb)$. For each $n\in\Zbb$, define the $n$-th \textbf{Fourier coefficient} \index{fn@$\wht f(n)$} to be
\begin{align*}
\wht f(n)=\frac 1{2\pi}\int_{-\pi}^\pi f(x)e^{-\im nx}dx
\end{align*}
We call $\dps \sum_{n=-\infty}^{+\infty}\wht f(n)e^{\im nx}$ the \textbf{Fourier series} \index{00@Fourier series} of $f$.
\end{df}



\begin{df}
For each $N\in\Nbb$, define the \textbf{Dirichlet kernel} \index{00@Dirichlet kernel}
\begin{align*}
D_N(x)=\sum_{n=-N}^Ne^{\im nx}=\frac{\sin(N+\frac 12)x}{\sin(x/2)}
\end{align*}
(When $x\in2\pi\Zbb$, the RHS above should be $2N+1$.) From $\frac 1{2\pi}\int_{-\pi}^\pi e^{\im nx}dx=\delta_{n,0}$ we easily see $\dps\frac 1{2\pi}\int_{-\pi}^\pi D_N(x)dx=1$. 
\end{df}


\begin{prob}\label{lb447}
Let $f\in\scr R(\Rbb,\Cbb)$ have period $2\pi$. Define $\dps s_N(f;x)=\sum_{n=-N}^N\wht f(n)e^{\im nx}$.
\begin{enumerate}
\item Prove that $s_N(f;x)=(f*D_N)(x)$ where the convolution is defined with respect to the integral $\frac 1{2\pi}\int_I$ where $I$ is any interval of length $2\pi$. (Note that both $f$ and $D_N$ have period $2\pi$. So translating $I$ does not affect the result.) In other words, prove that
\begin{align}
s_N(f;x)=\frac 1{2\pi}\int_{-\pi}^{\pi} f(t)D_N(x-t)dt=\frac 1{2\pi}\int_{-\pi}^{\pi} f(x-t)D_N(t)dt
\end{align}
\item Fix $x\in\Rbb$. Assume that there exist $A,B\in\Cbb$ such that 
\begin{align}
\limsup_{t\rightarrow 0^+}\Big|\frac{f(x-t)-A}{t}\Big|<+\infty\qquad \limsup_{t\rightarrow 0^+}\Big|\frac{f(x+t)-B}{t}\Big|<+\infty
\end{align}
(In other words, assume that there exist $\delta,M>0$ such that $|(f(x-t)-A)/t|\leq M$ and $|(f(x+t)-B)/t|\leq M$ for all $0<t<\delta$.) Prove that
\begin{align}
\lim_{N\rightarrow+\infty} s_N(f;x)=\frac{A+B}2
\end{align}
\end{enumerate}
\end{prob}


\begin{proof}[Hint for part 2]
Choose $g:[-\pi,\pi]\rightarrow\Cbb$ such that $g(t)=B$ if $t<0$, and $g(t)=A$ if $t>0$. Prove that
\begin{align}
\frac 1{2\pi}\int_{-\pi}^\pi g(t)D_N(t)dt=\frac{A+B}2  \label{eq193}
\end{align}
Let $\varphi:[-\pi,\pi]\rightarrow \Rbb$ such that $\dps\varphi(t)=\frac{f(x-t)-g(t)}{\sin(t/2)}$ if $t\neq 0$. Use Lebesgue's criterion to show that $\varphi\in\scr R([-\pi,\pi],\Cbb)$. Use Riemann-Lebesgue lemma to prove $\dps\lim_{N\rightarrow+\infty}\int_{-\pi}^\pi \varphi(t)\sin\big(N+\frac 12\big)t\cdot dt=0$. %Combine this with \eqref{eq193} to finish the proof.
\end{proof}


\begin{co}\label{lb448}
Let $g\in C^1([a-\pi,a+\pi],\Cbb)$. Let $f:\Rbb\rightarrow\Cbb$ be a $2\pi$-periodic function such that $f(x)=g(x)$ if $a-\pi<x<a+\pi$, and that $\dps f(a+\pi)=\frac{g(a-\pi)+g(a+\pi)}2$. Clearly $f\in\scr R(\Rbb,\Cbb)$. Then the Fourier series of $f$ converges pointwise to $f$.
\end{co}

\begin{proof}
Immediate from part 2 of Pb. \ref{lb447}.
\end{proof}

\begin{eg}
If we let $a=0$ and $g(x)=x$, then one can compute that the Fourier series of $f$ is \eqref{eq194}. It converges pointwise but not uniformly to $f$.
\end{eg}



















\newpage



\section{A topological proof of the Stone-Weierstrass theorem}



\subsection{*-algebras and subalgebras}

Fix $\Fbb\in\{\Rbb,\Cbb\}$.


\begin{df}
An \textbf{$\Fbb$-algebra} \index{00@Algebra}  is defined to be a ring $\scr A$ (not necessarily having $1$) which is at the same time also an $\Fbb$-vector space (where the vector addition is equal to the ring addition) such that the ring multiplication and the scalar multiplication satisfy the associativity: For every $\lambda\in\Fbb$ and $x,y\in\scr A$, we have
\begin{align}
\lambda(xy)=(\lambda x)y=x(\lambda y)
\end{align}

An $\Fbb$-algebra is called \textbf{unital} \index{00@Unital algebra} if $\scr A$, as a ring, has the identity $1$. In this case, we write $\lambda\cdot 1$ as $\lambda$ if $\lambda\in\Fbb$. 

An $\Fbb$-algebra is called \textbf{commutative} or \textbf{abelian} \index{00@Commutative algebra} \index{00@Abelian algebra} if $xy=yx$ for all $x,y\in\scr A$.

If $\scr A$ is an $\Fbb$-algebra, then an \textbf{($\Fbb$-)subalgebra} \index{00@Subalgebra} is a subset $\scr B$ which is invariant under the ring addition, ring multiplication, and scalar multiplication. (Namely, $\scr B$ is a subring and also a subspace of $\scr A$.) If $\scr A$ is unital, then a \textbf{unital ($\Fbb$-)subalgebra} of $\scr A$ is an $\Fbb$-subalgebra containing the identity of $\scr A$.  \hfill\qedsymbol
\end{df}


\begin{rem}
A unital $\Fbb$-algebra $\scr A$ is equivalently a ring with unit $1$, together with a ring homomorphism $\Cbb\rightarrow Z(\scr A)$ where $Z(\scr A)$ is the \textbf{center} of $\scr A$, i.e.
\begin{align*}
Z(\scr A)=\{x\in\scr A:xy=yx\text{ for every }y\in\scr A\}
\end{align*}
We leave it to the readers to check the equivalence.
\end{rem}


\begin{eg}
If $V$ is a $\Fbb$-vector space, then $\End(V)$, the set of $\Fbb$ linear maps $V\rightarrow V$, is naturally an $\Fbb$-algebra. If $V$ is a normed vector space, then $\fk L(V)$ is an $\Fbb$-algebra.
\end{eg}

\begin{df}\label{lb586}
A \textbf{(complex) *-algebra} \index{00@*-algebra} is defined to be a $\Cbb$-algebra together with an \textbf{antilinear map} \index{00@Antilinear map} $*:\scr A\rightarrow\scr A$ sending $x$ to $x^*$ (where ``antilinear" means that for every $a,b\in\Cbb$ and $x,y\in\scr A$ we have $(ax+by)^*=\ovl ax^*+\ovl by^*$) such that for every $x,y\in\scr A$, we have
\begin{align*}
(x^*)^*=x\qquad (xy)^*=y^*x^*
\end{align*}
Note that $*$ must be bijective. We call $*$ an \textbf{involution}. 
\index{00@Involution} A \textbf{*-subalgebra} \index{00@*-subalgebra} $\scr B$ is defined to be a subalgebra satisfying $x\in\scr B$ iff $x^*\in\scr B$. If $\scr A$ is a unital algebra with unit $\idt$, we say that $\scr A$ is a \textbf{unital *-algebra} if $\scr A$ is equipped with an involution $*:\scr A\rightarrow\scr A$ such that $\scr A$ is a *-algebra, and that
\begin{align*}
\idt^*=\idt
\end{align*}
A unital *-subalgebra is a unital subalegbra and also a *-subalgebra.
\end{df}


\begin{eg}
The set of complex $n\times n$ matrices $\Cbb^{n\times n}$ is naturally a unital $*$-algebra if for every $A\in\Cbb^{n\times n}$ we define $A^*=\ovl A^\tr$, the complex conjugate of the transpose of $A$.
\end{eg}

In this chapter, we are mainly interested in abelian algebras.

\begin{eg}
Let $X$ be a set. Then $\Fbb^X$ is naturally a unital $\Fbb$-algebra, and $l^\infty(X,\Cbb)$ is its unital $\Fbb$-subalgebra. If $X$ is a topological space, then $C(X,\Fbb)$ is a unital $\Fbb$-subalgebra of $\Fbb^X$. If $X$ is compact, then $C(X,\Fbb)$ is a unital $\Fbb$-subalgebra of $l^\infty(X,\Fbb)$.
\end{eg}



The following is our main example of this chapter.
\begin{eg}
Let $X$ be a set. Then $\Cbb^X$ is a unital *-algebra if for every $f\in\Cbb^X$ we define \index{f@$f^*(x)=\ovl{f(x)}$}
\begin{align}
f^*:X\rightarrow\Cbb\qquad f^*(x)=\ovl{f(x)}
\end{align}
Then $l^\infty(X,\Cbb)$ is a unital *-subalgebra of $\Cbb^X$. Assume that $X$ is a compact topological space. Then $C(X,\Cbb)$ is a unital *-subalgebra of $l^\infty(X,\Cbb)$. If $f_1,\dots,f_n\in C(X,\Cbb)$, then $\Cbb[f_1,\dots,f_n]$, the set of polynomials of $f_1,\dots,f_n$ with coefficients in $\Cbb$, is a unital subalgebra of $C(X,\Cbb)$. And $\Cbb[f_1,f_1^*,\dots,f_n,f_n^*]$ is a unital *-subalgebra of $C(X,\Cbb)$.
\end{eg}


More generally, we have:

\begin{eg}
Let $\scr A$ be an abelian unital $\Fbb$-algebra. Let $\fk S\subset\scr A$. Then \index{FS@$\Fbb[\fk S]$}
\begin{align}
\Fbb[\fk S]=\Span_\Fbb\{x_1^{n_1}\cdots x_k^{n_k}:k\in\Zbb_+,x_i\in\fk S,n_i\in\Nbb\}
\end{align}
the set of polynomials of elements in $\fk S$, is the smallest unital $\Fbb$-subalgebra containing $\fk S$, called the \textbf{unital $\Fbb$-subalgebra generated by $\fk S$}. \index{00@Subalgebra generated by...} (Here, we understand $x^0=1$ if $x\in\scr A$.) Thus, if $\scr A$ is an abelian unital *-algebra, then $\Cbb[\fk S\cup\fk S^*]$ (where $\fk S^*=\{x^*:x\in\fk S\}$) is the smallest unital *-algebra containing $\fk S$, called the \textbf{unital *-subalgebra generated by $\fk S$}.
\end{eg}



\subsection{Stone-Weierstrass (SW) theorems} \index{00@SW=Stone-Weierstrass}




The main goal of this chapter is to prove the following theorem.

\begin{thm}[\textbf{SW theorem, compact real version}]\index{00@SW theorem, compact real version}\label{lb442}
Let $X$ be a compact Hausdorff space. Let $\scr A$ be unital subalgebra of $C(X,\Rbb)$ separating points of $X$. Then $\scr A$ is dense in $C(X,\Rbb)$ (under the $l^\infty$-norm).
\end{thm}

\begin{eg}
Let $X=[a,b]$ be a compact interval in $\Rbb$. Then $\Rbb[x]$ is a unital subalgebra of $C(X,\Rbb)$. It separates points of $[a,b]$ because it contains $\id$. Thus, by SW theorem, $\Rbb[a,b]$ is dense in $C([a,b],\Rbb)$. This special case was proved in Thm. \ref{lb441}.

More generally, let $X=I_1\times\cdots\times I_N$ where each $I_i\subset\Rbb$ is a compact interval. Let $\pi_i:X\rightarrow I_i$ be the projection onto the $i$-th component, i.e., the $i$-th coordinate function. Then $\pi_1,\dots,\pi_N$ separate points of $X$, since $\pi_1\times\cdots\times\pi_N$ is the identity map of $X$. Thus, by SW theorem, $\Rbb[\pi_1,\dots,\pi_N]$ is dense in $C(X,\Rbb)$. In fact, we will first prove this special case before we prove the general SW theorem. \hfill\qedsymbol
\end{eg}

From the above version of SW theorem it is easy to prove:

\begin{thm}[\textbf{SW theorem, compact complex version}]\index{00@SW theorem, compact complex version}\label{lb450}
Let $X$ be a compact Hausdorff space. Let $\scr A$ be a unital *-subalgebra of $C(X,\Cbb)$ separating points of $X$. Then $\scr A$ is dense in $C(X,\Rbb)$ (under the $l^\infty$-norm).
\end{thm}

\begin{proof}
Let $\Real\scr A=\{\Real f=(f+f^*)/2:f\in\scr A\}$. Since for each $f\in\scr A$ we have $f^*\in\scr A$, we know that $\Real f\in\scr A$. This proves that $\Real\scr A\subset\scr A$. Since $f=\Real( f)-\im\Real(\im f)$, we conclude
\begin{align}
\scr A=\Real\scr A+\im\Real\scr A  \label{eq192}
\end{align}
In other words, elements of $\scr A$ are precisely of the form $\alpha+\im\beta$ where $\alpha,\beta\in\Real\scr A$. From this, it is clear that $\Real\scr A$ is a unital subalgebra of $C(X,\Rbb)$ separating points of $X$. Thus, by Thm. \ref{lb442}, $\Real\scr A$ is dense in $C(X,\Rbb)$. It is clear from \eqref{eq192} that $\scr A$ is dense in $C(X,\Cbb)$.
\end{proof}


\begin{eg}\label{lb443}
Let $\Sbb^1=\{z\in\Cbb:|z|=1\}$. For each $n\in\Zbb$, let $e_n:\Sbb^1\rightarrow \Cbb$ be defined by $e_n(e^{\im x})=e^{\im nx}$. In other words, $e_n(z)=z^n$. Then $\Cbb[e_1,e_{-1}]=\Span_\Cbb\{e_n:n\in\Zbb\}$ is a unital *-subalgebra of $C(\Sbb^1,\Cbb)$. It separates points of $\Sbb^1$ since $e_1$ does. Therefore, by SW theorem, $\Cbb[e_1,e_{-1}]$ is dense in $\Cbb(\Sbb^1,\Cbb)$. 

One often views
\begin{align}
C(\Sbb^1,\Cbb)=\{g\in C([-\pi,\pi],\Cbb):g(-\pi)=g(\pi)\}
\end{align}
since any $g$ in the RHS corrsponds bijectively to $f$ in the LHS by setting $f(e^{\im t})=g(t)$. Therefore, the above conclusion is that \textit{any $g\in C([-\pi,\pi],\Cbb)$ satisfying $g(-\pi)=g(\pi)$ can be approximated uniformly by functions of the form $\sum_{n=-N}^N a_n\cdot e^{\im nx}$ where $a_n\in\Cbb$}. This property is fundamental  to the theory of Fourier series. \hfill\qedsymbol
\end{eg}


\begin{eg}
We continue the above discussion. $\Cbb[e_1]=\Span_\Cbb\{e_n:n\in\Nbb\}$ is a unital subalgebra (though not a *-subalgebra) of $C(\Sbb^1,\Cbb)$ separating points of $\Sbb^1$. Let us prove that it is not dense in $C(\Sbb^1,\Cbb)$. 
\end{eg}

\begin{proof}
We view functions on $\Sbb^1$ as those on $[-\pi,\pi]$ having the same values at $\pi$ and at $-\pi$. We claim that $e_{-1}$ is not in the closure of $\Cbb[e_1]$. Indeed, it can be checked that $\int_{-\pi}^{\pi}e_{-1}e_1=2\pi$, and that $\int_{-\pi}^{\pi} p\cdot e_1=0$ for every $p\in\Cbb[e_1]$ (since this is true when $p=e_n$ and $n\geq0$). If $e_{-1}$ is in the closure of $\Cbb[e_1]$, then there is a sequence $(p_k)_{k\in\Zbb_+}$ in $\Cbb[e_1]$ converging uniformly on $[-\pi,\pi]$ to $e_{-1}$. By Cor. \ref{lb380}, $0=\int_{-\pi}^\pi p_ke_1$ converges  to $\int_{-\pi}^\pi e_{-1}e_1=2\pi$ as $k\rightarrow\infty$, impossible.  
\end{proof}

\begin{eg}\label{lb539}
Let $X,Y$ be compact metric spaces spaces. Let $\scr A$ be the subspace of $C(X\times Y,\Rbb)$ spanned by elements of the form $fg$ where $f\in C(X,\Rbb)$ and $g\in C(Y,\Rbb)$. (More precisely, one should understand $fg$ as $(f\circ\pi_X)\cdot(g\circ\pi_Y)$ where $\pi_X,\pi_Y$ are the projections of $X\times Y$ onto $X$ and $Y$ respectively.) By the Urysohn functions (cf. Rem. \ref{lb257}), we see that $C(X,\Rbb)$ resp. $C(Y,\Rbb)$ separates points of $X$ resp. $Y$. (This is in fact also true when $X,Y$ are compact Hausdorff spaces.) Thus $\scr A$ is a unital subalgebra of $C(X\times Y,\Rbb)$ separating points of $X\times Y$. By SW theorem, $\scr A$ is dense in $C(X\times Y,\Rbb)$. Nevertheless, we will prove SW theorem by first proving this special case: see Cor. \ref{lb471}.\footnote{More precisely, we don't need Exp. \ref{lb539} to prove SW. But our method of proving SW immediately implies Exp. \ref{lb539}. To put it differently, we don't need the full power of the SW theorem to prove Exp. \ref{lb539}.}
\end{eg}




\subsection{Proof of SW, I: polynomial approximation on $[0,1]^{\scr I}$}



Starting from this section, we begin our proof of SW Thm. \ref{lb442}. We first explain our strategy of the proof. Let $X$ be a compact Hausdorff space. 

We first consider the special case that $X$ is metrizable. Then by Thm. \ref{lb261}, $X$ is homeomorphic to a closed (and hence compact) subset of $[0,1]^{\Zbb_+}$. Therefore, we may assume that $X$ is a closed subset of $[0,1]^{\Zbb_+}$. As we will see, any continuous real-valued function on $X$ can be extended to a continuous function on  $[0,1]^{\Zbb_+}$. (This is due to \textbf{Tietze extension theorem}, which will be proved in the next section.) Thus, it suffices to prove SW theorem for $[0,1]^{\Zbb_+}$. In fact, as we will see, it suffices to prove that any element of $C([0,1]^{\Zbb_+},\Rbb)$ can approximated uniformly by polynomials, i.e., by elements of $\Rbb[\pi_1,\pi_2,\dots]$ where $\pi_n:[0,1]^{\Zbb_+}\rightarrow[0,1]$ is the projection onto the $n$-th component. This task will be achieved in this section.

The above method can be easily genearlized to the case that $X$ is not necessarily metrizable. In fact, since $\scr A\subset C(X,\Rbb)$ separates points of $X$, as in the proof of  Thm. \ref{lb261}, $\scr A$ enables us to embed $X$ onto a compact subset of $[0,1]^{\scr I}$ where $\scr I$ is an index set. Therefore, we need to show that $[0,1]^{\scr I}$ is compact. This is indeed due to:

%% Record #22 2023/12/6 three lectures  55

\begin{thm}[\textbf{Tychonoff theorem}]\index{00@Tychonoff theorem}\label{lb452}
Let $(X_\alpha)_{\alpha\in\scr I}$ be a family of compact topological spaces. Then the product space $S=\prod_{\alpha\in\scr I}X_\alpha$ (equipped with the product topology) is compact.
\end{thm}

Note that $S$ is obviously Hausdorff if each $X_\alpha$ is Hausdorff. We will prove Tychonoff theorem in Sec. \ref{lb503} using Zorn's lemma. Assuming Tychonoff theorem, we will show that elements of $C([0,1]^{\scr I},\Rbb)$ can be approximated uniformly by polynomials. This is the goal of this section:


\begin{pp}\label{lb451}
Let $(I_\alpha)_{\alpha\in\scr I}$ be a family of nonempty compact  intervals in $\Rbb$. Let $S=\prod_{\alpha\in\scr I}I_\alpha$. For each $\alpha$, define the \textbf{coordinate function}
\begin{align*}
\pi_\alpha:S\rightarrow I_\alpha\qquad x_\blt=(x_\mu)_{\mu\in\scr I}\mapsto x_\alpha
\end{align*}
Then $\Rbb[\{\pi_\alpha:\alpha\in\scr I\}]$, the unital subalgebra of $C(S,\Rbb)$ generated by all coordinate functions, is dense in $C(S,\Rbb)$.
\end{pp}


Since the coordinate functions separate points of $S$, Prop. \ref{lb451} is clearly a special case of SW theorem. %According to \cite{Sto48}, Prop. \ref{lb451} was due to Dieudonn\'e. Interestingly, M. Stone reproved this proposition \textit{as an application of SW theorem} (cf. Thm. 14 in \cite[p.342, Sec. 8]{Sto48}), since he didn't use Prop. \ref{lb451} to prove SW theorem.


\begin{lm}\label{lb456}
Prop. \ref{lb451} holds when $\scr I$ is a finite set.
\end{lm}

\begin{proof}
We prove by induction on $N$ that elements of $C(S,\Rbb)$, where $S=I_1\times\cdots\times I_N$, can be approximated uniformly by polynomials (i.e. by elements of $\Rbb[\pi_1,\dots,\pi_N]$). The case $N=1$ follows from Weierstrass approximation Thm. \ref{lb441}. Assume that case $N-1$ has been proved. Let use prove case $N$ where $N>1$. 

Write $S=I_1\times Y$ where $Y=I_2\times\cdots\times I_N$. Then by Thm. \ref{lb274}, we have a canonical equivalence of normed vector spaces
\begin{align*}
C(S,\Rbb)\simeq C(I_1,\mc V)\qquad\text{where}\qquad\mc V=C(Y,\Rbb)
\end{align*}
Choose any $f\in C(S,\Rbb)$, viewed as an element of $C(I_1,\mc V)$. Then by Thm. \ref{lb441}, $f$ can be approximated uniformly on $I_1$ by elements of $\mc V[\pi_1]$. Thus, for any $\eps>0$, there exist $n\in\Zbb_+$ and $g_0,\dots,g_n\in C(Y,\Rbb)$ such that $\Vert f-\sum_{i=0}^n g_i\cdot\pi_1^i\Vert_{l^\infty(I_1,\mc V)}<\eps$. Equivalently,
\begin{align*}
\Big\Vert f-\sum_{i=0}^n g_i\cdot \pi_1^i\Big\Vert_{l^\infty(S,\Rbb)}<\eps
\end{align*}
where $g_i$ actually means the composition of $g_i$ with the projection $S\rightarrow Y$. By case $N-1$, each $g_i$ can be approximated uniformly on $Y$ by elements of $\Rbb[\pi_2,\dots,\pi_N]$. So by triangle inequality, $f$ can be uniformly approximated by elements of the form $\sum_{i=1}^n h_i\cdot\pi_1^i$ where each $h_i$ is a polynomial of $\pi_2,\dots,\pi_N$. This finishes the proof of case $N$.
\end{proof}



The key to the transition from finite to general index sets is the following lemma.


\begin{lm}\label{lb455}
Let $(X_\alpha)_{\alpha\in\scr I}$ be a family of nonempty topological spaces. Let $S=\prod_{\alpha\in\scr I} X_\alpha$. Let $f\in C(S,\Rbb)$. Let $p_\blt=(p_\alpha)_{\alpha\in\scr I}\in S$. For each $A\in\fin(2^{\scr I})$, define a map $\varphi_A:S\rightarrow S$ such that for each $x_\blt=(x_\alpha)_{\alpha\in\scr I}$,
\begin{gather*}
\varphi_A(x_\blt)_\alpha=\left\{
\begin{array}{ll}
x_\alpha&\text{ if }\alpha\in A\\
p_\alpha&\text{ if }\alpha\notin A
\end{array}
\right.
\end{gather*}
Then $\varphi_A$ is continuous, and hence $f\circ\varphi_A$ is continuous. Moreover, for every $x_\blt\in S$ we have
\begin{align}\label{eq195}
\lim_{
\begin{subarray}{c}
(A,y_\blt)\in\fin(2^{\scr I})\times S\\
(A,y_\blt)\rightarrow (\infty,x_\blt)
\end{subarray}
}f\circ\varphi_A(y_\blt)=f(x_\blt)
\end{align}
in the sense of Prop. \ref{lb281}. Thus, if each $X_\alpha$ is compact, then $S$ is compact by Tychonoff Thm. \ref{lb452}, and hence $\dps\lim_{A\in\fin(2^{\scr I})}f\circ\varphi_A$ converges uniformly to $f$ by Thm. \ref{lb277}.
\end{lm}

In other words, $\varphi_A$ fixes the $\alpha$-th component if $\alpha\in A$, and changes the $\alpha$-th component to $p_\alpha$ if $\alpha\notin A$. \eqref{eq195} means that for every $\eps>0$ there exist $U\in\Nbh(x_\blt)$ and a finite $A\subset \scr I$ such that for every $y_\blt\in U$ and for every finite set $B$ satisfying $A\subset B\subset\scr I$, we have $|f(x_\blt)-f\circ\varphi_B(y_\blt)|<\eps$.


\begin{proof}
The proof is similar to that of Pb. \ref{lb453}. The continuity of $\varphi_A$ follows easily from the net-convergence description of product topology in Thm. \ref{lb192}. Choose any $x_\blt\in S$ and $\eps>0$. Since $f$ is continuous, there exists $U\in\Nbh(x_\blt)$ such that $|f(x_\blt)-f(y_\blt)|<\eps$ for all $y_\blt\in U$. By the definition of product topology by means of basis (Def. \ref{lb454}), we can shrink $U$ to a smaller neighborhood of $x_\blt$ of the form
\begin{align*}
U=\prod_{\alpha\in \scr I}V_\alpha
\end{align*}
where $V_\alpha\in\Nbh_{X_\alpha}(x_\alpha)$ for each $\alpha\in\scr I$, and $V_\alpha=X_\alpha$ for all $\alpha$ outside some $A\in\fin(2^{\scr I})$. Thus, $y_\blt\in S$ belongs to $U$ iff $y_\alpha\in V_\alpha$ for each $\alpha\in A$. Therefore, if $y_\blt\in U$, then for each finite $B$ satisfying $A\subset B\subset\scr I$, we have that
\begin{align*}
\varphi_B(y_\blt)_\alpha=y_\alpha\in V_\alpha\qquad(\forall\alpha\in A)
\end{align*}
and hence that $\varphi_B(y_\blt)\in U$. It follows that $|f(x_\blt)-f\circ\varphi_B(y_\blt)|<\eps$.
\end{proof}


\begin{proof}[\textbf{Proof of Prop. \ref{lb451}}]
Choose any continuous $f:S=\prod_{\alpha\in\scr I}I_\alpha\rightarrow\Rbb$. By Lem. \ref{lb455}, $f$ can be approximated uniformly by functions depending on finitely many variables. In other words, for any $\eps>0$, there exist $A\in\fin(2^{\scr I})$ and a continuous $g:S_A=\prod_{\alpha\in A}I_\alpha\rightarrow\Rbb$ such that $\Vert f-g\circ\pi_A\Vert_{l^\infty(S,\Rbb)}<\eps$, where $\pi_A:S\rightarrow S_A$ is the natural projection. By Lem. \ref{lb456}, $g$ can be approximated uniformly by polynomials of $\{\pi_\alpha:\alpha\in A\}$. This finishes the proof.
\end{proof}




\subsection{Partition of unity and the Tietze extension theorem}\label{lb464}


In this section, we fix a Banach space $\mc V$ over $\Fbb\in\{\Rbb,\Cbb\}$, and fix a nonempty LCH space $X$.


\begin{rem}\label{lb457}
Let $U$ be an open subset of $X$. Recall that $U$ is LCH by Prop. \ref{lb245}. Let $f\in C_c(U,\mc V)$ supported in $U$. Then by zero-extension, $f$ can be viewed as an element of $C_c(X,\mc V)$ supported in $U$. 
\end{rem}


\begin{proof}
Let $f$ take value $0$ outside $U$. Let  $A=\{x\in U:f(x)\neq 0\}$ and $K=\Cl_U(A)$, which is compact by assumption. In particular, $K$ is closed in $X$ by Cor. \ref{lb234}. Then $X=U\cup K^c$ is an open cover on $X$. By assumption, $f|_U$ is continuous. Also $f|_{K^c}=0$ is continuous. So $f$ is continuous by Exe. \ref{lb184}. To show that $f\in C_c(X,\mc V)$, it remains to prove that $\ovl A=\Cl_X(A)$ is compact. This fact follows from Rem. \ref{lb459}.
\end{proof}



\begin{rem}\label{lb459}
Let $W$ be a subset of a Hausdorff space $Y$. (In this section, we are mainly interested in the case that $Y$ is the LCH space $X$ and $W$ is open. In this case, $W$ is LCH by Prop. \ref{lb245}.)  Let $A\subset W$, and recall $\ovl A=\Cl_Y(A)$. Then \index{00@Precompact subset}
\begin{align}\label{eq196}
A\text{ is precompact in }W\quad\Longleftrightarrow\quad \ovl A\text{ is compact, and }\ovl A\subset W
\end{align}
Moreover, if $A$ is precompact in $W$, then $\ovl A$ equals $\Cl_W(A)$.
\end{rem}

Note that if $W$ is open, but if $A$ is not precompact in $W$, then $\Cl_Y(A)$ and $\Cl_W(A)$ are not necessarily equal: take $Y=\Rbb$ and $A=W=\Rbb_{>0}$.

\begin{proof}
``$\Leftarrow$" is obvious from the definition of precompactness (recall Def. \ref{lb458}).

``$\Rightarrow$": Clearly $\Cl_W(A)\subset\ovl A$ in general. Assume that $A$ is a precompact subset of $W$. By Def. \ref{lb458}, we have $A\subset\Cl_W(A)\subset W$ where $\Cl_W(A)$ is compact. In particular, $\Cl_W(A)$ is closed in $Y$ (by Cor. \ref{lb234}). So $\ovl A\subset\Cl_W(A)$. So $\ovl A=\Cl_W(A)$. Thus $\ovl A$ is a compact subset of $W$ since $\Cl_W(A)$ is so.
\end{proof}


\begin{df}
Let $W$ be a subset of a Hausdorff space $Y$. We write \index{AW@$A\Subset W$}
\begin{align}
A\Subset W
\end{align}
whenever $A$ is a precompact subset of $W$, or equivalently, whenever $A\subset Y$ satisfies that $\Cl_Y(A)$ is a compact subset of $W$.
\end{df}




\subsubsection{Tietze extension theorem}



The goal of this section is to prove the celebrated

\begin{thm}[\textbf{Tietze extension theorem}]\index{00@Tietze extension theorem}\label{lb468}
Let $K$ be a compact subset of $X$. Let $f\in C(K,\mc V)$. Then there exists $\wtd f\in C_c(X,\mc V)$ such that $\wtd f|_K=f$, and that $\Vert \wtd f\Vert_{l^\infty(X,\mc V)}=\Vert f\Vert_{l^\infty(K,\mc V)}$.
\end{thm}

The reader can first assume this theorem and read Sec. \ref{lb473} about the proof of SW theorem, and then return to this section to read the proof of Tietze extension theorem.

\begin{rem}
Tietze extension theorem is often used in the following form: Suppose that $U$ is an open subset of $X$ containing $K$. Applying Tietze extension to $U$ (which is LCH by Prop. \ref{lb245}) and noticing Rem. \ref{lb457}, we see that every $f\in C(K,\mc V)$ can be extended to some $\wtd f\in C_c(X,\mc V)$ supported in $U$ such that $\Vert \wtd f\Vert_{l^\infty(X,\mc V)}=\Vert f\Vert_{l^\infty(K,\mc V)}$. In other words, in Tietze extension theorem, we can assume that the extended function is compactly supported in a given open subset.
\end{rem}

\begin{co}\label{lb474}
$C_c(X,\Rbb)$ separates points of $X$.
\end{co}

\begin{proof}
Choose distinct points $x,y\in X$. Let $K=\{x,y\}$. Let $f:K\rightarrow\Rbb$ such that $f(x)=1$ and $f(y)=0$. By Thm. \ref{lb468}, $f$ can be extended to some $\wtd f\in C_c(X,\Rbb)$ which clearly separates $x$ and $y$.
\end{proof}



There is another version of Tietze extension theorem: If $A$ is a closed subset of a normal topological space $Y$, then any $f\in C(A,\Rbb)$ can be extended to some $\wtd f\in C(Y,\Rbb)$ without increasing the  $l^\infty$-norm. Its proof is not quite the same as the LCH version. See \cite[Sec. 35]{Mun}. We will not use this version in our course. 




\subsubsection{Urysohn's lemma}



The proof of Tietze extension theorem involves several steps. The first step is to prove a special case: If $K\subset X$ is compact, then the characteristic function $\chi_K:X\rightarrow\Rbb$ can be extended to a continuous $f\in C_c(X,\Rbb)$. Then $f|_K=1$. Replacing $f$ by $\max\{f,0\}$, we may assume that $f\geq0$. Replacing $f$ by $\min\{f,1\}$, we may assume $0\leq f\leq 1$. This special case is called


\begin{thm}[\textbf{Urysohn's lemma}]\index{00@Urysohn's lemma for LCH spaces}\label{lb711}
Let $K$ be a compact subset of $X$. Then there exists $f$ such that \index{Kf@@$K\prec f$}  \index{fX@$f\prec X$}
\begin{align}
K\prec f\prec X
\end{align}
\end{thm}

The meaning of the notations is explained below.

\begin{df}\label{lb465}
Let $U$ be an open subset of $X$. Let $K$ be a compact subset of $X$.
\begin{itemize}
\item $f\prec U$ means that $f\in C_c(X,[0,1])$ \index{Cc@$C_c(X,[0,1])$} (i.e. $f\in C_c(X,\Rbb)$ and $f(X)\subset[0,1]$) and that $\Supp(f)\subset U$.
\item $K\prec f$ means that $f\in C_c(X,[0,1])$ and that $f|_K=1$.
\end{itemize} 
\end{df}
 

The symbol ``$\prec$" is chosen for the following reason. Assume $f\in C_c(X,[0,1])$. Then $K\prec f$ means that $\chi_K\leq f$. However, the meaning of $f\prec U$ is slightly stronger than that of $f\leq \chi_U$: the latter means that $U$ contains $\{x:f(x)\neq0\}$, but not that $U$ contains its closure $\Supp(f)$.





To prove Urysohn's lemma we need some elementary observations:







\begin{lm}\label{lb460}
Let $W$ be an open subset of $X$. Let $K$ be a compact subset of $W$. Then there exists an open set $U$ of $X$ such that $K\subset U\Subset W$. 
\end{lm}


By Rem. \ref{lb459}, this lemma simply means that every compact $K\subset W$ is contained in a precompact open subset of $W$. To prove it, it suffices to assume $X=W$.


\begin{proof}
We assume $X=W$. Since $X$ is LCH, every $x\in K$ is contained in a precompact open subset $U_x$. Since $K$ is compact, $K$ is contained in $U=U_{x_1}\cup\cdots\cup U_{x_n}$ for some $x_1,\dots,x_n\in K$. Clearly $U$ is open and precompact.
\end{proof}


Recall Def. \ref{lb462} for the meaning of $\omega(f,x)$, the oscillation of a function $f$ at $x$.

\begin{lm}\label{lb463}
Let $Y$ be a topological space and $Z$ a metric space. Let $(f_n)$ be a sequence in $Z^Y$ converging uniformly to $f\in Z^Y$. Assume that for each $x\in Y$, $\lim_{n\rightarrow\infty}\omega(f_n,x)=0$. Then $f$ is continuous.
\end{lm}


Note that Thm. \ref{lb279} can be viewed as a special case of this lemma. However, this lemma is not used as often as Thm. \ref{lb279}. This is why we call this result only a lemma.

\begin{proof}
Choose any $\eps>0$. Then there is $N\in\Zbb_+$ such that for all $n\geq N$ we have $\Vert f-f_n\Vert_\infty<\eps$. Since $\lim_{n\rightarrow\infty}\omega(f_n,x)=0$, there is $n\geq N$ such that $\omega(f_n,x)<\eps$. Thus there exist $n\geq N$ and $U\in\Nbh_X(x)$ such that $\diam(f_n(U))<\eps$. Then by triangle inequality, we have $\diam(f(U))\leq 3\eps$. So $\omega(f,x)\leq3\eps$. Since $\eps$ is arbitrary, we conclude $\omega(f,x)=0$. So $f$ is continuous at $x$ by Prop. \ref{lb461}.
\end{proof}



\begin{proof}[$\star$ \textbf{Proof of Urysohn's lemma}]
By Lem. \ref{lb460}, we can choose $U_1\Subset X$ containing $K$. In the case that $X$ is metrizable, $f(x)=d(x,U_1^c)/(d(x,K)+d(x,U_1^c))$ gives a desired function. However, in the general case, we need to construct $f$ in a different way. 

We shall construct inductively a sequence of functions $f_n:X\rightarrow[0,1]$ such that the following conditions are satisfied:
\begin{enumerate}[label=(\alph*)]
\item $f_n|_K=1$ and $f_n|_{X\setminus U_1}=0$.
\item $\omega(f_n,x)\leq \frac 1{2^n}$ for all $x\in X$.
\item $\Vert f_{n+1}-f_n\Vert_{l^\infty}\leq \frac 1{2^{n+1}}$ for all $n$.
\end{enumerate}
Then $\Vert f_{n+k}-f_n\Vert_\infty\leq 1/2^n$ for all $n,k>0$. Thus $(f_n)_{n\in\Zbb_+}$ is a Cauchy sequence in $l^\infty(X,\Rbb)$, converging uniformly to some $f\in l^\infty(X,\Rbb)$. Clearly $f(X)\subset[0,1]$, $f|_K=1$, and $f|_{X\setminus U_1}=0$. So $f$ is compactly supported since $\ovl U_1$ is compact. By Lem. \ref{lb463}, $f\in C_c(X,[0,1])$, finishing the proof.

In fact, our construction of $f_n$ relies on
\begin{align*}
K~\subset~ U_0~\Subset~ U_{\frac 1{2^n}}~\Subset~ U_{\frac 2{2^n}}~\Subset~\cdots~\Subset ~U_{\frac{2^{n}-1}{2^n}}~\Subset ~U_1
\end{align*}
$f_0$ is simply defined to be $\chi_{U_0}$ where $U_0$ is a precompact open subset of $U_1$ containing $K$ (which exists due to Lem. \ref{lb460}). 

Suppose that $f_n$ and $U_{\frac j{2^n}}$ (where $0\leq j<2^n$) have been constructed. Clearly $U_{\frac j{2^{n+1}}}$ already exists when $j$ is even. Suppose that $j$ is odd, let $U_{\frac j{2^{n+1}}}$ be a precompact open subset of $U_{\frac {j+1}{2^{n+1}}}$ containing the closure of  $U_{\frac {j-1}{2^{n+1}}}$. Then $K\subset U_0\Subset U_{\frac 1{2^{n+1}}}\Subset U_{\frac 2{2^{n+1}}}\Subset\cdots\Subset U_1$. Let
\begin{align*}
h_{n+1}=\sum_{
\begin{subarray}{c}
0<j<2^{n+1}\\
j\text{ is odd}
\end{subarray}
}2^{-n-1}\cdot\chi_{\Delta_j}\qquad\text{where}\qquad \Delta_j=U_{\frac j{2^{n+1}}}\setminus U_{\frac{j-1}{2^{n+1}}}
\end{align*}
and let $f_{n+1}=f_n+h_{n+1}$. The best way to understand this construction is to look at the pictures:
\begin{gather*}
\vcenter{\hbox{{
			\includegraphics[height=3cm]{fig4.png}}}}
\qquad
\vcenter{\hbox{{
			\includegraphics[height=3cm]{fig5.png}}}}
\qquad
\vcenter{\hbox{{
			\includegraphics[height=3cm]{fig6.png}}}}
\end{gather*}


Clearly (a) and (c) are satisfied. Choose any $x\in X$. For each $n$, let $U_{\frac {2^n+1}{2^n}}=X$ and $U_{-\frac 1{2^n}}=\emptyset$. Then $X$ is a disjoint  union of $\Delta_j=U_{\frac j{2^{n+1}}}\setminus U_{\frac{j-1}{2^{n+1}}}$ over $0\leq j\leq 2^n+1$, and $f_n$ can be described by $f_n|_{\Delta_j}=\max\{1-\frac j{2^n},0\}$. For each $x\in \Delta_j$ (where $1\leq j\leq 2^n+1$), let
\begin{align*}
W=U_{\frac{j}{2^n}}\setminus\ovl{U_{\frac{j-2}{2^n}} }
\end{align*}
Then $W\in\Nbh_X(x)$, and $\diam(f_n(W))\leq \frac 1{2^n}$ since $W\subset \Delta_j\cup\Delta_{j-1}$. If $x\in\Delta_0$, then $\Delta_0\in\Nbh(x)$ and $\diam(f_n(\Delta_0))=0$. This proves (b).
\end{proof}



\subsubsection{Partition of unity}\label{lb467}


In the second step of the proof of Tietze extension theorem, we prove the theorem on partition of unity.

Recall that we defined Riemann integrals by partitioning a compact interval into small intervals. Thus, in order to define the integral of a function on $\Rbb^2$, one can partition a rectangle into smaller ones. However, it is more difficult to integrate a function defined on a more complicated space (a complicated compact surface $M$ in $\Rbb^3$, for example) by dividing $M$ into small pieces, since these small pieces may have complicated shapes.

Instead of partitioning $M$, a better way to define $\int_Mf$ is by partitioning $f$. Let $f:M\rightarrow\Rbb$ be continuous. Suppose first of all that $\Supp(f)$ is contained in a small enough neighborhood $U$ which can be ``parametrized by a rectangle". (More precisely: we can find a bijection $\varphi:R\rightarrow U$, where $R=(a,b)\times(c,d)$ is an open rectangle in $\Rbb^2$, such that $\varphi$ and $\varphi^{-1}$ are both smooth. Such $\varphi$ is called a \textbf{diffeomorphism}.) Then we can use integrals on rectangles to define $\int_Mf$ by ``pulling back $f$ to $R$". Now, in the general case, one can define $\int_Mf$ by writting $f$ as $f_1+\cdots+f_n$ where each $f_n\in C(M,\Rbb)$ has a small enough support such that $\int_Mf_i$ can be defined. Then $\sum_i\int_Mf_i$ gives the formula of $\int_Mf$. 

Notice that it suffices to write the constant function $1$ on $M$ as $h_1+\cdots+h_n$ where each $\Supp(h_i)$ is small enough. Then $f=fh_1+\cdots+fh_n$ gives a desired partition of $f$. Thus, $1=h_1+\cdots+h_n$ is called a \textbf{partition of unity} (where ``unity" means the constant function $1$).


%% Record #23 2023/12/11 two lectures  57

\begin{thm}\label{lb466}
Let $K$ be a compact subset of $X$. Let $\fk U=(U_1,\dots,U_n)$ be a finite set of open subsets of $X$ covering $K$ (i.e. $K\subset U_1\cup\cdots\cup U_n$). Then there exist $h_1,\dots,h_n\in C_c(X,\Rbb)$ such that the following conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item For each $1\leq i\leq n$, we have $h_i\geq 0$ and $\Supp (h_i)\subset U_i$.
\item  $\dps\sum_{i=1}^nh_i\big|_K=1$.
\item $\dps 0\leq\sum_{i=1}^nh_i\leq 1$. 
\end{enumerate}
Such $h_1,\dots,h_n$ are called a \textbf{partition of unity of $K$ subordinate to $\fk U$}. \index{00@Partition of unity in LCH spaces}
\end{thm}

In the language of Def. \ref{lb465}, the assumptions on $h_1,\dots,h_n$ are that $h_i\prec U_i$ for all $i$, and that $K\prec\sum_i h_i$. 

Among the three conditions in Thm. \ref{lb466}, (1) and (2) are more important. In fact, many people only assume (1) and (2) in the definition of partition of unity. We assume (3) since it is also useful. (Note also that (3) is automatic when $X=K$.) Looking at the function $\sum_i h_i$, one easily sees that Urysohn's lemma is a special case of Thm. \ref{lb466}. However, we shall prove Thm. \ref{lb466} using Urysohn's lemma. In fact, $h_1,\dots,h_n$ should be viewed as a \textit{partition of the Urysohn function} $\sum_i h_i$.


\begin{proof}
Step 1. Let us construct $G_i\in C_c(U_i,\Rbb_{\geq0})$ for each $1\leq i\leq n$ such that $G:=\sum_i G_i$ is $>0$ on $K$ (i.e., $G(K)\subset\Rbb_{>0}$).  

By Urysohn's lemma, for each $x\in X$ there exists $g_x\in C_c(X,[0,1])$ supported in $U_i$ such that $g_x(x)\neq 0$. Since $K$ is compact and is contained in $\bigcup_{x\in K}g_x^{-1}(\Rbb_{>0})$, there exists a finite subset $E\subset K$ such that $K\subset \bigcup_{x\in E}g_x^{-1}(\Rbb_{>0})$. So $\sum_{x\in E}g_x\big|_K>0$. 

Now, for each $1\leq i\leq n$, define
\begin{align}\label{eq197}
G_i=\sum_{
\begin{subarray}{c}
x\in E\\
\Supp(g_x)\subset U_i
\end{subarray}}
g_x
\end{align}
Since for each $x\in E$ there is some $U_i$ containing $\Supp(g_x)$, when summing up \eqref{eq197} over all $i$, each $g_x$ must appear at least once in the summand. So $G:=\sum_i G_i$ is $\geq \sum_i g_i$, and hence $G|_K>0$. Clearly each $G_i$ is $\geq0$ and is compactly supported in $U_i$. \\[-1ex]


Step 2. $G_1,\dots,G_n$ satisfy (1) but not necessarily (2) or (3). It is tempting to define $h_i=G_i/G$. Then $h_1,\dots,h_n$ satisfy all the desired conditions except the continuity. To remedy this issue, we define $h_i=G_i/\wtd G$, where $\wtd G\in C_c(X,\Rbb)$ satisfies that $\wtd G>0$ on $X$, that $\wtd G\geq G$ (so that $0\leq \sum_ih_i\leq 1$), and that $\wtd G|_K=G|_K$ (so that $\sum_i h_i|_K=1$). Then $h_1,\dots,h_n$ are the desired functions.

Let us prove the existence of such $\wtd G$. Let $W=\{x\in X:G(x)>0\}$. Let $F\in C(X,[0,1])$ such that $F|_K=0$ and that $F|_{X\setminus W}=1$. The existence of such $1-F$ is ensured by Urysohn's lemma. Then one can let $\wtd G=G+F$. 
\begin{align*}
\vcenter{\hbox{{
			\includegraphics[height=1.5cm]{fig7.png}}}}
\end{align*}
\end{proof}





\subsubsection{How to use partition of unity}\label{lb490}


In Sec. \ref{lb293}, we have discussed how to use the condition of compactness: Suppose that $K$ is compact and $f$ is a function on $K$, for instance, a continuous one. To prove that $f$ satisfies a global finiteness condition, we first prove that each $x\in K$ is contained in a neighborhood $U_x$ on which $f$ satisfies this finiteness condition. Then we pick finitely many $U_{x_1},U_{x_2},\dots$ covering $K$, and show that $f$ satisfies the finiteness condition globally. 

To summarize, we can use compactness to prove many \uline{finiteness properties} by a local-to-global argument. As pointed out in Sec. \ref{lb293}, usually, these finiteness properties can also be proved by contradiction using net-compactness or sequential compactness.

Now assume that $K$ is a compact subset of the LCH space $X$, and let $f$ be a function on $K$. \uline{With the help of partition of unity, one can construct new objects from $f$ using a local-to-global argument}. Moreover, these constructions are usually very difficult to obtain by using net-compactness or sequential compactness. The following are two typical examples:
\begin{enumerate}
\item (\textbf{Integral problems}) Construction of an integral $\int_Kf$. This was already mentioned in Subsec. \ref{lb467}.
\item (\textbf{Extension problems}) Construct a ``good" function $\wtd f$ on $X$ extending (or ``approximately extending") $f$. The idea is simple: Suppose that the extension exists locally, i.e., suppose that for each $x\in K$ there exists $U_x\in\Nbh_X(x)$ such that $f|_{U_x\cap K}$ can be extended to a good function $g_x$ on $U_x$. By compactness, $K$ is covered by $\bigcup_{x\in E}U_x$ where $E$ is a finite subset of $K$. Let $(h_x)_{x\in E}$ be a partition of unity of $K$ subordinate to this open cover.  Then $\wtd f=\sum_{x\in E}h_x\cdot g_x$ gives a good extension.
\end{enumerate}

In the study of measure theory in the second semester, we will use partition of unity extensively. Readers who want to get a head start can do the problems in Subsec. \ref{lb494}, \ref{lb534}, and \ref{lb495} to see how to build a theory of multivariable Riemann integrals using partitions of unity. For the moment, let's look at a simple example of extension problem before we prove the Tietze extension theorem. This example is not used elsewhere in this chapter, but it serves as a good illustration of how to use partitions of unity.


\begin{eg}\label{lb478}
Let $I$ be an open interval in $\Rbb$, and let $K$ be a nonempty compact subset of $I$. Let $r\in\Zbb_+\cup\{\infty\}$. Assume that $f:K\rightarrow\Rbb$ is $C^r$, which means that for each $x\in K$ there exist an open interval $U_x\subset I$ containing $x$ and $g_x\in C^r(U_x,\Rbb)$ extending $f|_{U_x\cap K}$. Then there exists $\wtd f\in C^r(I,\Rbb)$ extending $f$ and is compactly supported in $I$. 
\end{eg}

This example is the \textbf{smooth Tietze extension theorem} in dimension $1$, as mentioned in Rem. \ref{lb479}.

\begin{proof}
Let $U_x$ and $g_x$ be as in the example. Since $K$ is compact, it can be covered by $U_{x_1},\dots,U_{x_n}$. Call this open cover $\fk U$. Similar to the proof of Thm. \ref{lb466}, one can find a set of $C^r$-partition of unities $h_1,\dots,h_n$ of $K$ subordinate to $\mc U$. In other words, $h_1,\dots,h_n$ are $C^r$ functions and form a partition of unity. (Similar to the proof of Thm. \ref{lb466}, in order to find such $h_1,\dots,h_n$, it suffices to prove the $C^r$-version of Urysohn's lemma. But this has been done in Prop. \ref{lb440}, noting that $K$ is contained in a compact subinterval of $I$.) Now each $h_ig_{x_i}$ is an $C^r$-function on $I$ compactly supported in $U_{x_i}$. Then $\wtd f=\sum_i h_ig_{x_i}$ is a desired extension.
\end{proof}



You will see many more examples in the future when you study differential manifolds and sheaf theory. It is no exaggeration to say that partition of unity is one of the most important techniques in modern mathematics.




\subsubsection{Proof of Tietze extension theorem}




Now you may wonder: Under the assumption of Tietze extension Thm. \ref{lb468}, we don't even know how to extend $f\in C(K,\mc V)$ locally. Then how can we use the local-to-global argument? Here is the answer: you can find an \textit{approximate} extension locally. Therefore, you can first find an approximate global extension of $f$. Then, passing to the limit, you get the desired extension. This will be our strategy of the proof of Thm. \ref{lb468}.




\begin{lm}\label{lb469}
Let $K$ be a compact subset of $X$, and let $f\in C(K,\mc V)$. Then for every $\eps$, there exists $\varphi\in \Span(C_c(X,\Rbb)\mc V)$ such that
\begin{align*}
\Vert f-\varphi\Vert_{l^\infty(K,\mc V)}\leq\eps\qquad \Vert \varphi\Vert_{l^\infty(X,\mc V)}\leq \Vert f\Vert_{l^\infty(K,\mc V)}
\end{align*}
\end{lm}

The meaning of $\Span(C_c(X,\Rbb)\mc V)$ is clear: the smallest linear subspace of $C_c(X,\mc V)$ containing $C_c(X,\Rbb)\mc V$. Thus \index{SpanCX@$\Span(C_c(X,\Rbb)\mc V)$}
\begin{align}
\Span(C_c(X,\Rbb)\mc V)=\{g_1v_1+\cdots+g_nv_n:n\in\Zbb_+,g_i\in C_c(X,\Rbb),v_i\in\mc V\}
\end{align}


\begin{proof}
Let $M=\Vert f\Vert_{l^\infty(K,\mc V)}$. For each $p\in K$, since $f$ is continuous at $p$, there exists $U_p\in\Nbh_X(p)$ such that $\diam(f(U_p\cap K))\leq\eps$ (cf. Prop. \ref{lb461}). Then the contant function $f(p)$ gives a local approximate extension of $f|_{U_p\cap K}$.

Since $K$ is compact, there exists a finite subset $E\subset K$ such that $K$ is covered by $\fk U=\{U_p:p\in E\}$. By Thm. \ref{lb466}, there is a partition of unity $(h_p)_{p\in E}$ of $K$ subordinate to $\fk U$. Since $h_p\in C_c(U_p,\Rbb)$ can be viewed as a compactly supported continuous function on $X$ (Rem. \ref{lb457}), the function $\varphi=\sum_{p\in E}h_pf(p)$ is an element of $C_c(X,\mc V)$. 

If $x\in K$, then since $h_p\geq0$, we have
\begin{align*}
\Vert f(x)-\varphi(x)\Vert=\Big\Vert \sum_{p\in E}h_p(x)\cdot \big(f(x)-f(p)\big)  \Big\Vert\leq\sum_{p\in E}h_p(x)\Vert f(x)-f(p)\Vert
\end{align*}
In the RHS, if $h_p(x)\neq 0$, then $x\in U_p$, and hence $\Vert f(x)-f(p)\Vert\leq\eps$. So the RHS is no greater than $\sum_{p\in E}h_p(x)\eps$, and hence no greater than $\eps$ since $\sum_{p\in E}h_p(x)=1$. Finally, for every $x\in X$, we have
\begin{align*}
\Vert \varphi(x)\Vert\leq\sum_{p\in E}h_p(x)\Vert f(p)\Vert\leq \sum_{p\in E}h_p(x)M=M
\end{align*}
finishing the proof.
\end{proof}




The following special case of Lem. \ref{lb469} is more useful for application.

\begin{pp}\label{lb470}
Assume that $X$ is a compact Hausdorff space, and let $f\in C(X,\mc V)$. Then for every $\eps>0$, there exists $\varphi\in\Span(C_c(X,\Fbb)\mc V)$ such that $\Vert f-\varphi\Vert_\infty<\eps$.
\end{pp}



\begin{co}\label{lb471}
Let $X,Y$ be compact Hausdorff spaces. Then for every $f\in C(X\times Y,\Fbb)$ and $\eps>0$, there exist $g_1,\dots,g_n\in C(X,\Fbb)$ and $h_1,\dots,h_n\in C(Y,\Fbb)$ such that $\Vert f-g_1h_1-\cdots-g_nh_n\Vert_{l^\infty(X\times Y,\Fbb)}<\eps$.
\end{co}


\begin{proof}
By Thm. \ref{lb274}, we view $f$ as an element of $C(X,\mc V)$ where $\mc V=C(Y,\Rbb)$. Then the corollary follows immediately from Prop. \ref{lb470}. From this proof, we see that it is not necessarily to assume that $Y$ is Hausdorff. (This is not an important fact anyway.)
\end{proof}



Now we are ready to finish the

\begin{proof}[\textbf{Proof of Tietze extension Thm. \ref{lb468}}]
Recall that $K$ is compact in $X$ and $f\in C(K,\mc V)$, and that our goal is to extend $f$ to $\wtd f\in C_c(X,\mc V)$. Moreover, we need that $\Vert\wtd f\Vert_{l^\infty(X,\mc V)}$ equals $M=\Vert f\Vert_{l^\infty(K,\mc V)}$. We first note that the last requirement is easy to meet. Assume WLOG that $M>0$. Let $\wtd f\in C_c(X,\mc V)$ extend $f$, and define $g:X\rightarrow\Rbb_{\geq0}$ by $g(x)=\max\{M,\Vert\wtd f(x)\Vert\}$. Then $g$ is continuous, $g\geq M$, and $g|_K=M$. Then $M\wtd f/g$ is an element of $C_c(X,\mc V)$ extending $f$ and is $l^\infty$-bounded by $M$, finishing the proof.

Second, note that it suffices to extend $f$ to $\wtd f\in C(X,\mc V)$. By Urysohn lemma, there exists $h$ such that $K\prec h\prec X$. Then $\wtd fh\in C_c(X,\mc V)$ extends $f$.

We now construct $\wtd f\in C(X,\mc V)$ extending $f$. By Lem. \ref{lb469}, there exist $\varphi_1,\varphi_2,\dots\in C_c(X,\mc V)$ such that
\begin{gather*}
\Vert f-\varphi_1\Vert_{l^\infty(K,\mc V)}\leq \frac M2\qquad \Vert \varphi_1\Vert_{l^\infty(X,\mc V)}\leq M\\
\Vert f-\varphi_1-\varphi_2\Vert_{l^\infty(K,\mc V)}\leq \frac M4\qquad \Vert \varphi_2\Vert_{l^\infty(X,\mc V)}\leq \frac M2\\
\Vert f-\varphi_1-\varphi_2-\varphi_3\Vert_{l^\infty(K,\mc V)}\leq \frac M8\qquad \Vert \varphi_3\Vert_{l^\infty(X,\mc V)}\leq \frac M4\\
\vdots
\end{gather*}
Then $\sum_{n=1}^\infty\Vert \varphi_n\Vert_{l^\infty(X,\mc V)}\leq 2M$, and hence $\sum_{n=1}^\infty\varphi_n$ converges to some $\wtd f$ in the Banach space $C(X,\mc V)\cap l^\infty(X,\mc V)$ (Cor. \ref{lb101}). Clearly $\wtd f$ extends $f$.
\end{proof}



\subsection{Proof of SW, II: embedding into $[0,1]^{\scr I}$}\label{lb473}

In this section, we shall finish the proof of SW Thm. \ref{lb442}.


\begin{rem}\label{lb472}
Suppose that $\Phi:X\rightarrow Y$ is a homeomorphism of topological spaces. Then $X$ and $Y$ can be ``viewed as the same space" via $\Phi$. This means that a point $x\in X$ can be identified with $\Phi(x)\in Y$, that an open or closed subset $A\subset X$ can be identified with $\Phi(A)$, which is open or closed in $Y$. It also means, for example, that if $A\subset X$, then the closure of $A$ can be identified with the closure of $\Phi(A)$ in $Y$. More precisely: $\Phi$ restricts to a homeomorphism $\Cl_X(A)\rightarrow\Cl_Y(\Phi(A))$.

The continuous functions of $X$ and $Y$ can also be identified: If $g\in C(Y,Z)$ where $Z$ is a topological space (e.g. $Z=\Rbb$), then $g$ is equivalent to its \textbf{pullback under $\Phi$}, \index{00@Pullback of functions} which is an element of $C(X,Z)$ given by
\begin{align}
\Phi^*g:=g\circ\Phi\qquad\in C(X,Z)
\end{align}
This equivalence of functions can be illustrated by the commutative diagram
\begin{equation}
\begin{tikzcd}[column sep=tiny]
X \arrow[rr, "\Phi","\simeq"'] \arrow[rd, "\Phi^*g"'] &   & Y \arrow[ld, "g"] \\
                                            & Z &                  
\end{tikzcd}
\end{equation}
\hfill\qedsymbol
\end{rem}



\begin{proof}[\textbf{Proof of SW Thm. \ref{lb442}}]
Recall that $X$ is a compact Hausdorff space and $\scr A$ is a unital subalgebra of $C(X,\Rbb)$ separating points of $X$. For each $f\in\scr A$, let $M_f=\Vert f\Vert_\infty$ and $I_f=[-M_f,M_f]$. Define
\begin{gather*}
\Phi:X\rightarrow S=\prod_{f\in\scr A}I_f\qquad x\mapsto (f(x))_{f\in\scr A}
\end{gather*}
In other words, $\Phi=\bigvee_{f\in\scr A}f$, using the notation in Pb. \ref{lb258}. Thus, by Pb. \ref{lb258} (or by Thm. \ref{lb192}), $\Phi$ is continuous. The fact that $\scr A$ separates points of $X$ is equivalent to that $\Phi$ is injective. Since $X$ is compact, by Thm. \ref{lb236}, $\Phi$ restricts to a homeomorphism $\Phi:S\rightarrow\Phi(S)$. 

Recall that the coordinate function $\pi_f:S\rightarrow I_f$ is the projection onto the $f$-component. So $\Phi^*\pi_f=\pi_f\circ\Phi=f$. Therefore, by Rem. \ref{lb472}, $X$ is equivalent to $\Phi(X)$ under $\Phi$, and $\pi_f|_{\Phi(X)}\in C(\Phi(X),\Rbb)$ is equivalent to $f\in C(X,\Rbb)$ under $\Phi$. Therefore, we can identify $X$ with $\Phi(X)$ via $\Phi$ so that $f$ is identified with $\pi_f|_{\Phi(X)}$. Thus, in this case, $\scr A$ is the set of all $\pi_f|_X$, and clearly $\scr A$ contains all the polynomials of the coordinate functions $\{\pi_f|_X:f\in\scr A\}$ since $\scr A$ is a unital subalgebra. Thus, it suffices to prove that the polynomials of coordinate functions are dense in $C(X,\Rbb)$. 

Choose any $g\in C(X,\Rbb)$. Since $X$ is compact, and since $S$ is a compact Hausdorff space (by Tychonoff Thm. \ref{lb452}), by Tietze extension Thm. \ref{lb468}, $g$ can be extended to $\wtd g\in C(S,\Rbb)$. By Prop. \ref{lb451}, $\wtd g$ can be approximated uniformly by the polynomials of coordinate functions of $S$.
\end{proof}



\begin{rem}
If you feel that identifying $X$ and $\Phi(X)$ is cheating, it is easy to revise the proof without identifying them: Choose any $g\in C(X,\Rbb)$. One first concludes that $g\circ\Phi^{-1}\in C(\Phi(X),\Rbb)$ can be uniformly approximated by the polynomials of coordinate functions. Then, since the pullback of these polynomials under $\Phi$ are elements of $\scr A$, one concludes that $g$ can be approximated uniformly by elements of $\scr A$.
\end{rem}





\subsection{Summary of the proof of SW}





The key steps of the proof of SW Thm. \ref{lb442} are as follows.

\begin{enumerate}
\item We first prove the case that $X$ is a compact interval $I$ and $\scr A$ is the polynomial algebra: Let $f\in C(I,\Rbb)$, and extend $f$ to an element in $C_c(\Rbb,\Rbb)$. Let $g(x)=\pi^{-\frac 12}e^{-x^2}$ and $g_\eps(x)=\eps^{-1}g(x/\eps)$. On the one hand, $\lim_{\eps\rightarrow 0}f*g_\eps$ converges uniformly to $f$. On the other hand, for each $\eps$, since $g_\eps$ is approximated by polynomials uniformly on compact intervals (consider the Taylor series of $g_\eps$), one shows that $f*g_\eps$ is also approximated by polynomials uniformly on compact intervals.  
\item When $\mc V$ is a Banach space, the same method shows that $\mc V[x]$, the set of polynomials with coefficients in $\mc V$, is $l^\infty$-dense in $C(I,\mc V)$.  
\item Taking $\mc V=C(J,\Rbb)$ where $J$ is a compact interval, the above step shows that polynomials are dense in $C(I\times J,\Rbb)$.\footnote{Instead of using step 2, one can also use step 1 and the fact that elements in $C(I\times J,\Rbb)$ can be approximated by those of the form $f_1g_1+\cdots+f_ng_n$ where $f_i\in C(I,\Rbb)$ and $g_i\in C(J,\Rbb)$, cf. Cor. \ref{lb471}.} Similarly, by induction, one sees that polynomials are dense in $C(I_1\times\cdots\times I_n,\Rbb)$ if each $I_j$ is a compact interval.
\item Let $S=\prod_{\alpha\in\scr I}I_\alpha$ where each $I_\alpha$ is a compact interval. Then $S$ is a compact Hausdorff space by the Tychonoff theorem. One shows that any $f\in C(S,\Rbb)$ can be approximated by a function $g$ depending on finitely many variables (Lem. \ref{lb455}, or Pb. \ref{lb453} when $\scr I$ is countable). By the previous step, $g$ can be approximated by polynomials. So $f$ can be approximated by polynomials (of coordinate functions of $S$).
\item Let $X$ be a compact subset of $S=\prod_{\alpha\in\scr I}I_\alpha$, and let $f\in C(X,\Rbb)$. Then by the Tietze extension theorem, $f$ can be extended to $\wtd f\in C(S,\Rbb)$ where $\wtd f$ can be approximated by polynomials by the previous step. So $f$ can be approximated by polynomials.
\item Now let $X$ be a compact Hausdorff space, and let $\scr A$ be a unital subalgebra of $C(X,\Rbb)$ separating points of $X$. Then the map $\Phi=\bigvee_{f\in\scr A}f$ maps $X$ homeomorphically to a compact subspace of $S=\prod_{\alpha\in\scr A}I_f$ where $I_f=[-\Vert f\Vert_\infty,\Vert f\Vert_\infty]$. By the previous step, continuous functions on $\Phi(X)$ can be approximated by polynomials of the coordinate functions of $S$. This is equivalent to the density of $\scr A$ in $C(X,\Rbb)$. The proof is complete.
\end{enumerate}


The proof we have given, which is different from the proofs in most textbooks, has several advantages. First, it clearly shows that \uline{``$\scr A$ separates points of $X$" is an embedding condition, which ensures that the map $\Phi=\bigvee_{f\in\scr A}f$ is injective}. The embedding of spaces is a common theme in many branches of mathematics. In differential geometry, one can show that every (second countable) smooth manifold can be smoothly embedded into a Euclidean space. (This is the \textbf{Whitney embedding theorem}.) \textbf{Projective manifolds}, the
compact complex manifolds that can be holomorphically embedded into complex projective spaces $\Cbb\Pbb^n$, are among the most important examples in complex (algebraic) geometry.

Second, Urysohn's lemma and partition of unity are extremely important tools in differential manifolds and in measure theory, both of which will be studied next semester.  

Thus, although our proof is longer than those in the other textbooks,\footnote{The proofs in most textbooks (e.g. \cite[Sec. 4.7]{Fol}, \cite[Ch. 7]{Rud-P}, \cite[Sec. 16.4]{Zor-2}) are similar and are due to M. Stone \cite{Sto48}, which used the idea of lattices. A lattice is a set $L$ together with operations $\wedge,\vee$ satisfying $a \vee(a \wedge b)=a$ and $a \wedge(a \vee b)=a$. In analysis, one considers $L\subset C(X,\Rbb)$ where $f\vee g=\max\{f,g\}$ and $f\wedge g=\min\{f,g\}$. In Stone's day, lattices were relatively popular in functional analysis. But today it seems that they are a bit cold. The shorter proof in \cite{Sto48} is actually not the original proof of SW theorem. The original proof was given in \cite{Sto37} and is much more complicated.} the methods we used in the proof (convolutions, embedding of spaces, partition of unity, etc.) will appear frequently in the future study. Through our proof, the SW theorem is closely and organically related to other mathematical concepts.  



We close this section with an immediate consequence of the proof of SW theorem, which is parallel to Thm. \ref{lb261}.

\begin{thm}
Let $X$ be a topological space. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is a compact Hausdorff space.
\item $X$ is homeomorphic to a closed subset of $[0,1]^{\scr I}$ for some set $\scr I$.
\end{enumerate}
\end{thm}


\begin{proof}
Clearly $[0,1]^{\scr I}$ is Hausdorff. By Tychonoff Thm. \ref{lb452}, $[0,1]^{\scr I}$ is compact. So its closed subsets are compact Hausdorff. Conversely, let $X$ be compact Hausdorff. By Cor. \ref{lb474}, $C(X,\Rbb)$ separates points of $X$. Thus there is a subset $\scr I\subset C(X,\Rbb)$ separating points of $X$ such that $f(X)\subset[-1,1]$ for all $f\in\scr I$. Therefore $\Phi=\bigvee_{f\in\scr I}f:X\rightarrow[-1,1]^{\scr I}$ is a continuous injective map of $X$ into $[-1,1]^{\scr I}$. So it reduces to a homeomorphism $X\rightarrow\Phi(X)$ by Thm. \ref{lb236}.
\end{proof}









\subsection{Application: separability of $C(X,\Rbb)$}\label{lb513}


Our proof of SW theorem relies on Tychonoff Thm. \ref{lb452}, which in turn relies on Zorn's lemma, an uncountable version of mathematical induction. Though Zorn's lemma is equivalent to the axiom of choice, it is much more difficult to grasp intuitively than mathematical induction. Thus, one would like to find a proof without using Zorn's lemma if possible. 

In the following, we will show that when the compact Hausdorff space $X$ is second countable (equivalenty, metrizable), in the proof of SW theorem, it suffices to embed $X$ into a countable product of compact intervals. The latter is compact by countable Tychonoff theorem, whose proof does not rely on Zorn's lemma (cf. Thm. \ref{lb89} or Pb. \ref{lb241}). 

We first discuss a general fact about countability in compact Hausdorff spaces. The following theorem is one of the most important general properties about separability: It tells us that for a compact Hausdorff space $X$, the countability property of the topology of $X$ is equivalent to that of the uniform convergence topology of $C(X,\Rbb)$. %(Metrizability is also a countability property, since it implies that one can use sequences to study convergence.) 
This is in stark contrast to compactness, where the compactness of $X$ does not in general imply the compactness of bounded closed subsets of  $C(X,\Rbb)$  (cf. Exp. \ref{lb518}).



\begin{thm}\label{lb482}
Let $X$ be a compact Hausdorff space. The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is metrizable.
\item $X$ is second countable.
\item $C(X,\Rbb)$ is separable (equivalently, second countable, cf. Prop. \ref{lb250}).
\end{enumerate}
\end{thm}

Here, as usual, $C(X,\Rbb)$ is equipped with the $l^\infty$-norm. See Pb. \ref{lb484} for a generalization of Thm. \ref{lb482} to LCH spaces. Also, assuming (3), one can write down a metric and a countable basis explicitly; see Pb. \ref{lb533} for details.

\begin{proof}
By Thm. \ref{lb261}, we have (1)$\Leftrightarrow$(1') where
\begin{itemize}
\item[(1')] $X$ is homeomorphic to a closed subset of $[0,1]^{\Zbb_+}$. 
\end{itemize}
Let us prove that (2) and (3) are equivalent to (1').

(1')$\Rightarrow$(3): By Tietze extension theorem, it suffices to prove that $C(S,\Rbb)$ is separable where $S=[0,1]^{\Zbb_+}$. Let $\mc E=\Qbb[\{\pi_n:n\in\Zbb_+\}]$ be the set of polynomials of the coordinate functions of $S$ with coefficients in $\Qbb$. ($\pi_n:S\rightarrow[0,1]$ is the projection onto the $n$-th component.) Then $\mc E$ is countable (cf. Exe. \ref{lb476}), and is clearly dense in $\Rbb[\{\pi_n:n\in\Zbb_+\}]$. By Prop. \ref{lb451}, $\mc E$ is dense in $C(S,\Rbb)$.



(3)$\Rightarrow$(1'): Let $\mc E=\{f_1,f_2,\dots\}$ be a countable dense subset of $C(X,\Rbb)$. By enlarging $\mc E$, we assume WLOG that $\mc E$ is infinite. By Cor. \ref{lb474}, $C(X,\Rbb)$ separates points of $X$. So $\mc E$ also separates points of $X$. We scale $f_n\in\mc E$ by a nonzero real number such that $\Vert f_n\Vert_\infty\leq 1$. Then 
\begin{gather}
\Phi:X\rightarrow [-1,1]^{\Zbb_+} \qquad x\mapsto (f_1(x),f_2(x),\dots)  \label{eq199}
\end{gather}
is a continuous injective map, restricting to a homeomorphism $X\rightarrow\Phi(X)$ where $\Phi(X)$ is a compact (and hence closed) subset of $[-1,1]^{\Zbb_+}\simeq[0,1]^{\Zbb_+}$.

(1')$\Rightarrow$(2): Since $[0,1]$ is separable and hence second countable, by Pb. \ref{lb305}, $[0,1]^{\Zbb_+}$ is second countable. So its subsets are second countable by Prop. \ref{lb475}.\footnote{Alternatively, one can also use Thm. \ref{lb252} to prove directly (1)$\Rightarrow$(3). However, as the reader can feel in Rem. \ref{lb483}, I personally don't like the proof of Thm. \ref{lb252}. I would rather \textit{define} a metrizable compact space to be a topological space homeomorphic to a closed subset of $[0,1]^{\Zbb_+}$: adopting this definition, the proof of second countability will be more intuitive and less tricky. And after all, the method in the proof of Thm. \ref{lb252} will never be used in the future.}

(2)$\Rightarrow$(1'): Let $(U_n)_{n\in\Zbb_+}$ be an infinite countable basis of the topology of $X$. For each $m,n\in\Zbb_+$, if $\ovl U_n\subset U_m$, we choose $f_{m,n}$ such that $\ovl U_n\prec f\prec U_m$ (which exists by Urysohn lemma); otherwise, we let $f_{m,n}=0$. Then $\{f_{m,n}:m,n\in\Zbb_+\}$ separates points of $X$. (Proof: Choose distinct $x,y\in X$. Since $X\setminus\{y\}\in\Nbh_X(x)$, there exists $U_m$ containing $x$ and is contained in $X\setminus\{y\}$. By Lem. \ref{lb460}, there is $n$ such that $\{x\}\subset U_n\Subset U_m$. Then $f_{m,n}(x)=1$ and $f_{m,n}(y)=0$.) Thus, as in \eqref{eq199}, the map
\begin{align*}
X\rightarrow[0,1]^{\Zbb_+\times\Zbb_+}\qquad x\mapsto (f_{m,n}(x))_{m,n\in\Zbb_+}
\end{align*}
restricts to a homeomorphism from $X$ to a compact subset of $[0,1]^{\Zbb_+\times\Zbb_+}$.
\end{proof}





\begin{exe}\label{lb476}
Let $\scr A$ be a unital $\Rbb$-algebra (resp. unital $\Cbb$-algebra). Let $\Kbb$ be $\Qbb$ (resp. $\Qbb+\im\Qbb$). Let $x_1,x_2,\dots$ be a possibly finite sequence of elements of $\scr A$. Let $\mc E=\Kbb[x_1,x_2,\dots]$ be the set of polynomials of $x_1,x_2,\dots$ with coefficients in $\Kbb$. Prove that $\mc E$ is a countable set.
\end{exe}




\begin{rem}
Note that Prop. \ref{lb451} is used (and is only used) in the proof of (1')$\Rightarrow$(3) of Thm. \ref{lb482}. Our proof of Thm. \ref{lb482} relies on Tychonoff theorem, which in turn relies on Zorn's lemma. However, in the proof of (1')$\Rightarrow$(3) we only consider countable products of integrals. In this special case, the proof of Tychonoff theorem uses mathematical induction but not Zorn's lemma. (See the proof of Thm. \ref{lb89} or Pb. \ref{lb241}.) Therefore, our proof of Thm. \ref{lb482} does not rely on Zorn's lemma.
\end{rem}

\begin{rem}
Since the proof of Prop. \ref{lb451} uses Weierstrass approximation Thm. \ref{lb441}, the proof of ``$\Rightarrow$(3)" in Thm. \ref{lb482} also relies on Thm. \ref{lb441}. This is as expected. In fact, even if one wants to prove that $C([0,1],\Rbb)$ is separable, one needs Thm. \ref{lb441}. (See Cor. \ref{lb486}.) Therefore, it is fair to say that the separability of $C(X,\Rbb)$ is one of the most important applications of Weierstrass approximation (or SW) theorem.
\end{rem}


\begin{claim}
If $X$ is a compact Hausdorff space satisfying one of the equivalent conditions in Thm. \ref{lb482}, then the SW Thm. \ref{lb442} can be proved without using Zorn's lemma.
\end{claim}


\begin{proof}
Let $\scr A$ be a unital subalgebra of $C(X,\Rbb)$ separating points of $X$. Since $C(X,\Rbb)$ is separable (equivalently, second countable), so is $\scr A$ (by Prop. \ref{lb475}). Therefore, $\scr A$ has a countable dense subset $\mc E=\{f_1,f_2,\dots\}$ which clearly separates points of $X$. By enlarging $\mc E$ we assume that it is infinite. So \eqref{eq199} restricts to a homeomorphism from $X$ to a closed subset of $S=[0,1]^{\Zbb_+}$. As in the proof of (1')$\Rightarrow$(3) of Thm. \ref{lb482}, the polynomials of the coordinate functions of $S$, when restricted to $\Phi(X)$, form a dense subset of $C(\Phi(X),\Rbb)$. Pulling back this result to $X$, we conclude that polynomials of $f_1,f_2,\dots$ (which are in $\scr A$) form a dense subset of $C(X,\Rbb)$.
\end{proof}

%% Record #24 2023/12/13 three lectures  60

\subsection{Problems and supplementary material}


We always let $\mc V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.

\subsubsection{SW theorems for LCH spaces}

\begin{prob}\label{lb431}
Let $X$ be an LCH space with topology $\mc T$. Define a set $X^*=X\cup\{\infty\}$ where $\infty$ is a new symbol not in $X$. 
\begin{enumerate}
\item Prove that the set
\begin{align*}
\fk U=\mc T\cup\{X^*\setminus K:K\subset X\text{ is compact}\}
\end{align*}
is a basis for a topology on $X^*$. Let $\mc T^*$ be the topology on $X^*$ generated by $\fk U$. Prove that $\mc T$ is the subspace topology of $\mc T^*$. (Namely, prove that $U\subset X$ is open iff $U=X\cap V$ for some open $V\subset X^*$.)
\item Prove that $(X^*,\mc T^*)$ is a compact Hausdorff space. We call $(X^*,\mc T^*)$ (or simply call $X^*$) the \textbf{one-point compactification} \index{00@One-point compactification} of $X$.
\item Prove that if $X$ is not compact, then $X$ is dense in $X^*$.
\end{enumerate}
\end{prob}


More generally, we define:
\begin{df}
Let $X$ be LCH. A \textbf{one-point compactification (OPC)} of  $X$ is a compact Hausdorff space $\wht X$, together with an injective map $\varphi:X\rightarrow \wht X$ such that $\wht X\setminus \varphi(X)$ has exactly one element,  and that $\varphi$ restricts to a homeomorphism $\varphi:X\rightarrow\varphi(X)$. In particular, the $X^*$ constructed in Pb. \ref{lb431} is a one-point compactification of $X$.
\end{df}



\begin{prob}
Prove the uniqueness of one-point compactifications in the following sense. Let $X$ be LCH with one-point compactifications $(\wht X,\varphi)$ and $(\wtd X,\psi)$. Then there is a unique homeomorphism $\Phi:\wht X\rightarrow\wtd X$ such that the following diagram commutes:
\begin{equation}
\begin{tikzcd}[column sep=small]
                     & X \arrow[ld,"\varphi"'] \arrow[rd,"\psi"] &   \\
\wht X \arrow[rr, "\Phi","\simeq"'] &                         & \wtd X
\end{tikzcd}
\end{equation}
\end{prob}

\begin{eg}\label{lb433}
If $X$ is compact Hausdorff and contains at least two points, then for every $p\in X$, the subset $X\setminus\{p\}$ has OPC $X$ (together with the inclusion map). Thus $[0,1]$ has OPC $[0,1]\cup\{2\}$. $(0,1]$ has OPC $[0,1]$. $(0,1)$ has OPC $\Sbb^1$ (the unit circle in $\Cbb$) together with the map $\varphi:x\in(0,1)\mapsto e^{2\im\pi x}\in \Sbb^1$.
\end{eg}



\begin{cv}
Let $X$ be LCH and $f:X\rightarrow \mc V$. According to part 3 of Pb. \ref{lb431}, when $X$ is not compact, the limit of functions $\lim_{x\rightarrow\infty}f$ makes sense. In the case that $X$ is compact, we understand  $\lim_{x\rightarrow\infty}f$ as $0$.
\end{cv}




\begin{df}
Let $X$ be LCH with one-point compactification $X^*$. Define \index{C0@$C_0(X,\mc V)$}
\begin{align}
C_0(X,\mc V)=\big\{f\in C(X,\mc V):\lim_{x\rightarrow\infty}f(x)=0  \big\}
\end{align}
Since $\{X^*\setminus K:K\subset X\text{ is compact}\}$ is a neighborhood basis of $\infty\in X^*$ (recall Def. \ref{lb262}), by Def. \ref{lb197}-(2), it is clear that $f\in C(X,\mc V)$ belongs to $C_0(X,\mc V)$ iff
\begin{align*}
\tcboxmath{
\begin{array}{c}
\text{for every $\eps>0$ there exists a compact $K\subset X$ such that}\\
\text{for each $x\in X\setminus K$ we have $\Vert f(x)\Vert<\eps$}
\end{array}}
\end{align*}
For each $f:X\rightarrow \mc V$ define $\wtd f:X^*\rightarrow \mc V$ where
\begin{align}\label{eq184}
\wtd f(x)=\left\{
\begin{array}{ll}
f(x)& \text{ if }x\in X\\[1ex]
0& \text{ if }x=\infty
\end{array}
\right.
\end{align}
Then by Def. \ref{lb197}-(1),  $\wtd f$ is continuous at $\infty$ iff $\lim_{x\rightarrow\infty}f(x)=0$. Thus, we have
\begin{align}
C_0(X,\mc V)=\big\{\wtd f|_X:\wtd f\in C(X^*,\mc V)\text{ and }\wtd f(\infty)=0 \}
\end{align}
\end{df}

\begin{pp}\label{lb432}
Let $X$ be LCH with one-point compactification $X^*$. Then we have an linear isometry of Banach spaces (under the $l^\infty$-norms)
\begin{align}\label{eq185}
C_0(X,\mc V)\rightarrow C(X^*,\mc V)\qquad f\mapsto \wtd f=\eqref{eq184}
\end{align}
\end{pp}

\begin{proof}
Clearly $\Vert f\Vert_{l^\infty(X,\mc V)}=\Vert \wtd f\Vert_{l^\infty(X^*,\mc V)}$. So \eqref{eq185} is a linear isometry. The range of \eqref{eq185} is $\{g\in C(X^*,\mc V):g(\infty)=0\}$, which is clearly a closed subset of the Banach space $C(X,\mc V)$ and hence is complete by Prop. \ref{lb86}.
\end{proof}


\begin{cv}\label{lb449}
According to Prop. \ref{lb432}, people often identify $C_0(X,\mc V)$ with its image under \eqref{eq185}, i.e., identify $f\in C_0(X,\mc V)$ with $\wtd f$ defined by \eqref{eq184}.
\end{cv}








\begin{prob}\label{lb544}
Let $Y$ be a compact Hausdorff space. Let $E$ be a closed subspace of $Y$. Prove that 
\begin{align*}
C_0(Y\setminus E,\mc V)=\{g|_{Y\setminus E}:g\in C(Y,\mc V),g|_E=0\}
\end{align*}
\end{prob}



\begin{thm}[\textbf{SW theorem, LCH real version}]\index{00@SW theorem, LCH real version}\label{lb545}
Let $X$ be an LCH space. Let $\scr A$ be a subalgebra of $C_0(X,\Rbb)$. Assume that $\scr A$ separates points of $X$. Assume that $\scr A$ \textbf{vanishes nowhere on $X$} \index{00@Vanish nowhere} (i.e. for every $x\in X$ there is $f\in\scr A$ such that $f(x)\neq0$). Then $\scr A$ is dense in $C_0(X,\Rbb)$ (under the $l^\infty$-norm).
\end{thm}

\begin{prob}
Prove the above SW theorem by following the steps below. Let $X^*=X\cup\{\infty\}$ be the one-point compactification of $X$. By Conv. \ref{lb449}, $\scr A$ is naturally a subalgebra of $C(X^*,\Rbb)$. Define
\begin{align*}
\scr B=\{f+\lambda:f\in\scr A,\lambda\in\Rbb\}
\end{align*}
which is a unital subalgebra of $C(X^*,\Rbb)$. Use SW Thm. \ref{lb442} to prove that $\scr B$ is dense in $C(X^*,\Rbb)$. Use this fact to prove that $\scr A$ is dense in $C_0(X,\Rbb)$.
\end{prob}



\begin{thm}[\textbf{SW theorem, LCH complex version}]\index{00@SW theorem, LCH complex version}
Let $X$ be an LCH space. Let $\scr A$ be a *-subalgebra of $C_0(X,\Cbb)$. Assume that $\scr A$ separates points of $X$. Assume that $\scr A$ vanishes nowhere on $X$. Then $\scr A$ is dense in $C_0(X,\Cbb)$ (under the $l^\infty$-norm).
\end{thm}


\begin{proof}
This follows from the LCH real version, just as the compact complex version of SW theorem (Thm. \ref{lb450}) follows from the compact real version (Thm. \ref{lb442}).
\end{proof}





\begin{prob}\label{lb485}
Let $X$ be LCH. Prove that $C_c(X,\mc V)$ is dense in $C_0(X,\mc V)$. (This proves that $C_0(X,\mc V)$ is the Banach space completion of $C_c(X,\mc V)$.) Conclude that $C_0(X,\mc V)$ is separable iff $C_c(X,\mc V)$ is separable.
\end{prob}

\begin{proof}[Hint]
Use Urysohn's lemma or Tietze extension.
\end{proof}


\begin{df}
Let $\scr A$ be an $\Fbb$-algebra. A subset $J\subset \scr A$ is called an \textbf{ideal} if $J$ is an $\Fbb$-linear subspace of $\scr A$ such that $\scr A J\subset J$ (i.e. $xy\in J$ for all $x\in\scr A,y\in J$). 
\end{df}

The following problem gives an interesting application of SW Thm. \ref{lb545}.

\begin{sprob}
(\textbf{Nullstellensatz}) \index{00@Nullstellensatz for $C(X,\Rbb)$}  Let $X$ be a compact Hausdorff space. For each closed subset $A\subset X$, let
\begin{align*}
I(A)=\{f\in C(X,\Rbb):f|_A=0\}
\end{align*} 
Clearly $I(A)$ is a closed ideal of $C(X,\Rbb)$. For each closed ideal $J$ of $C(X,\Rbb)$, let
\begin{align*}
N(J)=\{x\in X:f(x)=0\text{ for all }f\in J\}
\end{align*}
which is a closed subset of $X$. Prove that $A\mapsto I(A)$ gives a bijection between the  closed subsets of $A$ and the closed ideals of $C(X,\Rbb)$, and that its inverse map is $J\mapsto N(J)$. In other words, for every closed subset $A\subset X$ and every closed ideal $J\subset C(X,\Rbb)$, prove that
\begin{gather}
N(I(A))=A\qquad I(N(J))=J \label{eq210}
\end{gather}
\end{sprob}
\begin{proof}[Hint]
For both parts of \eqref{eq210} it is easy to prove ``$\supset$". To prove $N(I(A))= A$, use Urysohn's lemma or the Tietze extension theorem. To prove $I(N(J))=J$, identify $I(N(J))$ with $C_0(X\setminus N(J),\Rbb)$ (cf. Pb. \ref{lb544}) and apply SW Thm. \ref{lb545} to the LCH space $X\setminus N(J)$.
\end{proof}




\subsubsection{Lebesgue measures of open sets}\label{lb494}


The theory of Riemann integrals on $\Rbb$ can be easily generalized to $\Rbb^N$ by partitioning boxes, i.e. $I_1\times\cdots\times I_N$ where each $I_j$ is a compact interval in $\Rbb$. In the following, we establish the basic theory of Riemann integrals on bounded subsets of $\Rbb^N$ using partitions of unity. Compared to partitioning boxes, the methods provided below are closer to those in measure theory.

\begin{df}
If $f\in C_c(\Rbb^N,\Rbb)$, define $\dps\int f\equiv\int_\Rbb f=\int_Bf$ where $B\subset\Rbb^N$ is any box containing $\Supp(f)$. (See Def. \ref{lb487}.)
\end{df}


\begin{prob}\label{lb713}
Let $U$ be an open subset of $\Rbb^N$. Define the \textbf{(Lebesgue) measure} of $U$ to be
\begin{align}
\mu(U)=\sup\Big\{\int_{\Rbb^N}f:f\in C_c(U,[0,1]) \Big\}
\end{align}
which is an element of $\ovl\Rbb_{\geq0}$. It is clear that if $V$ is an open subset of $U$ then $\mu(V)\leq\mu(U)$.
\begin{enumerate}
\item In the case that $N=1$ and $U=(a,b)$ (where $-\infty\leq a<b\leq+\infty$), prove that $\mu(U)=b-a$.
\item Suppose that $U$ has compact closure. Prove that $\mu(U)<+\infty$. (Hint: use Urysohn's lemma.)
\item Let $(U_\alpha)_{\alpha\in\scr A}$ be an increasing net of open subsets of $\Rbb^N$. Prove
\begin{align*}
\mu\Big(\bigcup_{\alpha\in\scr A}U_\alpha \Big)=\sup_{\alpha\in\scr A}\mu(U_\alpha)
\end{align*}
\item Let $(V_j)_{j\in\scr I}$ be a (non-necessarily increasing) family of open subsets of $\Rbb^N$. Prove that
\begin{align*}
\mu\Big(\bigcup_{j\in\scr I}V_j  \Big)\leq\sum_{j\in\scr I}\mu(V_j)
\end{align*}
(Hint: Use part 3 to reduce to the special case $\mu(U\cup V)\leq\mu(U)+\mu(V)$. Prove it using partition of unity.)
\item Let $(V_j)_{j\in\scr I}$ be a family of \textit{mutually disjoint} open subsets of $\Rbb^N$. Prove
\begin{align*}
\mu\Big(\bigcup_{j\in\scr I}V_j  \Big)=\sum_{j\in\scr I}\mu(V_j)
\end{align*}
\end{enumerate}
\end{prob}


\begin{df}
A subset $E\subset\Rbb^N$ is called a \textbf{(Lebesgue) null set} \index{00@Null set, Lebesgue} if for every $\eps$ there exists an open $U\subset\Rbb^N$ containing $E$ such that $\mu(E)<\eps$.
\end{df}

\begin{exe}
Show that a countable union of null sets is null.
\end{exe}

\begin{exe}
(This exercise is not need below.) Show that when $N=1$, the above definition of null sets agrees with Def. \ref{lb409}. More generally, for arbitrary $N$, show that $E\subset\Rbb^N$ is null iff for every $\eps>0$, $E$ can be covered by boxes whose total volumes are $<\eps$.
\end{exe}


\subsubsection{$\star$ Multiple Riemann integral}\label{lb534}

\begin{prob}\label{lb488}
Let $f\in C_c(\Rbb^N,\Rbb_{\geq0})$.
\begin{enumerate}
\item Let $M\geq0$. Assume that there is an open $U\subset\Rbb^N$ such that $f|_U\leq M$ and that $\Supp(f)\subset U$. Prove
\begin{align*}
\int_{\Rbb^N}f\leq\mu(U)\cdot M
\end{align*}
\item Let $\eps\geq0$. Assume that there is an open $U\subset\Rbb^N$ such that $f|_U\geq\eps$. Prove
\begin{align*}
\int_{\Rbb^N}f\geq\mu(U)\cdot \eps
\end{align*}
\end{enumerate}
\end{prob}


\begin{rem}
Pb. \ref{lb488} is easy but useful. It tells us that the values of $\int f$ and $\mu(U)$ are controlled by each other. For example, part 2 implies that if $\int f$ is small, then the measure of $\{x\in\Rbb^N:f(x)>\eps\}$ cannot be very big, and is converging to $0$ as $\eps\rightarrow+\infty$. 
\end{rem}


\begin{df}
Let $f\in l^\infty(\Rbb^N,\Rbb)$ have compact support. Define \textbf{upper integrals} and \textbf{lower integrals} to be
\begin{gather*}
\ovl\int f=\inf\Big\{\int g:g\in C_c(\Rbb^N,\Rbb),g\geq f  \Big\}\\
\underline\int f=\sup\Big\{\int h:h\in C_c(\Rbb^N,\Rbb),h\leq f  \Big\}
\end{gather*}
Clearly $\udl\int f\leq\ovl\int f$. Moreover, it is clear that if $f\in C_c(\Rbb^N,\Rbb)$ then $\udl\int f=\ovl\int f=\int f$. In general, if $\udl\int f=\ovl\int f$, we say that $f$ is \textbf{Riemann integrable}, \index{00@Riemann integrable} and define its integral $\int f$ to be $\ovl\int f$. Let \index{Rc@$\scr R_c(\Rbb^N,\Rbb)$}
\begin{align*}
\scr R_c(\Rbb^N,\Rbb)=\{f\in l^\infty(\Rbb^N,\Rbb):f\text{ has compact support and is Riemann integrable}\}
\end{align*}
\end{df}

\begin{rem}\label{lb489}
Let $f\in l^\infty(\Rbb^N,\Rbb)$ be compactly supported. It is clear from the definition that the following statements are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $f\in\scr R_c(\Rbb^N,\Rbb)$.
\item For every $\eps>0$ there exist $g,h\in C_c(\Rbb^N,\Rbb)$ satisfying $h\leq f\leq g$ and $\int(g-h)<\eps$.
\end{enumerate}
In fact, (2) is easier to use than the original definition of Riemann integrability.
\end{rem}


\begin{exe}
(This exercise is not needed below.) Prove that when $N=1$, the above definition of upper and lower integrals agrees with that in Pb. \ref{lb386}. 
\end{exe}


\begin{prob}\label{lb732}
Let $f,g\in l^\infty(\Rbb^N,\Rbb)$ have compact supports. Prove
\begin{align*}
\ovl\int(f+g)\leq\ovl\int f+\ovl\int g\qquad \udl\int(f+g)\geq\udl\int f+\udl\int g
\end{align*}
Prove that $\scr R_c(\Rbb^N,\Rbb)$ is a linear subspace of $\Rbb^{\Rbb^N}$. Prove that  $\int:\scr R_c(\Rbb^N,\Rbb)\rightarrow\Rbb$ is linear. Prove that if $f,g\in\scr R_c(\Rbb^N,\Rbb)$ then
\begin{align}
f\leq g\qquad\Longrightarrow\qquad\int f\leq\int g  \label{eq201}
\end{align}
\end{prob}


\begin{prob}\label{lb493}
(\textbf{Fubini's theorem})\index{00@Fubini's theorem for Riemann integrals} Let $X=\Rbb^M,Y=\Rbb^N$. Let $f\in\scr R_c(X\times Y,\Rbb)$.  Prove that $x\in X\mapsto \udl\int_Y f(x,y)dy $ and $x\in X\mapsto\ovl\int_Y f(x,y)dy$ are Riemann integrable, and that $\dps\int_{X\times Y}f=\int_X\udl\int_Yf=\int_X{\ovl\int_Y}f$.
\end{prob}

\begin{proof}[Hint]
Choose $g,h\in C_c(X\times Y,\Rbb)$ such that $g\leq f\leq h$ and $\int_{X\times Y}(h-g)<\eps$. Apply Fubini's theorem for compactly supported continuous functions (available due to Thm. \ref{lb399}) to $g,h,h-g$. (\eqref{eq201} is also useful.)
\end{proof}

\begin{rem}
In the next semester, we will prove Fubini's theorem for Lebesgue measurable functions using more complicated methods. The goal of Pb. \ref{lb493} is to show you how to prove Fubini's theorem for a large class of functions (sufficient for many applications) without using those methods. When studying mathematics, it is often important to know how to simplify a proof when the objects studied are simpler.
\end{rem}




\subsubsection{$\star$ Lebesgue's criterion for multiple Riemann integrals}\label{lb495} 

\begin{df}
As in \eqref{eq200}, for each $f:\Rbb^N\rightarrow\Rbb$ and $\eps>0$, define
\begin{align}
\Omega_\eps(f)=\{x\in\Rbb^N:\omega(f,x)\geq\eps\}
\end{align}
where $\omega(f,x)=\inf_{U\in\Nbh(x)}\diam f(U)$. Note that by Lem. \ref{lb414}, $\Omega_\eps(f)$ is a closed subset of $\Supp(f)$, and hence is compact when $f$ is compactly supported.
\end{df}


\begin{prob}
Let $f\in \scr R_c(\Rbb^N,\Rbb)$.   Prove that $\Omega_\eps(f)$ is a null set. (Thus, $\bigcup_{n\in\Zbb_+}\Omega_{1/n}(f)$ is null. This proves that the set of discontinuities of $f$ is null.)
\end{prob}

\begin{proof}[Hint]
For each $\delta>0$, choose $g,h\in C_c(\Rbb^N,\Rbb)$ such that $h\leq f\leq g$ and $\int(g-h)\leq\delta\eps$, which exist due to Rem. \ref{lb489}. Let $U=\{x\in\Rbb^N:g(x)-h(x)>\eps/2\}$. Use Pb. \ref{lb488} to give a small upper bound of $\mu(U)$. Prove that $\Omega_\eps(f)\subset U$.
\end{proof}


The following problem can be compared with Lem. \ref{lb415}.

\begin{prob}\label{lb491}
Let $f\in l^\infty(\Rbb^N,\Rbb)$ be supported in a compact set $K$. Let $W\subset\Rbb^N$ be an open set containing $K$. Let $\eps>0$, and assume that for every $x\in\Rbb^N$ we have $\omega(f,x)<\eps$. Prove that there exist $g,h\in C_c(W,\Rbb)$ such that
\begin{gather*}
g\leq f\leq h\qquad 0\leq g-h\leq\eps
\end{gather*}
\end{prob}

\begin{proof}[Hint]
First construct the functions locally. Then pass from local to global functions using a partition of unity of $K$ subordinate to an open cover.
\end{proof}


\begin{prob}\label{lb492}
Let $f\in l^\infty(\Rbb^N,\Rbb)$ be compactly supported. Assume that the set of discontinuities of $f$ is a null set. Prove that $f\in\scr R_c(\Rbb^N,\Rbb)$.
\end{prob}


\begin{proof}[Hint]
Choose any $\eps,\delta>0$. Choose an open set $U$ containing the compact set $\Omega_\eps(f)$ such that $\mu(U)<\delta$. Use a partition of unity of $K=\Supp(f)$ subordinate to $U$ and $V=\Rbb^N\setminus\Omega_\eps(f)$ to write $f=f_V+f_U$ where $f_V,f_U\in C_c(\Rbb^N,\Rbb)$ are supported in $K\cap V$ and $U$ respectively. Apply Pb. \ref{lb491} to $f_V$ to get $g_V\leq f_V\leq h_V$ with small $\int(h_V-g_V)$. Let $M=\Vert f\Vert_\infty$.  Find $g_U\in C_c(\Rbb^N,[0,M])$ such that $-g_U\leq f_U\leq g_U$. Show that $\int(g_U+h_V-(-g_U+g_V))$ is small.
\end{proof}



\begin{df}
Let $\Omega$ be a bounded subset of $\Rbb^N$. We say that $f:\Omega\rightarrow\Rbb$ is \textbf{Riemann integrable} \index{00@Riemann integrable} if $\wtd f\in\scr R_c(\Rbb^N,\Rbb)$, where $\wtd f$ is the zero extension of $f$ (i.e. $\wtd f|_\Omega=f$ and $\wtd f|_{X\setminus\Omega}=0$). If $f$ is Riemann integrable, we define
\begin{align*}
\int_\Omega f=\int_{\Rbb^N}\wtd f
\end{align*}
\end{df}


\begin{eg}
Let $D=\{(x,y)\in\Rbb^2:x^2+y^2\leq1\}$ and $f\in C(D,\Rbb)$. Then the set of discontinuities of the zero extension $\wtd f$ is a subset of $\Sbb^1=\{(x,y)\in\Rbb^2:x^2+y^2=1\}$. It is not hard to show that $\Sbb^1$ is a null subset of $\Rbb^2$. Since $\Vert f\Vert_\infty<+\infty$ by extreme value theorem, we conclude that $f$ is Riemann integrable thanks to Lebesgue's criterion (Pb. \ref{lb492}). Clearly $\int_\Rbb f(\cdot,y)dy$ is Riemann integrable. By Fubini's theorem (Pb. \ref{lb493}), we have
\begin{align*}
\int_D f=\int_{\Rbb^2}\wtd f=\int_{-1}^1\int_{-1}^1 \chi_D\wtd f(x,y)dydx=\int_{-1}^1\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}f(x,y)dydx
\end{align*}
\end{eg}








\subsubsection{Compactness and countability}


The following problem shows that the equivalence (2)$\Leftrightarrow$(3) in Thm. \ref{lb482} can be generalized to LCH spaces.

\begin{prob}\label{lb484}
Let $X$ be an LCH space with one-point compactification $X^*$. Prove that the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $X^*$ is second countable.
\item $X$ is second countable.
\item $C_0(X,\Rbb)$ is separable (equivalently, second countable).
\end{enumerate}
Here, as usual, $C_0(X,\Rbb)$ is equipped with the $l^\infty$-norm. Note also that by Pb. \ref{lb485}, (3) is equivalent to that $C_c(X,\Rbb)$ is separable.
\end{prob}


\begin{srem}
By Pb. \ref{lb484} and Thm. \ref{lb482}, if an LCH space $X$ is second countable, then $X^*$ is metrizable, and hence $X$ is metrizable. However, a metrizable LCH space is not necessarily second countable. For example, let $X$ be an uncountable set, equipped with the discrete topology $\mc T=2^X$. Then $\mc T$ is induced by a metric $d$ defined by $d(x,y)=1$ if $x\neq y$ and $d(x,x)=0$. $X$ is LCH, but not second countable.
\end{srem}


\begin{prob}\label{lb533}
Let $X$ be a compact Hausdorff space. Suppose that $(f_n)_{n\in\Zbb_+}$ is a sequence in $C(X,\Rbb)$ separating points of $X$. Assume for simplicity that $\Vert f_n\Vert_{l^\infty}\leq 1$ for all $n$. Use these functions to define an explicit metric on $X$ inducing its topology $\mc T$, and construct an explicit countable basis for $\mc T$. 
\end{prob}



\begin{proof}[Note]
This problem tests whether you truly understand the method of embedding and the proof of Thm. \ref{lb482}. Hint: How do you construct a metric and a countable basis for $[-1,1]^{\Zbb_+}$? What do their pullbacks look like under the embedding $X\rightarrow[-1,1]^{\Zbb_+}$ defined by $f_1,f_2,\dots$?
\end{proof}



\newpage




\section{Zorn's lemma and applications}\label{lb512}


\begin{displayquote}
\small The Axiom of Choice is obviously true, the well-ordering principle obviously false, and who can tell about Zorn's lemma?

\hfill ---- Jerry L. Bona
\end{displayquote}



\subsection{Zorn's lemma}



\begin{thm}[\textbf{Zorn's lemma}]\index{00@Zorn's lemma}
Let $(P,\leq)$ be a nonempty partially ordered set. Suppose that every totally oredered subset of $P$ has an upper bound in $P$. Then there is a maximal element $p\in P$.
\end{thm}


By a \textbf{totally ordered subset} \index{00@Totally ordered subset} $Q\subset P$, we mean that $Q$ satisfies that for every $x,y\in Q$, either $x\leq y$ or $y\leq x$. An upper bound of $Q$ (in $P$) means an element $p\in P$ such that $x\leq p$ for all $x\in Q$. A \textbf{maximal element} \index{00@Maximal element in a partially ordered set} $p\in P$ means that $p$ satisfies $\{x\in P:x\geq p\}=\{p\}$.


Zorn's lemma and the axiom of choice are equivalent. (They are both equivalent to the so called ``well-ordering principle".) Although it is easier to prove the axiom of choice from Zorn's lemma than the other way round, one may prefer to take the axiom of choice as the starting point, since axiom of choice is easier to grasp intuitively. Therefore, in the following, I will give a proof of Zorn's lemma under the assumption of axiom of choice, in order to comfort those who are obsessed with building a complete and rigorous mathematical theory in their heads from a few ``self-evident" axioms. However, it is recommended that you skip or only skim this proof, since it is safe enough to assume that Zorn's lemma is an axiom that needs no proof. Although Zorn's lemma is far from ``self-evident", knowing how to use it is much more important than knowing how to prove it. This is because in most areas of mathematics the ideas in the proof of Zorn's lemma are never used.


\begin{proof}[$\star\star$ \textbf{Proof of Zorn's lemma}]
Let $(P,\leq)$ satisfy the assumption in Zorn's lemma but has no maximal element. We shall find a contradiction. By assumption, every totally ordered subset $A\subset P$ has an upper bound $p\in P$, and $p$ is not maximal. So there exists $x_A>p$ in $P$. (Here, $x_A>p$ means that $x_A\geq p$ and $x\neq p$.) Thus, we have a function $A\mapsto x_A$ whose existence is due to axiom of choice.\\[-1ex]

Step 1. Fix $a\in P$ throughout the proof. For an arbitrary $\mc F\subset 2^P$, consider the following conditions:
\begin{enumerate}[label=(\alph*)]
\item $\{a\}\in\mc F$.
\item Every $A\in \mc F$ is a totally ordered subset of $P$.  %and satisfies $a\in A$
\item If $A\in\mc F$ then $A\cup\{x_A\}\in\mc F$.
\item If $\mc E$ is a nonempty totally ordered subset of $\mc F$ (under the partial order $\subset$), then $\bigcup_{A\in\mc E}A\in\mc F$. (Note that if every $A\in P$ is a totally ordered subset of $P$, then so is $\bigcup_{A\in\mc E}A$.)
\end{enumerate}
There exists at least one $\mc F$ satisfying the above conditions. For example, one can let $\mc F$ be the set of all totally ordered subsets of $P$ containing $a$. 

We let $\mc F$ be the intersection of all the subsets of $2^P$ satisfying the above four conditions. Then $\mc F$ clearly also satisfies these conditions. (In particular, $\mc F\neq\emptyset$ because $\{a\}\in\mc F$.) So $\mc F$ is the smallest subset of $2^P$ satisfying these four conditions.

We claim that $\mc F$ is a totally ordered subset of $2^P$. Suppose this is true. Let $B=\bigcup_{A\in\mc F}A$. Then $B\in\mc F$ by condition (d). In particular, by (b), $B$ is a totally ordered subset of $P$. Then $x_B$ is defined as at the beginning of the proof and satisfies $x_B>B$. By (c), we have $B\cup\{x_B\}\in\mc F$. By the definition of $B$, we get $x_B\in B$. This is impossible. So we are done with the proof.\\[-1ex]

Step 2. Let us prove that $\mc F$ is totally ordered. Let
\begin{align*}
\mc F_0=\{A\in\mc F:\text{every }B\in\mc F\text{ satisfies either }A\subset B\text{ or }B\subset A\}
\end{align*}
It is not hard to check that $\mc F_0$ satisfies (a,b,d). It suffices to prove that $\mc F_0$ satisfies (c). Then we will have $\mc F_0=\mc F$ and hence that $\mc F$ is totally ordered.

Let us prove (c) for $\mc F_0$. Choose any $A\in\mc F_0$. We need to prove that $A\cup\{x_A\}\in\mc F_0$. It suffices to prove that the following set equals $\mc F$:
\begin{align*}
\mc F_A=\{B\in\mc F:B\subset A\text{ or }A\cup\{x_A\}\subset B\}
\end{align*}
One checks easily that $\mc F_A$ satisfies (a,b,d). To check that $\mc F_A$ satisfies (c), we choose any $B\in\mc F_A$. Then there are two possible cases:
\begin{itemize}
\item $A\cup\{x_A\}\subset B$ or $B=A$. Then $A\cup\{x_A\}\subset B\cup\{x_B\}$. 
\item $B\subsetneq A$. Then, since $A\in\mc F_0$ and since $B\cup\{x_B\}\in\mc F$ (because $B\in\mc F$ and $\mc F$ satisfies (c)), we have either $A\subsetneq B\cup\{x_B\}$ or $B\cup\{x_B\}\subset A$. The former case is clearly impossible. So $B\cup\{x_B\}\subset A$.
\end{itemize}
Thus, in both cases we have $B\cup\{x_B\}\in\mc F_A$. This proves that $\mc F_A$ also satisfies (c). So $\mc F_A=\mc F$.
\end{proof}




\subsection{Comparison of Zorn's lemma and mathematical induction}



Zorn's lemma can be viewed as the uncountable version of mathematical induction. Therefore, the best way to understand Zorn's lemma is to first use it to prove some classical results traditionally proved by  mathematical induction.




\begin{eg}
Let $n\in\Nbb$ and $s_n=0+1+2+\cdots+n$. Then $s_n=\frac {n(n+1)}2$.
\end{eg}


\begin{proof}
Let $\Nbb^*=\Nbb\cup\{+\infty\}$ with the usual order. Let
\begin{align*}
P=\Big\{n\in\Nbb^*:s_i=\frac{i(i+1)}2\text{ for all }i\leq n\text{ and }i<+\infty \Big\}
\end{align*}
$P$ is nonempty since it contains $0$. (Checking that $P$ is nonempty is important, since it corresponds to checking the base case in mathematical induction.) $P$ is totally ordered, and clearly every nonempty subset  $Q\subset P$ has an upper bound $\sup Q$. Thus, by Zorn's lemma, $P$ has a maximal element. 

If $n<+\infty$, then from $s_n=\frac{n(n+1)}2$ and $s_{n+1}-s_n=n+1$ we have $s_{n+1}=\frac{(n+1)(n+2)}2$. (This step corresponds to checking ``case $n$ implies case $n+1$" in mathematical induction.) So $n+1\in P$, contradicting the maximality of $n$. So $n=+\infty$, and hence $+\infty\in P$, finishing the proof.
\end{proof}






\begin{eg}
Let $f:\Rbb\rightarrow\Rbb$ be a function not continuous at $0$. Assume that $f(0)=0$.  Then there exists $\eps>0$ and a sequence $(x_n)_{n\in\Zbb_+}$ in $\Rbb\setminus\{0\}$ converging to $0$ such that $|f(x_n)|>\eps$ for all $n$.
\end{eg}



\begin{proof}
By assumption, there is $\eps>0$ such that for every $\delta>0$, there exists $x\in(-\delta,\delta)\setminus\{0\}$ such that $|f(x)|>\eps$. Let
\begin{align*}
P=\{&\text{finite or countably infinite sequence }x_\blt=(x_1,x_2,\dots)\\
&\text{satisfying }0<|x_i|<1/i\text{ and }|f(x_i)|>\eps\text{ for all }i\}
\end{align*}
Then $P$ is nonempty. (This corresponds to checking the base case in mathematical induction.) View each element of $P$ as a subset of $\Zbb_+\times\Rbb$, and let $\subset$ be the partial order on $P$. Then every nonempty totally ordered subset $Q\subset P$ has an upper bound (defined by taking union). Thus, by Zorn's lemma, $P$ has a maximal element $x_\blt$.

Suppose that $x_\blt$ has finite length $N\in\Zbb_+$. (So $x_\blt=(x_1,\dots,x_N)$) Then by the first sentence of the proof, there exists $x_{N+1}$ satisfying $0<|x_{N+1}|<1/(N+1)$ satisfying $|f(x_{N+1})|>0$. (This step corresponds to checking ``case $n$ implies case $n+1$" in mathematical induction.) Then $(x_1,\dots,x_N,x_{N+1})$ belongs to $P$ and is $>x_\blt$, contradicting the maximality of $x_\blt$. So $x_\blt$ must be an infinite sequence. This finishes the construction of the sequence $(x_n)$.
\end{proof}




\subsection{Proof of the Tychonoff theorem}\label{lb503}





We first recall Pb. \ref{lb240}: Assume for simplicity that $X,Y$ are compact spaces. Let $(x_\alpha,y_\alpha)_{\alpha\in\scr I}$ be a net in $X\times Y$ such that $x\in X$ is a cluster point of $(x_\alpha)$. Then there exists $y\in Y$ such that $(x,y)$ is a cluster point of $(x_\alpha,y_\alpha)$.

The proof is easy. By the definition of cluster points (Pb. \ref{lb223}-(1)), $(x_\alpha)$ has a subnet $(x_{\alpha_\beta})_{\beta\in\scr J}$ converging to $x$. Since $Y$ is net-compact, $(y_{\alpha_\beta})$ has a subnet $(y_{\alpha_{\beta_\gamma}})_{\gamma\in\scr K}$ converging to some $y\in Y$. Then $(x_{\alpha_{\beta_\gamma}},y_{\alpha_{\beta_\gamma}})$ is a subnet of $(x_\alpha,y_\alpha)$ converging to $(x,y)$.



In the following, we present a proof of Tychonoff Thm. \ref{lb452} which was due to Chernoff \cite{Che92}. Our method is similar to the proof of the countable Tychonoff theorem (Pb. \ref{lb241}) which uses net-compactness. It is strongly recommended that you compare the proof with the one of Pb. \ref{lb241}, and even reprove Pb. \ref{lb241} using Zorn's lemma.

\begin{proof}[\textbf{Proof of Tychonoff Thm. \ref{lb452}}]
Recall the setting that $(X_\alpha)_{\alpha\in\scr I}$ is a family of compact topological spaces. Assume WLOG that $\scr I$ and each $X_\alpha$ are nonempty. We want to prove that $S=\prod_{\alpha\in\scr I}X_\alpha$ is compact. 

We first introduce a few notations. For each $I\subset\scr I$, let $S_I=\prod_{\alpha\in I}X_\alpha$. If $x\in S_I$, we write $x$ as $(x(\alpha))_{\alpha\in I}$, and view it as a function with domain $I$ and codomain $\fk X=\bigcup_{\alpha\in\scr I}X_\alpha$. We write $\Dom(x)=I$. If $x\in S_I$ and $J\subset I$, the restriction $x|_J=(x(\alpha))_{\alpha\in J}$ is clearly in $S_J$.\\[-1ex]

Step 1. Let $(f_\mu)_{\mu\in\fk M}$ be a net in $S$. Let
\begin{align*}
P=\bigcup_{I\subset\scr I}\big\{x\in S_I: x\text{ is a cluster point of }(f_\mu|_I)_{\mu\in\fk M} \text{ in }S_I \big\}
\end{align*}
Let ``$\subset$" be the partial order on $P$ (defined by identifying each element of $P$ with its graph, which is an element of $\scr I\times \fk X$). Thus $x\subset y$ iff $\Dom(x)\subset\Dom(y)$ and $y|_{\Dom(x)}=x$.


Clearly $P$ is nonempty: Choose any $\alpha\in\scr I$. Since $X_\alpha$ is compact, $(f_\mu(\alpha))_{\mu\in\fk M}$ has a cluster point in $X_\alpha$. This point, viewed as a function from $\{\alpha\}$ to this point, belongs to $P$.\footnote{This part corresponds to checking the base case in mathematical induction.} 

We claim that every totally ordered subset of $P$ has an upper bound. Suppose this is true. Then by Zorn's lemma, there is a maximal element $x\in P$. If $\Dom(x)\neq \scr I$, then by Pb. \ref{lb240} (applied to $S_{\Dom(x)}\times X_\beta$ where $\beta\in \scr I\setminus\Dom(x)$), $x$ can be extended to a function with larger domain which is again the cluster point of the restriction of $(f_\mu)_{\mu\in\fk M}$ to that domain.\footnote{This part corresponds to checking ``case $n$ implies case $n+1$" in mathematical induction. More precisely, it corresponds to constructing $(x(1),\dots,(x(n+1)))$ from $(x(1),\dots,x(n))$ in the proof of Pb. \ref{lb241}.} This proves that $P$ has an element strictly larger than $x$. This is impossible. So we must have $\Dom(x)=\scr I$, finishing the proof.\\[-1ex]

Step 2. Let $Q$ be a nonempty totally ordered subset of $P$. Let $x$ be the union of the elements of $Q$. Let $K=\Dom(x)$. Then $Q$ can be written in the form
\begin{align*}
Q=\{x|_I:I\in\scr U\}
\end{align*}
where $\scr U$ is a totally ordered subset of $2^K$ and $K=\bigcup_{I\in\scr U}I$.



To prove that $x\in P$,  let us use Pb. \ref{lb223}-(2) to prove that $x$ is a cluster point of $(f_\mu|_K)_{\mu\in\fk M}$.\footnote{This part corresponds to showing that $x$ is a cluster point of $(f_\alpha)$ in the proof of Pb. \ref{lb241}.} Let $W$ be a neighborhood of $x$ in $S_K$. By the definition of product topology (Def. \ref{lb454}), we can shrink $W$ to a smaller neighborhood of the form
\begin{align*}
W=\prod_{\alpha\in K}U_\alpha
\end{align*}
where each $U_\alpha$ is a neighborhood of $x(\alpha)$, and there exists a finite subset $E\subset K$ such that $U_\alpha=X_\alpha$ for all $\alpha\in K\setminus E$. Choose $I\in\scr U$ containing $E$. The fact that $x|_I$ is a cluster point of $(f_\mu|_I)_{\mu\in\fk M}$ implies (by Pb. \ref{lb223}-(2)) that $(f_\mu|_E)_{\mu\in\fk M}$ is frequently in $\prod_{\alpha\in E}U_\alpha$. Therefore $(f_\mu|_K)_{\mu\in\fk M}$ is frequently in $W$. This finishes the proof.
\end{proof}

%% Record #25 2023/12/18 two lectures  62


\subsection{Proof of the Hahn-Banach extension theorem}

Recall from Rem. \ref{lb560} that a \textbf{linear functional} on a vector space $V$ over a field $\Fbb$ is defined to be a linear map $V\rightarrow\Fbb$.

\begin{lm}\label{lb498}
Let $V$ be a normed vector space over $\Rbb$, and let $M$ be a linear subspace. Let $\varphi\in M^*=\fk L(M,\Rbb)$ with operator norm $\Vert\varphi\Vert\leq 1$. Assume that $e\in V\setminus M$, and let $\wtd M=M+\Rbb e$. Then $\varphi$ can be extended to a linear functional $\wtd\varphi:\wtd M\rightarrow\Rbb$ such that $\Vert\wtd \varphi\Vert\leq 1$. 
\end{lm}

\begin{proof}[$\star$ Proof]
Let $A\in\Rbb$ whose value will be determined later. Since any vector in $\wtd M$ can be written uniquely as  $x-\lambda e$ where $x\in M$ and $\lambda\in\Rbb$, we can define
\begin{align*}
\wtd\varphi:\wtd M\rightarrow\Rbb\qquad \wtd\varphi(x-\lambda e)=\varphi(x)-\lambda A
\end{align*}
It remains to prove $\Vert\wtd\varphi\Vert\leq 1$ (for some $A$). This means that we want to prove $|\varphi(x)-\lambda A|\leq \Vert x-\lambda e\Vert$ for all $x\in M,\lambda\in\Rbb$. Clearly this is true when $\lambda=0$. Assume $\lambda\neq 0$. Then replacing $x$ by $\lambda x$ and dividing both sides by $\lambda$, we see that it suffices to prove
\begin{align*}
|\varphi(x)-A|\leq \Vert x-e\Vert
\end{align*}
for all $x\in V$, or equivalently, 
\begin{align*}
\varphi(x)-\Vert x-e\Vert\leq A\leq \varphi(x)+\Vert x-e\Vert
\end{align*}
To prove the existence of $A$ satisfying the above inequalities for all $x\in V$, it suffices to prove
\begin{align}\label{eq227}
\sup_{x\in V}\big(\varphi(x)-\Vert x-e\Vert\big)\leq \inf_{y\in V}\big(\varphi(y)+\Vert y-e\Vert\big)
\end{align}
namely, to prove that $\varphi(x)-\Vert x-e\Vert\leq \varphi(y)+\Vert y-e\Vert$ for all $x,y\in V$. Using $\Vert\varphi\Vert\leq1$, we compute
\begin{align*}
\varphi(x)-\varphi(y)=\varphi(x-y)\leq \Vert(x-e)-(y-e)\Vert\leq \Vert x-e\Vert+\Vert y-e\Vert
\end{align*}
\end{proof}


\begin{thm}[\textbf{Hahn-Banach extension theorem}] \label{lb499} \index{00@Hahn-Banach extension theorem}
Let $V$ be a normed vector space over $\Fbb\in\{\Rbb,\Cbb\}$. Let $M$ be an $\Fbb$-linear subspace of $V$. Let $\varphi\in M^*=\fk L(M,\Fbb)$. Then there exists $\Phi\in V^*$ such that $\Phi|_M=\varphi$ and $\Vert\Phi\Vert=\Vert\varphi\Vert$.
\end{thm}


\begin{proof}
We first consider the case $\Fbb=\Rbb$. Assume WLOG that $\varphi\neq 0$. By scaling $\varphi$, we assume WLOG that $\Vert\varphi\Vert=1$. Let
\begin{align*}
P=\big\{(W,\Phi):&W\text{ is a linear subspace of }V\text{ and contains }M\\
&\Phi\in\Rbb^W\text{ is linear and satisfies }\Phi|_M=\varphi,\Vert\Phi\Vert=1\big\}
\end{align*}
Then $P$ is nonempty since it contains $(M,\varphi)$. Define a partial order on $P$ by setting $(W,\Phi)\leq(W',\Phi')$ whenever $W\subset W'$ and $\Phi=\Phi'|_W$.

Suppose that $Q$ is a totally ordered subset of $P$. Let $\wtd W=\bigcup_{(W,\Phi)\in Q}W$, and let $\wtd\Phi$ be the union of the functions $\Phi$ over all $(W,\Phi)\in Q$ (by taking the union of the graphs of the functions). So $\wtd\Phi:W\rightarrow\Rbb$ is the unique functions satisfying $\wtd \Phi|_W=\Phi$ for all $(W,\Phi)\in Q$. Then it is easy to see that $(\wtd W,\wtd\Phi)$ belongs to $P$ and is an upper bound of $Q$. 

Thus, we can use Zorn's lemma, which says that $P$ has a maximal element $(W,\Phi)$. If $W\neq V$, we let $e\in V\setminus W$. Then by Lem. \ref{lb498}, $\Phi$ can be extended to $\wtd\Phi\in \wtd W^*$ where $\wtd W=W+\Rbb e$, and $\Vert\wtd\Phi\Vert=1$. So $(\wtd W,\wtd\Phi)$ belongs to $P$ and is strictly larger than $(W,\Phi)$, impossible. So $W=V$.

We are done with the proof for the case $\Fbb=\Rbb$. Now assume $\Fbb=\Cbb$. By Pb. \ref{lb387}, the real part $\Real\varphi:M\rightarrow\Rbb$ sending $v$ to $\Real(\varphi(v))$ is linear with operator norm $\Vert\Real\varphi\Vert=\Vert\varphi\Vert$. By the real Hahn-Banach, $\Real\varphi$ can be extended to $\Lambda\in\fk L(V,\Rbb)$ with $\Vert\Lambda\Vert=\Vert\varphi\Vert$. By Pb. \ref{lb387}, there exists a unique $\Phi\in\fk L(V,\Cbb)$ with real part $\Lambda$ such that $\Vert\Phi\Vert=\Vert\varphi\Vert$. Since $\Real\Phi|_M=\Lambda|_M=\Real \varphi$, by Pb. \ref{lb387}, we have $\Phi|_M=\varphi$.
\end{proof}






In this course, we only use the following special case of Hahn-Banach theorem.

\begin{co}[\textbf{Hahn-Banach}]\label{lb502}
Let $V$ be a nonzero normed vector space over $\Fbb\in\{\Rbb,\Cbb\}$. Then for  every $v\in V$, there exists a nonzero $\varphi\in V^*$ such that $\bk{\varphi,v}=\Vert\varphi\Vert\cdot\Vert v\Vert$. Consequently, the linear map
\begin{align}\label{eq202}
V\rightarrow V^{**}\qquad v\mapsto \bk{\cdot,v}
\end{align}
is an isometry.
\end{co}

More precisely, the map \eqref{eq202} sends each $\varphi\in V^*$ to $\bk{\varphi,v}$. By scaling $\varphi$, Hahn-Banach implies that for each $v\in V$, there is $\varphi\in V^*$ with $\Vert\varphi\Vert=1$ such that $\bk{\varphi,v}=\Vert v\Vert$.

\begin{proof}
Let $v\in V$, and assume WLOG that $v\neq 0$. Let $\varphi:\Fbb v\rightarrow \Fbb$ send $\lambda v$ to $\lambda\Vert v\Vert$. Then $\varphi$ is linear and has operator norm $1$, and $\bk{\varphi,v}=\Vert v\Vert$. By Hahn-Banach Thm. \ref{lb499}, $\varphi$ can be extended to a bounded linear $V\rightarrow\Fbb$ with operator norm $1$. This is a desired linear functional.

Denote \eqref{eq202} by $\Psi$. To show that $\Psi$ is an isometry, we need to prove that for every $v\in V$, $\Psi(v):V^*\rightarrow\Fbb$ has operator norm $\Vert v\Vert$. Choose any $\varphi\in V^*$. Then 
\begin{align*}
\bk{\Psi(v),\varphi}=\bk{\varphi,v}\leq\Vert\varphi\Vert\cdot\Vert v\Vert
\end{align*}
where ``$\leq$" is ``$=$" for some nonzero $\varphi$ by the first paragraph. Therefore, by Rem. \ref{lb372}, we obtain $\Vert\Psi(v)\Vert=\Vert v\Vert$.
\end{proof}















\subsection{Problems and supplementary material}


\begin{prob}
Let $V$ be a vector space over a field $\Fbb$. Use Zorn's lemma to prove that $V$ has a basis, i.e. a set $E$ of linearly independent vectors such that any vector of $V$ can be written as a (finite) linear combination of elements of $E$.
\end{prob}



\begin{prob}
Let $X$ be a set. Let $E\subset X$ be an infinite subset such that $X\setminus E$ is countable. (Recall that finite sets are also countable.) Prove that $\card(X)=\card(E)$.
\end{prob}

\begin{prob}\label{lb496}
Let $X$ be an infinite set. Use Zorn's lemma to prove that $X$ can be written as a countably infinite union  of subsets $X=\bigsqcup_{n=1}^\infty X_n$ such that $\card(X_i)=\card(X_j)$ for each $i,j$.
\end{prob}


\begin{proof}[Hint]
Assume WLOG that $X$ is uncountable. Consider
\begin{align*}
P=\Big\{\big((A_n)_{n\in\Zbb_+},(\varphi_n)_{n\in\Zbb_+} \big):&A_1,A_2,\dots\text{ are mutually disjoint subsets of }X\\
&\varphi_n:A_n\rightarrow A_{n+1}\text{ is a bijection (for every $n\in\Zbb_+$)}\Big\}
\end{align*}
which is nonempty (why?). Define a suitable partial order on $P$.
\end{proof}



\begin{thm}\label{lb497}
Let $X$ be an infinite set, and let $Y$ be a nonempty countable set. Then $\card(X)=\card(X\times Y)$
\end{thm}

\begin{proof}
By Pb. \ref{lb496}, we have $X\approx A\times\Nbb$ for some subset $A\subset X$. Since  $\Nbb\times Y$ is infinite and countable, we have $\Nbb\times Y\approx \Nbb$. Therefore
\begin{align*}
X\times Y\approx A\times \Nbb\times Y\approx A\times \Nbb\approx X
\end{align*}
\end{proof}


\begin{sprob}
Let $E,F$ be two bases of a vector space $V$. Use Thm. \ref{lb497} to prove that $\card(E)=\card(F)$. (When one of $E,F$ is finite, this result was proved in linear algebra. You can assume this in your proof.)
\end{sprob}

\begin{prob}\label{lb501}
Let $V$ be a  \textit{separable} normed vector space over $\Rbb$. Prove Hahn-Banach Thm. \ref{lb499} for $V$ using mathematical induction instead of Zorn's lemma. (You will need Prop. \ref{lb500} in the proof.)
\end{prob}












\newpage




\section{Compactness and completeness revisited}





Tychonoff theorem asserts the compactness of function spaces under the pointwise convergence topology. In application, we are often interested in the compactness of function spaces satisfying certain additional condition such as the continuity. 

\begin{eg}\label{lb518}
$\fk X=C([0,1],[0,1])$ is not compact under either the pointwise convergence topology or the uniform convergence topology.
\end{eg}

\begin{proof}
We first choose the pointwise convergence topology. Choose any sequence $(f_n)$ in $\fk X$ converging pointwise to a non-continuous function $f$. Then $(f_n)$ has no subnet converging in $\fk X$, since any subnet converging to $g\in \fk X$ must satisfy $f=g$ and hence $f$ is continuous. This is impossible. So $\fk X$ is not compact.

Now we choose the uniform convergence topology for $\fk X$, which is metrizable by the $l^\infty$-norm. Let $(f_n)$ be a sequence in $\fk X$ such that $\Vert f_m-f_n\Vert_\infty=1$ whenever $m\neq n$. (For example, one chooses $f_n$ such that $\Supp f_n\subset I_n=(\frac 1{2n+2},\frac 1{2n+1})$ and that $f_n(\xi_n)=1$ for some $\xi_n\in I_n$.) Then every subsequence of $(f_n)$ is not Cauchy and hence not convergent. Therefore, $\fk X$ is not sequentially compact, and hence not compact.
\end{proof}



Note that this example does not contradict Tychonoff theorem. In fact, if we choose the pointwise convergence topology,  then Tychonoff theorem implies that every sequence $(f_n)$ in $C([0,1],[0,1])$ has a subnet convergent to some $f\in [0,1]^{[0,1]}$. However, one cannot deduce the continuity of $f$. To prove that the limit function is continuous, we need additional assumptions on the sequence $(f_n)$.  For example, Cor. \ref{lb303} tells us that we need the equicontinuity.




\subsection{Precompactness in function spaces}


Since $C(X,[0,1])$ is in general not compact, one should search for compact subsets of $C(X,[0,1])$, or more generally, subsets with compact closures in $C(X,[0,1])$. You know what precompact sets mean geometrically: In $\Rbb^N$, precompact subsets of $\Rbb^N$ are exactly bounded subsets of $\Rbb^N$. If $\Omega\subset\Rbb^N$, then precompact subsets of $\Omega$ are precisely bounded subsets of $\Rbb^N$ whose closures are inside $\Omega$, cf. Rem. \ref{lb459}. 

However, when studying function spaces, it is often more convenient to use another description of precompactness. As we shall see in Cor. \ref{lb508}, saying that $\scr A\subset C(X,[0,1])$ is precompact in $C(X,[0,1])$ is equivalent to saying that every net $(f_\alpha)$ in $\scr A$ converges to some $f\in C(X,[0,1])$.






Recall from Def. \ref{lb458} that a subset $A$ of a Hausdorff space $X$ is called \textbf{precompact} if $A$ is contained in a compact subset of $X$, or equivalently, if $\ovl A$ is compact. %Recall Rem. \ref{lb459} for two equivalent descriptions of precompactness in subsets of $X$.


\begin{pp}\label{lb505}
Let $X$ be a metrizable topological space, and let $A\subset X$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $A$ is precompact.
\item Every net in $A$ has a cluster point in $X$.
\item Every sequence in $A$ has a cluster point in $X$.
\end{enumerate}
\end{pp}

From the following proof, it is clear that (1)$\Rightarrow$(2) holds even without assuming that $X$ is metrizable.

\begin{proof}
(1)$\Rightarrow$(2): Since $\ovl A$ is compact, every net in $A$ has a cluster point in $\ovl A$ and hence in $X$. 

(2)$\Rightarrow$(3): Obvious.

(3)$\Rightarrow$(1): See Pb. \ref{lb287}.
\end{proof}



\subsubsection{$\star$ Precompactness in regular spaces}

(Since this section is a starred section, I will not use the results proved here in future sections. However, the material of this section is helpful for a better understanding of precompactness in function spaces.)

You may wonder to what general topological space the equivalence (1)$\Leftrightarrow$(2) generalizes. This equivalence is not true for an arbitrary topological space, but is true for regular spaces (recall Def. \ref{lb504}). Metrizable spaces are clearly regular. More generally, we have:

\begin{eg}\label{lb506}
Every subset of a regular space is regular. Every LCH space is regular. Products of regular spaces are regular.
\end{eg}

\begin{proof}%[$\star$ Proof]
Assume that $X$ is regular and $A\subset X$. For each $x\in A$, choose a neighborhood of $x$ in $A$, which must be of the form $U\cap A$ where $U\in\Nbh_X(x)$. Since $X$ is regular, there is $V\in\Nbh_X(x)$ such that $\ovl V\subset U$. So $\Cl_A(V\cap A)\subset \ovl V\cap A\subset U\cap A$. So $A$ is regular. That LCH spaces are regular follows from Lem. \ref{lb460} (together with Rem. \ref{lb459}).

Let $S=\prod_{\alpha\in\scr I}X_\alpha$ where each $X_\alpha$ is regular. Choose $x=(x_\alpha)_{\alpha\in\scr I}$ in $S$. Choose a neighborhood of $x$ which, after shrinking, is of the form $\prod_\alpha U_\alpha$ where $U_\alpha\in\Nbh_{X_\alpha}(x_\alpha)$, and there is a finite subset $E\subset\scr I$ such that $U_\alpha=X_\alpha$ if $\alpha\in\scr I\setminus E$. If $\alpha\in E$, let $V_\alpha$ be a neighborhood of $x$ such that $\ovl V_\alpha\subset U_\alpha$. If $\alpha\in\scr I\setminus E$, let $V_\alpha=X_\alpha$. Then one checks easily that $\prod_{\alpha\in\scr I} V_\alpha$ is a neighborhood of $x$ with closure $\prod_{\alpha\in\scr I} \ovl V_\alpha$, which is clearly inside $\prod_\alpha U_\alpha$. So $S$ is regular.
\end{proof}


\begin{thm}\label{lb507}
Let $X$ be a regular topological space, and let $A\subset X$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\ovl A$ is compact.
\item $A$ is contained in a compact subset of $X$.
\item Every net in $A$ has a cluster point in $X$.
\end{enumerate}
\end{thm}

Thus, although regular spaces are not necessarily Hausdorff, the two equivalent definitions of precompact subsets of Hausdorff spaces in Def. \ref{lb458} are also equivalent in regular spaces. (However, most important examples of regular spaces are also Hausdorff. Regular Hausdorff spaces are called \textbf{T3 spaces}.) \index{00@T3 spaces}

\begin{proof}%[$\star$ Proof]
``(1)$\Rightarrow$(2)" is obvious. ``(2)$\Rightarrow$(3)" is also obvious: if $X\subset K$ where $K$ is a compact subset of $X$, then every net in $A$ is a net in $K$, which has a cluster point in $K$ and hence in $X$. 

Assume (3). Let us show that $\ovl A$ is compact by showing that every nonempty net $(x_\alpha)_{\alpha\in I}$ in $\ovl A$ has a cluster point in $\ovl A$. 
For each $\gamma\in I$, let $E_\gamma=\{x_\alpha:\alpha\in I,\alpha\geq\gamma\}$. Define 
\begin{align*}
J=\big\{(U,\gamma)\in 2^X\times I:U\text{ is open and contains } E_\gamma \big\}
\end{align*}
It is not hard to check that $J$ is a directed set if we define its preorder ``$\leq$" to be
\begin{align*}
(U,\gamma)\leq(U',\gamma')\qquad\Longleftrightarrow\qquad U\supset U',\gamma\leq \gamma'
\end{align*}
For each $(U,\gamma)\in J$, since $E_\gamma\subset U$, $U$ intersects $\ovl A$, and hence intersects $A$. Therefore, we can choose some $y_{U,\gamma}\in A\cap U$. In this way, we get a net $(y_{U,\gamma})_{(U,\gamma)\in J}$ in $A$. By (3), this net has a cluster point $x\in X$. So clearly $x\in\ovl A$. Let us prove that $x$ is also a cluster point of $(x_\alpha)_{\alpha\in I}$.


Assume that $x$ is not a cluster point of $(x_\alpha)_{\alpha\in I}$. Then, by the definition of cluster points (Pb. \ref{lb223}-(2)), there exists $\Omega\in\Nbh_X(x)$ such that $(x_\alpha)$ is not frequently in $\Omega$, i.e., eventually outside $\Omega$. So there exists $\gamma\in I$ such that $E_\gamma\subset X\setminus \Omega$. Since $X$ is regular, there exists $V\in\Nbh_X(x)$ such that $\ovl V\subset \Omega$. Let $U=X\setminus \ovl V$. Then $E_\gamma\subset U$, and hence $(U,\gamma)\in J$.

Since $x$ is a cluster point of $y_\blt$, for the neighborhood $V$ of $x$, there exists $(U',\gamma')\geq (U,\gamma)$ in $J$ such that $y_{U',\gamma'}\in V$. By the definition of the net $y_\blt$, we have $y_{U',\gamma'}\in A\cap U'\subset U$. This is impossible, since $U\cap V=\emptyset$.
\end{proof}


\begin{co}\label{lb508}
Let $X$ be a topological space, and let $Y$ be a metric space. Equip $C(X,Y)$ with either the pointwise convergence topology or the uniform convergence topology. (Note that both topologies are Hausdorff.) Let $\scr A$ be a subset of $C(X,Y)$. Then the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\scr A$ is precompact.
\item Every net $(f_\alpha)_{\alpha\in\scr I}$  in $\scr A$ has a subnet converging to some $f\in C(X,Y)$.
\end{enumerate}
\end{co}

\begin{proof}
If the topology $\mc T$ on $C(X,Y)$ is the pointwise convergence topology, then $C(X,Y)$ is a subspace of $Y^X$ (equipped with the product topology). By Exp. \ref{lb506}, $C(X,Y)$ is regular. So one can use Thm. \ref{lb507} to prove (1)$\Leftrightarrow$(2). If $\mc T$ is the uniform convergence topology, then $C(X,Y)$ is metrizable (and hence also regular). So one can also use Thm. \ref{lb507} (or even Prop. \ref{lb505}) to prove (1)$\Leftrightarrow$(2). 
\end{proof}




%Compared to the original definition, it is better to view (2) as the definition of precompactness in $C(X,Y)$. In fact, when studying nonmetrizable regular topologies (such as the pointwise convergence topology), we will rarely use the original definition of precompactness.


\subsection{Equicontinuity and precompactness; Arzel\`a-Ascoli theorem}\label{lb517}


\subsubsection{Precompactness under pointwise convergence topology}


\begin{df}
Let $X$ be a set and $Y$ be a metric space. A subset $\scr A\subset Y^X$ is called \textbf{pointwise bounded} \index{00@Pointwise bounded} if
\begin{align}
\scr A(x)=\{f(x):f\in\scr A\}
\end{align}
is a bounded subset of $Y$ for every $x\in X$.
\end{df}





\begin{thm}\label{lb509}
Let $X$ be a topological space, and equip $C(X,\Rbb^N)$ with the pointwise convergence topology. Let $\scr A$ be an equicontinuous and pointwise bounded subset of $C(X,\Rbb^N)$. Then $\scr A$ is precompact in $C(X,\Rbb^N)$, and $\ovl{\scr A}$ is equicontinuous. 
\end{thm}




\begin{proof}
Write $Y=\Rbb^N$. Let us show that $\ovl{\scr A}=\Cl_{C(X,Y)}(\scr A)$ is equicontinuous. Since $\scr A$ is equicontinuous, for every $x\in X$ and $\eps>0$, there exists $U\in\Nbh(x)$ such that $\diam(f(U))\leq\eps$ for all $f\in\scr F$. Since each $g\in\ovl{\scr A}$ is the pointwise limit of a net in $\scr A$, we also have $\diam(g(U))\leq\eps$. This proves that $\ovl{\scr A}$ is equicontinuous at $x$.


Now let us show that $\ovl{\scr A}$ is compact. Choose any net $(f_\alpha)$ in $\ovl{\scr A}$. It is clear that $\ovl{\scr A}(x)$ is pointwise bounded. By Heine-Borel, for each $x\in X$, $\ovl{\scr A}(x)$ is contained in a compact subset $K_x\subset Y$. So $\ovl{\scr A}$ is contained in $S=\prod_{x\in X}K_x$ where $S$ is compact by Tychonoff theorem. Therefore, $(f_\alpha)$ has a subnet $(f_\beta)$ converging pointwise to some $f:X\rightarrow Y$. Since $(f_\beta)$ is equicontinuous (as $\ovl{\scr A}$ is equicontinuous), by Cor. \ref{lb303}, $f\in C(X,Y)$.\footnote{Alternatively, by the argument in the first paragraph, $\{f_\beta:\text{all }\beta\}\cup\{f\}$ is equicontinuous. So $f$ is continuous.}  So $f\in\Cl_{C(X,Y)}(\ovl{\scr A})=\ovl{\scr A}$ since $f$ can be approximated by elements of $\ovl{\scr A}$. This proves that $\ovl{\scr A}$ is compact.
\end{proof}

\begin{rem}\label{lb524}
Theorem \ref{lb509} is of fundamental importance because most compactness results about function spaces (such as Arzel\`a-Ascoli Thm. \ref{lb516}, Banach-Alaoglu Thm. \ref{lb519}) are derived from this theorem. As we will see in Claim \ref{lb510}, when $X$ is separable, the proof of Thm. \ref{lb509} uses only the countable version of Tychonoff theorem, and hence not using Zorn's lemma. This is in line with the history that Thm. \ref{lb509} (which is implicit in the proof of Arzel\`a-Ascoli theorem) appeared earlier than Zorn's lemma and was proved using diagonal method. (If you remember, the proof of countable Tychonoff theorem uses diagonal method, cf. Thm. \ref{lb89}. And we will use Thm. \ref{lb89} to prove Claim \ref{lb510}.) 

In the late 19th and early 20th centuries, the diagonal method was often used to derive compactness properties of function spaces. Prominent examples are Hilbert's and Schmidt's solutions of eigenvalue problems in integral equations (cf. Subset. \ref{lb370}) and F. Riesz's solution of moment problems (cf. Rem. \ref{lb526}). Thus, Thm. \ref{lb509} can be viewed as a summary of this method.  \hfill\qedsymbol
\end{rem}



\begin{comment}
\begin{rem}
In the above theorem, one can also prove that $\scr A$ is precompact without using Cor. \ref{lb508}: As in the above proof, we have $\scr A\subset S=\prod_{x\in X}K_x$ where $S$ is compact. So $\Cl_S(\scr A)$ is compact. Thus $\scr A$ is precompact in $C(X,Y)$ if we can prove that $\Cl_S(\scr A)$ is a subset of $C(X,Y)$. But this follows immediately from Cor. \ref{lb303}, or from the last paragraph of the above proof.
\end{rem}
\end{comment}


The following exercise is a variant of Thm. \ref{lb509}. Another variant, the Banach-Alaoglu theorem, will be discussed in the next section.



\begin{exe}\label{lb536}
Let $X$ be a metric space, and equip $C(X,\Rbb^N)$ with the pointwise convergence topology. Let $\scr A$ be a pointwise bounded subset of $C(X,\Rbb^N)$. Assume that $\scr A$ has a uniform Lipschitz constant $L<+\infty$. (Namely, $\Vert f(x)-f(y)\Vert\leq L\cdot d(x,y)$ for all $f\in\scr A$ and $x,y\in X$.) Prove that $\scr A$ is precompact in $C(X,\Rbb^N)$, and $\ovl{\scr A}$ has a uniform Lipschitz constant $L$. \footnote{In fact, instead of assuming that $\scr A$ is pointwise bounded, it suffices to assume that $\scr A(x)$ is bounded for some $x\in X$. Then the pointwise boundedness will follow automatically from the uniform Lipschitz continuity. Can you see why?}
\end{exe}



The proof of Thm. \ref{lb509} uses Tychonoff theorem for uncountable product spaces, and hence relies on Zorn's lemma. In the following, we shall show that Zorn's lemma is not needed when $X$ is separable. We first need a preparatory result: the following proposition is the equicontinuous analog of Prop. \ref{lb288}.



\begin{pp}\label{lb511}
Let $\mc V$ be a Banach space. Let $X$ be a topological space. Let $(f_\alpha)_{\alpha\in I}$ be an equicontinuous net in $C(X,\mc V)$ converging pointwise on a dense subset $E$ of $X$. Then $(f_\alpha)$ converges pointwise on $X$ to some $f\in C(X,\mc V)$.
\end{pp}

It follows that if $(f_\alpha)$ also converges pointwise on $E$ to some $g\in C(X,\mc V)$, then $(f_\alpha)$ converges pointwise on $X$ to $g$. (Indeed, since $f|_E=g|_E$, we have $f=g$ because $f,g$ are continuous and $E$ is dense.)  

\begin{proof}
Let $x\in X$. Since $\mc V$ is complete, to show that $(f_\alpha(x))$ converges, it suffices to prove that $(f_\alpha(x))_{\alpha\in I}$ is a Cauchy net. Choose any $\eps>0$. Since $(f_\alpha)$ is equicontinuous at $x$, there exists $U\in\Nbh_X(x)$ such that $\diam(f_\alpha(U))<\eps$ for all $\alpha$. Since $E$ is dense in $X$, $E$ intersects $U$. Pick $p\in E\cap U$. Since $(f_\alpha(p))$ is a Cauchy net, we have $\lim_{\alpha,\beta\in I}\Vert f_\alpha(p)-f_\beta(p)\Vert=0$. Then
\begin{align*}
&\Vert f_\alpha(x)-f_\beta(x)\Vert\leq \Vert f_\alpha(x)-f_\alpha(p)\Vert+ \Vert f_\alpha(p)-f_\beta(p)\Vert+  \Vert f_\beta(p)-f_\beta(x)\Vert\\
\leq&\Vert f_\alpha(p)-f_\beta(p)\Vert+2\eps
\end{align*}
where the RHS converges to $2\eps$ under $\limsup_{\alpha,\beta\in I}$. Therefore $\limsup_{\alpha,\beta\in I}\Vert f_\alpha(x)-f_\beta(x)\Vert$ is $\leq 2\eps$. Since $\eps$ is arbitrary, we conclude $\limsup_{\alpha,\beta\in I}\Vert f_\alpha(x)-f_\beta(x)\Vert=0$.

We have proved that $(f_\alpha)$ has a pointwise limit $f:X\rightarrow\mc V$. Since $(f_\alpha)$ is equicontinuous, by Cor. \ref{lb303}, $f$ is continuous.
\end{proof}

\begin{comment}
\begin{rem}\label{lb520}
Under the assumptions of Prop. \ref{lb511}, suppose we know that $(f_\alpha)$ converges pointwise on $E$ to $f\in C(X,\mc V)$, then $(f_\alpha)$ converges pointwise on $X$ to $f$.
\end{rem}


\begin{proof}
Let $\wtd f:X\rightarrow\mc V$ be the pointwise limit of $(f_\alpha)$, which is continuous by Cor. \ref{lb303}. So $\wtd f|_E=f|_E$. Since $f$ is assumed to be continuous, and since $E$ is dense, we conclude $\wtd f=f$.
\end{proof}

Thus, in order to show that a net of equicontinuous functions converges pointwise to some continuous function, it suffices to check the pointwise convergence on a convenient dense subset. This observation is often used for bounded linear maps:

\begin{co}
Let $\mc V,\mc W$ be normed vector spaces over $\Fbb\in\{\Rbb,\Cbb\}$ where $\mc W$ is Banach. Let $\mc E$ be a subset of $\mc V$ spanning a dense subspace of $\mc V$. Let $(T_\alpha)$ be a net in $\fk L(\mc V,\mc W)$ converging pointwise on $\mc E$ to $T\in\fk L(\mc V,\mc W)$. Assume that $\sup_\alpha\Vert T_\alpha\Vert<+\infty$. Then $(T_\alpha)$ converges pointwise on $\mc V$ to $T$.
\end{co}

In fact, there is no need to assume that $\mc W$ is complete: One can  embed $\mc W$ into its completion $\wht{\mc W}$, prove the pointwise convergence of $(T_\alpha)$ to $T$ in $\wht{\mc W}$, and then restrict the codomain to $\mc W$ (since we know that the ranges of $T_\alpha$ and $T$ are in $\mc W$).

\begin{proof}
Let $\mc F=\Span_\Qbb\mc E$ if $\Fbb=\Rbb$ and $\mc F=\Span_{\Qbb+\im\Qbb}\mc E$ if $\Fbb=\Cbb$. Then $\mc F$ is dense in $\mc V$. By linearity, $(T_\alpha)$ converges pointwise on $\mc F$ to $T$. Let $L=\sup_\alpha\Vert T_\alpha\Vert$, which is a Lipschitz constant for all $T_\alpha$ by Prop. \ref{lb313}. So $(T_\alpha)$ is an equicontinuous net. By Prop. \ref{lb511} and Rem. \ref{lb520}, $(T_\alpha)$ converges pointwise on $\mc V$ to $T$.
\end{proof}
\end{comment}






\begin{claim}\label{lb510}
When $X$ is a separable topological space, Thm. \ref{lb509} can be proved without using Zorn's lemma.
\end{claim}

We know that separable is equivalent to second countable when $X$ is metrizable, but is slightly weaker in general (cf. Sec. \ref{lb253}). In practice, however, almost all separable topological spaces you will encounter are Hausdorff and second countable. So there is no need to count the nuances of separability and second countability.

\begin{proof}
Let $Y=\Rbb^N$. As in the proof of Thm. \ref{lb509}, $\ovl{\scr A}=\Cl_{C(X,Y)}(\scr A)$ is equicontinuous and pointwise bounded. Let $(f_\alpha)$ be a net in $\ovl{\scr A}$. Since $X$ is separable, we can choose a countable dense subset $E\subset X$. For each $p\in E$, $\ovl{\scr A}(p)$ is contained in a compact $K_p\subset Y$. By the countable Tychonoff theorem (whose proof does not rely on Zorn's lemma, see Thm. \ref{lb89} or Pb. \ref{lb241}), $\prod_{p\in E}K_p$ is compact. Therefore, $(f_\alpha)$ has a subnet $(f_\beta)$ converging pointwise on $E$. Since $\ovl{\scr A}$ is equicontinuous, so is $(f_\beta)$. Therefore, by Prop. \ref{lb511}, $(f_\beta)$ converges pointwise on $X$ to some $f\in C(X,Y)$. So $f\in\Cl_{C(X,Y)}(\ovl{\scr A})=\ovl{\scr A}$. This proves that $\ovl{\scr A}$ is compact.
\end{proof}

\begin{rem}
Claim \ref{lb510} and its proof can be compared with Pb. \ref{lb501}. In particular,  Prop. \ref{lb500} plays the same role in the solution of Pb. \ref{lb501} as Prop. \ref{lb511} does in the proof of Claim \ref{lb510}. In both situations, if one wants to prove the separable case without using Zorn's lemma, one needs an extra analytic step to pass from a dense subset to the original space. 
\end{rem}


\begin{rem}
The readers may wonder why I often give two proofs of the same theorem, one using Zorn's lemma, which applies to a (slightly) larger setting and is somewhat simpler, and the other not using Zorn's lemma, but requires more extra steps. I have mentioned that Zorn's lemma is equivalent to the axiom of choice. However, whether or not to accept the axiom of choice is a matter of taste (or faith). After all, both the statements and the proof of Zorn's lemma are very hard to understand intuitively (at least to me).  

Nowadays, most mathematicians accept it because it often simplifies proofs and theories, it often proves theorems for a larger class of examples, and it is compatible with the theorems proved without using it. Therefore, the more theorems that can be proved both with and without Zorn's lemma, the more reason there is to believe in Zorn's lemma/axiom of choice. If there is any way to understand Zorn's lemma intuitively, it is to compare a proof using Zorn's lemma with a proof of the same theorem without using it, as we did in Sec. \ref{lb513} and Ch. \ref{lb512} and continue to do in this chapter.   \hfill\qedsymbol
\end{rem}



\subsubsection{Precompactness under uniform convergence topology}

In the last subsection, we see that equicontinuity implies precompactness under the pointwise convergence topology. The converse is not necessarily true. In this section, we will see that under reasonable assumptions, equicontinuity is equivalent to precompactness under the uniform convergence topology.


\begin{thm}\label{lb515}
Let $X$ be a topological space. Let $\mc V$ be a Banach space. Equip $C(X,\mc V)$ with the uniform convergence metric. Let $\scr A$ be a precompact subset of $C(X,\mc V)$. Then $\scr A$ is equicontinuous.
\end{thm}

\begin{proof}[First proof]
It suffices to prove that $\ovl{\scr A}$ is equicontinuous. Thus, by replacing $\scr A$ with $\ovl{\scr A}$, we assume that $\scr A$ is compact (under the uniform convergence metric).

Let us prove the equicontinuity of $\scr A$ at any $x\in X$. Choose any $\eps>0$. For each $f\in\scr A$, let $U_f$ be the open ball centered at $f$ with radius $\eps$, i.e.
\begin{align*}
W_f=\big\{g\in C(X,\mc V):\Vert g-f\Vert_{l^\infty(X,\mc V)}<\eps\big\}
\end{align*}
Since $\scr A$ is compact, there is a finite subset $\mc E\subset\scr A$ such that $\scr A\subset\bigcup_{f\in\mc E}W_f$. Since each $f\in\mc E$ is continuous at $x$, and since $\mc E$ is finite, there exists $U\in\Nbh_X(x)$ such that $\diam(f(U))<\eps$ for all $f\in\mc E$. Therefore, for each $g\in\scr A$, choose $f\in\mc E$ such that $g\in U_f$. Then since $\diam (f(U))<\eps$ for that particular $f$, by triangle inequality, we have $\diam (g(U))<3\eps$. Thus $\sup_{g\in\scr A}\diam(g(U))\leq 3\eps$. Since $\eps$ can be arbitrary, we conclude that $\scr A$ is equicontinuous at $x$.
\end{proof}



\begin{proof}[Second proof]
This is a fancy proof, just for entertainment. Again, we assume WLOG that $\scr A$ is compact. Consider the inclusion map $\scr A\mapsto C(X,\mc V)$ (sending $f$ to $f$). By Thm. \ref{lb274}, it can be viewed as a continuous map
\begin{gather*}
X\times\scr A\rightarrow \mc V\qquad (x,f)\mapsto f(x)
\end{gather*}
Since $\scr A$ is compact, by Thm. \ref{lb274}, the above map can be viewed as a continuous map
\begin{align*}
\Phi:X\rightarrow C(\scr A,\mc V)
\end{align*}
where for each $x\in X$, $\Phi(x):\scr A\rightarrow \mc V$ sends $f$ to $f(x)$. By enlarging the codomain of $\Phi$, we can view $\Phi$ as a map $X\rightarrow \mc V^{\scr A}$ where $\mc V^{\scr A}$ is equipped with the uniform convergence topology. The continuity of $\Phi$ means, by the very definition of equicontinuity (cf. Def. \ref{lb514}), that $\scr A$ is equicontinuous.
\end{proof}



\begin{thm}[\textbf{Arzel\`a-Ascoli (AA) theorem}]\index{00@Arzel\`a-Ascoli (AA) theorem} \label{lb516}
Let $X$ be a compact topological space. Equip $C(X,\Rbb^N)$ with the uniform convergence topology. Let $\scr A$ be a subset of $C(X,\Rbb^N)$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\scr A$ is a precompact subset of $C(X,\Rbb^N)$.
\item $\scr A$ is pointwise bounded and equicontinuous.
\end{enumerate}
\end{thm}

%Note that ``uniformly bounded" means that $\sup_{f\in\scr A}\Vert f\Vert_\infty<+\infty$, or equivalently, that $\bigcup_{x\in X}\scr A(x)$ is bounded. 


\begin{proof}
Assume (1). For each $x\in X$, since the map $f\in\scr A\mapsto f(x)$ is continuous and $\scr A$ is compact, by the extreme value theorem, we have $\sup_{f\in\scr A}\Vert f(x)\Vert<+\infty$. So $\scr A$ is pointwise bounded. By Thm. \ref{lb515}, $\scr A$ is equicontinuous.

Assume (2). Write $Y=\Rbb^N$. Note that the uniform convergence topology is metrizable. Thus, to prove (1), by Prop. \ref{lb505} it suffices to choose an arbitrary net $(f_\alpha)$ in $\scr A$ and show that it has a cluster point in $C(X,Y)$. By Thm. \ref{lb509}, $\scr A$ is precompact under the pointwise convergence topology. So $(f_\alpha)$ has a subnet $(f_\beta)$ converging pointwise to some $f\in C(X,Y)$. Since $X$ is compact and $(f_\beta)$ is equicontinuous, by Cor. \ref{lb284}, $(f_\beta)$ converges uniformly to $f$. 
\end{proof}


In the next chapter, we will use the AA theorem to study differential equations. See Thm. \ref{lb557}.


\begin{rem}
The proof of AA theorem relies on Thm. \ref{lb509}, and hence in turn relies on Zorn's lemma. If $X$ is separable, then AA theorem does not rely on Zorn's lemma since Thm. \ref{lb509} does not (cf. Claim \ref{lb510}).
\end{rem}

\begin{srem}
One may wonder if AA theorem still holds when $\Rbb^N$ is replaced by an arbitrary normed vector space $\mc V$ (or even a metric space). In fact, in this case, if we assume that $\scr A$ is \textbf{pointwise precompact} (i.e., for each $x\in X$, $\scr A(x)$ is precompact in $\mc V$), then Thm. \ref{lb509} still holds, as one can check by reading the proof of Thm. \ref{lb509}. Therefore, AA theorem also holds if ``pointwise bounded" is replaced by ``pointwise precompact". However, in most applications, $\mc V$ is $\Rbb^N$.
\end{srem}

%% Record #26 2023/12/20 three lectures  65

\subsection{Operator norms and compactness: Banach-Alaoglu theorem}\label{lb571}


\begin{displayquote}
\small Every interesting topological space is a metric space. Every interesting Banach space is separable. Every interesting real-valued function is Baire/Borel measurable.

\hfill ---- Barry Simon \cite[Preface of Part 1]{Sim-R}
\end{displayquote}


In this section, we fix a normed vector space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$.

We are going to apply the results and methods in Sec. \ref{lb517} to linear maps on $V$. Note that if $W$ is a normed vector space over $\Fbb$, the operator norm on $\fk L(V,W)$ is defined to be $\Vert T\Vert=\Vert T\Vert_{l^\infty(\ovl B_V(0,1),W)}$. Therefore, if $(T_\alpha)$ is a net in $\fk L(V,W)$ and $T\in\fk L(V,W)$, then
\begin{align}
\lim_\alpha \Vert T_\alpha-T\Vert=0\qquad\Longleftrightarrow\qquad T_\alpha\rightrightarrows T\text{ on }\ovl B_V(0,1) 
\end{align}
In other words, \uline{operator norms describe the uniform convergence of linear maps on the closed unit balls}.

To apply the results in the last section, we must let $W$ be $\Fbb^N$. Let us consider the case $W=\Fbb$. Then $\fk L(V,\Fbb)=V^*$. The closed unit ball $\ovl B_V(0,1)$ is not compact unless when $V$ is finite dimensional. (See Thm. \ref{lb565}.) We have seen such an example in Exp. \ref{lb518} where $V=C([0,1],\Rbb)$. Therefore, the Arzel\`a-Ascoli theorem is not available. Thus, one cannot expect general compactness in $V^*$ if the topology on $V^*$ is the uniform  convergence topology on $\ovl B_V(0,1)$, i.e. the topology induced by the operator norm. To get compactness in $V^*$, one must consider the pointwise convergence topology.



\subsubsection{Weak-* topology and Banach-Alaoglu theorem}


\begin{df}\label{lb625}
The topology on $V^*=\fk L(V,\Fbb)$ inherited from the product topology on $\Fbb^V$ is called the \textbf{weak-* topology}. \index{00@Weak-* topology} A net $(\varphi_\alpha)$ in $V^*$ is said to \textbf{converge weak-*} to $\varphi\in V^*$ if $(\varphi_\alpha)$ converges to $\varphi$ under the weak-* topology, equivalently, if $(\varphi_\alpha)$ converges pointwise to $\varphi$ when viewed as functions $V\rightarrow\Fbb$. By contrast, the \textbf{norm topology} \index{00@Norm topology} (cf. Def. \ref{lb529}) on $V^*$ is the topology induced by the (operator) norm of $V^*$.
\end{df}

Note that since $\ovl B_V(0,1)$ spans $V$, a net of linear maps converges pointwise on $V$ iff it converges pointwise on $\ovl B_V(0,1)$. Therefore, the weak-* topology is also the one induced by the product topology on $\Fbb^{\ovl B_V(0,1)}$.


We first prove the normed vector space version of Prop. \ref{lb511}.


\begin{pp}\label{lb520}
Let $V$ and $W$ be normed vector spaces over $\Fbb$ where $W$ is a Banach space. Let $M\in\Rbb_{\geq0}$, and let $(T_\alpha)$ be a net in $\fk L(V,W)$ such that $\Vert T_\alpha\Vert\leq M$ for all $\alpha$. Let $E$ be a subset of $V$ spanning a dense subspace of $V$. Assume that $(T_\alpha)$ converges pointwise on $E$. Then $(T_\alpha)$ converges pointwise on $V$ to some $T\in\fk L(V,W)$ satisfying $\Vert T\Vert\leq M$.
\end{pp}

It follows that if $(T_\alpha)$ also converges pointwise on $E$ to some $T'\in\fk L(V,W)$, then $(T_\alpha)$ converges pointwise on $V$ to $T'$. (This is because both $T$ and $T'$ are bounded linear, and they are equal on the dense subset $\Span_\Fbb E$ of $V$. So $T=T'$.)

\begin{proof}
By assumption, $F=\Span_\Fbb E$ is dense in $V$. By linearity, $(T_\alpha)$ converges pointwise on $F$. By Prop. \ref{lb313}, $(T_\alpha)$ has uniform Lipschitz constant $M$. So $(T_\alpha)$ is an equicontinuous net. By Prop. \ref{lb511} and the completeness of $W$, $(T_\alpha)$ converges pointwise on $V$ to $T\in C(V,W)$.

For each $u,v\in V,a,b\in\Fbb$ we have
\begin{align*}
T(au+bv)=\lim_\alpha T_\alpha(au+bv)=\lim_\alpha (a T_\alpha(u)+b T_\alpha(v))=a T(u)+b T(v)
\end{align*}
So $T$ is linear. For each $v\in V$, since $\Vert T_\alpha(v)\Vert\leq \Vert T_\alpha\Vert\cdot\Vert v\Vert\leq M \Vert v\Vert$, we have
\begin{align*}
\Vert T(v)\Vert=\lim_\alpha\Vert T_\alpha(v)\Vert\leq M\Vert v\Vert
\end{align*}
So $\Vert T\Vert\leq M$ by Rem. \ref{lb372}.
\end{proof}


Roughly speaking, Prop. \ref{lb520} says that if a net of bounded linear operators have a uniform upper bound for their operator norms, then pointwise convergence on a dense subset (or more generally, on a subset spanning a dense subspace) implies pointwise convergence on the whose space. The assumption on the uniform upper bound cannot be removed:
\begin{eg}
For each $n\in\Zbb_+$, define $T_n:l^1(\Zbb_+,\Rbb)\rightarrow\Rbb$ sending each $f$ to $n^3f(n)$. Let $E=\{\chi_{\{k\}}:k\in\Zbb_+\}$. Then $E$ spans a dense subspace of $l^1(\Zbb_+,\Rbb)$. For each $k\in\Zbb_+$ we have $\lim_n T_n\chi_{k}=0$. However, $\Vert T_n\Vert=n^3$ has no uniform upper bounds. Define $f\in l^1(\Zbb_+,\Rbb)$ by $f(n)=n^{-2}$. Then $T_nf=n$ does not converge in $\Rbb$ as $n\rightarrow\infty$. So $(T_n)$ does not converge pointwise on $l^1(\Zbb_+,\Rbb)$, although it converges pointwise on $E$ (and hence on $\Span E$) to $0$.
\end{eg}



The following Banach-Alaoglu theorem can be viewed as the normed vector space version of Thm. \ref{lb509}.

\begin{thm}[\textbf{Banach-Alaoglu theorem}]\index{00@Banach-Alaoglu theorem}\label{lb519}
$\ovl B_{V^*}(0,1)$ is \textbf{weak-* compact}, \index{00@Weak-* compact} i.e., it is compact under the weak-* topology. 
\end{thm}

By our notations, $\ovl B_{V^*}(0,1)$ is the set of all $\varphi\in V^*$ satisfying $\Vert\varphi\Vert\leq 1$. Note that weak-* topology is clearly Hausdorff.

\begin{proof}
Let $\scr A=\ovl B_{V^*}(0,1)$. Since elements in $\scr A$ have operator norms $\leq 1$, they have Lipschitz constant $1$ by Prop. \ref{lb313}. So $\scr A$ is equicontinuous on $V$. For each $v\in V$, $\scr A(v)$ is bounded since $\scr A(v)\subset\ovl B_\Fbb(0,\Vert v\Vert)$. Thus, by Thm. \ref{lb509}, $\scr A$ has compact closure in $C(V,\Fbb)$ under the pointwise convergence topology. Therefore, to show that $\scr A$ is compact, it suffices to show that $\scr A$ is closed in $C(V,\Fbb)$. Let $(\varphi_\alpha)$ be a net in $\scr A$ converging pointwise to $\varphi\in C(V,\Fbb)$. By Prop. \ref{lb520}, $\varphi\in V^*$ and $\Vert\varphi\Vert\leq 1$. So $\scr A$ is closed.
\end{proof}


\begin{rem}\label{lb525}
Similar to Arzel\`a-Ascoli theorem, the proof of the Banach-Alaoglu theorem relies on Thm. \ref{lb509}, and hence relies on Zorn's lemma. If $V$ is a separable normed vector space, the Banach-Alaoglu theorem does not rely on Zorn's lemma because the proof of Thm. \ref{lb509} does not, cf. Claim \ref{lb510}.
\end{rem}







\subsubsection{Application: embedding into $C(X,\Fbb)$}

Recall that $V$ is a normed vector space over $\Fbb$.





\begin{thm}\label{lb521}
There is a compact Hausdorff space $X$ and a linear isometry $\Phi:V\rightarrow C(X,\Fbb)$. Moreover, if $V$ is separable, then $X$ can be chosen to be metrizable.\footnote{In fact, every separable normed vector space can be embedded into $C([0,1],\Fbb)$. This is called the \textbf{Banach-Mazur theorem}, whose proof is more involved. Cf. \cite[Sec. 1.4]{AK}.} 
\end{thm}

In other words, $V$ is isomorphic to a linear subspace of $C(X,\Fbb)$ (namely, $\Phi(V)$). Clearly, if $V$ is Banach, then $\Phi(V)$ is complete and hence closed. So each Banach space is isomorphic to a closed linear subspace of $C(X,\Fbb)$ for some $X$.

A similar embedding for metric spaces is given in Pb. \ref{lb537}.

\begin{proof}
We let $X=\ovl B_{V^*}(0,1)$, equipped with the weak-* topology. By Banach-Alaoglu, $X$ is a compact Hausdorff space. The linear map $\Phi:V\rightarrow C(X,\Fbb)$ is defined by sending each $v$ to the function
\begin{align*}
\Phi(v):X\rightarrow \Fbb\qquad\varphi\mapsto \bk{\varphi,v}
\end{align*}
To check the continuity of $\Phi(v):X\rightarrow\Fbb$, we let $(\varphi_\alpha)$ be any net in $X$ converging weak-* to $\varphi\in X$. Then $\Phi(v)(\varphi_\alpha)=\bk{\varphi_\alpha,v}$ converges to $\bk{\varphi,v}=\Phi(v)(\varphi)$, proving that $\Phi(v)$ is continuous.

Let $v\in V$. For each $\varphi\in X$, we have
\begin{align*}
|\bk{\Phi(v),\varphi}|=|\bk{\varphi,v}|\leq\Vert\varphi\Vert\cdot\Vert v\Vert=\Vert v\Vert
\end{align*}
This proves that $\Vert \Phi(v)\Vert_{l^\infty(X,\Fbb)}\leq \Vert v\Vert$. (Recall Rem. \ref{lb372}.) By Hahn-Banach Cor. \ref{lb502}, there is $\varphi\in X$ with $\Vert\varphi\Vert=1$ such that $\bk{\varphi,v}=\Vert v\Vert$. Thus $\Vert \Phi(v)\Vert_{l^\infty(X,\Fbb)}=\Vert v\Vert$. This proves that $\Phi$ is a linear isometry.

Suppose that $V$ is separable. Then we can find a sequence $(v_n)_{n\in\Zbb_+}$ dense in $V$.  It is clear that $\Phi(V)$ separates points of $X$. So $\Phi(v_1),\Phi(v_2),\dots$ separate points of $X$.
Therefore,
\begin{align}
X\rightarrow S=\Fbb^{\Zbb_+}\qquad \varphi\mapsto \big(\Phi(v_n)(\varphi)\big)_{n\in\Zbb_+}
\end{align} 
is a continuous injective map of $X$ into $S$. Since $X$ is compact, $X$ is homeomorphic to $\Phi(X)$. Since $S$ is metrizable, so is $X=\ovl B_{V^*}(0,1)$.
\end{proof}


As an immediate consequence of the above proof we have:

\begin{thm}\label{lb523}
$V$ is separable iff $\ovl B_{V^*}(0,1)$ is metrizable (under the weak-* topology).
\end{thm}

Recall Thm. \ref{lb482} for equivalent descriptions of metrizable compact Hausdorff spaces. In fact, Thm. \ref{lb523} is closely related to Thm. \ref{lb482}, since the relationship between $V$ and $\ovl B_{V^*}(0,1)$ is similar to that between $C(X,\Rbb)$ and $X$, as implied by the proof of Thm. \ref{lb521}. 

\begin{proof}
The proof of Thm. \ref{lb521} shows that if $V$ is separable then $X=\ovl B_{V^*}(0,1)$ is metrizable. Conversely, assume that $X$ is metrizable. The proof of Thm. \ref{lb521} shows that $V$ is isomorphic to a linear subspace of $C(X,\Rbb)$. By Thm. \ref{lb482}, $C(X,\Rbb)$ is second countable. So $V$ is second countable, equivalently, separable.
\end{proof}

\begin{rem}
It follows from Thm. \ref{lb523} that if $V$ is separable then $\ovl B_{V^*}(0,1)$ is sequentially compact. In history, at a time when sequential compactness was still the primary way for people to understand compactness, there were good reasons for studying the sequential compactness of $\ovl B_{V^*}(0,1)$. This is because early examples of Banach spaces that people focused on were separable. 
\end{rem}

\begin{rem}
A typical example of a non-separable Banach space is $l^\infty(\Zbb,\Fbb)$, cf. Pb. \ref{lb567}. Precisely for this reason, the dual space of $l^\infty(\Zbb,\Fbb)$ is not much studied, and the norm topology on $l^\infty(\Zbb,\Fbb)$ is not good enough. The weak-* topology on (the unit ball of) $l^\infty(\Zbb,\Fbb)$ is more natural since, given the equivalence $l^1(\Zbb,\Fbb)^*\simeq l^\infty(\Zbb,\Fbb)$ (cf. Thm. \ref{lb527}) and the separability of $l^1(\Zbb,\Fbb)$, the closed unit ball of $l^\infty(\Zbb,\Fbb)$ is weak-* metrizable (equivalently, secound countable).\footnote{If $V$ is infinite dimensional, the weak-* topology of $V^*$ is indeed not first countable, and hence is neither metrizable nor second countable. \textit{Therefore}, the weak-* topology of $\ovl B_{V^*}(0,1)$ is more natural than that of $V^*$.} In the study of modern analysis, it is helpful to keep in mind the following two principles:
\begin{itemize}
\item Metrizability and second-countability are tests for whether or not a topological space is natural (e.g. whether or not it is reasonable from a natural science point of view% \footnote{For example, almost all topological vector spaces or their meaningful subsets related to quantum physics are metrizable and second countable.}
).
\item However, proving theorems only for metrizable and second-countable spaces will actually make the theory more complicated. It is mainly for the purpose of simplifying the theory (e.g. making the assumptions in the theorems shorter) that we prove the theorems in general, regardless of whether a topological space is metrizable/second-countable or not.\footnote{For example, the fact that $(l^1)^*\simeq l^\infty$ shows that separability is not closed under taking dual. Thus, if we stick to separable Banach spaces, the use of Hahn-Banach and Banach-Alaoglu will be more restricted (e.g. when discussing the relationship between $V$ and $V^{**}$, see Hahn-Banach Cor. \ref{lb502} and Goldstine's Thm. \ref{lb568}).}
\end{itemize}
In Rem. \ref{lb569}, I will say more about the significance of Thm. \ref{lb523}.
\end{rem}




\subsection{Banach-Alaoglu for $L^p$ and $l^p$ spaces}


So far, we have discussed $V^*$ and its weak-* topology on a very abstract level. In the following, we shall understand the results proved in Sec. \ref{lb571} in a more concrete setting.

\subsubsection{Weak-* topology in context}

Let $1<p\leq +\infty$ and $1\leq q<+\infty$ satisfy $\frac 1p+\frac 1q=1$. Let $I$ be an interval in $\Rbb$. Let $L^p(I)$ be the set of Lebesgue measurable functions \footnote{Lebesgue measurable functions are more general than Riemann integrable functions. We will discuss them in the next semester. To understand the material of this section, it is not important to know the precise meaning of them.} $f\rightarrow\Cbb$ satisfying that the \textbf{$L^p$-(semi)norm} $\Vert f\Vert_p=\sqrt[p]{\int_0^1 |f|^p}$ is finite. A remarkable \textbf{representation theorem} of F. Riesz says that we have an (isometric) isomorphism of Banach spaces
\begin{align*}
\Phi:L^p(I)\rightarrow L^q(I)^*
\end{align*}
such that for each $f\in L^p(I)$, $\Phi(f)$ is the linear map sending each $g\in L^q(I)$ to
\begin{align*}
\bk{\Phi(f),g}=\int fg
\end{align*}
Therefore, if $(f_\alpha)$ is a net in $L^p(I)$ and $f\in L^p(I)$, then $(f_\alpha)$ converges weak-* to $f$ (more precisely, $(\Phi(f_\alpha))$ converges weak-* to $\Phi(f)$) iff $\lim_\alpha\int_I f_\alpha g=\int_I fg$ for all $g\in L^q(I)$. 

In fact, when $I=[-\pi,\pi]$, using the $l^\infty$-density of $\Span\{e^{\im nx}:n\in\Zbb\}$ in $C([-\pi,\pi])$ (Exp. \ref{lb443}), it can be proved that if $(f_\alpha)$ is a net in $L^p([-\pi,\pi])$ satisfying $\sup_\alpha \Vert f_\alpha\Vert_p<+\infty$, then $(f_\alpha)$ converges weak-* to $f\in L^p([-\pi,\pi])$ iff
\begin{align*}
\lim_\alpha \int_{-\pi}^\pi f_\alpha(x)e^{-\im nx}dx=\int_{-\pi}^\pi f(x)e^{-\im nx}dx
\end{align*}
i.e., iff the Fourier coefficients of $(f_\alpha)$ converge to the corresponding ones of $f$.

\begin{rem}\label{lb569}
Let me discuss the importance of Thm. \ref{lb523} in the context of $L^p$ spaces. As we will see in the future, Lebesgue measure (and measure theory in general) is not very compatible with net convergence. The main reason is that measure theory is countable by nature, as one can feel in Sec. \ref{lb522}. For example, the pointwise limit of a sequence of Lebesgue measurable functions is Lebesgue measurable, but the pointwise limit of a net of measurable functions is not necessarily so. Lebesgue's dominated convergence theorem, a powerful theorem about the commutativity of limits and integrals, applies only to sequences but not nets of functions. 

However, it is true that $L^q(I)$ is separable. Therefore, by Thm. \ref{lb523}, the closed unit ball of $L^p(I)$ is a metrizable compact space under the weak-* topology. Therefore, to study the weak-* convergence for functions $f\in L^p(I)$ satisfying $\Vert f\Vert_p\leq 1$, it suffices to use sequences instead of nets, because metrizable topologies and their compactness are determined by sequential convergence. Therefore, one can use all the results in measure theory to study the weak-* topology on $\ovl B_{L^p(I)}(0,1)$.  \hfill\qedsymbol
\end{rem}

\subsubsection{Weak-* topology on $l^p$ spaces}

Let $\Fbb\in\{\Rbb,\Cbb\}$, and let $X$ be a set. Recall that $1<p\leq +\infty$ and $1\leq q<+\infty$ satisfy $\frac 1p+\frac 1q=1$. 

In this subsection, we prove that the linear isometry $\Psi$ in Thm. \ref{lb369} is surjective, thus establishing the isomorphism $l^p(X,\Fbb)\simeq l^q(X,\Fbb)^*$. Using this isomorphism, we give a concrete (and historically important) description of weak-* topology on the norm-bounded subsets of $l^p(X,\Fbb)$. We introduce a temporary notation
\begin{align}
\mc S(X,\Fbb)=\{f\in\Fbb^X:f=0\text{ except at finitely many points}\}
\end{align}
Then $\mc S(X,\Fbb)$ is clearly a subspace of $l^q(X,\Fbb)$.


\begin{lm}\label{lb528}
$\mc S(X,\Fbb)$ is dense in $l^q(X,\Fbb)$ under the $l^q$-norm.
\end{lm}

\begin{proof}
Let $f\in l^q(X,\Fbb)$. Then $\lim_{A\in\fin(2^X)}\sum_A|f|^q=\sum_X |f|^q<+\infty$. Thus, for every $\eps>0$ there is $A\in\fin(2^X)$ such that $\sum_{X\setminus A}|f|^q<\eps^q$, and hence $\Vert f-f\chi_A\Vert_q<\eps$. This finishes the proof, since $f\chi_A\in\mc S(X,\Fbb)$.
\end{proof}


\begin{exe}
Show that $\mc S(X,\Fbb)$ is not dense in $l^\infty(X,\Fbb)$ if $X$ is an infinite set.
\end{exe}

\begin{thm}\label{lb527}
The linear isometry
\begin{gather*}
\Psi:l^p(X,\Fbb)\rightarrow l^q(X,\Fbb)^*\\
\bk{\Psi(f),g}=\sum_{x\in X}f(x)g(x)
\end{gather*}
in Thm. \ref{lb369} is an isomorphism of Banach spaces.
\end{thm}

In the special case that $p=2$, Thm. \ref{lb527} is called the \textbf{Riesz-Fr\'echet representation theorem}. \index{00@Riesz-Fr\'echet representation theorem} Recall that we are assuming $1<p\leq+\infty$ in this theorem. When $X$ is infinite and $p=1$, $\Psi$ is not surjective. See Pb. \ref{lb532} for details.

%% Record #27 2023/12/25 two lectures  67

\begin{proof}
It remains to prove that $\Psi$ is surjective. Choose a nonzero bounded linear $\Lambda:l^q(X,\Fbb)\rightarrow\Fbb$. We want to show that $\Lambda$ is in the range of $\Psi$. By scaling $\Lambda$ we assume for simplicity that $\Vert\Lambda\Vert=1$.  Define 
\begin{align}
f:X\rightarrow\Fbb\qquad  f(x)=\bk{\Lambda,\chi_{\{x\}}}
\end{align}
Let us prove that $f\in l^p(X,\Fbb)$. If $p=+\infty$, then $|f(x)|\leq \Vert\Lambda\Vert\cdot\Vert\chi_{\{x\}}\Vert_q=1$, and hence $\Vert f\Vert_\infty\leq 1$. Assume $p<+\infty$. We understand $\ovl f(x)/|f(x)|$ as $0$ if $f(x)=0$. Choose any $A\in\fin(2^X)$. Define $g:X\rightarrow\Fbb$ to be $g=(\ovl f/|f|)\cdot |f|^{p-1}\chi_A$. Then clearly $g\in l^q(X,\Fbb)$. We compute
\begin{align*}
\bk{\Lambda,g}=\bigbk{\Lambda,\sum_{x\in A}g(x)\chi_{\{x\}}}=\sum_{x\in A}g(x)\bk{\Lambda,\chi_{\{x\}}}=\sum_{x\in A}f(x)g(x)=\sum_A|f|^p
\end{align*}
On the other hand,
\begin{align*}
\Vert g\Vert_q=\Big(\sum_A|f|^p \Big)^{1/q}
\end{align*}
Since $|\bk{\Lambda,g}|\leq \Vert\Lambda\Vert\cdot\Vert g\Vert_q=\Vert g\Vert_q$, we obtain $(\sum_A|f|^p)\leq (\sum_A|f|^p)^{1/q}$, and hence 
\begin{align*}
\sum_A |f|^p\leq 1
\end{align*}
Applying $\lim_{A\in\fin(2^X)}$, we get $\Vert f\Vert_p\leq 1$.

Now choose any $g\in l^q(X,\Fbb)$. Since $1\leq q<+\infty$, as in the proof of Lem. \ref{lb528}, it is easy to see that $\sum_{x\in X}g(x)\chi_{\{x\}}=\lim_{A\in\fin(2^X)}\sum_{x\in A}g(x)\chi_{\{x\}}$ converges to $g$ (under the $l^q$-norm). Thus, since $\Lambda$ is continuous, we have
\begin{align*}
&\bk{\Lambda,g}=\Bigbk{\Lambda,\lim_{A\in\fin(2^X)}\sum_{x\in A}g(x)\chi_{\{x\}}}=\lim_{A\in\fin(2^X)}\sum_{x\in A}\Bigbk{\Lambda,g(x)\chi_{\{x\}}}\\
=&\lim_{A\in\fin(2^X)}\sum_{x\in A}f(x)g(x)=\sum_X fg
\end{align*}
This proves that $\Psi(f)=\Lambda$.
\end{proof}

We are now able to give an explicit characterization of the weak-* topology on \uwave{norm-bounded subsets} (e.g. the unit ball) of $l^p(X,\Fbb)$.

\begin{thm}\label{lb530}
Let $(f_\alpha)$ be a net in $l^p(X,\Fbb)$ satisfying $\sup_\alpha\Vert f_\alpha\Vert_p<+\infty$, and let $f\in l^p(X,\Fbb)$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $(f_\alpha)$ converges weak-* to $f$. (More precisely, $(\Psi(f_\alpha))$ converges weak-* to $\Psi(f)$.)
\item $(f_\alpha)$ converges pointwise to $f$ as functions $X\rightarrow\Fbb$.
\end{enumerate}
\end{thm}

We warn the reader that (1) is not equivalent to (2) if $\sup_\alpha\Vert f_\alpha\Vert_p=+\infty$.

\begin{proof}
For each $x\in X$ we have $f_\alpha(x)=\bk{\Psi(f_\alpha),\chi_{\{x\}}}$ and $f(x)=\bk{\Psi(f),\chi_{\{x\}}}$. Therefore, (2) is equivalent to that $\Psi(f_\alpha)$ converges to $\Psi(f)$ when acting on each $\chi_{\{x\}}$.  Since $\Psi$ is an isometry, $\sup_\alpha \Vert\Psi(f_\alpha)\Vert=\sup_\alpha\Vert f_\alpha\Vert_p<+\infty$. Therefore, by Prop. \ref{lb520}, (2) is equivalent to that $\Psi(f_\alpha)$ converges pointwise on $l^q(X,\Fbb)$ to $\Psi(f)$, because functions of the form $\chi_{\{x\}}$ span $\mc S(X,\Fbb)$, a norm-dense subspace of $l^q(X,\Fbb)$ due to Lem. \ref{lb528}.
\end{proof}


\begin{rem}
Weak-* convergence and the Banach-Alaoglu theorem were first studied for (the closed unit ball of) $l^2(\Zbb,\Fbb)$ by Hilbert in his study of integral equations and eigenvalue problem (cf. Ch. \ref{lb310}). Of course, Hilbert didn't have the modern definition of weak-* topology. For him, the weak-* convergence on the unit ball simply means condition (2) of Thm. \ref{lb530}. Taking Thm. \ref{lb530}-(2) as the definition of weak-* convergence, the Banach-Alaoglu theorem can be proved quite easily for $l^2(\Zbb,\Fbb)$ (cf. Pb. \ref{lb531}). Therefore, as with many abstract definitions, the notion of weak-* topology has its origins in very concrete forms of expression. 
\end{rem}

After Hilbert's study of $l^2$ spaces (a.k.a. Hilbert spaces), F. Riesz generalized weak-* topology and Banach-Alaoglu theorem to $l^p$ and $L^p$ spaces for arbitrary $1<p\leq+\infty$, as we shall see in the next section.

\begin{rem}
If $1<p<+\infty$, the weak-* topology on $l^p(X)$ is also called the \textbf{weak topology}. See Rem. \ref{lb628}.
\end{rem}







\subsection{The birth of operator norms, I: moment problems}\label{lb543}

It can be said that F. Riesz made the first crucial contribution to the notion of dual spaces of Banach spaces. According to Riesz, bounded linear functionals on $L^q(I)$ (when $1<q<+\infty$ and $I$ is an interval in $\Rbb$) are nothing abstract. They are simply elements of $L^p(I)$. But what is the advantage of viewing $f\in L^p(I)$ not only as a function on $I$, but also as a linear functional on $L^q(I)$? Why was Riesz interested in characterizing $L^q(I)^*$ at all?


In fact, Riesz's study of dual spaces is related to the \textbf{moment problems} which have applications to probability. Around 1910, Riesz studied the following type of moment problem: Let $g_1,g_2,\dots$ be a sequence in $L^q(I)$, let $c_1,c_2,\dots\in\Cbb$, and find some $f\in L^p(I)$ such that
\begin{align}
\int fg_j=c_j\qquad(\text{for all } j) \label{eq204}
\end{align}
(For example, in the classical moment problem, $g_n(x)=x^n$, and $f$ is understood as a ``probability distribution". Then $\int f(x)xdx=c_1$ is the mean (i.e., expected value) of $f$, and $\int f(x)(x-c_1)^2dx$ is the variance of $f$. The number $c_n=\int f(x)x^ndx$ is called the \textbf{$n$-th moment} of $f$.)


By H\"older's inequality, if such $f$ exists, then there must be some $M\in\Rbb_{\geq0}$ (e.g. $M=\Vert f\Vert_{L^p}$), such that 
\begin{align}\label{eq203}
|a_1c_1+\cdots+a_nc_n|\leq M\cdot \Vert a_1g_1+\cdots+a_ng_n\Vert_{L^q}\qquad(\forall n\in\Zbb_+,a_1,\cdots,a_n\in\Cbb)
\end{align}
Riesz proved that the existence of $M$ satisfying \eqref{eq203} is also a sufficient condition for the existence of $f\in L^p(I)$ satisfying \eqref{eq204}. 

From the modern viewpoint, Riesz's result can be proved in the following way: Let $V$ be spanned by $g_1,g_2,\dots$. By \eqref{eq203}, there exists a unique linear functional $\varphi:V\rightarrow\Cbb$ with operator norm $\leq M$ satisfying $\varphi(g_j)=c_j$ for all $j$. By Hahn-Banach Thm. \ref{lb499}, $\varphi$ can be extended to a bounded linear functional $\varphi:L^q(I)\rightarrow\Cbb$ also with operator norm $\leq M$. Then, by the isomorphism $L^p(I)\simeq L^q(I)^*$, $\varphi$ can be realized by some $f\in L^p(I)$, which is the desired function.


\begin{rem}\label{lb526}
In fact, Riesz didn't find the linear functional $L^q(I)\rightarrow\Cbb$ in this way. He didn't see this problem as extending linear functionals. And Hahn-Banach theorem didn't exist before Riesz solved the moment problem. (Riesz's solution is actually an important motivation for the Hahn-Banach theorem.)

Riesz found $\varphi:L^q(I)\rightarrow\Cbb$ in the following way (cf. \cite[Sec. VI.2]{Die-H}). In the first step, by using complicated methods, he could find $f_n\in L^p(I)$ satisfying $\Vert f_n\Vert_{L^p}\leq M$ and 
\begin{align*}
\int_I f_ng_i=c_i\qquad(\text{for every }1\leq i\leq n)
\end{align*}
In Sec. \ref{lb548}, we will explain more about how to find these $f_n$.

In the second step, Riesz considered $\Phi(f_n):L^q(I)\rightarrow\Cbb$ sending each $g\in L^q(I)$ to $\int f_ng$. Then, for each $j$, we have $\lim_{n\rightarrow\infty}\bk{\Phi(f_n),g_j}=c_j$. Riesz made the following crucial steps:
\begin{itemize}
\item He proved the Banach-Alaoglu theorem for $L^q(I)^*$ using diagonal method. %\footnote{Cf. Rem. \ref{lb524} and \ref{lb525}, and note that $L^q(I)$ is actually separable.} 
Therefore, since $\sup_n \Vert\Phi(f_n)\Vert=\sup_n\Vert f_n\Vert_p\leq M$, one has a subsequence $\Phi(f_{n_k})$  converging weak-* to some $\varphi\in L^q(I)^*$.\footnote{Since $L^q(I)$ is separable, the closed ball of $L^q(I)^*$ with radius $M$ is compact metrizable by Thm. \ref{lb523}, and hence is sequentially compact.}  Then clearly $\bk{\varphi,g_j}=c_j$ for all $j$.\footnote{More precisely, the following is what Riesz did: Using the diagonal method as in the proof of Thm. \ref{lb89}, he found $\Phi(f_{n_k})$ whose evaluations with the elements in a given countable dense subset of $L^q(I)$ converge. Since $\sup_k\Vert\Phi(f_{n_k})\Vert<+\infty$, by Prop. \ref{lb520}, one concludes that $(\Phi(f_{n_k}))$ converges weak-* to some $\varphi\in L^q(I)^*$.}
\end{itemize}
Then $\varphi$ can be represented by some $f\in L^p(I)$ thanks to $L^p(I)\simeq L^q(I)^*$. \hfill\qedsymbol
\end{rem}

Therefore, Riesz's study of moment problems contributed to:
\begin{enumerate}[label=(\alph*)]
\item The discovery of important special cases of Banach-Alaoglu theorem.
\item The realization that \uline{operator norms play an important role in the compactness of dual Banach spaces}.
\item The discovery of $L^p(I)\simeq L^q(I)^*$.
\end{enumerate}
After the work of Riesz, operator norms became a central concept in modern analysis.  


We refer the readers to \cite[Ch. VI]{Die-H} and \cite{NB97} for more details about the relevant history, and to \cite[Sec. 4.17 \& 5.6]{Sim-R} for a modern treatment of moment problems.



\subsection{Operator norms and completeness: functional calculus}\label{lb610}


In this section, all normed vector spaces are over $\Fbb\in\{\Rbb,\Cbb\}$.

That one can do a lot of analysis on the \textit{linear maps} of function spaces is a remarkable fact. In fact, in the early history of functional analysis, people were more interested in \textit{nonlinear} functionals on function spaces, for example, the expression $S(f)$ in the calculus of variations (cf. \eqref{eq24}), for which one searched for the extreme values. The problem of finding extreme values is almost trivial if the functional $S$ is linear and defined on a vector space of functions. Even Hilbert studied the eigenvalue problem for the integral operator $(Tf)(x)=\int_0^1 K(x,y)f(x)dx$ (cf. Subsec. \ref{lb370}) by transforming it to the extreme value problem of the functional $S(f)=\eqref{eq206}$ defined for all $f$ on the closed unit ball of $L^2([0,1],\Cbb)$, viewing $S(f)$ as an (infinite-dimensional) \textbf{quadratic form}.



F. Riesz first recognized the importance of \textbf{operator norms}, thus calling people's attention to bounded linear maps of function spaces. Thus, Riesz is undoubtedly one of the most prominent figures in the early history of \textit{linear} functional analysis. We have defined bounded linear maps between arbitrary Banach spaces $V\rightarrow W$. However, to understand the nature of operator norms, we have to look at the following two important special cases:
\begin{enumerate}[label=(\arabic*)]
\item Bounded linear maps $V\rightarrow\Fbb$. By Banach-Alaoglu, one can use the operator norm to get a weak-* \uwave{compact} set $\ovl B_{V^*}(0,1)$. This has been discussed in previous sections.
\item Bounded linear maps $V\rightarrow V$. As we will see below, the operator norm makes $\fk L(V):=\fk L(V,V)$ a Banach space which is compatible with its $\Fbb$-algebra structure. Namely, $\fk L(V)$ is a \textbf{Banach algebra}. Here, the crucial analytic property is \uwave{completeness} rather than compactness. 
\end{enumerate} 
Thus, in these two cases, the operator norms are playing different roles: compactness in the first case, and completeness in the second one. For example, as I will argue in the future, Hilbert and Schmidt noticed the importance of $l^2(\Zbb,\Cbb)$ not so much because of its completeness, but because of the weak-* compactness of its closed unit ball.


\subsubsection{The Banach algebra $\fk L(V)$}


Let me explain the meaning of the statement ``$\fk L(V)$ is a Banach algebra". First, we observe:


\begin{thm}\label{lb540}
Assume that $V$ is a normed vector spaces and $W$ is a Banach space. Recall that $\fk L(V,W)$ is a linear subspace of $W^V$ (cf. Prop. \ref{lb314}). Then $\fk L(V,W)$, equipped with the operator norm, is a Banach space.
\end{thm}


\begin{proof}
By Prop. \ref{lb314}, it remains to prove that $\fk L(V,W)$ is complete. Let $(T_n)$ be a Cauchy sequence in $\fk L(V,W)$. So $\lim_{m,n\rightarrow\infty}\Vert T_m-T_n\Vert=0$. For each $v\in V$, we have $\Vert T_mv-T_nv\Vert\leq \Vert T_m-T_n\Vert\cdot\Vert v\Vert$ which converges to $0$ under $\lim_{m,n}$. So $(T_nv)$ is a Cauchy sequence in $W$, converging to a vector $Tv\in W$. Thus, we have proved that $(T_n)$ converges pointwisely to $T$. Clearly $M:=\sup_{n\in\Zbb_+}\Vert T_n\Vert$ is a finite number. Therefore, by Prop. \ref{lb520}, we have $T\in\fk L(V,W)$ and $\Vert T\Vert\leq M$. 

Note that $\lim_n\Vert T-T_n\Vert=0$ means precisely that $T_n\rightrightarrows T$ on $B_V(0,1)$. Since $(T_n)$ converges pointwise to $T$, to prove that $\lim_n\Vert T-T_n\Vert=0$, it suffices to prove that $(T_n)$ converges uniformly to some function on $B_V(0,1)$. This is due to the completeness of $l^\infty(B_V(0,1),W)$ and the Cauchyness of $(T_n|_{B_V(0,1)})_{n\in\Zbb_+}$. 
\end{proof}


\begin{df}
Let $\scr A$ be an $\Fbb$-algebra. Suppose that $\scr A$, as a vector space, is equipped with a norm $\Vert\cdot\Vert$ so that $\scr A$ is a normed vector space, and that
\begin{align}
\Vert xy\Vert\leq\Vert x\Vert\cdot\Vert y\Vert \label{eq208}
\end{align}
for all $x,y\in\scr A$. Then we call $\scr A$ a \textbf{normed algebra} over $\Fbb$. \index{00@Normed algebra} If the norm is complete, we say that $\scr A$ is a \textbf{Banach algebra} \index{00@Banach algebra} over $\Fbb$. A \textbf{unital Banach algebra} \index{00@Unital Banach algebra} is a unital algebra (with unit $\idt$) which is also a Banach algebra and satisfies
\begin{align*}
\Vert\idt\Vert=1
\end{align*}
\end{df}




As mentioned above, the most important reason for considering operator norms on $V^*$ is due to the Banach-Alaoglu theorem. The most important reason for considering operator norms on $\fk L(V)$ is due to the following elementary but important fact:


\begin{thm}
Let $V$ be a Banach space. Then $\fk L(V)$, equipped with the operator norm, is a unital Banach algebra.
\end{thm}

Recall that the multiplication in $\fk L(V)$ is defined by the composition of linear operators.

\begin{proof}
This is immediate from Thm. \ref{lb540} and Prop. \ref{lb541}.
\end{proof}

\begin{pp}\label{lb541}
Let $S:U\rightarrow V$ and $T:V\rightarrow W$ be linear maps of normed vector spaces. Then
\begin{align}
\Vert TS\Vert\leq\Vert T\Vert \cdot\Vert S\Vert  \label{eq207}
\end{align}
\end{pp}


\begin{proof}
For each $u\in U$ we have $\Vert TSu\Vert\leq \Vert T\Vert\cdot \Vert Su\Vert\leq \Vert T\Vert\cdot\Vert S\Vert\cdot\Vert u\Vert$. According to Rem. \ref{lb372}, we get \eqref{eq207}.
\end{proof}


\subsubsection{Power series functional calculus}



The word ``functional calculus" refers in general to the procedure of replacing the variable $x$ or $z$ in the $\Fbb$-valued function $f(x)$ or $f(z)$ by $T$ to get $f(T)$, where $T$ is an element in a unital Banach algebra $T$. The notation $T$ suggests that the most important case is where $T\in\fk L(V)$ for some Banach space $V$. Depending on whether $f$ is a continuous/analytic/integrable/measurable function, $f(T)$ is defined in different ways. Let us consider the simplest case. In the following, we fix a complex unital Banach algebra $\scr A$. (For example, $\scr A=\fk L(V)$ where $V$ is a complex Banach space.)


Let $f(z)=\sum_{n=0}^\infty a_nz^n$ be a power series in $\Cbb$ with radius of convergence $R$. If $T\in\scr A$ and $\Vert T\Vert < R$, we can define
\begin{align}
f(T)=\sum_{n=0}^\infty a_nT^n  \label{eq209}
\end{align}
By \eqref{eq208}, we have $\Vert T^n\Vert\leq \Vert T\Vert^n$. Therefore, by root test, we have $\sum_n |a_n|\cdot \Vert T^n\Vert<+\infty$. So the RHS of \eqref{eq209} converges absolutely, and hence converges in $\scr A$. So \eqref{eq209} makes sense. 


The most important general fact about functional calculus is that $f\mapsto f(T)$ is a homomorhism of unital algebras:

\begin{pp}\label{lb573}
If $f(z)=\sum_n a_nz^n$ and $g(z)=\sum_n b_nz^n$ be power series in $\Cbb$ with radius of convergence $R_1,R_2$. Let $R=\min\{R_1,R_2\}$. If $\scr A$ is a complex unital Banach algebra with unit $\idt$, and if $T\in\scr A$ satisfies $\Vert T\Vert<R$, then for each $a,b\in\Cbb$ we have
\begin{align*}
1(T)=\idt\qquad (af+bg)(T)=af(T)+bg(T)\qquad   (fg)(T)=f(T)g(T)
\end{align*}
\end{pp}


\begin{proof}
The first two equations are obvious. We prove the last one. Let $c_n=\sum_{k=0}^n a_kb_{n-k}$. Then $h(z)=\sum_n c_nz^n$ equals $f(z)g(z)$ on $B_\Cbb(0,R)$. Cor. \ref{lb153} immediately implies $h(T)=f(T)g(T)$.
\end{proof}

Let me give a simple application of Prop. \ref{lb573}. Recall that one can use determinants to prove that the set of $n\times n$ complex matrices is open in $\Cbb^{n\times n}$. However, in the infinite dimensional case one clearly cannot use determinants. Functional calculus provides an easy method to treat this problem.

\begin{eg}\label{lb542}
Let $T\in\scr A$ such that $\Vert T\Vert<1$. Then $1+T$ is invertible.
\end{eg}

\begin{proof}
Let $f(z)=1+z$. Let $g(z)=\sum_{n=0}^\infty (-z)^n$, which has radius of convergence $1$ and equals $(1+z)^{-1}$ when $|z|<1$. Since $fg=gf=1$, by Prop. \ref{lb573} we have $(1+T)S=S(1+T)=1$ if we let $S=g(T)=\sum_{n=0}^\infty T^n$.
\end{proof}


\begin{pp}
The set $\scr A^\times$ of invertible elements is an open subset of $\scr A$.
\end{pp}

\begin{proof}
Let $T\in\scr A$ be invertible. Let $\delta=\Vert T^{-1}\Vert^{-1}$. Then for each $S\in\scr A$ satisfying $\Vert T-S\Vert<\delta$ we have
\begin{align*}
\Vert T^{-1}(S-T)\Vert\leq \Vert T^{-1}\Vert\cdot \Vert S-T\Vert <1
\end{align*}
Thus, by Exp. \ref{lb542}, $1+T^{-1}(S-T)$ is invertible. So its multiplication with $T$ (which is $S$) is invertible.
\end{proof}


Functional calculus is also useful in differential equations, even in the finite dimensional case:


\begin{exe}\label{lb582}
Let $A,B\in\scr A$.  Suppose that $AB=BA$. Show that $e^{A+B}=e^A\cdot e^B$. Show that
\begin{align*}
\frac {d e^{At}}{dt}=Ae^{At}=e^{At}A
\end{align*}
\end{exe}

\begin{exe}
Let $V$ be a complex Banach space. Let $v\in V$. Let $T\in\fk L(V)$. Show that there is a unique differentiable $f:\Rbb\rightarrow V$ satisfying the differential equation
\begin{align*}
f'(t)=Tf(t)\qquad f(0)=v
\end{align*}
Show that $f(t)=e^{Tt}v$ satisfies this differential equation.
\end{exe}


\subsection{The birth of operator norms, II: diagonalization}\label{lb551}




The power series functional calculus is the easiest example of functional calculus. In history, F. Riesz realized that $\fk L(V)$ is useful because one can use the completeness of $\fk L(V)$ under the operator norm to establish the \textbf{continuous functional calculus}, a more difficult version of functional calculus. Riesz's realization of the usefulness of operator norms is probably motivated by his study of moment problems, in which he introduced operator norms to $V^*$. (See Sec. \ref{lb543}.)

Let $V$ be the Hilbert space $l^2(\Zbb,\Cbb)$. For each $T\in\fk L(V)$, if $m,n\in\Zbb$, note that $T\chi_{\{n\}}$ is a function $\Zbb\rightarrow\Cbb$. We let $T_{m,n}=(T\chi_{\{n\}})(m)$, which is a complex number. (You can view $T$ as an $\infty\times\infty$ matrix. Then $T_{m,n}$ is its $m\times n$ entry.) It can be shown by the $l^2$-version of Thm. \ref{lb527} that there is a unique $T^*\in\fk L(V)$ such that $(T^*)_{m,n}=\ovl{T_{n,m}}$. We say that $T$ is \textbf{self-adjoint} if $T=T^*$. Then Riesz's continuous functional calculus says that for each self-adjoint $T$ satisfying $\Vert T\Vert\leq 1$, if we let $I=[-1,1]$, then there is continuous unital $*$-homomorphism
\begin{align*}
C(I,\Cbb)\rightarrow \fk L(V)\qquad f\mapsto f(T)
\end{align*}
sending the function $\id_I:x\mapsto x$ to $\id_I(T)=T$. By saying that the above map is a unital $*$-homomorphism, we mean that for every $a,b\in\Cbb,f,g\in C(I,\Cbb)$ we have
\begin{gather*}
1(T)=\id_V\qquad (af+bg)(T)=af(T)+bg(T)\\
(fg)(T)=f(T)g(T)\qquad f^*(T)=f(T)^*
\end{gather*}
Recall that $f^*(x)=\ovl {f(x)}$. 

Riesz's continuous functional calculus, together with his characterization of the dual space of $C(I,\Cbb)$ (this is the celebrated \textbf{Riesz representation theorem}, one of the deepest results in measure theory), enabled him to give an elegant proof of Hilbert's \textbf{spectral theorem} for bounded self-adjoint operators on $l^2(\Zbb,\Cbb)$. Riesz's proof by functional calculus is still the standard method today. 

Roughly speaking, spectral theorem can be viewed as the continuous version of the diagonalization theorem for complex self-adjoint matrices: If $T\in \Cbb^{n\times n}$ is self-adjoint, then there exists $U\in\Cbb^{n\times n}$ satisfying $U^*U=1$ (i.e. $U$ is unitary), such that $UTU^*$ is a diagonal matrix. In the case of $l^2(\Zbb,\Cbb)$, the eigenvalues and the diagonalizability of $T$ should be understood in a continuous way. (We will talk more about this in the future.) The spectral theorem generalizes Hilbert's and Schmidt's work on integral equations and paved the way for von Neumann's establishment of abstract Hilbert spaces and their (unbounded) self-adjoint operators as the mathematical foundation of quantum mechanics.




If you remember, when studying the diagonalization and the Jordan forms of (finite dimensional) matrices, you were already using functional calculus: Take $T\in\Cbb^{n\times n}$. Then one has an obvious unital homomorphism of $\Cbb$-algebras $\Phi:\Cbb[z]\mapsto\Cbb^{n\times n},f\mapsto f(T)$. Then the set of eigenvalues of $T$ is 
\begin{align*}
E(T)=\{\lambda\in\Cbb:f(z)=0\text{ for all }f\in\Ker(\Phi)\}
\end{align*}
the zero set of the ideal $\Ker(\Phi)$. For each $\lambda\in E(T)$,  the subspace of the generalized vectors with eigenvalue $\lambda$ (i.e., the set of all $v\in \Cbb^n$ such that $(T-\lambda)^kv=0$ for some $k\in\Nbb$) is the range of some projection operator $f(T)$ where $f\in\Cbb[z]$. Thus, it is no surprising that functional calculus is also used to study the diagonalization of bounded linear operators on function spaces.


\begin{rem}
In analysis, the most important reason for studying the topological properties (e.g. compactness, completeness) of a space is to find the solutions. Thus, if the space is a function space $\fk X$ (for example, a subspace of $C([0,1])$), and if $S:\fk X\rightarrow \Rbb$ is a \textit{fixed} functional, one can use the topological properties of $\fk X$ to find the elements of $\fk X$ minimizing or maximizing the value of $S$. This pattern is deeply influenced by the calculus of variations and is nonlinear by nature. (See Sec. \ref{lb55}.) 

Riesz's study of operator norms undoubtedly brought a paradigm shift to modern analysis. After Riesz's work on operator norms, dual spaces, and functional calculus, \textit{linear} functionals and \textit{linear} maps \footnote{Even Hilbert did not take the viewpoint of linear maps in his study of integral equations. Instead, he took the viewpoint of quadratic forms (cf. \eqref{eq206}), which was partly influenced by the calculus of variations. We will say more about this in the second semester.} took the stage of analysis. Since then, people were interested not only in the topological properties of function spaces (e.g. $C([0,1]),L^2([0,1]),l^2(\Zbb)$), \uwave{but also in the topological properties of the spaces of linear functionals and linear maps defined on these function spaces} (e.g. $C([0,1])^*,\fk L(L^2([0,1])),\fk L(l^2(\Zbb))$). Since then, functionals and maps on function spaces were not fixed from beginning to end. Instead, one should find the correct functionals or maps that solve the problems by using the topological properties of the spaces of linear functionals or linear maps. \hfill\qedsymbol
\end{rem}

 

Many modern people will have a hard time understanding how nontrivial the notion of operator norms is, since they feel that it follows naturally from the study of the continuity of linear operators $V\rightarrow V$. But these people are probably unaware that the notion of linear operators on a Banach space is nontrivial, and that it is nontrivial to view an integral operator as an operator on the function space but not (for example) as a quadratic form \eqref{eq206}. It is precisely because functional calculus depends on operator norms, and operator norms depend on the perspective of linear operators, that the concept of linear operators has received great attention from people. See Fig. \ref{lb572}.

\begin{figure}[h]
	\centering

\begin{equation*}
\begin{tikzcd}[row sep=small,column sep=tiny]
\tcboxmath{\text{Banach-Alaoglu}} && \tcboxmath{\text{functional calculus}}\\
\text{moment problems}\arrow[d] && \text{diagonalization}\arrow[d] \\
\begin{array}{c}
\text{infinite dimensional}\\
\text{compactness}
\end{array}
\arrow[d]&
\text{Arzel\`a-Ascoli}\arrow[l,dashed,dash pattern=on 3pt off 3pt] \arrow[ld,dashed,dash pattern=on 3pt off 3pt,shorten=12]
& 
\begin{array}{c}
\text{infinite dimensional}\\
\text{completeness}
\end{array}
\arrow[dd]\\
\text{equicontinuity}
\arrow[d]& &\\
\text{operator norm on $V^*$}\arrow[dr] && \text{operator norm on $\fk L(V)$}\arrow[dl]\\
&
\begin{array}{c}
\text{the need to}\\
\text{study linear maps}\\
\text{on function spaces}
\end{array}
&
\end{tikzcd}
\end{equation*}
	\caption{~The order of motivation in history}
\label{lb572}
\end{figure}

%% Record #28 2023/12/27 three lectures  70


\subsection{Contraction theorem}



\begin{df}
Let $f:X\rightarrow Y$ be a map of metric spaces. Suppose that $f$ has Lipschitz constant $L\in[0,1)$, we say that $f$ is a \textbf{contraction}.\index{00@Contraction}
\end{df}


\begin{thm}[\textbf{Contraction theorem}] \index{00@Contraction theorem}\label{lb555}
Let $X$ be a nonempty complete metric space. Let $T:X\rightarrow X$ be a contraction. Then $T$ has a unique \textbf{fixed point}, \index{00@Fixed point} i.e., there exists a unique $x\in X$ satisfying $T(x)=x$. 
\end{thm}

In the following proof, we let $L\in[0,1)$ be a Lipschitz constant of $T$.

\begin{proof}
Uniqueness: Suppose $x,y\in X$ satisfy $T(x)=x$ and $T(y)=y$. Then
\begin{align*}
0\leq d(x,y)=d(T(x),T(y))\leq Ld(x,y)
\end{align*}
showing that $d(x,y)$ must be $0$.

Existence: Choose $x_0\in X$. Define $(x_n)_{n\in\Nbb}$ inductively by $x_{n+1}=T(x_n)$. Then $d(x_{n+1},x_n)=d(T(x_n),T(x_{n-1}))\leq L d(x_n,x_{n-1})$. From this, we conclude
\begin{align*}
d(x_{n+1},x_n)\leq Ld(x_n,x_{n-1})\leq L^2 d(x_{n-1},x_{n-2})\leq\cdots\leq L^n d(x_1,x_0)
\end{align*}
Therefore, for each $k\in\Zbb_+$ we have
\begin{align*}
&d(x_{n+k},x_n)\leq d(x_{n+k},x_{n+k-1})+d(x_{n+k-1},x_{n+k-2})+\cdots+d(x_{n+1},x_n)\\
\leq& (L^{n+k-1}+L^{n+k-2}+\cdots+L^n)d(x_1,x_0)\leq \frac {L^n}{1-L}d(x_1,x_0)
\end{align*}
This proves that $(x_n)_{n\in\Nbb}$ is a Cauchy sequence in $X$. So it converges to some $x\in X$. Since $(T(x_n))=(x_{n+1})$ also converges to $x$, by the continuity of $T$ we conclude $T(x)=x$.
\end{proof}


The contraction theorem is also called the \textbf{Banach fixed-point theorem}. In the next chapter, we will use the contraction theorem to study differential equations. In the next semester, we will use the contraction theorem to prove the inverse function theorem for multivariable functions.












\subsection{Problems and supplementary material}


Let $1<p,q\leq +\infty$ satisfy $\frac 1p+\frac 1q=1$. %Unless otherwise stated, assume $p>1$ and hence $q<+\infty$. 
Let $\Fbb\in\{\Rbb,\Cbb\}$. 

\begin{sprob}\label{lb570}
Let $1\leq p<+\infty$. Let $X$ be a set. Let $\scr A$ be a subset of $l^p(X,\Fbb)$ equipped with the $l^p$-norm. Prove that the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\scr A$ is precompact.
\item For each $x\in X$ we have $\sup_{f\in\scr A}|f(x)|<+\infty$. Moreover, for every $\eps>0$, there exists a finite subset $K\subset X$ such that for each $f\in\scr A$ we have $\Vert f|_{X\setminus K}\Vert_{l^p}<\eps$; in other words,
\begin{align}
\lim_{K\in\fin(2^X)}\sup_{f\in\scr A}\sum_{x\in X\setminus K}|f(x)|^p=0
\end{align}
\end{enumerate}
\end{sprob}

\begin{proof}[Hint]
(1)$\Rightarrow$(2): Mimic the first proof of Thm. \ref{lb515}. (2)$\Rightarrow$(1): Choose any net $(f_\alpha)$ in $\scr A$. First find a subnet $(f_\mu)$ converging pointwise to a function $f:X\rightarrow\Fbb$. Then show that $f\in l^p(X,\Fbb)$ and $\lim_\mu \Vert f-f_\mu\Vert_p=0$.
\end{proof}


Pb. \ref{lb570} is the $l^p$-version of the \textbf{Fr\'echet-Kolmogorov theorem}. \index{00@Fr\'echet-Kolmogorov theorem}

\begin{sexe}
In Pb. \ref{lb570}, assume that $\scr A$ is precompact. Prove that $E=\bigcup_{f\in\scr A}\Supp(f)\equiv\bigcup_{f\in\scr A}\{x\in X:f(x)\neq 0\}$ is a countable set.
\end{sexe}


\begin{proof}[Hint]
Method 1: Show that $\scr A$ is separable. Let $\mc E$ be a countable dense subset of $\scr A$. Prove $E=\bigcup_{f\in\mc E}\Supp(f)$.

Method 2: For each $\eps=1/n$, write the $K$ in (2) as $K_n$. Prove  $E\subset\bigcup_nK_n$.
\end{proof}


\begin{sprob}
Let $V$ be a Banach space. Let $\xi\in C(\Rbb,V)$ such that $\Vert \xi\Vert_{L^1}=\int_\Rbb \Vert \xi(t)\Vert dt<+\infty$. Let $\fk X=C(\Rbb,\Rbb)\cap l^\infty(\Rbb,\Rbb)$, equipped with the $l^\infty$-norm. Define a linear map
\begin{align*}
\Phi:\fk X\rightarrow V\qquad f\mapsto \int_I f(t)\xi(t)dt
\end{align*}
Show that $\Phi$ is a bounded linear map. Show that $\Phi$ is a \textbf{compact operator}, which means that $\Phi(\ovl B_{\fk X}(0,1))$ is a precompact subset of $V$.
\end{sprob}

\begin{proof}[Hint]
By Thm. \ref{lb521}, we can view $V$ as a closed linear subspace of $C(X,\Rbb)$ where $X$ is a compact Hausdorff space. So $\xi$ can be viewed as a function on $\Rbb\times X$. Use Arzel\`a-Ascoli to prove that $\Phi$ is a compact operator.
\end{proof}



\begin{prob}\label{lb535}
Let $V$ be a separable normed vector space over $\Fbb$. Let $(v_n)_{n\in\Zbb_+}$ be a dense sequence in $\ovl B_V(0,1)$. (The density is with respect to the norm topology.\footnote{Recall that subsets of separable (equivalently, second countable) metric spaces are separable.}) Use $(v_n)_{n\in\Zbb_+}$ to construct an explicit metric on $\ovl B_{V^*}(0,1)$ inducing its weak-* topology, and construct an explicit countable basis for the weak-* topology on $\ovl B_{V^*}(0,1)$.
\end{prob}

\begin{proof}[Hint]
Check the proof of Thm. \ref{lb523} (or more precisely, Thm. \ref{lb521}). This problem is related to Pb. \ref{lb533}.
\end{proof}


\begin{rem}
The metric you are asked to find in Pb. \ref{lb535} can actually be found in many analysis textbooks, although their authors do not tell you how they find it. The point of this problem (together with Pb. \ref{lb533}) is to tell you that the correct geometric viewpoint (i.e., embedding into Hilbert cubes) can lead you to the formula of the metric. 

Another goal of this problem is to justify the point in Rem. \ref{lb483}: For all concrete examples of compact metrizable spaces, you can explicitly construct countable bases for their topologies. Therefore, there is no need to use the indirect proof of the second countability in Thm. \ref{lb252}.\footnote{You can feel how much I hate the proof of Thm. \ref{lb252} :-)}  \hfill\qedsymbol
\end{rem}





\begin{comment}
\begin{exe}
Let $f:X\rightarrow Y$ be a continuous map of topological spaces. Show that if $X$ is separable, then $f(X)$ is separable. (This property is not true if ``separable" is replaced by ``second countable".)
\end{exe}
\end{comment}

\begin{prob}\label{lb537}
Let $Y$ be a metric space. Prove that there is a compact Hausdorff space $X$ and an isometry $\Phi:Y\rightarrow C(X,\Fbb)$ following the hint below. 
\end{prob}

In particular, any metric space can be (isometrically) embedded into a Banach space. \footnote{If we simply want to embed $Y$ into a Banach space $V$, there is a simpler method called \textbf{Kuratowski embedding}: Assume $Y$ is nonempty and fix $a\in Y$. Let $V=l^\infty(Y,\Rbb)$, and define the embedding sending each $y\in Y$ to the function $p\in Y\mapsto d(y,p)-d(a,p)$.}

\begin{proof}[Hint]
This problem is similar to Thm. \ref{lb521}. Since $C(X,\Rbb)\subset C(X,\Cbb)$, it suffices to assume $\Fbb=\Rbb$. In  Thm. \ref{lb521}, $X$ is constructed to be the set of linear functionals with operator norms $\leq 1$. The assumption on the operator norms ensures the equicontinuity and hence the (pre)compactness of $X$. Thus, in the current situation, in order to get a compact $X$, one should consider a pointwise bounded set of functions with a uniform Lipschitz constant  (cf. Thm. \ref{lb509} or Exe. \ref{lb536}). For example, assume $Y$ is nonempty and fix $a\in Y$, and define
\begin{align}\label{eq212}
X=\{f\in \Rbb^Y:f(a)=0\text{, and $f$ has Lipschitz constant }1\}
\end{align}
equipped with the pointwise convergence topology.
\end{proof}


\begin{rem}
Similar to the proof of Thm. \ref{lb523}, one can show that $X=\eqref{eq212}$ is metrizable (equivalently, second countable) iff $Y$ is separable.
\end{rem}

\begin{rem}
Pb. \ref{lb537} tells us that any metric space can be embedded into a Banach space. This fact is useful in the same way that the existence of the completions of normed vector spaces is useful, cf. Rem. \ref{lb538}. (Interestingly, Pb. \ref{lb537} gives a new proof that every metric space $Y$ has completion, since one can restrict $\Phi:Y\rightarrow V$ to $Y\rightarrow\ovl{\Phi(Y)}$.) 

For example, assume that $X,Y$ are topological spaces, and $Z$ is a metric space. Equip $C(Y,Z)$ with a uniform convergence metric (cf. Exp. \ref{lb272}). By Pb. \ref{lb537}, $Z$ can be viewed as a metric subspace of a Banach space $V$. By Thm. \ref{lb274}, there is a canonical injection $\Psi:C(X,C(Y,V))\rightarrow C(X\times Y,V)$ which is bijective when $Y$ is compact. It is easy to see that $\Psi$ restricts to an injective map $C(X,C(Y,Z))\rightarrow C(X\times Y,Z)$, and that this restriction is surjective when $Y$ is compact. Therefore, Thm. \ref{lb274} can be generalized to the case that the codomain is a metric space, but not necessarily a normed vector space.\footnote{Of course, we can prove the case of metric spaces for Thm. \ref{lb274} at the very beginning. We didn't do it just to avoid distraction.} Similarly, Thm. \ref{lb277} can be generalized to functions whose codomains are metric spaces, and the Moore-Osgood theorem can be generalized to functions whose codomains are complete\footnote{You need completeness because in the proof you need the fact that $Y$ is closed in $V$.} metric spaces.   \hfill\qedsymbol
\end{rem}








\begin{prob}\label{lb531}
Let $X$ be a set. By the Banach-Alaoglu theorem and Thm. \ref{lb530}, when $1< p\leq +\infty$, the closed unit ball
\begin{align}
B=\{f\in l^p(X,\Fbb):\Vert f\Vert_p\leq 1\}
\end{align}
is compact under the pointwise convergence topology (i.e. the product topology inherited from $\Fbb^X$). Give a direct proof of this fact for all $1\leq p\leq +\infty$ (including the case $p=1$) without using Banach-Alaoglu Thm. \ref{lb519}.
\end{prob}


Now you may wonder if the pointwise convergence topology on the closed unit ball of $l^1$ can be realized by a weak-* topology. The answer is yes:

\begin{sprob}
Let $X$ be a set. Let
\begin{align*}
c_0(X,\Fbb)=\{f\in\Fbb^X: \text{for all $\eps>0$ there exists $A\in\fin(2^X)$ such that }\Vert f\Vert_{l^\infty(A^c,\Fbb)}<\eps\}
\end{align*}
equipped with the $l^\infty$-norm. By Prop. \ref{lb432}, $c_0(X,\Fbb)$ is a Banach space. (You can also check it directly.) Find a natural isomorphism of Banach spaces
\begin{align}
\Psi:l^1(X,\Fbb)\xlongrightarrow{\simeq} c_0(X,\Fbb)^*
\end{align}
Let $(f_\alpha)$ be a net in $l^1(X,\Fbb)$ satisfying $\sup_\alpha \Vert f_\alpha\Vert_{l^1}<+\infty$, and let $f\in l^1(X,\Fbb)$. Prove that $\Psi(f_\alpha)$ converges weak-* to $\Psi(f)$ iff $(f_\alpha)$ converges pointwise to $f$ as functions $X\rightarrow\Fbb$.
\end{sprob}


\begin{sprob}\label{lb567}
Prove that $l^q(\Zbb_+,\Fbb)$ is separable (where $1\leq q<+\infty$), and $l^\infty(\Zbb_+,\Fbb)$ is not separable.
\end{sprob}

\begin{proof}[Hint]
Every subset of a second countable space is second countable and hence Lindel\"of. Find an uncountable discrete (and hence non-Lindel\"of) subset of $l^\infty(\Zbb_+,\Fbb)$.
\end{proof}



\begin{sprob}\label{lb532}
By Thm. \ref{lb369}, there is a linear isometry 
\begin{gather}
\Psi:l^1(\Zbb_+,\Fbb)\rightarrow l^\infty(\Zbb_+,\Fbb)^*  \label{eq205}
\end{gather}
such that $\bk{\Psi(f),g}=\sum_n f(n)g(n)$. Let $\mc B=\ovl B_{l^1(\Zbb_+,\Fbb)}(0,1)$, the closed unit ball of $l^1(\Zbb_+,\Fbb)$. If $\Psi$ is surjective (and hence an isomorphism), then $\Psi(\mc B)$ is the closed unit ball of $l^\infty(\Zbb_+,\Fbb)^*$, and hence is weak-* compact by Banach-Alaoglu. 

Let $f_n=\chi_{\{n\}}$, viewed as an element of $\mc B$. Prove that the sequence $(\Psi(f_n))_{n\in\Zbb_+}$ has no weak-* convergent subnet in $\Psi(\mc B)$.\footnote{It is not enough to consider subsequences, since $l^\infty(\Zbb_+,\Fbb)$ is not separable and hence the unit ball of $l^\infty(\Zbb_+,\Fbb)^*$ is not weak-* metrizable, cf. Thm. \ref{lb523}.} Conclude that the map $\Psi$ is not surjective.  \hfill\qedsymbol
\end{sprob}


\begin{comment}
\begin{sprob}
Let $V$ be a normed vector space such that $V^*$ is separable (under the operator norm). Prove that $V$ is separable. Use this fact to give another proof that \eqref{eq205} is not surjective.
\end{sprob}


\begin{proof}[Hint]
Show that $\ovl B_{V^*}(0,1)$ is second countable under the weak-* topology. Then use Thm. \ref{lb482} and \ref{lb523}. 
%Note that before proving that a space is metrizable, you cannot identify separability with second countability.
\end{proof}
\end{comment}



\subsection{$\star$ Supplementary material: a discussion of Riesz's treatment of moment problems}\label{lb548}

Unless otherwise stated, $V$ is a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.


\subsubsection{Quotient Banach spaces in moment problems}




\begin{prob}
Let $U$ be a closed linear subspace of $V$. Let $V/U$ be its quotient vector space. Prove that $V/U$ has a norm defined by
\begin{align}
\Vert v+U\Vert=\inf_{u\in U}\Vert v+u\Vert  \label{eq216}
\end{align}
for all $v\in V$. Prove that $V/U$ is complete under this norm. We call $V/U$ the \textbf{quotient Banach space} \index{00@Quotient Banach space} of $V$ by $U$.
\end{prob}

\begin{proof}[Hint]
Use Pb. \ref{lb566} to prove that $V/U$ is complete.
\end{proof}

\begin{pp}[Abstract moment problem, finite version]\label{lb546}
Let $\varphi_1,\dots\varphi_n\in V^*$. Let $c_1,\dots,c_n\in\Fbb$. Suppose that there exists $M\in\Rbb_{\geq0}$ such that
\begin{align}
|a_1c_1+\cdots+a_nc_n|\leq M\cdot\Vert a_1\varphi_1+\cdots+a_n\varphi_n\Vert\qquad(\forall a_1,\dots,a_n\in\Fbb)
\end{align}
Then for every $\eps>0$ there exists $v\in V$ satisfying that $\Vert v\Vert\leq M+\eps$ and that
\begin{align}
\bk{\varphi_i,v}=c_i\qquad(\text{for all }1\leq i\leq n) \label{eq213}
\end{align}
\end{pp}

The above proposition is credited to Helly.

\begin{prob}\label{lb547}
Assume the setting of Prop. \ref{lb546}. Define a bounded linear map
\begin{gather}\label{eq214}
\Phi:V\rightarrow \Fbb^n\qquad v\mapsto(\varphi_1(v),\dots,\varphi_n(v))
\end{gather}
The following two steps will lead you to prove Prop. \ref{lb546}.
\begin{enumerate}
\item Consider the special case that $\Ker(\Phi)=0$. In particular, $V$ is a finite dimensional normed vector space, and $\dim V=\dim\Span(\varphi_1,\dots,\varphi_n)$. We have $\dim V^*=\dim V<+\infty$ because all linear maps $V\rightarrow\Fbb$ are bounded. Similarly $\dim V^{**}=\dim V^*$. So the canonical linear isometry
\begin{gather}
V\rightarrow V^{**}\qquad v\mapsto \bk{\cdot,v}
\end{gather}
(cf. Cor. \ref{lb502}) must be bijective. Use this observation to prove that there exists $v\in V$ satisfying $\Vert v\Vert\leq M$ and \eqref{eq213}.

\item Reduce the general case to the special case in part 1 by considering $V/\Ker(\Phi)$ and its bounded linear functionals $\psi_1,\dots,\psi_n$ defined by
\begin{align}
\bk{\psi_i,v+\Ker(\Phi)}=\bk{\varphi_i,v}
\end{align}
(Note that you need to prove $\Vert\psi_i\Vert=\Vert\varphi_i\Vert$.)
\end{enumerate}
\end{prob}

\begin{rem}\label{lb552}
Recall from Sec. \ref{lb543} that Riesz wanted to solve the moment problem: Let $I=[a,b]$ be a compact interval in $\Rbb$. The scalar field is chosen to be $\Cbb$. Let $g_1,g_2,\dots\in L^q(I)=L^q(I,\Cbb)$ (where $1<q<+\infty$) and $c_1,c_2,\dots\in\Cbb$ satisfying
\begin{align*}
|a_1c_1+\cdots+a_nc_n|\leq M\cdot \Vert a_1g_1+\cdots+a_ng_n\Vert_{L^q}\qquad(\forall n\in\Zbb_+,a_1,\cdots,a_n\in\Cbb)
\end{align*}
The goal is to find $f\in L^p(I)$ (where $p^{-1}+q^{-1}=1$) satisfying $\int_I fg_j=c_j$ for all $j=1,2,\dots$. As mentioned in Rem. \ref{lb526}, Riesz's first step in solving this problem is to find $f_n\in L^p(I)$ for each $n\in\Zbb_+$ satisfying $\Vert f_n\Vert_{L^p}\leq M$ and
\begin{align}
\int_I f_ng_i=c_i\qquad(\text{for every }1\leq i\leq n) \label{eq215}
\end{align}
The second step is then to find a weak-* convergent subsequence of $(f_n)_{n\in\Zbb_+}$. 

In Sec. \ref{lb543}, I didn't explain how Riesz solved step 1. Some of the key ideas can now be explained. (For simplicity, the reader can replace $L^q(I),L^p(I)$ by $l^q(\Zbb),l^p(\Zbb)$ and replace the integrals by the series. The main idea remains the same.) 

Let us slightly weaken the above assumption to $\Vert f_n\Vert_{L^p}\leq M+\eps$ where $\eps>0$ is fixed at the beginning. (Later, I will explain why $\eps$ can be chosen to be $0$.) Then, in view of the canonical isomorphism $\Psi:L^q(I)\rightarrow L^p(I)^*$, Riesz's first step can be proved by directly applying Prop. \ref{lb546} to the special case that $V=L^p(I)$ and $\varphi_i=\Psi(g_i)$. \hfill\qedsymbol
\end{rem}

\begin{rem}
It should be noted that the proof of Prop. \ref{lb546} relies on the fact that $V\rightarrow V^{**}$ is an isometry (cf. Pb. \ref{lb547}-1) when $V$ is finite-dimensional. This follows from Hahn-Banach in the general case. As I said in Rem. \ref{lb526}, the Hahn-Banach theorem did not exist by the time Riesz was studying the moment problems.

In the case of moment problems, according to Pb. \ref{lb547}, one should take $V$ to be $L^p(I)/\Ker(\Phi)$ for some bounded linear $\Phi:L^p(I)\rightarrow\Cbb^n$. In this case, without using Hahn-Banach, one can at least show easily that $V\rightarrow V^{**}$ is bijective. But it is quite hard to give a direct (i.e. function-theoretic) proof that $V\rightarrow V^{**}$ is an isometry.  Nevertheless, Riesz circumvented this problem by using a complicated method due to Schmidt (cf. \cite[Sec. 6.2 and 5.3]{Die-H}), which was rarely used after the appearance of Hahn-Banach. In some sense, one can say that Riesz proved a weak version of Hahn-Banach theorem for the finite-dimensional Banach space $L^p(I)/\Ker(\Phi)$. Anyway, \uwave{the infinite-dimensional Hahn-Banach theorem is not needed in Riesz's method}. (So Hahn-Banach is nontrivial enough in the finite-dimensional case.) %(To put it differently, Riesz's method, which is Banach-Alaoglu plus finite-dimensional Hahn-Banach, implies the general Hahn-Banach.\footnote{Notice the following interesting fact: The finite-dimensional Hahn-Banach does not rely on Zorn's lemma. However, both Banach-Alaoglu and the general Hahn-Banach rely on Zorn's lemma.})
\hfill\qedsymbol
\end{rem}

\begin{rem}
The notion of quotient Banach spaces introduced in this section is also implicit in Riesz's work. Indeed, Riesz wanted to find $f_n\in L^p(I)$ satisfying \eqref{eq215} for small enough $\Vert f_n\Vert_{L^p}$. This amounts to the fact that in \eqref{eq216}, the value of  $\Vert v+U\Vert$ can be estimated by finding $u\in U$ such that $\Vert v+u\Vert$ is small enough. 

In fact, Riesz was able to find $f_n$ satisfying \eqref{eq215} \textit{and minimizing $\Vert f_n\Vert$}; moreover, for such $f_n$, one has $\Vert f_n\Vert\leq M$ (but not just $\Vert f_n\Vert\leq M+\eps$). This follows from Pb. \ref{lb564}, applied to $V=L^q(I)$ and $E=\{g_1,\dots,g_n\}$.%\footnote{Riesz did not use the method provided in Pb. \ref{lb564} to find a minimizing $f_n$. He used Lagrange multipliers instead, according to \cite[Sec. 6.2]{Die-H}.}  
\hfill\qedsymbol
\end{rem}


\begin{prob}\label{lb564}
Let $E$ be a subset of $V$. Let
\begin{align}
E^\perp=\{\varphi\in V^*:\varphi|_E=0\}
\end{align}
which is clearly a weak-* closed linear subspace of $V^*$. Let $\varphi\in V^*$. Prove that there exists $\psi\in E^\perp$ such that $\Vert\varphi+\psi\Vert$ equals $\Vert\varphi+E^\perp\Vert\equiv\inf_{\eta\in E^\perp}\Vert\varphi+\eta\Vert$.
\end{prob}

It is in fact true that any weak-* closed linear subspace of $V^*$ is of the form $E^\perp$. To prove it one needs a more general version of the Hahn-Banach theorem. We will not discuss it in our notes.

\begin{proof}[Hint]
Choose a sequence $(\psi_n)$ in $V^*$ such that  $\lim_n\Vert \varphi+\psi_n\Vert=\Vert \varphi+E^\perp\Vert$. Since this sequence is clearly norm-bounded, by Banach-Alaoglu, it has a subnet $(\psi_\alpha)$ converging weak-* to some $\psi\in V^*$. Clearly $\psi\in E^\perp$. For each $\eps>0$, choose $v\in V$ with $\Vert v\Vert=1$ such that $|\bk{\varphi+\psi,v}|\geq \Vert\varphi+\psi\Vert-\eps$. Show that $|\bk{\varphi+\psi,v}|\leq \lim_\alpha\Vert\varphi+\psi_\alpha\Vert$. Conclude that $\Vert\varphi+\psi\Vert-\eps\leq\Vert\varphi+E^\perp\Vert$ for all $\eps>0$.
\end{proof}







\subsubsection{Some consequences}


The goal of this subsection is to give some quick consequences of the results and the methods introduced in the previous subsections. These important properties are mainly about compactness, and are not difficult to find in many books on functional analysis. The reason I present these results is to show that they can (or should?) be understood in the light of Riesz's treatment of moment problems. (Unfortunately, this point is often not emphasized in many textbooks.)



\begin{thm}[\textbf{Goldstine's theorem}] \index{00@Goldstine's theorem}\label{lb568}
Let $\Gamma:V\rightarrow V^{**}$ be the linear isometry (cf. Cor. \ref{lb502}) sending $v$ to the bounded linear functional $\varphi\in V^*\mapsto\bk{\varphi,v}$. Then $\Gamma(\ovl B_V(0,1))$ is weak-* dense in $\ovl B_{V^{**}}(0,1)$. 
\end{thm}

Consequently, since $\ovl B_V(0,1)$ is a compact Hausdorff space by Banach-Alaoglu, we conclude that $\Gamma$ is bijective iff $\Gamma(\ovl B_V(0,1))$ is weak-* compact.\footnote{The \textbf{weak topology} \index{00@Weak topology} on $V$ is defined to be the pullback of the weak-* topology of $V^{**}$ by $V\rightarrow V^{**}$. So a net $(v_\alpha)$ in $V$ converges weakly to $v\in V$ iff $\lim_\alpha\bk{v_\alpha,\varphi}=\bk{v,\varphi}$ for all $\varphi\in V^*$. Thus, the conclusion is that $\Gamma:V\rightarrow V^{**}$ is bijective iff $\ovl B_V(0,1)$ is weakly compact.} A Banach space satisfying these two equivalent conditions is called \textbf{reflexive}. \index{00@Reflexive Banach spaces} For example, if $1<p<+\infty$, then Thm. \ref{lb527} shows that $l^p(X,\Fbb)$ is reflexive. It is also true that $L^p([a,b],\Fbb)$ is reflexive.

\begin{proof}
Since $\bigcup_{0<r<1}\ovl B_{V^{**}}(0,r)$ is norm dense and hence weak-* dense in  $\ovl B_{V^{**}}(0,1)$, it suffices to prove that for each $0<r<1$ and $\fk v\in\ovl B_{V^{**}}(0,r) $, $\fk v$ can be approximated weak-* by elements of $\Gamma(\ovl B_V(0,1))$.

We shall prove that for each $E\in\fin(2^{V^*})$ there exists $v_E\in \ovl B_V(0,1)$ such that $\bk{\fk v,\varphi}=\bk{v_E,\varphi}$ for all $\varphi\in E$. Then the net $(\Gamma(v_E))_{E\in\fin(2^{V^*})}$ converges weak-* in $\ovl B_{V^{**}}(0,1)$ to $\fk v$, finishing the proof. To find $v_E$, we write $E=\{\varphi_1,\dots,\varphi_n\}$. Let $c_i=\bk{\fk v,\varphi_i}$. Since $\Vert\fk v\Vert\leq r$, for each $a_1,\dots,a_n\in\Fbb$ we have
\begin{align*}
|a_1c_1+\cdots+a_nc_n|=|\bk{\fk v,a_1\varphi_1+\cdots+a_n\varphi_n}|\leq r\cdot\Vert a_1\varphi_1+\cdots+a_n\varphi_n\Vert
\end{align*}
Therefore, by Prop. \ref{lb546}, there exists $v_E\in \ovl B_V(0,1)$ such that $\bk{v_E,\varphi_i}=c_i$ for all $1\leq i\leq n$. This finishes the construction of $v_E$. 
\end{proof}

%The content of Goldstine's theorem is natural in view of Riesz's solution of moment problems: If we translate Rem. \ref{lb526} to the abstract language of Banach spaces, then the moment problem can be formulated as follows: Let $V$ a Banach space. If $\fk Y$ is a closed subspace of 

\begin{comment}
\begin{rem}
We will show in Exe. \ref{lb562} that if $V$ is reflexive and $U$ its closed linear subspace, then for each $v\in V$ there exists $u\in U$ minimizing $\Vert v+u\Vert$.  It follows immediately from Pb. \ref{lb547} that in Prop. \ref{lb546} one can choose $v\in V$ satisfying $\Vert v\Vert\leq M$ (but not just $\Vert v\Vert\leq M+\eps$) and \eqref{eq213}. Since $L^p(I)$ is reflexive (when $1<p<+\infty$), we thus have an explanation of why in Rem. \ref{lb552} one can choose $f_n$ satisfying $\Vert f\Vert_{L^p}\leq M$ and \eqref{eq215}.
\end{rem}
\end{comment}



%We end this section with a fun application of quotient Banach spaces.

\begin{prob}\label{lb561}
Let $M$ be a linear subspace of $V$. Suppose that $e\in V$ is not in $\ovl M=\Cl_V(M)$. Prove that there exists $\psi\in V^*$ such that $\psi|_M=0$ and $\psi(e)\neq 0$. 
\end{prob}

This problem is a special case of the \textbf{Hahn-Banach separation theorem}. \index{00@Hahn-Banach separation theorem} As a consequence of this exercise, we know that $M$ is dense in $V$ iff every $\psi\in V^*$ vanishing on $M$ is zero.


\begin{comment}
As in the proof of Thm. \ref{lb499}, reduce to the case that $\Fbb=\Rbb$. By using Thm. \ref{lb499}, it suffices to find $\wtd\varphi\in \wtd M^*$ satisfying $\wtd\varphi|_M=0$ and $\wtd\varphi(e)=A\neq 0$, where $\wtd M=M+\Rbb e$. Check the proof of Lem. \ref{lb498} for the special case that $\varphi=0$. Use the fact that the LHS of \eqref{eq227} is $<0$ and the RHS of \eqref{eq227} is $>0$ to find $A$.
\end{comment}

\begin{proof}[Hint]
Use Hahn-Banach Cor. \ref{lb502} to prove the special case that $M=0$. Then reduce the general case to the special case by considering $V/\ovl M$.
\end{proof}

\begin{prob}\label{lb562}
Let $V$ be a reflexive Banach space. Let $U$ be a closed linear subspace of $V$. Let $v\in V$. Show that there exists $u\in U$ such that $\Vert v+U\Vert=\Vert v+u\Vert$. 
\end{prob}


\begin{proof}[Hint]
The canonical linear isometry $\Gamma:V\rightarrow V^{**}$ is an isomorphism of Banach spaces. Use the Hahn-Banach separation theorem (Pb. \ref{lb561}) to show that $\Gamma(U)=W^\perp$ where $W=\{\psi\in V^*:\psi|_U=0\}$. Then apply Pb. \ref{lb564}.
\end{proof}


\begin{comment}
\begin{pp}\label{lb563}
Suppose that $V$ is reflexive. Then in Prop. \ref{lb546} the condition $\Vert v\Vert\leq M+\eps$ can be strengthened to $\Vert v\Vert\leq M$.
\end{pp}

\begin{proof}
This is immediate from the proof of Prop. \ref{lb546} (cf. Pb. \ref{lb547}) and Pb. \ref{lb562} (applied to $U=\Ker(\Phi)$).
\end{proof}

\end{comment}

\begin{thm}[\textbf{Riesz's theorem}]\label{lb565}
Let $V$ be a Banach space. Then the closed unit ball $\ovl B_V(0,1)$ is compact under the norm topology iff $\dim_\Fbb V<+\infty$.
\end{thm}

\begin{proof}
If $\dim_\Fbb V<+\infty$, then the norm on $V$ is equivalent to the Euclidean norm by Pb. \ref{lb559}. So $\ovl B_V(0,1)$ is compact by Bolzano-Weierstrass. 

Now we assume $\dim_\Fbb V$ is not finite. We fix any $0<\eps<1$. (For example, choose $\eps=1/2$.) Notice that for each closed linear subspace $U\subsetneq V$, there clearly exists $v\in U$ such that $\Vert v+U\Vert=1-\eps$. By replacing $v$ by $v+u$ for some $u\in U$, we assume moreover that $\Vert v\Vert\leq 1$.

Now we construct an infinite linearly-independent sequence $(v_n)_{n\in\Zbb_+}$ in $\ovl B_V(0,1)$ as follows. $v_1$ is arbitrary. Suppose $v_1,\dots,v_n$ have been constructed. Let $U_n=\Span_\Fbb(v_1,\dots,v_n)$. Then $U_n$ is a closed linear subspace of $V$ since $U_n$ is complete. (Any finite dimensional normed vector space is equivalent as a metric space to $\Fbb^n$ by Pb. \ref{lb559}, and hence is complete.) Then $U_n$ is a closed proper linear subspace of $V$ since $\dim_\Fbb V<+\infty$. Therefore, by the previous paragraph, there exists $v_{n+1}\in\ovl B_V(0,1)$ such that $\Vert v_{n+1}+U_n\Vert=1-\eps$. So $\Vert v_{n+1}-v_j\Vert\geq 1-\eps$ for all $j\leq n$.

The sequence constructed above satisfies that $\Vert v_m-v_n\Vert\geq 1-\eps$ for all $m\neq n$. So $(v_n)_{n\in\Zbb_+}$ has no Cauchy subsequence, and hence no norm-convergent subsequence. Therefore $\ovl B_V(0,1)$ is not sequentially compact under the norm topology, and hence not compact.
\end{proof}


\begin{rem}
If $V$ is a reflexive infinite dimensional Banach space (i.e. if $V$ is $L^p([0,1],\Fbb)$ or $l^p(X,\Fbb)$ where $1<p<+\infty$), the above proof that $\ovl B_V(0,1)$ is not compact can be simplified by choosing $\eps=0$. This is due to Pb. \ref{lb562}.
\end{rem}

\begin{comment}
\begin{rem}
In the above proof, when the infinite dimensional $V$ is reflexive, one can choose $\eps$ to be $0$ thanks to Pb. \ref{lb562}. Therefore, for example, if $V$ is $L^p([0,1],\Fbb)$ or $l^p(\Zbb,\Fbb)$ (where $1<p<+\infty$), one can choose a sequence $(f_n)$ in $\ovl B_V(0,1)$ inductively as follows: First choose $f_{n+1}\in V$ outside $U_n=\Span(f_1,\dots,f_n)$. Then choose $f'_{n+1}\in U_n$ minimizing $\Vert f_{n+1}-f'_{n+1}\Vert$. (This step is very intuitive: in the case that $V$ is an inner product space, $f_{n+1}'$ is just the projection of $f_{n+1}$ to $U_n$.) Replace $f_n$ by $f_{n+1}-f'_{n+1}$. Then scale $f_n$ so that $\Vert f_n\Vert=1$. Then we have $\Vert f_n-f_n\Vert\geq1$ for all $m\neq n$. Since Thm. \ref{lb565} is credited to Riesz, I guess Riesz first figured out this construction for $L^p$ and $l^p$ spaces.
\end{rem}

\end{comment}










\newpage


\section{Application to differential equations}



\subsection{Picard-Lindel\"of theorem}

We fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Let $I$ be an interval in $\Rbb$ containing at least two points.


\begin{df}
Let $T$ be a set. Let $X,Y$ be metric spaces. We say that a function $f:T\times X\rightarrow Y$ is \textbf{Lipschitz continuous on $X$} \index{00@Lipschitz continuous} if there exists $L\in\Rbb_{\geq0}$ (called the \textbf{Lipschitz constant}) such that for every $t\in T$ and $x_1,x_2\in X$ we have
\begin{align*}
d(f(t,x_1),f(t,x_2))\leq Ld(x_1,x_2)
\end{align*}  
\end{df}



\begin{thm}\label{lb554}
Let $E\subset V$. Let $\varphi\in C(I\times E,V)$ be Lipschitz continuous on $E$. Fix $t_0\in I,\xi\in E$. Then there exists at most one differentiable $f:I\rightarrow E$ satisfying
\begin{gather}\label{eq217}
f'(t)=\varphi(t,f(t))\qquad f(t_0)=\xi
\end{gather} 
for all $t\in I$.
\end{thm}



\begin{proof}
This theorem generalizes Lem. \ref{lb553}. In fact, we will prove this theorem in a similar way as we proved Lem. \ref{lb553}. Let $f,g:I\rightarrow$ be differentiable and assume that they both satisfy \eqref{eq217}. Then $\Omega=\{t\in I:f(t)=g(t)\}$ is a closed subset of $I$ since $f,g$ are continuous. $\Omega$ is nonempty since $t_0\in\Omega$. Since $I$ is connected, to prove that $\Omega=I$, it suffices to prove that $\Omega$ is open.

Choose any $x\in\Omega$. We want to prove that $x\in\Int_I(\Omega)$. Let us prove that if $x<\sup I$ then there exists $\delta>0$ such that $[x,x+\delta)\subset \Omega$. Then a similar argument shows that if $x>\inf I$ then there exists $\delta>0$ such that $(x-\delta,x]\subset\Omega$. This proves $x\in\Int_I(\Omega)$ whether $x$ is an endpoint of $I$ or not.

We first choose $\delta>0$ such that $[x,x+\delta]\subset I$. Since $f,g$ are continuous, both $\varphi(t,f(t))$ and $\varphi(t,g(t))$ are continuous in $t$. Therefore, by the fundamental theorem of calculus, we have
\begin{gather*}
f(t)=f(x)+\int^t_x\varphi(s,f(s))ds\qquad g(t)=g(x)+\int_x^t\varphi(s,g(s))ds
\end{gather*}
Since $f(x)=g(x)$, we have
\begin{align*}
\Vert f(t)-g(t)\Vert\leq\int_x^t \Vert \varphi(s,f(s))-\varphi(s,g(s))\Vert ds\leq L\int_x^t\Vert f(s)-g(s)\Vert ds
\end{align*}
where $L\in\Rbb_{\geq0}$ is a Lipschitz constant of $\varphi$ on $E$. Let $\dps A=\sup_{t\in[x,x+\delta]}\Vert f(t)-g(t)\Vert$. Then the above inequality implies $\Vert f(t)-g(t)\Vert\leq LA\delta$ for all $t\in[x,x+\delta]$, and hence $A\leq LA\delta$. Now we shrink $\delta$ so that $L\delta<1$. Then we must have $A=0$. So $f=g$ on $[x,x+\delta]$, and hence $[x,x+\delta)\subset\Omega$.
\end{proof}



\begin{eg}
The function $\varphi(x)=x^{\frac 13}$ is not Lipschitz continuous on any compact interval containing $0$. For every $c\geq0$, the function
\begin{align*}
f(x)=\left\{
\begin{array}{ll}
\big(\frac 23(x-c)\big)^{\frac 32}&\text{ if }x\geq c\\[1ex]
0 &\text{ if }x<c
\end{array}
\right.
\end{align*}
is differentiable on $\Rbb$ and satisfies the differential equation
\begin{align*}
f'(t)=(f(t))^{\frac 13}\qquad f(0)=0
\end{align*}
Therefore, Thm. \ref{lb554} does not hold without assume the Lipschitz continuity.
\end{eg}



\begin{thm}[\textbf{Picard-Lindel\"of theorem}]\index{00@Picard-Lindel\"of theorem}\label{lb556}
Let $\xi\in V$ and $0<R<+\infty$. Let $I=[a,b]$ where $-\infty<a<b<+\infty$. Assume that $\varphi \in C\big(I\times\ovl B_V(\xi,R),V\big)$ satisfies the following conditions:
\begin{enumerate}[label=(\arabic*)]
\item $\varphi$ is Lipschitz continuous on $\ovl B_V(\xi,R)$.
\item $M=\Vert\varphi\Vert_{l^\infty}$ is $<+\infty$.
\end{enumerate}
Assume that 
\begin{align}
|I|\leq \frac RM  \label{eq219}
\end{align} 
where $|I|=b-a$. Then there exists a unique differentiable function $f:I\rightarrow \ovl B_V(\xi,R)$ satisfying the differential equation
\begin{align}
f'(t)=\varphi(t,f(t))\qquad f(a)=\xi  \label{eq218}
\end{align}
for all $t\in I$. The same conclusion holds if $f(a)=\xi$ is replaced by $f(b)=\xi$.
\end{thm}


Note that the assumption $M<+\infty$ is automatic when $V=\Fbb^N$, since in that case $I\times\ovl B_V(\xi,R)$ is compact.  


\begin{proof}
We only prove the existence, since the uniqueness follows from Thm \ref{lb554}. We treat the case $f(a)=\xi$. The other case is similar. Also, by translating $f$, it suffices to assume $a=0$. So we let $I=[0,b]$. Then $bM\leq R$. Let $L$ be the Lipschitz constant of $\varphi$ on $\ovl B_V(\xi,R)$. Note that if we can find $f\in C\big(I,\ovl B_V(\xi,R)\big)$ satisfying the integral equation
\begin{align}
f(t)=\xi+\int_0^t \varphi(s,f(s))ds  \label{eq220}
\end{align}
for all $t\in I$, then $f$ clearly satisfies \eqref{eq218}.\\[-1ex]

Step 1. We first consider the special case that $bL<1$. Define a map
\begin{gather*}
T:C\big(I,\ovl B_V(\xi,R)\big)\rightarrow C(I,V)\\
(Tf)(t)=\xi+\int_0^t\varphi(s,f(s))ds
\end{gather*}
Then $T$ has range inside $X=C\big(I,\ovl B_V(\xi,R)\big)$ since for each $f\in X$ and $t\in I=[0,b]$ we have
\begin{align}
\Vert (Tf)(t)-\xi\Vert\leq \int_0^t\Vert\varphi(s,f(s))\Vert ds\leq bM\leq R
\end{align}
by \eqref{eq219}. Therefore, $T$ can be viewed as a map $X\rightarrow X$. 

Any fixed point of $T$ satisfies \eqref{eq220}. Therefore, by the contraction Thm. \ref{lb555}, it suffices to prove that $T$ is a contraction. Here, the metric on $X$ is defined by the $l^\infty$-norm on $C(I,V)$. Then for each $f,g\in X$. We compute that for each $t\in[0,b]$,
\begin{align*}
&\Vert (Tf)(t)-(Tg)(t)\Vert=\Big\Vert\int_0^t \varphi(s,f(s))ds-\int_0^t\varphi(s,g(s))ds\Big\Vert\\
\leq&\int_0^t\big\Vert\varphi(s,f(s))-\varphi(s,g(s))\big\Vert ds\leq L\int_0^t\big\Vert f(s)-g(s)\big\Vert ds\leq bL\Vert f-g\Vert_\infty
\end{align*}
Applying $\sup_{t\in[0,b]}$ to the LHS, we get $\Vert Tf-Tg\Vert_\infty\leq bL\Vert f-g\Vert_\infty$. Since $bL<1$, $T$ is a contraction.\\[-1ex]

Step 2. We consider the general case. Choose $N\in\Zbb_+$ such that $bL/N<1$. Let us prove by induction that for each $n=0,1,\dots,N$ there exists
\begin{align*}
f_n\in C\big([0,nb/N],\ovl B_V(\xi,nR/N) \big)
\end{align*}
satisfying \eqref{eq220} for all $t\in[0,nb/N]$. Then $f_N$ gives the desired function satisfying \eqref{eq220} for all $t\in I=[0,b]$.

The case $n=0$ is obvious. Assume that case $n-1$ has been proved where $1\leq n\leq N$. Let $\xi_{n-1}=f_{n-1}((n-1)b/N)$, which is inside $\ovl B_V(\xi,(n-1)R/N)$. Now we apply step 1, but replace the $I$ in step 1 by $I_n=[(n-1)b/N,nb/N]$ and replace the $\ovl B_V(\xi,R)$ by $\ovl B_V(\xi_{n-1},R/N)$. Note that by triangle inequality we have
\begin{align*}
\ovl B_V(\xi_{n-1},R/N)\subset \ovl B_V(\xi,nR/N)
\end{align*}
Note also that assumption \eqref{eq219} becomes $b/N\leq R/NM$, which is still satisfied. Thus, according to step 1, there exists $\dps g_n\in C\big(I_n,\ovl B_V(\xi_{n-1},R/N)\big)$ satisfying
\begin{align*}
g_n(t)=\xi_{n-1}+\int_{(n-1)b/N}^t \varphi(s,g(s))ds
\end{align*}
for all $t\in I_n$. Then (the graph of) $f_n$ is defined to be the union of (the graphs of) $f_{n-1}$ and $g_n$.
\end{proof}


Thm. \ref{lb556} still holds if we assume $R=+\infty$ and $I=\Rbb$:

\begin{co}\label{lb558}
Let $\xi\in V$. Assume that $\varphi\in C(\Rbb\times V,V)$ satisfies the following conditions:
\begin{enumerate}[label=(\arabic*)]
\item $\varphi$ is Lipschitz continuous on $V$.
\item $M=\Vert\varphi\Vert_{l^\infty}$ is $<+\infty$.
\end{enumerate}
Choose any $t_0\in\Rbb$. Then there exists a unique differentiable $f:\Rbb\rightarrow V$ satisfying
\begin{align}
f'(t)=\varphi(t,f(t))\qquad f(t_0)=\xi \label{eq221}
\end{align}
for all $t\in\Rbb$.
\end{co}

\begin{proof}
By Thm. \ref{lb556}, for each $a>0$ there exists a unique differentiable $f:[t_0-a,t_0+a]\rightarrow V$ satisfying \eqref{eq221}. The union of the graphs of these $f_a$ (for all $a>0$) gives the graph of the desired function.
\end{proof}






\subsection{Peano's existence theorem}





\begin{thm}\label{lb557}
Let $\xi\in\Rbb^N$. Let $\varphi\in C(\Rbb\times\Rbb^N,\Rbb^N)$ satisfy $M:=\Vert\varphi\Vert_\infty<+\infty$. Let $t_0\in\Rbb$. Then there exists a differentiable $f:\Rbb\rightarrow\Rbb^N$ satisfying
\begin{align}
f'(t)=\varphi(t,f(t))\qquad f(t_0)=\xi \label{eq222}
\end{align}
for all $t\in\Rbb$.
\end{thm}


\begin{proof}
Assume WLOG that $t_0=0$. It suffices to find $f\in C([0,1],\Rbb^N)$ satisfying
\begin{align}
f(t)=\xi+\int_0^t\varphi(s,f(s))ds \label{eq224}
\end{align}
on $[0,1]$. Then $f$ is differentiable and satisfies \eqref{eq222}. Then we can similarly find $f_1:[1,2]\rightarrow\Rbb^N$ satisfying $f_1'(t)=\varphi(t,f_1(t))$ and $f_1(1)=f(1)$. Namely, $f$ can be extended to a function $[0,2]\rightarrow\Rbb^N$ satisfying \eqref{eq222}. Repeating this argument, we get $f:[0,+\infty)\rightarrow\Rbb^N$ satisfying \eqref{eq222}, and similarly $f:\Rbb\rightarrow\Rbb^N$ satisfying \eqref{eq222}.\\[-1ex]

Step 1. Let $X=\ovl B_{\Rbb^N}(\xi,M)$. Since $I\times X$ is compact Hausdorff, by Stone-Weierstrass Thm. \ref{lb442}, we have a sequence of multivariable polynomials $(\varphi_n)_{n\in\Zbb_+}$ (where $\varphi_n\in\Rbb^N[t,x_1,\dots,x_N]$) converging uniformly on $[0,1]\times X$ to $\varphi$. Since $M=\Vert\varphi\Vert_\infty$, we have $\lim_n\Vert\varphi_n\Vert_{l^\infty(I\times X,\Rbb^N)}=M$. Therefore, by scaling each $\varphi_n$, we assume that $\Vert\varphi_n\Vert_{l^\infty(I\times X,\Rbb^N)}\leq M$. Since polynomials are clearly Lipschitz continuous, by Picard-Lindel\"of Thm. \ref{lb556}, for each $n$ there exists a differentiable $f_n:[0,1]\rightarrow X$ satisfying $f_n'(t)=\varphi_n(t,f_n(t))$ and $f_n(0)=\xi$. Equivalently, $f_n\in C([0,1],X)$ and
\begin{align}
f_n(t)=\xi+\int_0^t\varphi_n(s,f_n(s))ds \label{eq223}
\end{align}
for all $t\in[0,1]$.\\[-1ex]

Step 2. Recall that $\Vert\varphi_n\Vert\leq M$. For each $0\leq t_1\leq t_2\leq 1$, by \eqref{eq223} we have
\begin{align*}
\Vert f_n(t_2)-f_n(t_1)\Vert\leq\int_{t_1}^{t_2}\Vert\varphi_n(s,f_n(s))\Vert ds\leq M(t_2-t_1)
\end{align*}
Therefore $(f_n)_{n\in\Zbb_+}$ is an equicontinuous sequence in $C([0,1],\Rbb^N)$ and is uniformly bounded because $f_n([0,1])\subset X$. By Arzel\`a-Ascoli Thm. \ref{lb516}, $\{f_n:n\in\Zbb_+\}$ is a precompact subset of $C([0,1],\Rbb^N)$. Thus, by Prop. \ref{lb505}, $(f_n)$ has a uniformly convergent subsequence. By replacing $(f_n)$ with this subsequence, we assume WLOG that $(f_n)$ converges uniformly to some $f\in C([0,1],\Rbb^N)$. Since $f_n\in C([0,1],X)$, we have $f\in C([0,1],X)$.

To prove \eqref{eq224}, in view of \eqref{eq223}, it suffices to prove for each $t\in[0,1]$ that
\begin{align*}
\lim_{n\rightarrow\infty}\int_0^t\varphi_n(s,f_n(s))ds=\int_0^t\varphi(s,f(s))ds
\end{align*}
By Cor. \ref{lb380}, it suffices to prove $\lim_n \varphi_n(\cdot,f_n(\cdot))=\varphi(\cdot,f(\cdot))$ in $C([0,1],\Rbb^N)$ under the $l^\infty$-norm. Note that
\begin{align*}
\Vert \varphi(\cdot,f(\cdot))-\varphi_n(\cdot,f_n(\cdot))\Vert\leq \Vert \varphi(\cdot,f(\cdot))-\varphi(\cdot,f_n(\cdot))\Vert+\Vert \varphi(\cdot,f_n(\cdot))-\varphi_n(\cdot,f_n(\cdot))\Vert
\end{align*}
The first term on the RHS converges uniformly to $0$ since $f_n\rightrightarrows f$ and since $\varphi$ is uniformly continuous (Thm. \ref{lb294}). The second term converges uniformly to $0$ since $\varphi_n\rightrightarrows \varphi$ on $[0,1]\times X$. So the LHS converges uniformly to $0$.
\end{proof}

\begin{comment}
By Thm. \ref{lb120}, it suffices to prove
\begin{align}\label{eq225}
\lim_{m,n\rightarrow\infty} \varphi_m(\cdot,f_n(\cdot))=\varphi(\cdot,f(\cdot)) 
\end{align}
For each $m$, $\varphi_m$ is uniformly continuous on the compact set $[0,1]\times X$ (by Thm. \ref{lb294}). Therefore, since $(f_n)$ converges uniformly to $f$, we obtain
\begin{align*}
\lim_{n\rightarrow\infty}\varphi_m(\cdot,f_n(\cdot))=\varphi_m(\cdot,f(\cdot))
\end{align*}
On the other hand, since $\lim_m\varphi_m$ converges uniformly to $\varphi$, we have
\begin{align*}
\lim_{m\rightarrow\infty}\sup_{n\in\Zbb_+} \big\Vert \varphi_m(\cdot,f_n(\cdot))-\varphi(\cdot,f_n(\cdot))\big\Vert_{C([0,1],\Rbb^N)}=0
\end{align*}
Thus, by Moore-Osgood Thm. \ref{lb289}, we get \eqref{eq225}.
\end{comment}



Thm. \ref{lb557} is parallel to Cor. \ref{lb558} since $\varphi$ is defined on the whole space $\Rbb\times\Rbb^N$. However, in applications it is common that $\varphi$ is only defined on a subset of $\Rbb\times\Rbb^N$. Thus, we want to prove an existence theorem similar to Thm. \ref{lb556} without assuming the Lipschitz continuity, and hence without concluding the uniqueness. We shall state this result for a finite dimensional real Banach space $V$. This means that we consider $V=\Rbb^N$, but not necessarily equipped with the Euclidean norm. Since all norms on $\Rbb^N$ are equivalent (cf. Pb. \ref{lb559}), we conclude that the closed balls under any norm of $\Rbb^N$ is compact (since they are closed subsets of standard closed balls of $\Rbb^N$). In practice, it is useful to consider non-Euclidean norms of $\Rbb^N$. For example, under the $l^\infty$-norm on $\Rbb^N=l^\infty(\{1,\dots,N\},\Rbb)$, the closed balls are actually the cubes. 




\begin{thm}[\textbf{Peano's existence theorem}]\index{00@Peano's existence theorem}
Let $V$ be a finite-dimensional real Banach space. Let $\xi\in V$ and $0<R<+\infty$. Let $I=[a,b]$ where $-\infty<a<b<+\infty$. Let $\varphi \in C\big(I\times\ovl B_V(\xi,R),V\big)$. Assume
\begin{align}
|I|\leq \frac RM  
\end{align} 
where $|I|=b-a$ and $M=\Vert\varphi\Vert_\infty$. Then there exists a differentiable function $f:I\rightarrow \ovl B_V(\xi,R)$ satisfying the differential equation
\begin{align}
f'(t)=\varphi(t,f(t))\qquad f(a)=\xi  \label{eq226}
\end{align}
for all $t\in I$. The same conclusion holds if $f(a)=\xi$ is replaced by $f(b)=\xi$.
\end{thm}


\begin{proof}
Since $\Rbb\times V\simeq \Rbb^{N+1}$ is LCH and $I\times \ovl B_V(\xi,R)$ is compact, by Tietze extension Thm. \ref{lb468}, $\varphi$ can be extended to an element in $C_c(I\times V,V)$ still satisfying $\Vert\varphi\Vert_\infty=M$. Therefore, by Thm. \ref{lb557}, there exists a differentiable $\xi:\Rbb\rightarrow V$ satisfying \eqref{eq226}. It remains to check that $f(I)\subset \ovl B_V(\xi, R)$: For each $t\in I=[a,b]$, since $f(t)=\xi+\int_a^t\varphi(s,f(s))dt$, we have
\begin{align*}
\Vert f(t)-\xi\Vert\leq\int_a^t\Vert\varphi(s,f(s))\Vert dt\leq (t-a)M\leq |I|\cdot M\leq R
\end{align*}
and hence $f(t)\in \ovl B_V(\xi,R)$.
\end{proof}

%% Record #29 2024/1/3 three lectures  73

\newpage




\section{Differential calculus on $\Rbb^N$}

In this chapter, we fix a Banach space $V$ over $\Rbb$. However, we will be mainly interested in the case that $V$ is finite dimensional.


\subsection{Differentiability and $C^1$}

A motivating question of this short chapter is the following: Suppose that $f$ is a function on an open subset $\Omega\subset\Rbb^N$, and $\gamma:(a,b)\rightarrow\Omega$ is differentiable. How to calculate $(f\circ\gamma)'$? The key to the answer of this question is the following definition:


\begin{df}\label{lb576}
Let $\Omega\subset\Rbb^N$ be open. Let $p\in\Omega$. Let $f:\Omega\rightarrow V$. Assume that there is an $\Rbb$-linear map $A:\Rbb^N\rightarrow V$ such that for all $v\in\Rbb^N$ we have
\begin{align*}
f(p+v)=f(p)+Av+o(\Vert v\Vert)
\end{align*}
(recall Def. \ref{lb574} for the meaning of $o(\Vert v\Vert)$). Namely,
\begin{align}
\lim_{v\rightarrow 0}\frac{\Vert f(p+v)-f(p)-Av\Vert}{\Vert v\Vert}=0 \label{eq228}
\end{align}
Then we say that $f$ is \textbf{differentiable at $p$}. \index{00@Differentiable on $\Rbb^N$} We write
\begin{align*}
A=df|_p=df(p):\Rbb^N\rightarrow V
\end{align*}
and call $A$ the \textbf{differential} of $f$ at $p$. If $f$ is differentiable at every point of $\Omega$, we simply say that $f$ is \textbf{differentiable on $\Omega$}. 
\end{df}

\begin{comment}
For simplicity, in the future, we write
\begin{align*}
o(\Vert v\Vert)=o(v)
\end{align*}
\end{comment}

\begin{rem}\label{lb577}
Every linear map $A:\Rbb^N\rightarrow V$ is bounded since, if $v=a_1e_1+\cdots+a_Ne_N$ where $e_1,\dots,e_N$ are the standard basis of $\Rbb^N$, then
\begin{align*}
\Vert Av\Vert \leq \sum_i |a_i|\cdot \Vert Ae_i\Vert\leq \Vert v\Vert \Big(\sum_i\Vert Ae_i\Vert^2\Big)^{\frac 12}
\end{align*}
by Minkowski's inequality.
\end{rem}

\begin{rem}\label{lb585}
If $f$ is differentiable at $p$, then $f$ is continuous at $p$ since, by the continuity of $A$, we have $\lim_{v\rightarrow0} (Av+o(\Vert v\Vert))=0$.
\end{rem}





Whenever $f$ is differentiable at $p$, its differential can be computed explicitly:

\begin{pp}\label{lb580}
Let $\Omega\subset\Rbb^N$ be open. Let $f:\Omega\rightarrow V$ be differentiable at $p\in\Omega$. Then $\partial_1f,\dots,\partial_Nf$ exist at $p$. Define the \textbf{Jacobian (matrix)} \index{00@Jacobian matrix} \index{Jac@$\Jac f$, Jacobian matrix}
\begin{align}
\Jac f|_p=(\partial_1f(p),\dots,\partial_Nf(p))
\end{align}
viewed as an $1\times N$ matrix with entries in $V$. (When $V=\Rbb^M$, we view $\Jac f|_p$ as an $M\times N$ real matrix.) Then for each $v=(a_1,\dots,a_n)^\tr\in\Rbb^N$ (viewed as an $N\times 1$ matrix) we have 
\begin{align}
df|_p\cdot v=\Jac f|_p\cdot v=\sum_{i=1}^N a_i\partial_i f(p)\label{eq229}
\end{align}
\end{pp}

Note that when $V=\Rbb^M$ and $f=(f^1,\dots,f^N)^\tr$, \eqref{eq229} reads
\begin{align}
df|_p\cdot v=\begin{pmatrix}
\partial_1 f^1 & \cdots & \partial_N f^1\\
 & \vdots & \\
\partial_1 f^M & \cdots & \partial_N f^M
\end{pmatrix}_p
\cdot
\begin{pmatrix}
a_1 \\
\vdots\\
a_N
\end{pmatrix}
\end{align}

\begin{proof}
Let $e_1,\dots,e_N\in\Rbb^N$ be the standard basis of $\Rbb^N$. So $v=a_1e_1+\cdots+a_Ne_N$. Let $A=df|_p$. By \eqref{eq228}, we have
\begin{align*}
\lim_{t\rightarrow 0}\Big\Vert \frac{f(p+te_i)-f(p)}{t}-Ae_i\Big\Vert=\lim_{t\rightarrow 0}\frac{\Vert f(p+te_i)-f(p)-tAe_i\Vert}{\Vert t e_i\Vert}=0
\end{align*}
which shows that $\partial_i f(p)=Ae_i$. Therefore
\begin{align*}
Av=\sum_i a_iAe_i=\sum_i a_i \partial_i f(p)
\end{align*}
\end{proof}






\begin{co}
$f$ has at most one differential at a point $p$. 
\end{co}

\begin{proof}
Immediate from \eqref{eq229}.
\end{proof}


\begin{eg}
When $N=1$, the above definition of differentiability in Def. \ref{lb576} agrees with the one in Sec. \ref{lb575}. Moreover, we clearly have $\Jac f|_p=f'(p)$. 
\end{eg}



\begin{thm}[\textbf{Chain rule}] \index{00@Chain rule in $\Rbb^N$}\label{lb578}
Let $\Gamma\subset\Rbb^M$ and $\Omega\subset\Rbb^N$ be open. Let $g:\Gamma\rightarrow\Omega$ be differentiable at $p\in\Gamma$. Let $f:\Omega\rightarrow V$ be differentiable at $g(p)$. Then $f\circ g:\Gamma\rightarrow V$ is differentiable at $p$, and 
\begin{align}
d(f\circ g)|_p=df|_{g(p)}\cdot dg|_p
\end{align}
Equivalently, we have
\begin{align}
\Jac(f\circ g)|_p=\Jac f|_{g(p)}\cdot \Jac g|_p\label{eq230}
\end{align}
\end{thm}

Note that when $V=\Rbb^L$, \eqref{eq230} is of type $L\times M=(L\times N)(N\times M)$, and reads
\begin{align}
\begin{pmatrix}
\partial_1 (f\circ g)^1 & \cdots & \partial_M (f\circ g)^1\\
 & \vdots & \\
\partial_1 (f\circ g)^L & \cdots & \partial_M (f\circ g)^L
\end{pmatrix}_p
=\begin{pmatrix}
\partial_1 f^1 & \cdots & \partial_N f^1\\
 & \vdots & \\
\partial_1 f^L & \cdots & \partial_N f^L
\end{pmatrix}_{g(p)}
\cdot\begin{pmatrix}
\partial_1 g^1 & \cdots & \partial_M g^1\\
 & \vdots & \\
\partial_1 g^N & \cdots & \partial_M g^N
\end{pmatrix}_{p}
\end{align} 


\begin{proof}
We write
\begin{align*}
g(p+v)=g(p)+Bv+\beta(v)\qquad f(g(p)+w)=f\circ g(p)+Aw+\alpha(w)
\end{align*}
where $A:\Rbb^N\rightarrow V$ and $B:\Rbb^M\rightarrow\Rbb^N$ are linear, and
\begin{align}
\lim_{v\rightarrow 0}\beta(v)/\Vert v\Vert=\lim_{w\rightarrow 0}\alpha(w)/\Vert w\Vert=0  \label{eq232}
\end{align}
Then
\begin{align*}
&f\circ g(p+v)=f(g(p)+Bv+\beta(v))=f\circ g(p)+A(Bv+\beta(v))+\alpha(Bv+\beta(v))\\
=&f\circ g(p)+ABv+A\beta(v)+\alpha(g(p+v)-g(p))
\end{align*}
Note that $A,B$ are bounded by Rem. \ref{lb577}. So $\Vert A\beta(v)\Vert\leq\Vert A\lVert \cdot\Vert\beta(v)\Vert$, and hence $\lim_{v\rightarrow 0}\Vert A\beta(v)\Vert/\Vert v\Vert=0$. To finish the proof, it remains to prove
\begin{align*}
\lim_{v\rightarrow0}\Vert \alpha(g(p+v)-g(p))\Vert/\Vert v\Vert=0
\end{align*}

Since $g$ is continuous at $p$, there is a neighborhood $\Delta$ of $0\in\Rbb^M$ such that $g(p+v)-g(p)$ is defined and is in the domain of $\alpha$ (which is $\{q-g(p):q\in\Omega\}$) for every $v\in \Delta$. Define $\gamma:\Delta\rightarrow V$ to be
\begin{align*}
\gamma(v)=\left\{
\begin{array}{ll}
\dps\frac{\alpha\big(g(p+v)-g(p)\big)}{\Vert g(p+v)-g(p)\Vert}&\text{ if }g(p+v)-g(p)\neq0\\[2ex]
0&\text{ otherwise}
\end{array}
\right.
\end{align*}
Then $\gamma$ is continuous at $v=0$ and $\gamma(0)=0$ by \eqref{eq232}. (This part is similar to the construction \eqref{eq231}.) Thus
\begin{align*}
&\frac{\Vert\alpha(g(p+v)-g(p))\Vert}{\Vert v\Vert}=\Vert\gamma(v)\Vert\cdot\frac{\Vert g(p+v)-g(p)\Vert}{\Vert v\Vert}\\
=&\Vert\gamma(v)\Vert\cdot\frac{\Vert Bv+\beta(v)\Vert}{\Vert v\Vert}\leq \Vert\gamma(v)\Vert\cdot\Vert B\Vert+\Vert \gamma(v)\Vert\cdot \frac{\Vert\beta(v)\Vert}{\Vert v\Vert}
\end{align*}
where the RHS converges to $0$ as $v\rightarrow 0$.
\end{proof}




\begin{co}[\textbf{Chain rule}]\label{lb581}
Let $I$ be an interval. Let $\Omega\subset\Rbb^N$ be open. Let $t_0\in I$. Let $\gamma:I\rightarrow\Omega$ be differentiable at $t_0$. Let $f:\Omega\rightarrow V$ be differentiable at $p=\gamma(t_0)$. Then $f\circ\gamma$ is differentiable at $t_0$, and 
\begin{align}
(f\circ\gamma)'(t_0)=\Jac f|_{\gamma(t_0)}\cdot \gamma'(t_0)=\sum_{i=1}^N (\gamma^i)'(t_0)\cdot\partial_i f(\gamma(t_0))
\end{align}
\end{co}

\begin{proof}
This follows easily from Thm. \ref{lb578} when $t_0$ is an interior point of $I$. Suppose that $t_0$ is an end point of $I$. For example, consider $t_0=b$ where $I$ is $[a,b]$ or $(a,b]$. Then we can easily extend $\gamma$ to a function on $[a,b+1)$ or $(a,b+1)$ which is differentiable at $t_0$. (For example, if $t\in(b,b+1)$, we can define $\gamma(t)=\gamma(b)+(t-b)\gamma'(b)$.) Then we can apply Thm. \ref{lb578} again to finish the proof.
\end{proof}



\begin{eg}\label{lb579}
Let $\Omega\subset\Rbb^N$ be open. Let $f:\Omega\rightarrow V$ be differentiable at $p\in\Omega$. Then for each $v\in\Rbb^N$, the \textbf{directional derivative} \index{00@Directional derivative $\nabla_vf$}
\begin{align*}
(\nabla_vf)(p)=\lim_{t\rightarrow 0}\frac{f(p+tv)-f(p)}{t}
\end{align*}
exists and equals $df|_p\cdot v=\Jac f|_p\cdot v$.
\end{eg}

\begin{proof}
Apply the chain rule to $\gamma(t)=p+tv$.
\end{proof}


We have thus proved the chain rule, one of the most important reasons for introducing differentiability on $\Rbb^N$. We know that differentiable functions have partial derivatives of first order. However, having first order partial derivatives does not imply differentiability or even continuity:


\begin{eg}
Let $\dps f(x,y)=\frac{xy}{x^2+y^2}$ when $(x,y)\neq (0,0)$, and $f(0,0)=0$. Then $\lim_{r\rightarrow 0}f(r,r)=\frac 12$ and $\lim_{r\rightarrow 0}f(r,0)=0$, showing that $f$ has no limit and is not continuous $(0,0)$ at. However, $f(x,0)=f(0,y)=0$. So $\partial_1f$ and $\partial_2f$ are both equal to $0$ at $(0,0)$.
\end{eg}

\begin{eg}
Let $\dps f(x,y)=\frac{x^3}{x^2+y^2}$ when $(x,y)\neq (0,0)$, and $f(0,0)=0$. Since $|f(x,y)|\leq |x|$ and $\lim_{(x,y)\rightarrow(0,0)}|x|=0$, we conclude that $f$ is continuous at $(0,0)$ (and hence is continuous everywhere). We have $\partial_1 f|_{(0,0)}=\frac d{dx}x|_{x=0}=1$ and $\partial_2 f|_{(0,0)}=0$. Let $\gamma(t)=(at,bt)$ where $(a,b)\in\Rbb^2\setminus\{(0,0)\}$. Then
\begin{align*}
(\nabla_{(a,b)} f)(0)=(f\circ\gamma)'(0)=\frac{a^3}{a^2+b^2}
\end{align*}
is not equal to $\Jac f|_{(0,0)}\cdot (a,b)^\tr=a$. Thus, by Exp. \ref{lb579}, $f$ is not differentiable at $(0,0)$.
\end{eg}



To have differentiability, we need the continuity of partial derivatives of first order.


\begin{df}
Let $\Omega\subset\Rbb^N$ be open. For each $r\in\Nbb$, we let \index{Cr@$C^r$ on $\Rbb^N$}
\begin{align*}
C^r(\Omega,V)=\big\{& f\in C(\Omega,V):\partial_{i_1}\cdots\partial_{i_k}f\text{ exists and is in }C(\Omega,V)  \\
&\text{for all $0\leq k\leq r$ and $1\leq i_1,\dots,i_k\leq N$}
\big\}
\end{align*}
In particular, we understand $C^0(\Omega,V)$ as $C(\Omega,V)$, and
\begin{align*}
C^\infty(\Omega,V)=\bigcap_{r\in\Nbb} C^r(\Omega,V)
\end{align*}
Elements in $C^\infty(\Omega,V)$ are called \textbf{smooth functions}. \index{00@Smooth functions, $\Rbb^N$}
\end{df}

It is clear that $C^r(\Omega,V)\subset C^{q}(\Omega,V)$ if $r\geq q$.

\begin{rem}
Suppose that $r\in\Zbb_+$ and $f\in C^r(\Omega,V)$. Let $1\leq k\leq r$, and let $\sigma:\{1,\dots,k\}\rightarrow \{1,\dots,k\}$ be a bijection. Then by Thm. \ref{lb406} or \ref{lb407}, for each $1\leq i_1,\dots, i_k\leq N$ we have
\begin{align*}
\partial_{i_1}\cdots\partial_{i_k}f=\partial_{i_{\sigma(1)}}\cdots\partial_{i_{\sigma(k)}}f
\end{align*}
\end{rem}



\begin{thm}
Let $\Omega$ be an open subset of $\Rbb^N$. Let $f\in C^1(\Omega,V)$. Then $f$ is differentiable on $\Omega$.
\end{thm}

\begin{proof}
We prove this by induction. The case $N=1$ is obvious. Assume that the case $N$ is true where $N\in\Zbb_+$. Let $\Omega\subset \Rbb^{N+1}$ be open, and let $(p_0,p_1,\dots,p_N)=(p_0,p_\blt)\in\Omega$. Let $v=(a_0,a_\blt)=(a_0,a_1,\dots,a_N)\in\Omega$. Then
\begin{align*}
&f(p_0+a_0,p_\blt+a_\blt)-f(p_0,p_\blt)\\
=&f(p_0+a_0,p_\blt+a_\blt)-f(p_0,p_\blt+a_\blt)+f(p_0,p_\blt+a_\blt)-f(p_0,p_\blt)\\
=&\int_{0}^{a_0}\partial_0 f(p_0+t,p_\blt+a_\blt)dt+f(p_0,p_\blt+a_\blt)-f(p_0,p_\blt)
\end{align*}
By case $N$, there exist $\lambda_1,\dots,\lambda_N\in\Rbb$ such that
\begin{align*}
f(p_0,p_\blt+a_\blt)-f(p_0,p_\blt)=\sum_{i=1}^N \lambda_ia_i+o(\Vert a_\blt\Vert)
\end{align*}
(In fact, $\lambda_i=\partial_i f(p_0,p_\blt)$ by Prop. \ref{lb580}.) Let
\begin{align*}
&h(a_0,a_\blt)=\int_{0}^{a_0}\partial_0 f(p_0+t,p_\blt+a_\blt)dt-\partial_0 f(p_0,p_\blt)\cdot a_0\\
=&\int_{0}^{a_0}\big(\partial_0 f(p_0+t,p_\blt+a_\blt)-\partial_0 f(p_0,p_\blt)\big)dt
\end{align*}
Since $\partial_1f$ is continuous, for every $\eps>0$ there exists $\delta>0$ such that for all $(t,a_\blt)\in\Rbb^N$ with norm $\leq \delta$ we have
\begin{align*}
\Vert \partial_0 f(p_0+t,p_\blt+a_\blt)-\partial_0 f(p_0,p_\blt)\Vert\leq\eps
\end{align*}
and hence $\Vert h(a_0,a_\blt)\Vert\leq\eps |a_0|$ for all $(a_0,a_\blt)$ with norm $\leq\delta$. Thus $h(a_0,a_\blt)=o(|a_0|)$. Therefore
\begin{align*}
&f(p_0+a_0,p_\blt+a_\blt)-f(p_0,p_\blt)-\partial_0 f(p_0,p_\blt)\cdot a_0-\sum_{i=1}^N \lambda_ia_i\\
=&o(|a_0|)+o(\Vert a_\blt\Vert)=o(\Vert(a_0,a_\blt)\Vert)
\end{align*}
\end{proof}


\subsection{Applications of the chain rule}

\begin{comment}
\begin{eg}
Every $A\in\mc L(\Rbb^N)$ is differentiable everywhere, and $A$ is the diffferential of itself. It follows from chain rule that if $I$ is an open interval and $f\in C^1(I,\Rbb^N)$, then
\begin{align*}
\frac d{dt}Af(t)=Af'(t)
\end{align*}
\end{eg}
\end{comment}
\begin{eg}\label{lb583}
Let $I,J$ be nonempty open intervals in $\Rbb$. Choose $f\in C(I\times J,V)$ such that $\partial_2f$ exists and is in $C(I\times J,V)$. Let $\alpha,\beta\in C^1(J,\Rbb)$ such that $\alpha(J)\subset I,\beta(J)\subset I$. Then
\begin{align}\label{eq234}
\frac d{dy}\int_{\alpha(y)}^{\beta(y)}f(x,y)dx=\int_{\alpha(y)}^{\beta(y)}\partial_2 f(x,y)dx+f(\beta(y),y)\beta'(y)-f(\alpha(y),y)\alpha'(y)
\end{align}
\end{eg}


\begin{proof}
Define $F:I\times I\times J\rightarrow V$ by
\begin{align*}
F(r,s,t)=\int_r^s f(x,t)dx=\int_e^s f(x,t)dx-\int_e^r f(x,t)dx
\end{align*}
where $e$ is any element of $I$. By Exe. \ref{lb408}, $F$ is continuous. Since $\partial_2f$ is assumed to be continuous, by Thm. \ref{lb405}, we have
\begin{align*}
\partial_3 F(r,s,t)=\int_r^s \partial_2 f(x,t)dx
\end{align*}
By the fundamental theorem of calculus, we have
\begin{align*}
\partial_1 F(r,s,t)=-f(r,t)\qquad\partial_2 F(r,s,t)=f(s,t)
\end{align*}
By Exe. \ref{lb408}, $\partial_1F,\partial_2F,\partial_3F$ are continuous, and hence $F$ is a $C^1$-function. Therefore, by the chain rule (Cor. \ref{lb581}), the LHS of \eqref{eq234} can be calculated by setting $(r,s,t)=(\alpha(y),\beta(y),y)$:
\begin{align*}
&\frac d{dy}F(\alpha(y),\beta(y),y)\\
=&\partial_1 F(\alpha(y),\beta(y),y)\alpha'(y)+\partial_2 F(\alpha(y),\beta(y),y)\beta'(y)+\partial_3F(\alpha(y),\beta(y),y)
\end{align*}
which equals the RHS of \eqref{eq234}.
\end{proof}


\begin{eg}
Fix $v\in V$ and $\varphi\in C(\Rbb,V)$. Let $A\in\fk L(V)$. Then $\frac d{dt}e^{At}=Ae^{At}=e^{At}A$ by Exe. \ref{lb582}. By Exp. \ref{lb583} and Thm. \ref{lb392}, we have
\begin{align*}
\frac d{dt}\int_0^t e^{A(t-s)}\varphi(s)ds=\varphi(t)+\int_0^tAe^{A(t-s)}\varphi(s)ds=\varphi(t)+A\int_0^te^{A(t-s)}\varphi(s)ds
\end{align*}
It follows that
\begin{align}
f(t)=e^{At}v+\int_0^t e^{A(t-s)}\varphi(s)ds \label{eq235}
\end{align}
is a solution of the differential equation
\begin{align*}
f'(t)=Af(t)+\varphi(t)\qquad f(0)=v
\end{align*}
It is the unique solution by Picard-Lindel\"of Cor. \ref{lb558}. %The formula \eqref{eq235} is called the \textbf{variation of constants formula}.
\end{eg}


\begin{thm}[\textbf{Finite-increment theorem}]\index{00@Finite-increment theorem on $\Rbb^N$}\label{lb584}
Let $\Omega$ be an open subset of $\Rbb^N$. Assume that $f:\Omega\rightarrow V$ is differentiable. Assume that $x,y\in\Omega$ satisfy $[x,y]\subset\Omega$ where
\begin{align*}
[x,y]=\{(1-t)x+ty:0\leq t\leq 1\}
\end{align*}
Then
\begin{align}\label{eq236}
\Vert f(y)-f(x)\Vert \leq \Big(\sup_{z\in[x,y]}\Vert df|_z\Vert  \Big)\cdot \Vert y-x\Vert
\end{align}
where $\Vert df|_z\Vert$ is the operator norm of $df|_z$.
\end{thm}

\begin{proof}
This can be proved in a similar way as Cor. \ref{lb335}. Let $\gamma:[0,1]\rightarrow\Omega$ be $\gamma(t)=(1-t)x+ty$. Then by chain rule we have $(f\circ\gamma)'(t)=df|_{\gamma(t)}\cdot\gamma'(t)=df|_{\gamma(t)}\cdot (y-x)$. Thus $\Vert (f\circ\gamma)'(t)\Vert$ is $\leq$ the RHS of \eqref{eq236}. Therefore, by the single-variable finite-increment Thm. \ref{lb333}, we have
\begin{align*}
\Vert f(y)-f(x)\Vert=\Vert f\circ\gamma(1)-f\circ\gamma(0)\Vert\leq\sup_{t\in[0,1]}\Vert (f\circ\gamma)'(t)\Vert
\end{align*}
which is $\leq$ the RHS of \eqref{eq236}. 
\end{proof}



The following corollary provides a lot of examples satisfying the assumptions on $\varphi$ in Picard-Lindel\"of Thm. \ref{lb556}. Recall Def. \ref{lb364} for the meaning of convex sets.


\begin{co}
Let $\Omega$ be an open subset of $\Rbb^N$. Let $f\in C^1(\Omega,V)$. Let $K$ be a compact convex subset of $\Omega$. Then $f|_K$ is Lipschitz continuous.
\end{co}

\begin{proof}
Since $df:\Omega\rightarrow \fk L(\Rbb^N,V)$ is continuous (because it is essentially $\Jac f$ by Prop. \ref{lb580}), there exists $L\in\Rbb_{\geq0}$ such that $\Vert df|_x\Vert\leq L$ for all $x\in K$. By Thm. \ref{lb584}, $L$ is a Lipschitz constant of $f|_K$.
\end{proof}


\begin{co}
Let $\Omega$ be a nonempty open connected subset of $\Rbb^N$. Assume that $f:\Omega\rightarrow V$ satisfies that $df=0$ everywhere on $\Omega$. Then $f$ is a constant.
\end{co}

\begin{proof}
Fix $p\in\Omega$, and let $v=f(p)$. Let us prove that $f=v$ on $\Omega$. Let $\Delta=\{x\in\Omega:f(x)=v\}$, which is a closed subset of $\Omega$ by the continuity of $f$ (Rem. \ref{lb585}). $\Delta$ is nonempty since $p\in\Delta$. Thus, if we can show that $\Delta$ is open in $\Omega$, then $\Delta=\Omega$ because $\Omega$ is connected, and hence the proof is complete.

To see that $\Delta$ is open, we choose any $x\in\Delta$, and choose $r>0$ such that $U=B_{\Rbb^N}(x,r)$ is contained in $\Omega$. Since $U$ is path-connected and hence connected, by Thm. \ref{lb584} and the fact that $df=0$, for every $y\in U$ we have $\Vert f(x)-f(y)\Vert=0$ and hence $f(y)=v$. So $U\subset\Delta$. This proves that $x$ is an interior point of $\Delta$.
\end{proof}

%% Record #1 2024/02/26 two lectures  2


\section{Inner product spaces}

Hilbert spaces and measure theory are two parallel but deeply connected theories that arose in the study of Fourier series and differential equations. We will spend half the semester learning these two theories in turn. The climax of this entire story is the Riesz-Fischer theorem, which establishes a connection between these two theories.

In the following three chapters, we develop the basic theory of Hilbert spaces. A Hilbert space is defined to be a complete inner product space. This definition indicates that Hilbert spaces have two important aspects:
\begin{itemize}
\item Geometry: orthogonal and orthonormal vectors.
\item Analysis: completeness, and more.
\end{itemize}
We will focus on the geometric aspect in this chapter, and leave the discussion of the analytic aspect to the next chapter. As we shall see, the geometry of orthogonal vectors (together with the powerful \textbf{Gram-Schmidt process}) provides a uniform understanding of many identities and inequalities: Bessel's inequality (and its special case, Cauchy-Schwarz inequality), Parserval's identity. We will apply this geometric understanding to Fourier series. Surprisingly, the geometry of inner product spaces provides an elegant proof of the following analytic result: Every Riemann integrable function on $[-\pi,\pi]$ is the limit of its Fourier series under the $L^2$-norm.


When referring to Hilbert spaces and inner product spaces, it is usually assumed that the field is $\Cbb$, since complex Hilbert spaces are more useful than real ones. We will present the theory only for complex Hilbert spaces, although it can be easily adapted to real ones. 



Starting from this chapter, we adopt the notations \index{lp@$l^p(X)=l^p(X,\Cbb)$} \index{CX@$C(X)=C(X,\Cbb)$}
\begin{gather}
\begin{gathered}
l^p(X)=l^p(X,\Cbb)\qquad C(X)=C(X,\Cbb)
\end{gathered}
\end{gather}




\subsection{Inner product spaces}


We fix a complex vector space $V$.





\begin{df}
A map of complex vector spaces $T:V\rightarrow W$ is called \textbf{antilinear} or \textbf{conjugate linear} \index{00@Antilinear map} if for every $a,b\in\Cbb$ and $u,v\in V$ we have
\begin{align*}
T(au+bv)=\ovl au+\ovl bv
\end{align*}
where $\ovl a,\ovl b$ are the complex conjugates of $a,b$.
\end{df}


For example, the involution $*:\scr A\rightarrow\scr A$ of a $*$-algebra is antilinear (recall Def. \ref{lb586}).

\begin{df}
A function $\bk{\cdot|\cdot}:V\times V\rightarrow\Cbb$ (sending $u\times v\in V^2$ to $\bk{u|v}$) is called a \textbf{sesquilinear form} \index{00@Sesquilinear form} if it is linear on the first variable, and antilinear on the second one.\footnote{Physicists prefer the opposite convention, i.e., their sesquilinear forms are antilinear on the first variables.} Namely, for each $a,b\in\Cbb$ and $u,v,w\in V$ we have
\begin{gather*}
\bk{au+bv|w}=a\bk{u|w}+b\bk{v|w}\qquad \bk{w|au+bv}=\ovl a\bk{w|u}+\ovl b\bk{w|v}
\end{gather*}
More generally, if $V,W$ are complex vector spaces, a map $V\times W\rightarrow\Cbb$ is also called \textbf{sesquilinear} if it is linear on the $V$-component and antilinear on the $W$-component.
\end{df}


Notice the difference between the notations $\bk{u|v}$ and $\bk{u,v}$: the latter always means a bilinear form, i.e., a function which is linear on both variables.


\begin{pp}\label{lb587}
Suppose that $\bk{\cdot|\cdot}$ and $(\cdot|\cdot)$ are sesquilinear forms on $V$ satisfying $\bk{v|v}=(v|v)$ for all $v\in V$. Then $\bk{u|v}=(u|v)$ for all $u,v\in V$.
\end{pp}


\begin{proof}
Let us prove that $\bk{u|v}$ can be written in terms of expressions of the form $\bk{\xi|\xi}$ where $\xi\in V$. Choose any $t\in[-\pi,\pi]$. Let
\begin{align*}
f(t):=\bk{u+e^{\im t}v|u+e^{\im t}v}=\bk{u|u}+\bk{v|v}+e^{-\im t}\bk{u|v}+e^{\im t}\bk{v|u}
\end{align*}
Then $\bk{u|v}$ is a Fourier coefficient of $f$. Namely, using the fact that $\frac 1{2\pi}\int_{-\pi}^\pi e^{\im nt}dt=\delta_{n,0}$ (where $n\in\Zbb$), we have
\begin{align}\label{eq237}
\bk{u|v}=\frac 1{2\pi}\int_{-\pi}^\pi \bk{u+e^{\im t}v|u+e^{\im t}v}e^{\im t}dt
\end{align}
This finishes the proof.
\end{proof}

\begin{rem}
In practice, it is sometimes more convenient to have a discrete version of \eqref{eq237}: We view $f(t)$ as a function on the finite abelian group $\{0,\frac \pi 2,\pi,\frac{3\pi}2\}\simeq\Zbb/4\Zbb$. Then $\bk{u|v}$ is a coefficient of the ``discrete Fourier transform" of $f$:
\begin{align}\label{eq238}
\begin{aligned}
&\bk{u|v}=\frac 14\sum_{t=0,\frac \pi 2,\pi,\frac{3\pi}2}~ \bk{u+e^{\im t}v|u+e^{\im t}v}e^{\im t}\\
=&\frac 14\Big(\bk{u+v|u+v}-\bk{u-v|u-v}+\im\bk{u+\im v|u+\im v}-\im\bk{u-\im v|u-\im v}\Big)
\end{aligned}
\end{align}
We call \eqref{eq238} the \textbf{polarization identity}. \index{00@Polarization identity}
\end{rem}






\begin{df}
A function $\bk{\cdot|\cdot}:V\times V\rightarrow\Cbb$ is called a \textbf{Hermitian form} \index{00@Hermitian form} if it is linear on the first variable and satisfies $\ovl{\bk{u|v}}=\bk{v|u}$ for all $u,v\in V$. Then $\bk{\cdot|\cdot}$ is automatically antilinear on the second variable, i.e., a Hermitian form is automatically a sesquilinear form.
\end{df}

\begin{eg}
Let $A\in\Cbb^{N\times N}$ be a complex $N\times N$ matrix. Define $\bk{\cdot|\cdot}:\Cbb^N\times\Cbb^N\rightarrow\Cbb$ by $\bk{u|v}=\ovl{v^\tr}\cdot A\cdot u$ where $u,v$ are viewed as column vectors. Then $\bk{\cdot|\cdot}$ is a sesquilinear form on $\Cbb^N$. (It is an easy linear algebra exercise that every sesquilinear form on $\Cbb^N$ is of this form.) Moreover, $\bk{\cdot|\cdot}$ is a Hermitian form iff $A$ is a \textbf{Hermitian matrix}, i.e., $A=\ovl{A^\tr}$.
\end{eg}



The following is our first application of the polarization identity:

\begin{pp}\label{lb588}
Let $\bk{\cdot|\cdot}$ be a sesquilinear form on $V$. The following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\bk{\cdot|\cdot}$ is a Hermitian form.
\item For each $v\in V$ we have $\bk{v|v}\in\Rbb$.
\end{enumerate}
\end{pp}


\begin{proof}
(1)$\Rightarrow$(2): Obvious. (2)$\Rightarrow$(1): Let $(u|v)=\ovl{\bk{v|u}}$. Then $(\cdot|\cdot)$ is a sesquilinear form on $V$. Assuming (2), we have $\bk{v|v}=(v|v)$ for all $v\in \Rbb$. Therefore, by Prop. \ref{lb587}, we have $\bk{u|v}=(u|v)=\ovl{\bk{v|u}}$ for all $u,v$. So (1) is true.
\end{proof}

\begin{df}
A sesquilinear form $\bk{\cdot|\cdot}$ on $V$ is called \textbf{positive semi-definite} (or simply \textbf{positive}), \index{00@Positive sesquilinear form} if $\bk{v|v}\geq0$ for all $v\in V$. If a positive sesquilinear form $\bk{\cdot|\cdot}$ on $V$ is fixed, we define
\begin{align}
\Vert v\Vert=\sqrt{\bk{v|v}}\qquad\text{ for all }v\in V
\end{align} 
A vector $v\in V$ satisfying $\Vert v\Vert=1$ is called a \textbf{unit vector}. \index{00@Unit vector} 
\end{df}

By Prop. \ref{lb588}, a positive sesquilinear form is Hermitian.

\begin{df}
Let $\bk{\cdot|\cdot}$ be a positive sesquilinear form on $V$. By sesquilinearity, we clearly have $\bk{0|0}=0$. We say that $\bk{\cdot|\cdot}$ is an \textbf{inner product} \index{00@Inner product} if it is also \textbf{non-degenerate}, i.e., if the only $v\in V$ satisfying $\bk{v|v}=0$ is $v=0$. We call the pair $(V,\bk{\cdot|\cdot})$ (or simply call $V$) an \textbf{inner product space} or a \textbf{pre-Hilbert space} \index{00@Inner product space, also called pre-Hilbert space}.
\end{df}



\begin{eg}
$\Cbb^N$, equipped with the Euclidean inner product $\bk{u|v}=\ovl{v^\tr}u$, is an inner product space. 
\end{eg}

\begin{eg}
Let $X$ be a set. Then $l^2(X,\Cbb)$, together with the standard inner product
\begin{align}
\bk{f|g}=\sum_{x\in X}f(x)\ovl{g(x)}
\end{align}
(where the RHS converges by Thm. \ref{lb369}), is an inner product space.
\end{eg}

\begin{eg}
Let $-\infty<a<b<+\infty$. Then $C([a,b],\Cbb)$, together with the inner product
\begin{align}\label{eq241}
\bk{f|g}=\int_a^b fg^*=\int_a^b f(x)\ovl{g(x)}dx
\end{align}
is an inner product space.
\end{eg}





The gap between positive forms and inner products is not very big:
\begin{pp}\label{lb589}
Let $\bk{\cdot|\cdot}$ be a positive sesquilinear form on $V$. Let
\begin{align}
\scr N=\{v\in V:\Vert v\Vert=0\}
\end{align} 
Then $\scr N$ is a linear subspace of $V$. Moreover, there is an inner product $(\cdot|\cdot)$ on the quotient space $V/\scr N$ satisfying
\begin{align}
(u+\scr N|v+\scr N)=\bk{u|v}\qquad\text{ for all }u,v\in V \label{eq239}
\end{align}
\end{pp}

\begin{proof}
\eqref{eq239} suggests that if $\Vert v\Vert=0$, then $\bk{u|v}=(u+\scr N|v+\scr N)=(u+\scr N|\scr N)=\bk{u|0}=0$ for all $u\in V$. Motivated by this observation, let us prove
\begin{align}
\scr N=\{v\in V:(u|v)=0\text{ for all }u\in V \} \label{eq240}
\end{align}
Then the linearity of $\scr N$ follows immediately from \eqref{eq240}. Moreover, if we define a function $(\cdot|\cdot)$ on $V/\scr N$ by \eqref{eq239}, then it is well-defined: If $v+\scr N=v'+\scr N$, then $v-v'\in\scr N$. Thus, by \eqref{eq240}, we have $\bk{u|v-v'}=0$. So 
\begin{align*}
(u+\scr N|v+\scr N)=\bk{u|v}=\bk{u|v'}=(u+\scr N|v'+\scr N)
\end{align*}
A similar argument shows that if $u+\scr N=u'+\scr N$ then $(u+\scr N|v+\scr N)=(u'+\scr N|v+\scr N)$. This proves the well-definedness. It is easy to check that $(\cdot|\cdot)$ is sesquilinear and positive. If $(v+\scr N|v+\scr N)=0$, then $\bk{v|v}=0$ and hence $v\in \scr N$. This proves that $(\cdot|\cdot)$ is non-degenerate.


Let us prove \eqref{eq240}. Clearly $\Vert v\Vert=\sqrt{\bk{v|v}}=0$ holds whenever $\bk{u|v}=0$ for all $u$. Conversely, suppose that $\Vert v\Vert=0$, i.e., $\bk{v|v}=0$. Choose any $u\in V$. Then for each $t\in\Rbb$ we have
\begin{align*}
0\leq\bk{u+tv|u+tv}=\Vert u\Vert^2+2t\cdot \Real\bk{u|v}
\end{align*}
where the RHS is a linear function of $t$. Any linear function which is always $\geq0$ must be zero. So $\Real\bk{u|v}=0$. Similarly, $\Imag\bk{u|v}=-\Real\bk{\im u|v}=0$. So $\bk{u|v}=0$.
\end{proof}

\begin{eg}\label{lb599}
Let $-\infty<a<b<+\infty$ and $\scr V=\scr R[a,b]=\scr R([a,b],\Cbb)$ (the space of Riemann integrable complex functions on $[a,b]$). Then $\scr V$ has a positive sesquilinear form defined by \eqref{eq241}. Let $\scr N=\{f\in V:\bk{f|f}=0\}$. Then
\begin{align}\label{eq249}
\scr N=\{f\in\scr V:\text{ $f$ is zero outside a null subset of $[a,b]$}\}
\end{align}
%We will see in the future that $\scr N$ is the set of all $f:[a,b]\rightarrow\Cbb$ such that $\{x:f(x)\neq 0\}$ is a Lebesgue null set. 
Thus, $V=\scr V/\scr N$ is the set of all $f\in\scr R[a,b]$ such that $f,g\in\scr R[a,b]$ are viewed as the same element of $V$ iff $f=g$ almost everywhere (i.e. $\{x:f(x)\neq g(x)\}$ is null).
\end{eg}

Eq. \eqref{eq249} is almost obvious after we learn Lebesgue theory. Here, we give an elementary proof.

\begin{proof}
Let $g=|f|^2$. We want to show that $\int g=0$ iff $\Delta=\{x\in[a,b]:g(x)> 0\}$ is null. Assume that $\int g>0$. Then $f$ has a strictly positive lower Darboux sum (recall Thm. \ref{lb444}). This implies that there exist $c,d$ satisfying $a\leq c<d\leq b$ such that $(d-c)\cdot \inf_{\xi\in[c,d]}g(\xi)>0$. So $g>0$ on $[c,d]$. So $\Delta$ contains $[c,d]$, and hence is not null.

Assume that $\int g=0$. By Lebesgue's criterion \ref{lb411}, $g$ is continuous outside a null subset of $[a,b]$. Thus, it suffices to prove that $g(p)=0$ for any $p\in[a,b]$ at which $g$ is continuous. Suppose that $\eps:=g(p)>0$. Then by the continuity, there is an interval $I\subset [a,b]$ containing $p$ with $|I|>0$ such that $g>\eps/2$ on $I$. Thus $g\geq\frac\eps 2\cdot\chi_I$, and hence $0=\int g\geq\frac\eps 2\cdot |I|$, impossible.
\end{proof}

\begin{comment}
For each $\mu>0$, let us prove that $\Delta_\mu=\{x\in[-\pi,\pi]:g(x)>\mu\}$ is null. Then $\Delta=\bigcup_{n\in\Zbb_+}\Delta_{1/n}$ is null by Prop. \ref{lb412}. Choose any $\eps,\delta>0$. By the proof of (2)$\Rightarrow$(1) in Lebesgue's Thm. \ref{lb411}, there is a partition of $[-\pi,\pi]$ cutting it into subintervals $I_1,I_2,\dots,J_1,J_2,\dots$ such that $\diam(g(I_i))<\eps$ for all $i$, and that $\sum_j |J_j|<\delta$. For each $I_i$ we let $M_i=\sup_{\xi\in I_i}g(\xi)$ and $m_i=\inf_{\xi\in I_i}g(\xi_i)$. So $g\geq \sum_i m_i\cdot\chi_i$. Since $\int g=0$, we conclude $m_i=0$. Since $\diam (g(I_i))=M_i-m_i$, we get $M_i<\eps$. Now, we choose $\eps$ such that $\eps<\mu$. Then $M_i<\mu$, and hence $I_i\cap \Delta_\mu=\emptyset$ for all $i$. Therefore $\Delta_\mu\subset\bigcup_j |J_j|$. Thus, for each $\delta>0$, $\Delta_\mu$ can be covered a union of closed invertals with total length $<\delta$. So $\Delta_\mu$ is null.
\end{comment}


We close this section with an elementary but important fact. It says that linear maps are determined by their associated sesquilinear forms.


\begin{exe}
Suppose that $S,T:U\rightarrow V$ are linear maps of inner product spaces. 
\begin{enumerate}
\item Prove that $S=T$ iff $\bk{Su|v}=\bk{Tu|v}$ for all $u\in U,v\in V$. 
\item Assume that $U=V$. Prove that $S=T$ iff $\bk{Sv|v}=\bk{Tv|v}$ for all $v\in V$.
\end{enumerate}
\end{exe}





\subsection{Pythagorean and Gram-Schmidt}

Unless otherwise stated, we fix an inner product $\bk{\cdot|\cdot}$ on a complex vector space $V$ so that $V$ is an inner product space.

\begin{df}
A set $\fk S$ of vectors of $V$ are called \textbf{orthogonal} \index{00@orthogonal} if $\bk{u|v}=0$ for any distinct $u,v\in V$. An orthogonal set $\fk S$ is called \textbf{orthonormal} \index{00@orthonormal} if $\Vert v\Vert=1$ for all $v\in V$. 
\end{df}

\begin{rem}
We will also talk about an \textbf{orthogonal} resp.  \textbf{orthonormal family of vectors} $(e_i)_{i\in I}$. This means that $\bk{e_i|e_j}=0$ for any distinct $i,j\in I$ (resp. $\bk{e_i|e_j}=\delta_{i,j}$ for any $i,j\in I$). 
\end{rem}

In particular, two vectors $u,v\in V$ are called orthogonal when $\bk{u|v}=0$. A fundamental fact about orthogonal vectors is
\begin{pp}[\textbf{Pythagorean identity}]\index{00@Pythagorean identity}
Suppose that $u,v\in V$ are orthogonal. Then
\begin{align}\label{eq243}
\Vert u+v\Vert^2=\Vert u\Vert^2+\Vert v\Vert^2
\end{align}
In particular,
\begin{align}\label{eq245}
\Vert v\Vert\leq\Vert u+v\Vert
\end{align}
\end{pp}



\begin{proof}
$\Vert u+v\Vert^2=\bk{u+v|u+v}=\bk{u|u}+\bk{v|v}+2\Real \bk{u|v}=\bk{u|u}+\bk{v|v}$.
\end{proof}

Note that by applying \eqref{eq243} repeatedly, we see that if $v_1,\dots,v_n\in V$ are orthogonal, then
\begin{align}\label{eq244}
\Vert v_1+\cdots+v_n\Vert^2=\Vert v_1\Vert^2+\cdots+\Vert v_n\Vert^2
\end{align}

\begin{rem}
Suppose that $\fk S$ is an orthonormal set of vectors of $V$. Then $\fk S$ is clearly linearly independent. (If $e_1,\dots,e_n\in\fk S$ and $\sum_i a_ie_i=0$, then $a_j=\sum_i\bk{a_ie_i|e_j}=\bk{0|e_j}=0$.) Thus, by linear algebra, if $\fk S=\{e_1,\dots,e_n\}$ is finite, then one can find uniquely $a_1,\dots,a_n\in\Cbb$ and $u\in V$ such that $v=a_1e_1+\cdots+a_ne_n+u$ and that $u$ is orthogonal to $e_1,\dots,e_n$. The expressions of $a_1,\dots,a_n,u$ can be expressed explicitly:
\end{rem}


\begin{pp}[\textbf{Gram-Schmidt}]\index{00@Gram-Schmidt}
Let $e_1,\dots,e_n$ be orthonormal vectors in $V$. Let $v\in V$. Then
\begin{align}
v-\sum_{i=1}^n \bk{v|e_i}\cdot e_i
\end{align}
is orthogonal to $e_1,\dots,e_n$.
\end{pp}

\begin{proof}
This is a direct calculation and is left to the readers.
\end{proof}

\begin{rem}\label{lb593}
``Gram-Schmidt" usually refers to the following process. Let $v_1,\dots,v_n$ be a set of linearly independent vectors of $V$. Then there is an algorithm of finding an orthonormal basis of $U=\Span\{v_1,\dots,v_n\}$: Let $e_1=v_1/\Vert v_1\Vert$. Suppose that a set of orthonormal vectors $e_1,\dots,e_k$ in $U$ have been found. Then $e_{k+1}$ is defined by $\wtd v_{k+1}/\Vert\wtd v_{k+1}\Vert$ where $\wtd v_{k+1}=v_{k+1}-\sum_{i=1}^k \bk{v_{k+1}|e_i}\cdot e_i$.
\end{rem}



Combining Pythagorean with Gram-Schmidt, we have:
\begin{co}[\textbf{Bessel's inequality}]\label{lb634}
Let $(e_i)_{i\in I}$ be a family of orthonormal vectors of $V$. Then for each $v\in V$ we have
\begin{align}\label{eq242}
\sum_{i\in I}|\bk{v|e_i}|^2\leq \Vert v\Vert^2
\end{align}
In particular, the set $\{i\in I:\bk{v|e_i}\neq0\}$ is countable.
\end{co}

\begin{proof}
The LHS of \eqref{eq242} is $\lim_{J\in\fin(2^I)}\sum_{j\in J}|\bk{v|e_j}|^2$. Thus, it suffices to that for each $J\in\fin(2^I)$ we have $\sum_{j\in J}|\bk{v|e_j}|^2\leq \Vert v\Vert^2$. Let 
\begin{align*}
u_1=\sum_{j\in J}\bk{v|e_j}\cdot e_j\qquad u_2=v-u_1
\end{align*}
(Namely, $v=u_1+u_2$ is the orthogonal decomposition of $v$ with respect to $\Span\{e_j:j\in J\}$.) By Gram-Schmidt, we have $\bk{u_1|u_2}=0$. By Pythagorean, we have $\Vert u_1\Vert^2\leq\Vert v\Vert^2$. But Pythagorean \eqref{eq244} also implies
\begin{align*}
\Vert u_1\Vert^2=\sum_{j\in J}|\bk{v|e_j}|^2
\end{align*}
The last statement about countability follow from Pb. \ref{lb413}.
\end{proof}


\begin{thm}[\textbf{Cauchy-Schwarz inequality}]\index{00@Cauchy-Schwarz inequality}\label{lb590}
For each $u,v\in V$ we have
\begin{align}
|\bk{u|v}|\leq \Vert u\Vert\cdot\Vert v\Vert
\end{align}
\end{thm}

\begin{proof}
If $v=0$, then the inequality trivially holds. Assume $v\neq 0$. Then $\Vert v\Vert\neq0$. By dividing $v$ by $\Vert v\Vert$, we assume $\Vert v\Vert=1$. Then $\{v\}$ is a set of orthonormal vector. By Bessel's inequality, we have $|\bk{u|v}|\leq\Vert u\Vert$. 
\end{proof}

\begin{rem}\label{lb654}
In the general case that $\bk{\cdot|\cdot}$ is a positive sesquilinear form, the Cauchy-Schwarz inequality $|\bk{u|v}|^2\leq\bk{u|u}\cdot\bk{v|v}$ still holds.
\end{rem}

\begin{proof}
We use the notations in Prop. \ref{lb589}. Applying Thm. \ref{lb590} to $V/\scr N$, we have $|(u+\scr N|v+\scr N)|^2\leq(u+\scr N|u+\scr N)\cdot(v+\scr N|v+\scr N) $. This proves $|\bk{u|v}|^2\leq\bk{u|u}\cdot\bk{v|v}$.
\end{proof}

\begin{co}
$V$ is a normed vector space if we define $\Vert v\Vert=\sqrt{\bk{v|v}}$. 
\end{co}

\begin{proof}
By Cauchy-Schwarz, we have
\begin{align*}
\Vert u+v\Vert^2=\Vert u\Vert^2+\Vert v\Vert^2+2\Real\bk{u|v}\leq \Vert u\Vert^2+\Vert v\Vert^2+2\Vert u\Vert\cdot\Vert v\Vert=(\Vert u\Vert+\Vert v\Vert)^2
\end{align*}
This proves the triangle inequality. The other conditions are obvious.
\end{proof}

\begin{co}\label{lb594}
The map $\bk{\cdot|\cdot}:V\times V\rightarrow\Cbb$ is continuous if $V$ is equipped with the norm topology.
\end{co}

\begin{proof}
For each $u,u_0,v,v_0\in V$ such that $\Vert u-u_0\Vert\leq\eps,\Vert v-v_0\Vert\leq\eps$ where $0<\eps<1$, we have by Cauchy-Schwarz that
\begin{align*}
|\bk{u|v}-\bk{u_0|v_0}|\leq|\bk{u-u_0|v}|+|\bk{u_0|v-v_0}|\leq \eps (\Vert v_0\Vert+1)+\eps\Vert u_0\Vert
\end{align*}
\end{proof}



\begin{rem}
Since $V$ is a normed vector space, $V$ is also a metric space with $d(u,v)=\Vert u-v\Vert=\sqrt{\bk{u-v|u-v}}$. Now, the polarisation identity (Prop. \ref{lb587}) says that for inner product spaces, \uwave{norms determine inner produts}, and hence \uwave{metrics determine inner products}. Therefore, if $W$ is an inner product space, and if $T:V\rightarrow W$ is a linear isometry, then
\begin{align*}
\bk{Tu|Tv}=\bk{u|v}
\end{align*}
for all $u,v\in V$. In particular, if $T$ is an isomorphism of normed vector sapces (i.e., $T$ is a linear surjective isometry, cf. Def. \ref{lb302}), then $T$ is an equivalence of inner product spaces. In this case, we say that $T$ is a \textbf{unitary map}, \index{00@Unitary maps} and say that $V,W$ are \textbf{isomorphic inner product spaces} (or that $V,W$ are \textbf{unitarily equivalent}). \index{00@Unitarily equivalent} \hfill\qedsymbol
\end{rem}



\subsection{Orthogonal decompositions}

Fix an inner product space $V$. In the last section, we used Gram-Schmidt process and Pythagorean inequality \eqref{eq245} to derive many useful inequalities. In this section, we will have a deeper understanding of the geometry behind Gram-Schmidt process and orthogonal projections.


\begin{df}\label{lb615}
Let $U$ be a linear subspace of $V$. Let $v\in V$. An \textbf{orthogonal decomposition} \index{00@Orthogonal decomposition and orthonal projection} of $v$ with respect to $U$ is an expression of the form $v=u+w$ where $u\in U$ and $w\perp U$ (i.e. $w$ is orthogonal to every vector of $U$). Orthogonal decompositions of $v$ are unique if exist. We call $u$ the \textbf{orthogonal projection} of $v$ onto $U$.
\end{df}

\begin{proof}[Proof of uniqueness]
Suppose that $v=u'+w'$ is another orthogonal decomposition. Then $u-u'$ equals $w'-w$. Let $\xi=u-u'$. Then $\xi\in U$ and $\xi\perp U$. So $\bk{\xi|\xi}=0$, and hence $\xi=0$. So $u=u'$ and $w=w'$.
\end{proof}

\begin{eg}
Let $e_1,\dots,e_n$ be orthonormal vectors of $V$. Let $U=\Span\{e_1,\dots,e_n\}$. Choose any $v\in V$. Then by Gram-Schmidt, $v=u+w$ is the orthogonal decomposition if we let $u=\sum_{i=1}^n\bk{v|e_i}e_i$ and $w=v-u$.
\end{eg}

The inequalities in the last section relies on the Pythagorean inequality $\Vert u\Vert\leq \Vert v\Vert$ for an orthogonal decomposition $v=u+w$. In this section, we need an optimization property about orthogonal decompositions:

\begin{pp}\label{lb591}
Let $U$ be a linear subspace of $V$. Suppose that $v\in V$ has orthogonal decomposition $v=u+w$ with respect to $U$. Then
\begin{align}
\Vert v-u\Vert=\inf_{\xi\in U}\Vert v-\xi\Vert
\end{align}
\end{pp}

\begin{proof}
Clearly ``$\geq$" holds. Choose any $\xi\in U$. Then $v-\xi=v-u+u-\xi=w+(u-\xi)$. Since $u-\xi\in U$, we have $w\perp u-\xi$. Thus, by Pythagorean, we have $\Vert w\Vert\leq\Vert v-\xi\Vert$.
\end{proof}

\begin{co}\label{lb592}
Let $e_1,\dots,e_n$ be orthonormal vectors of $V$. For each $v\in V$ and $\lambda_1,\dots,\lambda_n\in\Cbb$ we have
\begin{align}
\Vert v\Vert^2-\sum_{i=1}^n|\bk{v|e_i}|^2=\Big\Vert v-\sum_{i=1}^n\bk{v|e_i}e_i\Big\Vert^2\leq \Big\Vert v-\sum_{i=1}^n \lambda_ie_i\Big\Vert^2
\end{align}
\end{co}

\begin{proof}
By Gram-Schmidt, we have orthogonal decomposition $v=u+w$ where $w=\sum_i\bk{v|e_i}e_i$. The Pythagorean identity $\Vert v\Vert^2-\Vert u\Vert^2=\Vert w\Vert^2$ proves the first equality. Prop. \ref{lb591} proves the ``$\leq$".
\end{proof}


We now give several applications of Cor. \ref{lb592}.


\begin{df}\label{lb611}
A set $\fk S$ (or a family $(e_i)_{i\in I}$) of orthonormal vectors of $V$ is called an \textbf{orthonormal basis} \index{00@Orthonormal basis} of $V$ if it spans a dense subspace of $V$.  
\end{df}

\begin{eg}
If $X$ is a set, by Lem. \ref{lb528}, $l^2(X)$ has an orthonormal basis $(\chi_{\{x\}})_{x\in X}$.
\end{eg}





\begin{eg}\label{lb613}
If $V$ is separable, then $V$ has a countable orthonormal basis.
\end{eg}

\begin{proof}
Let $\{v_1,v_2,\dots\}$ be a dense subset of $V$ where $v_1\neq 0$. Then by Gram-Schmidt (Rem. \ref{lb593}), we can find $e_1,e_2,\dots\in V$ such that the set $\{e_1,e_2,\dots\}$ is orthnormal (after removing the duplicated terms), and that $\Span\{v_1,\dots,v_n\}=\Span\{e_1,\dots,e_n\}$ for each $n$. Then $\{e_1,e_2,\dots\}$ clearly spans a dense subspace of $V$.
\end{proof}

We remark that there are non-separable and non-complete inner product spaces that do not have orthonormal bases. See \cite{Gud74}.


\begin{thm}\label{lb595}
Suppose that $(e_i)_{i\in I}$ is an orthonormal basis of $V$. Then for each $v\in V$, the RHS of the following converges (under the norm of $V$) to the LHS:
\begin{align}
v=\sum_{i\in I}\bk{v|e_i}\cdot e_i
\end{align}
\end{thm}

\begin{proof}
Note that for $J\in\fin(2^I)$, the expression
\begin{align*}
\Big\Vert v-\sum_{j\in J}\bk{v|e_j}e_j\Big\Vert^2=\Vert v\Vert^2-\sum_{j\in J}|\bk{v|e_j}|^2
\end{align*}
decreases when $J$ increases. Thus, it suffices to prove that the $\inf_{J\in \fin(2^I)}$ of this expression is $0$. This follows immediately from Cor. \ref{lb592} and the fact that we can find $J$ and $(\lambda_j)_{j\in J}$ in $\Cbb$ such that $\Vert v-\sum_{j\in J}\lambda_je_j\Vert$ is small enough.
\end{proof}

\begin{co}[\textbf{Parseval's identity}]\index{00@Parseval's identity}\label{lb601}
Suppose that $(e_i)_{i\in I}$ is an orthonormal basis of $V$. Then for each $u,v\in V$ we have
\begin{align}\label{eq246}
\bk{u|v}=\sum_{i\in I}\bk{u|e_i}\cdot\bk{e_i|v}
\end{align}
In particular,
\begin{align}
\Vert v\Vert^2=\sum_{i\in I}|\bk{v|e_i}|^2
\end{align}
\end{co}

\begin{proof}
By Thm. \ref{lb595}, $u=\lim_{J\in\fin(2^I)}u_J$ where $u_J=\sum_{j\in J}\bk{u|e_j}\cdot e_j$. By the continuity of $\bk{\cdot|\cdot}:V\times V\rightarrow\Cbb$ (Cor. \ref{lb594}), we have
\begin{align*}
\bk{u|v}=\lim_{J\in\fin(2^I)}\bk{u_J|v}=\lim_{J\in\fin(2^I)}\sum_{j\in J}\bk{u|e_j}\cdot\bk{e_j|v}=\sum_{i\in I}\bk{u|e_i}\cdot\bk{e_i|v}
\end{align*}
\end{proof}



\begin{rem}
In Hilbert's original definition, an orthonormal basis $(e_i)_{i\in I}$ (called by Hilbert a complete orthogonal system of functions) is a set of orthonormal vectors satisfying Parseval's identity \eqref{eq246} for all $u,v\in V$. Hilbert did not have the topological understanding of orthonormal basis as in Def. \ref{lb611} (i.e., a set of orthonormal vectors spanning a \textit{dense} subspace of $V$). See \cite[Sec. 8]{BK84}.
\end{rem}



\begin{co}\label{lb602}
Suppose that $(e_x)_{x\in X}$ is an orthonormal basis of $V$. Then there is a linear isometry
\begin{gather}\label{eq247}
\Phi:V\rightarrow l^2(X)\qquad v\mapsto \big(\bk{v|e_x})_{x\in X}
\end{gather} 
whose range is dense in $l^2(X)$.
\end{co}

Since $l^2(X)$ is complete (Thm. \ref{lb596}), it follows that $\Phi$ gives a Banach space completion of $V$, cf. Def. \ref{lb597}.


\begin{proof}
Parseval's identity shows that $(\bk{v|e_x})_{x\in X}$ has finite $l^2$-norm $\Vert v\Vert$. So the map $\Phi$ defined by \eqref{eq247} is clearly a linear isometry.
\end{proof}



\subsection{Application to Fourier series}

In this section, we apply the results about inner product spaces to the study of Fourier series.

Let $\scr V=\scr R[-\pi,\pi]=\scr R([-\pi,\pi],\Cbb)$, the vector space of (strongly) Riemann integrable complex valued functions on $[-\pi,\pi]$. For each $f\in\scr V$ and $n\in\Zbb$, define its $n$-th \textbf{Fourier coefficient} to be
\begin{align}\label{eq248}
\wht f(n)=\frac 1{2\pi}\int_{-\pi}^\pi f(x)e^{-\im nx}dx=\frac 1{2\pi}\int_{-\pi}^\pi fe_{-n}
\end{align}
where $e_n(x)=e^{\im nx}$. Then the \textbf{Fourier series} of $f$ is
\begin{align}\label{eq293}
\sum_{n\in\Zbb}\wht f(n)e_n
\end{align}
We view $\wht f$ as a function on $\Zbb$.

In Fourier analysis, one asks whether the Fourier series of $f$ converges to $f$, and if so, in which sense does it converge? We have seen in Subsec. \ref{lb598} that \eqref{eq293} might not converge uniformly to $f$. We have also mentioned there that \eqref{eq293} might be divergent at many points of $[-\pi,\pi]$, although in many cases it converges pointwise to $f$ (cf. Pb. \ref{lb447}). In this section, we will see that \eqref{eq293} converges to $f$ under the $L^2$-norm. Another important result of this section is the classification of all $f$ whose Fourier modes are all $0$. %Historically, this is a main motivation for Cantor to study set theory.



Let $V$ be the vector space of all $f\in\scr R[-\pi,\pi]$, where $f,g\in\scr R[-\pi,\pi]$ are equal elements in $V$ iff $\{x\in[-\pi,\pi]:f(x)\neq g(x)\}$ is null. By Exp. \ref{lb599}, $V$ is an inner product space whose inner product $\bk{\cdot|\cdot}$ is defined by
\begin{align}\label{eq250}
\bk{f|g}=\frac{1}{2\pi}\int_{-\pi}^\pi fg^*=\frac{1}{2\pi}\int_{-\pi}^\pi f(x)\ovl{g(x)}
\end{align}
It follows that
\begin{align}
\wht f(n)=\bk{f|e_n}
\end{align}

\begin{pp}\label{lb600}
$\{e_n:n\in\Zbb\}$ is an orthonormal basis of $V$.
\end{pp}

\begin{proof}
Clearly $\{e_n:n\in\Zbb\}$ is an orthonormal set of vectors. Choose any $f\in\scr R[-\pi,\pi]$. By Stone-Weierstrass, there is a sequence $(f_k)_{k\in\Zbb_+}$ of elements in $\Span\fk S$ converging uniformly on $[-\pi,\pi]$ to $f$ (cf. Exp. \ref{lb443}). Therefore $\lim_{k\rightarrow\infty}\int|f-f_k|^2=0$ by Cor. \ref{lb380}. This proves that $\Span\fk S$ is dense in $V$.
\end{proof}



\begin{co}
Let $f\in\scr R[-\pi,\pi]$. Then the following are equivalent
\begin{enumerate}[label=(\arabic*)]
\item Each Fourier coefficient $\wht f(n)$ is $0$.
\item $f$ is zero outside a null set.
\end{enumerate}
\end{co}

\begin{proof}
This is immediate from Prop. \ref{lb600} and the obvious fact that a vector in an inner product space (equipped with an orthonormal basis) is zero iff its inner product with any element in the orthonormal basis (or more generally, any element in a densely-spanning subset) is zero.
\end{proof}


\begin{thm}
Let $f,g\in\scr R[a,b]$. Then we have \textbf{Parseval's identity}
\begin{align}
\frac 1{2\pi}\int_{-\pi}^\pi fg^*=\sum_{n=-\infty}^{+\infty} \wht f(n)\ovl{\wht g(n)}
\end{align}
where the RHS converges absolutely. In particular,
\begin{align}
\frac 1{2\pi}\int_{-\pi}^\pi|f|^2=\sum_{n=-\infty}^{+\infty}|\wht f(n)|^2
\end{align}
Moreover, we have
\begin{align}
\lim_{m,n\rightarrow+\infty} \int_{-\pi}^\pi \Big|f-\sum_{k=-m}^n \wht f(k)e_k\Big|^2=0
\end{align}
\end{thm}

\begin{proof}
Immediate from Cor. \ref{lb601} and Thm. \ref{lb595}.
\end{proof}

The following result was mentioned in Subsec. \ref{lb370}.

\begin{co}\label{lb603}
The space $C[-\pi,\pi]$, under the inner product \eqref{eq250}, has a Banach space completion
\begin{gather}
C[-\pi,\pi]\rightarrow l^2(\Zbb)\qquad f\mapsto \wht f
\end{gather}
The same is true if $C[-\pi,\pi]$ is replaced by $V$.
\end{co}
\begin{proof}
Prop. \ref{lb600} implies that $\{e_n:n\in\Zbb\}$ is also an orthonormal basis of $C[-\pi,\pi]$. Thus, the corollary follows from Cor. \ref{lb602}.
\end{proof}










\subsection{Problems and supplementary material}


\subsubsection{$\star$ The Sobolev space $H^s(\Sbb^1)$}


For each $s\geq0$, and for each $\varphi:\Zbb\rightarrow\Cbb$, let 
\begin{align}
\Vert \varphi\Vert_{h^s}=\sum_{n\in\Zbb}(1+n^2)^s|\varphi(n)|^2
\end{align}
It is clear that $\Vert \varphi\Vert_{h^0}=\Vert \varphi\Vert_2$, and that
\begin{align*}
s\leq t\qquad\Longrightarrow\qquad \Vert \varphi\Vert_{h^s}\leq \Vert \varphi\Vert_{h^t}
\end{align*} 
Define 
\begin{gather}
h^s(\Zbb)=\{\varphi\in \Cbb^\Zbb:\Vert \varphi\Vert_{h^s}<+\infty\}
\end{gather}
which is clearly a subset of $l^2(\Zbb)$. Clearly $h^s(\Zbb)\supset h^t(\Zbb)$ if $s\leq t$.

\begin{prob}
Prove that $h^s(\Zbb)$ is a linear subspace of $l^2(\Zbb)$, that $h^s(\Zbb)$ has a well-defined inner product described by
\begin{align}
\bk{\varphi|\psi}_{h^s}=\sum_{n\in\Zbb}(1+n^2)^s\varphi(n)\ovl{\psi(n)}
\end{align}
and that $h^s(\Zbb)$ is complete under this inner product.
\end{prob}


Let $C(\Sbb^1)$ be the set of complex continuous functions on $\Sbb^1$, equivalently, continuous $2\pi$-periodic functions on $\Rbb$. More generally, for each $n\in\Nbb\cup\{\infty\}$, let
\begin{align}
C^n(\Sbb^1)=\{2\pi\text{-periodic }f\in C^n(\Rbb)\}
\end{align}
Equip $C(\Sbb^1)$ with the inner product $\bk{f|g}=\frac 1{2\pi}\int_{-\pi}^\pi fg^*$. The norm determined by this inner product is called the $L^2$-norm. By Cor. \ref{lb603}, we have a Hilbert space completion
\begin{gather}
\Phi:C(\Sbb^1)\rightarrow l^2(\Zbb)\qquad f\mapsto \wht f
\end{gather}
Recall that $e_n(x)=e^{\im nx}$.

\begin{prob}\label{lb604}
Prove that $l^1(\Zbb)\subset\Phi(C(\Sbb^1))$, and that the linear injection
\begin{align}
\Phi^{-1}:l^1(\Zbb)\rightarrow C(\Sbb^1)
\end{align}
can be described by
\begin{align}
\Phi^{-1}(\varphi)=\sum_{n\in\Zbb} \varphi(n)e_n
\end{align}
where the RHS converges under the $l^\infty(\Sbb^1)$-norm (and hence under the $L^2$-norm).
\end{prob}

\begin{prob}\label{lb605}
Let $s>\frac 12$. Prove that $h^s(\Zbb)\subset l^1(\Zbb)$.
\end{prob}

\begin{proof}[Hint]
Use Cauchy-Schwarz or H\"older's inequality.
\end{proof}

\begin{df}
Let $s>\frac 12$. Define
\begin{align}
H^s(\Sbb^1)\xlongequal{\mathrm{def}}\Phi^{-1}(h^s(\Zbb))
\end{align}
According to Pb. \ref{lb604} and \ref{lb605}, we have 
\begin{align}\label{eq251}
H^s(\Sbb^1)\subset\Phi^{-1}(l^1(\Zbb)) \subset C(\Sbb^1)
\end{align}
and $\Phi$ restricts to a linear bijection
\begin{align}
\Phi: H^s(\Sbb^1)\xlongrightarrow{\simeq} h^s(\Zbb)
\end{align}
whose inverse sends
\begin{align*}
\varphi\mapsto \sum_{n\in\Zbb}\varphi(n)e_n
\end{align*}
where the RHS converges uniformly. Define inner product $\bk{\cdot|\cdot}_{H^s}$ on $H^s(\Sbb^1)$ to be the pullback of  $h^s$, i.e.
\begin{align}
\bk{f|g}_{H^s}=\bk{\wht f|\wht g}_{h^s}=\sum_{n=-\infty}^{+\infty} (1+n^2)^s\wht f(n)\ovl{\wht g(n)}
\end{align}
Then $H^s(\Sbb^1)$ is a Hilbert space since $h^s(\Zbb)$ is so. We call $H^s(\Sbb^1)$ a \textbf{Sobolev space} \index{00@Sobolev space $H^s(\Sbb^1)$} of $\Sbb^1$. Thus $\Phi$ is a unitary map.
\end{df}


\begin{prob}\label{lb607}
Let $k\in\Zbb_+$. Prove that $C^k(\Sbb^1)\subset H^k(\Sbb^1)$. Prove for each $f\in C^{2k}(\Sbb^1),g\in H^k(\Sbb^1)$ that
\begin{align}
\bk{f|g}_{H^k}=\frac 1{2\pi}\int_{-\pi}^\pi (1-\partial^2)^kf\cdot g^* \label{eq252}
\end{align}
where $\partial f=f'$.
\end{prob}


\begin{proof}[Hint]
For each $f\in C^k(\Sbb^1)$, prove by induction on $k$ that
\begin{align}
\wht{f^{(k)}}(n)=(\im n)^k\wht f(n)
\end{align}
Apply Parserval's identity to $\int |f^{(k)}|^2$ and to the RHS of \eqref{eq252}.
\end{proof}

As an application, we obtain a useful criterion for the uniform convergence of Fourier series:

\begin{co}
If $f\in C^1(\Sbb^1)$, then $\sum_{n\in\Zbb}\wht f(n)e_n$ converges uniformly to $f$.
\end{co}

\begin{proof}
By Pb. \ref{lb607} and \eqref{eq251}, we have $f\in C^1(\Sbb^1)\subset H^1(\Sbb^1)\subset \Phi^{-1}(l^1(\Zbb))$. Since $\wht f=\Phi(f)$ and hence $f=\Phi^{-1}(\wht f)$, by Pb. \ref{lb604} (the description of $\Phi^{-1}$ on $l^1(\Zbb)$), $\sum_{n\in\Zbb}\wht f(n)e_n$ converges uniformly to $f$. 
\end{proof}

\begin{thm}
Let $k\in\Zbb_+$. Then $H^k(\Sbb^1)$ is the Hilbert space completion of $C^\infty(\Sbb^1)$ under the inner product defined by \eqref{eq252} for all $f,g\in C^\infty(\Sbb^1)$.
\end{thm}

\begin{proof}
Equip $C^\infty(\Sbb^1)$ with the inner product defined by \eqref{eq252}. We know that
\begin{align}
\Phi:C^\infty(\Sbb^1)\rightarrow h^k(\Zbb)\qquad f\mapsto \wht f  \label{eq253}
\end{align}
is a linear isometry. Since $\Phi(e_n)=\delta_{\{n\}}$, the range of $\Phi$ contains $\Span\{\chi_{\{n\}}:n\in\Zbb\}$. From this it follows easily that the range is dense in $h^k(\Zbb)$. Thus, $\Phi$ gives a completion of $C^\infty(\Sbb^1)$. This is equivalent to saying that the inclusion map $C^\infty(\Sbb^1)\hookrightarrow H^k(\Sbb^1)$ is a completion of $C^\infty(\Sbb^1)$ (since \eqref{eq253} extends to a unitary map $\Phi:H^k(\Sbb^1)\rightarrow h^k(\Zbb)$).
\end{proof}



\begin{prob}\label{lb606}
Suppose that $s>\frac 32$. Prove that the map of derivative $\partial:C^1(\Sbb^1)\rightarrow C(\Sbb^1),f\mapsto f'$ restricts to
\begin{align}
\partial: H^s(\Sbb^1)\rightarrow H^{s-1}(\Sbb^1)
\end{align}
(Namely, each $f\in H^s(\Sbb^1)$ is differentiable, and has derivative in $H^{s-1}(\Sbb^1)$.) Prove for each $f\in H^s(\Sbb^1)$ that
\begin{align}
\Phi(f')(n)=\im n\wht f(n)
\end{align}
\end{prob}

\begin{proof}[Hint]
By Pb. \ref{lb604}, we have uniform convergence $f=\sum_{n\in\Zbb}\wht f(n)e_n$ where $\wht f\in h^s(\Zbb)$. Show that $\sum_{n\in\Zbb}\im n\wht f(n)e_n$ converges uniformly. Then use Thm. \ref{lb336}.
\end{proof}

\begin{thm}[\textbf{Sobolev embedding}]\label{lb608}
If $s>\frac 12$, then $H^s(\Sbb^1)\subset C^{\lceil s-\frac 32\rceil}(\Sbb^1)$ where $\lceil s-\frac 32\rceil$ is the smallest integer $\geq s-\frac 32$.
\end{thm}

\begin{proof}
Choose $f\in H^s(\Sbb^1)$. By \eqref{eq251} we have $f\in C(\Sbb^1)$. Thus the theorem is proved when $\frac 12<s\leq \frac 32$. Suppose that $s>\frac 32$. Let $k=\lceil s-\frac 32\rceil$. Then $\frac 12<s-k\leq \frac 32$. By Pb. \ref{lb606}, we have $f'\in H^{s-1}(\Sbb^1),f''\in H^{s-2}(\Sbb^1),\dots$, and hence $f^{(k)}\in H^{s-k}(\Sbb^1)\subset C(\Sbb^1)$. This proves $f\in C^k(\Sbb^1)$.
\end{proof}


\begin{co}
We have $C^\infty(\Sbb^1)=\bigcap_{k\in\Zbb_+}H^k(\Sbb^1)=\bigcap_{s>\frac 12}H^s(\Sbb^1)$.
\end{co}
\begin{proof}
By Pb. \ref{lb607}, Thm. \ref{lb608}, and the fact that $H^s(\Sbb^1)$ decreases as $s$ increases.
\end{proof}


\newpage




\section{Hilbert spaces}\label{lb665}

\begin{df}\label{lb609}
An inner product space $\mc H$ is called a \textbf{Hilbert space} \index{00@Hilbert space} if it is complete under the norm defined by $\Vert\xi\Vert=\sqrt{\bk{\xi|\xi}}$.
\end{df}

\begin{rem}
Let $\mc H$ be a Hilbert space. Since a subset of a complete metric space is complete iff it is closed (Prop. \ref{lb86}), the phrases ``closed linear subspaces of $\mc H$" and ``Hilbert subspaces of $\mc H$" are synonymous.
\end{rem}

\begin{eg}
If $\mc H$ is a finite-dimensional inner product space, then $\mc H$ is a Hilbert space. 
\end{eg}

\begin{proof}
Since $\mc H$ is spanned by finitely many vectors, by Gram-Schmidt process, $\mc H$ has an orthonormal basis $(e_x)_{x\in X}$ where $X$ is a finite set. The canonical linear isometry $\Phi:\mc H\rightarrow l^2(X)$ in Cor. \ref{lb602} must be surjective. So $\mc H\simeq l^2(X)$. Hence $\mc H$ is complete.
\end{proof}




\begin{thm}
Let $V$ be an inner product space. Then $V$ has a \textbf{Hilbert space completion}, \index{00@Hilbert space completion} i.e., a Hilbert space $\mc H$ and a linear isometry $\Phi:V\rightarrow\mc H$ with dense range.

Moreover, Hilbert space completions are unique up to unitary equivalences: If $\Psi:V\rightarrow\mc K$ is another Hilbert space completion, then there is a unitary $\Gamma:\mc H\rightarrow\mc K$ such that the following diagram commutes:
\begin{equation}
\begin{tikzcd}[column sep=small]
                     & V \arrow[ld,"\Phi"'] \arrow[rd,"\Psi"] &   \\
\mc H \arrow[rr, "\Gamma","\simeq"'] &                         & \mc K
\end{tikzcd}
\end{equation}
\end{thm}

When no confusion arises, we identify $V$ with $\Phi(V)$ so that $V$ can be viewed as a dense inner product subspace of $\mc H$.

\begin{proof}
Since $V$ is a normed vector space, by Thm. \ref{lb312}, we have a Banach space $\mc H$ (with norm $\Vert\cdot\Vert_{\mc H}$) and a linear isometry $\Phi:V\rightarrow\mc H$ with dense range. We assume WLOG that $V$ is a normed vector subspace of $\mc H$ so that the norm $\Vert\cdot\Vert_{\mc H}$ restricts to that of $V$. Define a function $\bk{\cdot|\cdot}_{\mc H}:\mc H\times\mc H\rightarrow\Cbb$ using the polarization identity \eqref{eq238}, i.e., for each $\xi,\eta\in\mc H$ we set
\begin{align*}
\bk{\xi|\eta}_{\mc H}=\frac 14\sum_{t=0,\frac \pi 2,\pi,\frac{3\pi}2}~ \Vert\xi+e^{\im t}\eta\Vert_{\mc H}^2\cdot e^{\im t}
\end{align*}
Then $\bk{\cdot|\cdot}_{\mc H}$ restricts to the inner product $\bk{\cdot|\cdot}$ of $V$. Moreover, the function $\bk{\cdot|\cdot}_{\mc H}$ is continuous (with respect to the norm $\Vert\cdot\Vert_{\mc H}$), and $V\times V$ is a dense subset of $\mc H\times\mc H$. Therefore, as in the proof of Thm. \ref{lb312}, we can use the fact that $\bk{\cdot|\cdot}$ is an inner product on $V$ to prove that $\bk{\cdot|\cdot}_{\mc H}$ is a positive sesquilinear form on $\mc H$, and we can use the fact that the norm of $V$ is defined by the inner product of $V$ to prove that $\Vert\xi\Vert_{\mc H}^2=\bk{\xi|\xi}_{\mc H}$. Therefore, $\bk{\cdot|\cdot}_{\mc H}$ is non-degenerate, i.e., an inner product, and this inner product defines the complete norm $\Vert\cdot\Vert_{\mc H}$. It follows that $\mc H$ is a Hilbert space, and hence a Hilbert space completion of $V$.

The uniqueness of Hilbert space completions (up to unitary equivalences) follows directly from that of Banach space completions, cf. Thm. \ref{lb312}.
\end{proof}








\subsection{Introduction: completeness, the most familiar stranger}\label{lb671}

\begin{comment}
\begin{displayquote}
\small Young man, in mathematics you don't understand things. You just get used to them.

\hfill ---- John von Neumann
\end{displayquote}
\end{comment}



\begin{displayquote}
\small What is familiar is what we are used to; and what we are used to is most difficult to ``know"---that is, to see as a problem; that is, to see as strange, as distant, as ``outside us".

\hfill ---- Friedrich Nietzsche (cf. \cite[Sec. 355]{Nie})
\end{displayquote}

We introduced completeness at the beginning of last semester and applied it to function spaces. It has allowed us to give a unified understanding of many analytic problems, especially those related to uniform convergence: the uniform convergence of series of functions, the commutativity of two limit processes and its relationship with uniform convergence, etc.. In fact, the vast majority of Banach spaces we considered last semester were defined by the $l^\infty$-norm.


From this perspective, it is perfectly natural to consider completeness for inner product spaces. However, we refuse to take lightly the consideration of completeness as natural for the following reasons:
\begin{itemize}
\item The notion of completeness was originally applied only to $\Rbb$. Historically, however, the idea of applying completeness to function spaces was not entirely inspired by the study of uniform convergence, since uniform convergence is not too far from pointwise convergence.\footnote{As a matter of fact, most results learned in the last semester can be formulated and proved without introducing completeness to function spaces, as implied by history and by many analysis textbooks.} {The apparently successful application of $l^\infty$-completeness to the problems of uniform convergence does not justify the \textit{a priori} value of completeness in the study of other norms} (such as the $L^2$-norm). \footnote{That $l^\infty$ and $L^2$ are both called ``norms" is a \textit{result}, not a starting point, of the observation in history that different types of problems can be treated in a similar fashion.} 

%\item It is therefore important to know the math problems that fundamentally rely on (and gave birth to) the concept of completeness of function spaces. They are not about uniform convergence, but about $L^2$-convergence (and more generally, $L^p$-convergence).  

%Applying completeness to other norms such as $L^2$ or $L^p$ norms is an unusual and significant historical event. What are the benefits of studying completeness under the $L^2$-norm?
%\item  Therefore, we must ask: 
\item For inner product spaces, some analytic properties are equivalent to completeness. The most important one is the weak (or weak-*) compactness of the unit ball.\footnote{We have learned that $l^2(X)$ satisfies this property due to $l^2(X)\simeq l^2(X)^*$ (cf. Thm. \ref{lb527}) and Banach-Alaoglu, or more directly, by Pb. \ref{lb531}.} The main reason that the Hilbert space $l^2(\Zbb)$ was introduced in history by Hilbert is due to this compactness rather than completeness. 
\end{itemize}


As I will explain below, the Hilbert space techniques based on completeness were developed much later: F. Riesz made the first systematic use of completeness with the help of his novel (at his time) idea of operator norms.

\subsubsection{The late-coming concept of completeness}


The modern definition of Hilbert spaces as complete inner product spaces was due to von Neumann \cite{vNeu30} in late 1920s, many years after the introduction of $l^2(\Zbb)$ by Hilbert and Schmidt (same person as the Schmidt in the Gram-Schmidt process!\footnote{Thus, the discovery of Gram-Schmidt process is not only for the purpose of solving linear algebra problems. It has a deep background in analysis.}) to the study of integral equations in 1900s. We will see in this chapter that all separable Hilbert spaces are isomorphic to $\Cbb^N$ or $l^2(\Zbb)$, and that all Hilbert spaces are isomorphic to $l^2(X)$. Therefore, one can equivalently define a Hilbert space to be an inner product space isomorphic to $l^2(X)$ for some set $X$. This is indeed closer to how people originally understood Hilbert spaces than the modern definition.

As opposed to 1900s, by the time von Neumann gave Def. \ref{lb609}, the importance of completeness in function spaces was fully recognized. One of the most important contributors to this change of ideas is F. Riesz, who applied the completeness of function spaces to the study of the following problems in 1910s:
\begin{itemize}
\item[(a)] The completeness of $\fk L\big(l^2(\Zbb)\big)$ (relying on the completeness of $l^2(\Zbb)$) makes possible the technique of continuous functional calculus and its application to the problem of diagonalizing self-adjoint operators on $l^2(\Zbb)$. See Sec. \ref{lb610} and \ref{lb551}.
\item[(b)] The study of eigenvalue problems of compact operators between Banach spaces such as $C(X),l^p(X),L^p(X)$, generalizing Hilbert-Schmidt's results on the Hilbert space $l^2(\Zbb)$.
\end{itemize}
Of course, the second problem is a mixture of completeness and compactness. We will not study the second problem in this course. (But see Sec. \ref{lb677} for a discussion of how the notion of compact operators evolved from Hilbert's original definition of completely continuous forms during Riesz's study of problem (b).) However, to show you at least one application of completeness to $l^2$ and $L^2$ norms, we will discuss the first problem in the middle of this semester. (Surprisingly, as we will see, the first problem is also related to the moment problems.)

\subsubsection{Scalar-valued functions vs. vector-valued functions, compactness vs. completeness, Hilbert vs. Riesz}\label{lb621}


It is worth noting that the idea of completeness in function spaces (especially under the $L^2$ and $l^2$ norms) is related to the recognition of the importance of studying linear maps between function spaces. Given inner product spaces $U,V$, a bounded linear map $T:U\rightarrow V$ can also be viewed as a continuous sesquilinear map
\begin{align}\label{eq255}
\omega_T:U\times V\rightarrow\Cbb\qquad (u,v)\mapsto \bk{Tu|v}
\end{align}
%(This equivalence of $T$ and $\omega_T$ is close in spirit to the equivalence $C(X,C(Y))\simeq C(X\times Y)$ in Thm. \ref{lb274}.) 
In the special case that $U=V$, $T$ can be viewed as a continuous \textbf{quadratic form}
\begin{align}\label{eq254}
V\times V\rightarrow \Cbb\qquad v\mapsto \bk{Tv|v}
\end{align}
(Note that \eqref{eq254} completely determines \eqref{eq255} by the polarization identity (Prop. \ref{lb587}), and hence determines $T$.) This viewpoint, insisting on the study of scalar-valued functions, was hold by people before Riesz, especially by Hilbert (and his students). 

In Hilbert's scalar-valued function viewpoint, completeness plays a very marginal role (with very few exceptions, see Rem. \ref{lb639}), and emphasis was put on the compactness. To empathize with this phenomenon, compare it with the equivalence $C(X,C(Y))\simeq C(X\times Y)$ in Thm. \ref{lb274} (where $Y$ is compact): If we take the viewpoint of $C(X\times Y)$, we put more emphasis on the compactness of $Y$. However, if we take the viewpoint of $C(X,C(Y))$, we view $C(Y)$ as an abstract Banach space $V$. Therefore, we forget about the compactness of $Y$ and focus on the completeness of $V$.



Riesz's systematic use of the completeness of $l^2$ and $L^2$ spaces goes hand in hand with his emphasis on the viewpoint of vector-valued functions. Thus, the following three ideas were intertwined in Riesz's hands:
\begin{itemize}
\item[(1)] The completeness of function spaces.
\item[(2)] The study of linear maps between function spaces $U\rightarrow V$, as opposed to the study of scalar-valued functions on $U\times V$.
\item[(3)] The operator norm on $\fk L(U,V)$.
\end{itemize}
We have mentioned the relationship between (1) and (3) in Sec. \ref{lb610} and \ref{lb551}. %(The key property behind this relationship is Thm. \ref{lb540}.) 
Here, let me briefly explain why (1) and (2) are related by recalling two fundamental facts learned before: 

Suppose that $U,V$ are normed vector spaces and $U_0$ is a dense linear subspace of $V$. To extend a bounded linear map $T:U_0\rightarrow V$ to a bounded linear $U\rightarrow V$, one needs the completeness of $V$. (Cf. Prop. \ref{lb500}.) Given a net of bounded linear maps $(T_\alpha)$ from $U$ to $V$ satisfying $\sup_\alpha\Vert T_\alpha\Vert<+\infty$, to show that the pointwise convergence of $(T_\alpha)$ on $U_0$ implies the pointwise convergence on $U$, one also needs the completeness of $V$. (Cf. Prop. \ref{lb520}.) %\footnote{The discussion here does not imply that completeness is useless in the viewpoint of scalar-valued functions. One important method related to the completeness of domains (rather than codomains) is Baire's category theorem/Banach-Steinhaus theorem. However, there was no systematic application of this method to function spaces by the time Hilbert-Schmidt initiated the study of Hilbert spaces. And I don't really know whether  F. Riesz has used this method, especially when dealing with Hilbert spaces.}


%% Proofread


\subsection{Key property 1: convergence of summing orthogonal vectors}\label{lb640}

In this section, we fix a Hilbert space $\mc H$.

Hilbert and Schmidt introduced $l^2(\Zbb)$ to the study of integral equations. As we have mentioned, the main interesting analytic property for them is not completeness. Indeed, they mainly used the following conditions, both equivalent to the completeness:
\begin{enumerate}
\item If $\sum_i|a_i|^2<+\infty$ and if $(e_i)_{i\in I}$ is orthormal, then $\sum a_ie_i$ converges.
\item The weak(-*) compactness of the closed unit ball.
\end{enumerate}
(We will show the equivalence in Thm. \ref{lb612} and \ref{lb637}.) In this section, we study the first property. The second property will be studied in the next section. The first property is indeed a direct consequence of the second one; see Thm. \ref{lb630}. Thus, one may also say that the second property is the main analytic property used by Hilbert and Schmidt. In the next chapter, we will see how these two properties are used to study integral equations.

\begin{thm}\label{lb612}
Let $V$ be an inner product space. The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $V$ is complete.
\item For each orthonormal family $(e_i)_{i\in I}$ in $V$, and for each $(a_i)_{i\in I}$ in $\Cbb$ satisfying $\sum_{i\in I}|a_i|^2<+\infty$, the discrete integral $\sum_{i\in I}a_ie_i$ converges (under the norm of $V$).
\item $V$ is isomorphic to $l^2(X)$ for some $X$.
\end{enumerate}
\end{thm}


\begin{proof}
(3)$\Rightarrow$(1): This is because $l^2(X)$ is complete, cf. Thm. \ref{lb596}.

(1)$\Rightarrow$(2): Since $\sum_i |a_i|^2<+\infty$, by Rem. \ref{lb133}, for each $\eps>0$ there exists $J\in\fin(2^I)$ such that for all finite $K\subset I\setminus J$ we have $\sum_{k\in K}|a_k|^2<\eps$, and hence, by the Pythagorean identity,
\begin{align*}
\Big\Vert \sum_{k\in K}a_ke_k\Big\Vert^2=\sum_{k\in K}|a_ke_k|^2<\eps
\end{align*}
By Rem. \ref{lb133} again and the completeness of $V$, we see that $\sum_{i\in I}a_ie_i$ converges.

(2)$\Rightarrow$(3): Assume (2). We first show that $V$ has an orthonormal basis. By Zorn's lemma, we can find a maximal (with respect to the partial order $\subset$) set of orthonormal vectors, written as a family $(e_i)_{i\in I}$. The maximality implies that every nonzero vector $\xi\in V$ is not orthogonal to some $e_i$. (Otherwise, $\{e_i:i\in I\}$ can be extended to $\{e_i:i\in I\}\cup\{\xi/\Vert\xi\Vert\}$.)

Let us prove that $(e_i)_{i\in I}$ is an orthonormal basis. Suppose not. Then $U=\Span\{e_i:i\in I\}$ is not dense in $X$. Let $\xi\in X\setminus \ovl{U}$. By Bessel's inequality, we have
\begin{align*}
\sum_{i\in I}|\bk{\xi|e_i}|^2<+\infty
\end{align*}
Therefore, by (2),
\begin{align}
\sum_{i\in I}\bk{\xi|e_i}\cdot e_i
\end{align}
converges to some vector $\eta\in V$. By the continuity of $\bk{\cdot|\cdot}$, we see that $\bk{\eta|e_i}=\bk{\xi|e_i}$ for all $i$, and hence
\begin{align}
\bk{\xi-\eta|e_i}=0\qquad\text{ for all }i\in I
\end{align}
Since $\eta\in\ovl U$ and $\xi\notin\ovl U$, we conclude that $\xi-\eta$ is a nonzero vector orthogonal to all $e_i$. This contradicts the maximality of $(e_i)_{i\in I}$.

Now we have an orthonormal basis $(e_i)_{i\in I}$. By Cor. \ref{lb602}, we have a linear isometry
\begin{align*}
\Phi:V\rightarrow l^2(I)\qquad \xi\mapsto \big(\bk{\xi|e_i}\big)_{i\in I}
\end{align*}
with dense range. If $(a_i)_{i\in I}$ belongs to $l^2(I)$, by (2), the discrete integral $\sum_{i\in I}a_ie_i$ converges to some $\xi\in V$. Clearly $\Phi(\xi)=(a_i)_{i\in I}$. This proves that $\Phi$ is surjective, and hence is a unitary map. So $V\simeq l^2(I)$.
\end{proof}



Condition (2) of Thm. \ref{lb612} is the main subject of this section. From the above proof, we see that this condition is an easy special case of the completeness. This special case is often sufficient for applications without fully utilizing the completeness of $l^2(\Zbb)$. (To understand how nontrivial the direction (2)$\Rightarrow$(1) is even in the separable case, try to give a direct proof of it!)

In the following, we show that many well-known properties about Hilbert spaces follow Thm. \ref{lb612}-(2). Recall that we have fixed a Hilbert space $\mc H$.




\begin{co}\label{lb616}
$\mc H$ has an orthonormal basis. Moreover, $\mc H$ is separable iff the orthonormal basis can be chosen to be countable.
\end{co}

\begin{proof}
That $\mc H$ has an orthonormal basis follows from the proof of Thm. \ref{lb612} or from the fact that $l^2(X)$ has an orthonormal basis $(\chi_{\{x\}})_{x\in X}$. If $X$ is countable, then $l^2(X)$ has dense subset $\Span_{\Qbb+\im\Qbb}\{\chi_{\{x\}}:x\in X\}$ (by Lem. \ref{lb528}) and hence is separable. Conversely, we have proved in Exp. \ref{lb613} that every separable inner product space has a countable orthonormal basis.
\end{proof}



\begin{thm}
Let $(e_x)_{x\in X}$ be an orthonormal basis of $\mc H$. Then we have a unitary map
\begin{gather}
\mc H\xlongrightarrow{\simeq} l^2(X)\qquad \xi\mapsto\big(\bk{\xi|e_x}\big)_{x\in X}
\end{gather}
\end{thm}



\begin{proof}
This is clear from the proof of Thm. \ref{lb612}.
\end{proof}


\begin{df}
If $\mc K$ is a linear subspace of $\mc H$, we define its \textbf{orthogonal complement} \index{00@orthogonal complement}
\begin{align*}
\mc K^\perp=\{\xi\in\mc H:\bk{\xi|\eta}=0\text{ for all }\eta\in\mc K\}
\end{align*}
It is clear that 
\begin{align}\label{eq257}
{\mc K}^\perp={\ovl{\mc K}}^\perp
\end{align}
\end{df}
\begin{proof}[Proof of \eqref{eq257}]
If $\xi\in\mc H$ is orthogonal to $\ovl{\mc K}$, then $\xi$ is clearly orthogonal to $\mc K$. Conversely, if $\xi$ is orthogonal to $\mc K$, the continuity of $\eta\in\mc H\mapsto\bk{\xi|\eta}$ implies that  $\xi\perp\ovl{\mc K}$.
\end{proof}



\begin{df}\label{lb614}
Let $\mc H_1,\mc H_2$ be Hilbert spaces. Consider the direct sum of vector spaces $\mc H_1\oplus\mc H_2$. Namely, $\mc H_1\oplus\mc H_2$ equals $\mc H_1\times\mc H_2$ as a set, $(\xi,\eta)$ is also written as $\xi\oplus\eta$, and the linear structure is defined by $(\xi\oplus\eta)+(\xi'\oplus\eta')=(\xi+\xi')\oplus(\eta+\eta')$ and $\lambda(\xi\oplus\eta)=\lambda\xi\oplus\lambda\eta$ (where $\xi,\xi'\in\mc H_1,\eta,\eta'\in\mc H_2,\lambda\in\Cbb$), and the zero vector is $0\oplus 0$. Equip $\mc H_1\oplus\mc H_2$ with inner product defined by
\begin{align*}
\bk{\xi\oplus\eta|\xi'\oplus\eta'}=\bk{\xi|\eta}\oplus\bk{\xi'|\eta'}
\end{align*}
Then $\mc H_1\oplus\mc H_2$ is clearly a Hilbert space. We call $\mc H_1\oplus\mc H_2$ the \textbf{(Hilbert space) direct sum}\index{00@Hilbert space direct sums} of $\mc H_1,\mc H_2$.
\end{df}


\begin{rem}
In Def. \ref{lb614}, we clearly have linear isometries
\begin{gather*}
\mc H_1\rightarrow\mc H_1\oplus\mc H_2\qquad\xi\mapsto \xi\oplus 0\\
\mc H_2\rightarrow \mc H_1\oplus\mc H_2\qquad\eta\mapsto 0\oplus\eta
\end{gather*}
with ranges $\mc H_1\oplus 0$ and $0\oplus\mc H_2$ respectively. It is clear that $\mc H_1\oplus0$ and $0\oplus\mc H_2$ are orthogonal complements of each other. We often identify $\mc H_1$ with $\mc H_1\oplus0$ and $\mc H_2$ with $0\oplus\mc H_2$. Then $\mc H_1$ and $\mc H_2$ are Hilbert subspaces of $\mc H_1\oplus\mc H_2$, and are orthogonal complements of each other.
\end{rem}



\begin{thm}\label{lb617}
Let $\mc K$ be a closed linear subspace of $\mc H$. Note that $\mc K$ and $\mc K^\perp$ are both Hilbert subspaces of $\mc H$. Then there is a unitary map
\begin{align}\label{eq256}
\Psi: \mc K\oplus\mc K^\perp\xlongrightarrow{\simeq}\mc H\qquad \xi\oplus\eta\mapsto\xi+\eta
\end{align}
\end{thm}

\begin{proof}
It is a routine check that $\Psi$ is a linear isometry. It remains to prove that $\Psi$ is surjective. This means that we need to write each $\psi\in\mc H$ in the form $\psi=\xi+\eta$ where $\xi\in\mc K,\eta\in\mc K^\perp$. Thus, the surjectivity of $\Psi$ means that every $\psi\in\mc H$ has an orthogonal decomposition with respect to $\mc K$ (recall Def. \ref{lb615}).

Let $\psi\in\mc H$. By Cor. \ref{lb616}, $\mc K$ has an orthonormal basis $(e_i)_{i\in I}$. As in the proof of (2)$\Rightarrow$(3) of Thm. \ref{lb612}, Bessel's inequality implies that  $\sum_{i\in I}\bk{\psi|e_i}\cdot e_i$ converges to some vector $\xi\in\mc K$. Then one checks easily that $\eta=\psi-\xi$ is orthogonal to all $e_i$, and hence is orthogonal to $\mc K_0=\Span\{e_i:i\in I\}$. Thus $\eta\perp\mc K$ by \eqref{eq257}. 
\end{proof}

\begin{rem}
Due to Thm. \ref{lb617}, given a Hilbert subspace $\mc K$, people often write
\begin{align}
\mc H=\mc K\oplus\mc K^\perp
\end{align}
\end{rem}


\begin{co}\label{lb618}
Let $\mc K$ be a closed linear subspace of $\mc H$. Then every vector $\psi\in\mc H$ has an orthogonal decomposition with respect to $\mc K$. Moreover, we have $(\mc K^\perp)^\perp=\mc K$.
\end{co}

\begin{proof}
The existence of the orthogonal decomposition follows from Thm. \ref{lb617}. Clearly $\mc K\subset(\mc K^\perp)^\perp$. By Thm. \ref{lb617} (applied to $\mc K^\perp$ instead of $\mc K$), there is a unitary $\mc K^\perp\oplus(\mc K^\perp)^\perp\rightarrow\mc H$ sending $\eta\oplus\xi\mapsto\eta+\xi$, and hence a unitary $\Gamma:(\mc K^\perp)^\perp\oplus\mc K^\perp \rightarrow\mc H$ sending  $\xi\oplus\eta\mapsto\xi+\eta$. By Thm. \ref{lb617} again, $\Gamma$ restricts to the unitary map $\Psi=\eqref{eq256}$. So we must have $(\mc K^\perp)^\perp\oplus\mc K^\perp=\mc K\oplus\mc K^\perp$, and hence $\mc K=(\mc K^\perp)^\perp$.
\end{proof}

\begin{co}
Let $V$ be a linear subspace of $\mc H$. Then $V$ is dense iff the only vector of $\mc H$ orthogonal to $V$ is $0$. 
\end{co}

This corollary gives a useful criterion for the density of linear subspaces.

\begin{proof}
Cor. \ref{lb618} implies for every closed linear subspaces $\mc K_1,\mc K_2$ that
\begin{align}
\mc K_1=\mc K_2\qquad\Longleftrightarrow\qquad \mc K_1^\perp=\mc K_2^\perp
\end{align}
Therefore $\ovl V=\mc H$ iff $\ovl V^\perp=\mc H^\perp=0$ iff (by \eqref{eq257}) $V^\perp=0$.
\end{proof}


\begin{rem}
The existence of orthogonal decompositions with respect to closed linear subspaces (Thm. \ref{lb617} or Cor. \ref{lb618}) is a key feature of Hilbert spaces that is not satisfied by general inner product spaces. A different (and fancier) proof can be found in many textbooks which proves the existence of the orthogonal decomposition $\psi=\xi+\eta$ by defining $\xi$ to be the vector in $\mc K$ such that $\Vert\psi-\xi\Vert$ attains its minimum: the existence of such $\xi$ relies on the completeness of $\mc H$ and the parallelogram law
\begin{align}
\Vert u+v\Vert^2+\Vert u-v\Vert^2=2\Vert u\Vert^2+2\Vert v\Vert^2
\end{align}
cf. \cite[Thm. 5.24]{Fol} or \cite[Thm. 4.11]{Rud-R} for instance.\footnote{The proof of the existence of this minimizing $\xi$ makes use of the convexity of $\mc K$ and thus holds for $\mc K$ being an arbitrary closed convex subset. Convexity in Banach spaces is a very important topic, but its importance was not realized until long after the birth of Hilbert spaces and much of F. Riesz's work. We believe that introducing convexity (even if secretly introduced) at the first encounter with Hilbert spaces is off-topic and distracting, because it hinders the understanding of the roles played by the other central properties in Hilbert spaces, especially the analytic properties.}

The proof we give here is closer to Schmidt's method in that we used the Gram-Schmidt process instead of the parallelogram law. With this proof, we want to convey the idea that the existence of orthogonal decompositions is a direct consequence of the convergence of summing orthogonal vectors (condition (2) of Thm. \ref{lb612}).   \hfill\qedsymbol
\end{rem}




\subsection{Key property 2: weak(-*) compactness of the closed unit balls}\label{lb641}

We fix Hilbert spaces $\mc H,\mc K$ in this section. 

\begin{lm}\label{lb623}
For each $\xi\in\mc H$, we have
\begin{align}\label{eq263}
\Vert \xi\Vert=\sup_{\eta\in\ovl B_{\mc H}(0,1)}|\bk{\xi|\eta}|
\end{align}
\begin{proof}
Clearly ``$\geq$" holds by Cauchy-Schwarz. To prove ``$\leq$", assume WLOG that $\xi\neq0$. Then $\Vert\xi\Vert=\bk{\xi|\eta}$ if we choose $\eta=\xi/\Vert\xi\Vert$.
\end{proof}
\end{lm}



Recall that if $T:V\rightarrow W$ is a linear map of normed vector spaces, its operator norm is $\Vert T\Vert=\sup_{v\in \ovl B_V(0,1)}\Vert Tv\Vert$. Operator norms describe uniform convergence on $\ovl B_V(0,1)$: If $(T_\alpha)$ is a net of bounded linear operators $V\rightarrow W$, then $\lim_\alpha\Vert T-T_\alpha\Vert=0$ iff $T_\alpha$ converges uniformly on $\ovl B_V(0,1)$ to $T$. 

We now show that in the case of Hilbert spaces, the operator norms of linear maps can be translated to certain upper bounds about sesquilinear forms, which are closer in spirit to Hilbert's original understanding of ``boundedness" (cf. Pb. \ref{lb619} and Rem. \ref{lb622}). 

\begin{pp}\label{lb624}
Let $T:\mc H\rightarrow\mc K$ be a linear map. Then
\begin{align}\label{eq262}
\Vert T\Vert=\sup_{
\begin{subarray}{c}
\xi\in\ovl B_{\mc H}(0,1)\\
\eta\in\ovl B_{\mc K}(0,1)
\end{subarray}
}\big|\bk{T\xi|\eta}\big|
\end{align}
\end{pp}

\begin{proof}
By Lem. \ref{lb623}, the RHS of \eqref{eq262} equals $\dps\sup_{\xi\in\ovl B_{\mc H}(0,1)}\Vert T\xi\Vert=\Vert T\Vert$.
\end{proof}



\begin{co}\label{lb646}
Let $T:\mc H\rightarrow\mc K$, and let $(T_\alpha)$ be a net of linear maps $\mc H\rightarrow\mc K$. Define their associated sesquilinear maps $\omega_T,\omega_{T_\alpha}:\mc H\times\mc K\rightarrow\Cbb$ by
\begin{gather}
\omega_T(\xi|\eta)=\bk{T\xi|\eta}\qquad\omega_{T_\alpha}(\xi|\eta)=\bk{T_\alpha\xi|\eta}
\end{gather} 
Then $\lim_\alpha\Vert T-T_\alpha\Vert=0$ iff $(\omega_{T_\alpha})$ converges uniformly on $\ovl B_{\mc H}(0,1)\times\ovl B_{\mc K}(0,1)$ to $\omega_T$.
\end{co}


\subsubsection{Weak topology}

We know that, contrary to the norm topology, the weak-* topology describes poinwise convergence. Let us recall several basic facts about weak-* topology.

Recall from Def. \ref{lb625} that if $V$ is a normed vector space, the weak-* topology of $V^*$ is the unique topology such that a net $(\varphi_\alpha)$ in $V^*$ converges to $\varphi$ under this topology iff $\lim_\alpha\bk{\varphi_\alpha,v}=\bk{\varphi,v}$ for all $v\in V$. It is more important to study the weak-* topology on $\ovl B_{V^*}(0,1)$ (or on any bounded closed ball of $V^*$): By Banach-Alaoglu Thm. \ref{lb519}, the closed unit ball $\ovl B_{V^*}(0,1)$ is weak-* compact. Moreover, the following elementary fact allows for a flexible characterization of weak-* topology:

\begin{lm}\label{lb626}
Let $V$ be a normed vector space. Assume that $E\subset V$ spans a dense subspace of $V$. Let $(\varphi_\alpha)$ be a net in $\ovl B_{V^*}(0,1)$, and let $\varphi\in\ovl B_{V^*}(0,1)$. Then $(\varphi_\alpha)$ converges weak-* to $\varphi$ iff $\lim_\alpha\bk{\varphi_\alpha,v}=\bk{\varphi,v}$ for all $v\in E$.
\end{lm}

\begin{proof}
Immediate from Prop. \ref{lb520}.
\end{proof}
The most relevant example of this chapter is $l^2(X)$, which can be viewed as the dual space of $l^2(X)$ through the Banach space isomorphism (cf. Thm. \ref{lb527})
\begin{align}\label{eq265}
\Psi:l^2(X)\rightarrow l^2(X)^*\qquad f\mapsto \bk{\cdot|f^*}
\end{align}
(Recall that $f^*(x)=\ovl{f(x)}$. So $\bk{g|f^*}=\sum_{x\in X}f(x)g(x)$.) In this case, one can choose $E=\{\chi_{\{x\}}:x\in X\}$. Then Lem. \ref{lb626} says that for any net $(f_\alpha)$ in $l^2(X)$ satisfying $\Vert f_\alpha\Vert_{l^2}\leq 1$ and any $f\in l^2(X)$ satisfying $\Vert f\Vert_{l^2}\leq 1$,
\begin{align}\label{eq264}
f_\alpha\xlongrightarrow{\text{weak-*}}f\qquad\Longleftrightarrow\qquad \lim_\alpha f_\alpha(x)=f(x)\text{ for all }x\in X
\end{align}
This is the content Thm. \ref{lb530}. In view of \eqref{eq264}, the Banach-Alaoglu for $l^2(X)$ says that any net $(f_\alpha)$ in $\ovl B_{l^2(X)}(0,1)$ has a subnet converging pointwise on $X$ to some $f\in\ovl B_{l^2(X)}(0,1)$, which can be checked directly using  the Tychonoff theorem. (You were asked to give this direct proof in Pb. \ref{lb531}.)


It should be kept in mind that whenever talking about the weak-* topology on a Banach space $V$, we should specify a Banach space $U$ and an isomorphism of Banach spaces $\Psi:V\rightarrow U^*$ so that $V$ can be viewed as a dual Banach space. The data $(\Psi,U)$ (or simply written as $U$) is called a \textbf{predual} of $V$. It is customary to write a predual $U$  of $V$ as $V_*$.

Since we know that every Hilbert space $\mc H$ is isomorphic to $l^2(X)$ for some $X$, the isomorphism \eqref{eq265} suggests that we can talk about the weak-* topology of $\mc H$, which should be the unique topology whose convergence $\xi_\alpha\rightarrow\xi$ means that $\bk{\xi_\alpha|\eta}\rightarrow\bk{\xi|\eta}$ for all $\eta$. However, calling this topology a weak-* topology is confusing. In fact, there is no \uwave{standard choice} of isomorphism $\mc H\rightarrow\mc H^*$. The isomorphism  \eqref{eq265} relies on the antiunitary map $f\mapsto f^*$.
\begin{df}
A linear map of inner product spaces $T:U\rightarrow V$ is called \textbf{antiunitary} \index{00@Antiunitary map} if it is an antilinear surjective isometry.
\end{df}
There is no canonical antiunitary operator on a Hilbert space: any antiunitary operator, composed with a unitary one, is again antiunitary. However, we do have a canonical antiunitary map $\mc H\rightarrow\mc H^*$: \index{00@Riesz-Fr\'echet representation theorem}

\begin{thm}[\textbf{Riesz-Fr\'echet representation theorem}]
For any Hilbert space $\mc H$, the (operator) norm of the dual Banach space $\mc H^*$ is induced by a unique inner product. (So $\mc H^*$ is a Hilbert space.) Moreover, we have an antiunitary map
\begin{gather}\label{eq266}
\Phi:\mc H\rightarrow\mc H^*\qquad \xi\mapsto\bk{\cdot|\xi}
\end{gather}
\end{thm}

\begin{proof}
Define a linear map $\Phi$ by \eqref{eq266}. By Thm. \ref{lb612}-(3), we assume WLOG that $\mc H=l^2(X)$. Then $\Phi$ is related to the Banach space isomorphism \eqref{eq265} by $\Phi(\xi)=\Psi(\xi^*)$. Since $\xi\mapsto\xi^*$ is antiunitary, $\Phi$ must be an antilinear surjective isometry. Define an inner product on $\mc H^*$ by
\begin{align}
\bk{\varphi|\mu}=\bk{\Phi^{-1}\mu|\Phi^{-1}\psi}\qquad(\forall \varphi,\mu\in\mc H^*)
\end{align}
Then this inner product induces the operator norm $\Vert\cdot\Vert$ on $\mc H^*$ because
\begin{align*}
\Vert\varphi\Vert^2=\Vert\Phi^{-1}\varphi\Vert^2=\bk{\Phi^{-1}\varphi|\Phi^{-1}\varphi}=\bk{\varphi|\varphi}
\end{align*}
where the first identity is due to the fact that $\Phi$ is an isometry. The uniqueness of the inner product follows from the polarization identity \eqref{eq238}.
\end{proof}

Recall that any linear map $T:\mc H\rightarrow\mc K$ is determined by the expressions $\bk{T\xi|\eta}$.

\begin{co}\label{lb631}
For every $T\in\fk L(\mc H,\mc K)$ there is a unique bounded linear map $T^*:\mc K\rightarrow\mc H$ (called the \textbf{adjoint} \index{00@Adjoint operator} of $T$) satisfying for all $\xi\in\mc H,\eta\in\mc K$ that
\begin{align}\label{eq269}
\bk{T\xi|\eta}=\bk{\xi|T^*\eta}
\end{align}
Moreover, we have $\Vert T\Vert=\Vert T^*\Vert$, and $(T^*)^*=T$.
\end{co}


\begin{proof}
For each $\eta\in\mc K$, the linear map $\xi\in\mc H\mapsto \bk{T\xi|\eta}\in\Cbb$ is bounded since $|\bk{T\xi|\eta}|\leq \Vert\xi\Vert\cdot\Vert T\Vert\cdot\Vert\eta\Vert$. Therefore, by Riesz-Fr\'echet, there is a unique $\psi\in\mc H$ such that $\bk{T\xi|\eta}=\bk{\xi|\psi}$ for all $\xi\in\mc H$. We let $T^*\eta=\psi$. This gives a map $T^*:\mc K\rightarrow\mc H$ satisfying \eqref{eq269} for all $\xi\in\mc H,\eta\in\mc K$. The formula \eqref{eq269} clearly shows that $T^*$ is linear, and that $T^{**}=T$ if $T^*$ is bounded. That $\Vert T\Vert=\Vert T^*\Vert$ (and in particular, the boundedness of $T^*$) follows from  Prop. \ref{lb624}.
\end{proof}








\begin{sexe}
Recall that for every normed vector space $V$ there is a canonical bounded linear map $V\rightarrow V^{**},v\mapsto\bk{v,\cdot}$ which is a linear isometry by Hahn-Banach Cor. \ref{lb502}. Now let $V=\mc H$. Show that this map $\mc H\rightarrow\mc H^{**}$ is equal to the composition of the antiunitary maps $\mc H\rightarrow\mc H^*$ and $\mc H^*\rightarrow\mc H^{**}$, both defined by Riesz-Fr\'echet. Conclude that the canonical linear isometry $\mc H\rightarrow\mc H^{**}$ is unitary.
\end{sexe}
\begin{proof}[Hint]
You may prove it in a general fashion. But the easiest way to think about this question is to assume WLOG that $\mc H=l^2(X)$. 
\end{proof}


According to this exercise, we can view $\mc H$ as the dual space of $\mc H^*$, and talk about the weak-* topology of $\mc H$. Indeed, it is more customary to call it weak topology in this case:
\begin{df}
For any normed vector space $V$, the \textbf{weak topology} \index{00@Weak topology} of $V$ is defined to be the pullback of the weak-* topology of $V^{**}$ through the canonical linear isometry $V\rightarrow V^{**}$. Convergence under the weak topology is called \textbf{weak convergence}. \index{00@Weak convergence} Thus, weak topology is described by the condition that for every net $(v_\alpha)$ in $V$ and every $v\in V$,
\begin{align}
v_\alpha\xlongrightarrow{\text{weakly}}v\qquad\Longleftrightarrow\qquad \lim_\alpha\bk{v_\alpha,\varphi}=\bk{v,\varphi}\text{ for all }\varphi\in V^*
\end{align}
If $(\xi_\alpha)$ is a net in $\mc H$ and $\xi\in \mc H$, then by Riesz-Fr\'echet,
\begin{align}
\xi_\alpha\xlongrightarrow{\text{weakly}}\xi\qquad\Longleftrightarrow\qquad \lim_\alpha\bk{\xi_\alpha|\eta}=\bk{\xi|\eta}\text{ for all }\eta\in \mc H
\end{align}
\end{df}

\begin{rem}\label{lb628}
Assume that $1<p,q<+\infty$ and $p^{-1}+q^{-1}=1$. The weak-* topology on $l^p(X)$ defined by the isomorphism $l^q(X)^*\simeq l^p(X)$ in Thm. \ref{lb527} is clearly equal to the weak topology of $l^p(X)$. Therefore:
\begin{align*}
\tcboxmath{
\begin{array}{c}
\text{We will not distinguish between weak topology}\\
\text{and weak-* topology for $l^p(X)$ where $1<p<+\infty$.}
\end{array}
}
\end{align*}
\end{rem}


\begin{co}\label{lb636}
The closed unit ball $\ovl B_{\mc H}(0,1)$ of $\mc H$ is weakly compact. Moreover, $\mc H$ is separable iff $\ovl B_{\mc H}(0,1)$ is weakly metrizable.
\end{co}

It follows that if $\mc H$ is separable, then $\ovl B_{\mc H}(0,1)$ is sequentially compact.

\begin{proof}
By Cor. \ref{lb616}, we may assume $\mc H=l^2(X)$ where $X$ is countable if $\mc H$ is separable. By Banach-Alaoglu and $l^2(X)\simeq l^2(X)^*$, or by Pb. \ref{lb531} and Thm. \ref{lb530}, $\ovl B_{\mc H}(0,1)$ is weakly compact. By $l^2(X)\simeq l^2(X)^*$ and Thm. \ref{lb523}, $l^2(X)$ is separable iff $\ovl B_{l^2(X)}(0,1)$ is metrizable under the weak(-*) topology. 
\end{proof}



\begin{srem}
Assume that $X$ is countable, say, $X$ is $\{1,2,\dots,n\}$ or $\Zbb_+$. Then explicit metrics of $\ovl B_{l^2(X)}(0,1)$ can be found, e.g., $d$ and $\delta$ defined by
\begin{align*}
d(f,g)=\sup_n n^{-1}|f(n)-g(n)|\qquad \delta(f,g)=\sum_n 2^{-n}|f(n)-g(n)|
\end{align*}
for each $f,g\in \ovl B_{l^2(X)}(0,1)$. This is because, by Cor. \ref{lb260}, both $d$ and $\delta$ induce the pointwise convergence topology, and hence the weak topology on $\ovl B_{l^2(X)}(0,1)$ by Thm. \ref{lb530}. (Compare also Pb. \ref{lb535} or Pb. \ref{lb533}.)
\end{srem}




\begin{srem}
A normed vector space $V$ is called \textbf{reflexive} \index{00@Reflexive Banach space} if the canonical linear isometry $V\rightarrow V^{**}$ is surjective (and hence an isomorphism). A reflexive space must be complete since $V^{**}$ is complete by Thm. \ref{lb540}. By Thm. \ref{lb540}, if $1<p<+\infty$ then $l^p(X)$ is reflexive. In particular, every Hilbert space is reflexive.

If $V$ is reflexive, the canonical isomorphism $V\simeq V^{**}$ allows us to view $V$ as a dual space of $V^*$, and talk about its weak-* topology. This is clearly equal to the weak topology. If $V$ is not reflexive, we do not view the weak topology of $V$ as the weak-* topology, since $V$ might have a meaningful predual such that the weak-* topology defined by this predual is different from the weak topology. 

For example, consider $V=l^\infty(\Zbb)$ with predual $l^1(\Zbb)$. The weak-* topology defined by $l^1(\Zbb)$ is different from the weak topology (which is defined by elements of $l^\infty(\Zbb)^*$), cf. Pb. \ref{lb532}.  \hfill\qedsymbol
\end{srem}

\subsubsection{Strong and weak convergence}


The following proposition clarifies the relationship between norm and weak convergence in Hilbert spaces:
\begin{pp}\label{lb635}
Let $(\xi_\alpha)$ be a net in an inner product space $V$, and let $\xi\in V$. The following are equivalent:
\begin{enumerate}
\item[(1)] $\lim_\alpha\xi_\alpha=\xi$.
\item[(2)] $(\xi_\alpha)$ converges weakly to $\xi$, and
\begin{align}\label{eq268}
\lim_\alpha\bk{\xi_\alpha|\xi_\alpha}=\bk{\xi|\xi}
\end{align}
\end{enumerate}
\end{pp}
\begin{proof}
By the (norm-)continuity of $\bk{\cdot|\cdot}$ we clearly have $(1)\Rightarrow(2)$. Assume (2). Then
\begin{align*}
\bk{\xi-\xi_\alpha|\xi-\xi_\alpha}=\bk{\xi|\xi}+\bk{\xi_\alpha|\xi_\alpha}-\bk{\xi|\xi_\alpha}-\bk{\xi|\xi_\alpha}
\end{align*}
converges to $\bk{\xi|\xi}+\bk{\xi|\xi}-\bk{\xi|\xi}-\bk{\xi|\xi}=0$ by \eqref{eq268} and the weak convergence. So (1) is true.
\end{proof}


That \eqref{eq268} does not always hold means that the norm function $\Vert\cdot\Vert$ is not continuous under the weak topology:
\begin{eg}\label{lb661}
Suppose that $(e_n)_{n\in\Zbb_+}$ is an orthormal sequence in an inner product space $V$. Then $\lim_n e_n$ converges weakly to $0$, for instance, by the fact that $\sum_n |\bk{\xi|e_n}|^2<+\infty$ for all $\xi\in\mc H$ (which implies $\lim_n\bk{\xi|e_n}=0$). However, $\lim_n\Vert e_n\Vert=1\neq0$.
\end{eg}



Prop. \ref{lb635} will be used to prove Thm. \ref{lb630}, the main result of the next section.

\begin{comment}
\begin{slm}
Let $V$ be a normed vector space. The norm function $\Vert\cdot\Vert:V\rightarrow\Rbb$ is weakly \textbf{lower semicontinuous}, i.e., if $(v_\alpha)$ is a net in $V$ converging weakly to $v\in V$, then $\liminf_\alpha\Vert v_\alpha\Vert\geq\Vert v\Vert$.
\end{slm}

\begin{proof}
By Hahn-Banach Cor. \ref{lb502}, there exists $\varphi\in V^*$ such that $\Vert\varphi\Vert=1$ and $\varphi(v)=\Vert v\Vert$. (In the case that $V$ is an inner product space, one assumes WLOG that $v\neq0$, and let $\varphi=\bk{\cdot|u}$ where $u=v/\Vert v\Vert$.) Then
\begin{align}
\Vert v\Vert=\bk{\varphi,v}=\lim_\alpha\bk{\varphi,v_\alpha}\leq\liminf_\alpha \Vert\varphi\Vert\cdot\Vert v_\alpha\Vert=\liminf_\alpha \Vert v_\alpha\Vert
\end{align}
\end{proof}

Consequently, if $(v_\alpha)$ is a net in an inner product space converging weakly to $v\in V$, then $(v_\alpha)$ converges to $v$ iff $\limsup_\alpha\Vert v_\alpha\Vert\leq\Vert v\Vert$.
\end{comment}





\subsection{$\star$ Key property 1 is a direct consequence of key property 2}




In this starred section, we shall give a direct proof that an inner product space $V$ satisfies Condition (2) of Thm. \ref{lb612} (the convergence of summing orthogonal vectors) if $\ovl B_V(0,1)$ is weakly compact. The word "direct" means that the proof will clearly show how weak compactness leads almost directly to the conclusion. Let us begin the discussion by an easy observation:

\begin{rem}
If $W$ is a normed vector space and $V$ is a linear subspace, then we have a canonical linear map $W^*\rightarrow V^*,\varphi\mapsto\varphi|_V$. It is easy to see (cf. Prop. \ref{lb500}) that this map is an isomorphism of Banach spaces if $V$ is dense in $W$. In particular, if $V$ is an inner product space with completion $\mc H$, there is a canonical isomorphism $V^*\simeq\mc H^*$.
\end{rem}

The following lemma tells us that given an inner product space $V$ with completion $\mc H$, the weak topology of $\ovl B_V(0,1)$ can be described in terms of the vectors of $V$ but not necessarily of $\mc H$ or $V^*$. Thus, you may take Lem. \ref{lb629}-(2) as the \textit{definition} of the weak compactness of $\ovl B_V(0,1)$ if you want to invoke Occam's razor.

\begin{lm}\label{lb629}
Let $V$ be an inner product space. Let $(v_\alpha)$ be a net in $\ovl B_V(0,1)$ and $v\in \ovl B_V(0,1)$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $(v_\alpha)$ converges weakly to $v$.
\item $\lim_\alpha\bk{v_\alpha|w}=\bk{v|w}$ for all $w\in V$.
\end{enumerate}
\end{lm}

Similar to Prop. \ref{lb520}, this lemma is not true if $\ovl B_V(0,1)$ is replaced by $V$.

\begin{proof}
Let $\mc H$ be the Hilbert space completion of $V$. So $V^*\simeq\mc H^*$. By Riesz-Fr\'chet,  (1) means that $\lim\bk{v_\alpha|\xi}=\bk{v|\xi}$ for all $\xi\in\mc H$. Thus, one checks easily that (1)$\Longleftrightarrow$(2) using the density of $V$ in $\mc H$.
\end{proof}



%Thus, we will used condition (2) of Lem. \ref{lb629} to understand the weak topology of inner product spaces, which is more convenient.


\begin{thm}\label{lb630}
Let $V$ be an inner product space. Suppose that the unit ball $\ovl B_V(0,1)$ is weakly compact. Then $V$ satisfies condition (2) of Thm. \ref{lb612}. 
\end{thm}
 



\begin{proof}
Let $(e_i)_{i\in I}$ be an orthonormal family of vectors in $V$. Let $(a_i)_{i\in I}$ be in $\Cbb$ satisfying $\lambda:=\sum_i|a_i|^2<+\infty$. By scaling $(a_i)_{i\in I}$, assume WLOG that $\lambda=1$. For each $J\in\fin(2^I)$, let $v_J=\sum_{j\in J}a_je_j$. Then $\Vert v_J\Vert^2=\sum_{j\in J}|a_j|^2$. Therefore, $(v_J)_{J\in\fin(2^I)}$ is a net in $\ovl B_V(0,1)$, and  $\lim_{J\in\fin(2^I)}\Vert v_J\Vert^2=1$.

Since $\ovl B_V(0,1)$ is weakly compact, $(v_J)$ has a subnet $(v_{J_\beta})$ converging weakly to some $v\in\ovl B_V(0,1)$. In particular, $\Vert v\Vert^2\leq 1$. For each $i\in I$ we have
\begin{align*}
\bk{v|e_i}=\lim_\beta \bk{v_{J_\beta}|e_i}=\lim_J\bk{v_J|e_i}=a_i
\end{align*}
Therefore, by Bessel's inequality (Cor. \ref{lb634}), we have $\Vert v\Vert^2\geq \sum_{i\in I}|a_i|^2=1$. Thus $\Vert v\Vert^2=1=\lim_\beta \Vert v_{J_\beta}\Vert^2$. Therefore, by Prop. \ref{lb635}, we have $\lim_\beta v_{J_\beta}=v$. It is now easily to see (e.g. by Prop. \ref{lb127}) that $\lim_J v_J=v$, i.e., $\sum_{i\in I}a_ie_i$ converges to $v$.
\end{proof}


\begin{co}\label{lb637}
Let $V$ be an inner product space. Then $V$ is a Hilbert space iff $\ovl B_V(0,1)$ is weakly compact.
\end{co}

\begin{proof}
Cor. \ref{lb636} implies ``$\Rightarrow$". Thm. \ref{lb630} and \ref{lb612} imply ``$\Leftarrow$".
\end{proof}


\begin{comment}
Let $\mc H$ be the Hilbert space completion of $V$. Choose any $\xi\in\mc H$. We want to show that $\xi\in V$. Assume WLOG that $\Vert\xi\Vert= 1$. Since $V$ is dense, there is a net $(v_\alpha)$ in $V$ converging (in norm) to $\xi$. So $\lim_\alpha\Vert v_\alpha\Vert=\lVert \xi\Vert=1$. Thus, by dividing $v_\alpha$ by $\Vert v_\alpha\Vert$, we assume WLOG that $\Vert v_\alpha\Vert=1$ for all $\alpha$. 

Now, by the weak compactness of $V$, $(v_\alpha)$ has a subnet $(v_\beta)$ converging weakly to some $v\in V$. (Note that since since $V^*=\mc H^*$, a net in $V$ converges weakly as a net in $V$ to $v$ iff it converges weakly as a net in $\mc H$ to $v$.) Since $(v_\beta)$ converges in norm to $\xi$, it also converges weakly to $\xi$. So $\xi=v$, and hence $\xi\in V$.

\begin{rem}
Thm. \ref{lb630} is a very special case of the (difficult) \textbf{Banach-Dieudonn\'e theorem}, which says the following: If $V$ is a normed vector space, and if $W$ is a linear subspace of $V^*$, then $W$ is weak-* closed in $V^*$ iff $\ovl B_W(0,1)$ is weak-* compact. A proof can be found in \cite[Sec. 3.3]{BS}.
\end{rem}

\end{comment}














\subsection{Problems and supplementary material}

Fix Hilbert spaces $\mc H,\mc K$.

\subsubsection{Basic facts about Hilbert spaces}


\begin{sprob}
Prove that any two orthonormal bases of $\mc H$ have the same cardinality.
\end{sprob}

\begin{proof}[Hint]
The case $\dim\mc H<+\infty$ is obvious by linear algebra. In the case that either one of the two orthonormal bases is infinite, use Thm. \ref{lb497}.
\end{proof}






\begin{prob}
Assume that $\mc K$ is a closed linear subspace of $\mc H$. Define a map $P:\mc H\rightarrow\mc H$ such that for each $\xi\in\mc H$, $\xi=P\xi+(1-P)\xi$ is the orthogonal decomposition with respect to $\mc K$. (So $P\xi\in\mc K$ and $(1-P)\xi\in\mc K^\perp$.) Prove that $P$ is a bounded linear map, and $P^2=P$, $P^*=P$. We call $P$ the \textbf{projection operator} \index{00@Projection} of $\mc K$. 
\end{prob}



\begin{sprob}
Let $P\in\fk L(\mc H)$ satisfying $P^2=P$ and $P=P^*$. Prove that $P(\mc H)$ is a closed linear subspace of $\mc H$, and $P$ is the projection operator of $P(\mc H)$.
\end{sprob}











\subsubsection{Hilbert's notion of boundedness}\label{lb643}


Let $X,Y$ be sets. 

\begin{prob}\label{lb649}
Let $T:l^2(Y)\rightarrow l^2(X)$ be a bounded linear map. Prove that $T$ is uniquely determined by its \textbf{matrix representation} \index{00@Matrix representation}
\begin{align}\label{eq260}
K:X\times Y\rightarrow\Cbb\qquad K(x,y)=\bk{T\chi_{\{y\}}|\chi_{\{x\}}}
\end{align}
Prove that for each $x\in X$, the RHS of the following converges to the LHS:
\begin{gather}\label{eq258}
(T\xi)(x)=\sum_{y\in Y} K(x,y)\xi(y)
\end{gather}
\end{prob}

\begin{proof}[Hint]
Show that the RHS of \eqref{eq258} (together with its convergence) is equal to $\lim_{B\in\fin(2^Y)} \bk{T(\chi_B\xi)|\chi_{\{x\}}}$.
\end{proof}



Let's study the question of which $\infty\times\infty$ matrices are the matrix representations of bounded linear maps. We first consider the special case that $l^2(X)=\Cbb$, and write $K$ as a function $f:Y\rightarrow\Cbb$: 

\begin{exe}\label{lb632}
Let $f\in\Cbb^Y$. Prove that
\begin{align}\label{eq261}
\Vert f\Vert_{l^2(Y)}=\sup_{B\in\fin(2^Y)}~\sup_{g\in \ovl B_{l^2(Y)}(0,1)}\bk{f\chi_B|g}
\end{align}
In particular, we have $f\in l^2(Y)$ iff the RHS of \eqref{eq261} is finite.
\end{exe}
\begin{proof}[Hint]
Use Lem. \ref{lb623} to prove $\dps\sum_{y\in B}|f(y)|^2=\sup_{g\in \ovl B_{l^2(B)}(0,1)}\bk{f\chi_B|g}$.
\end{proof}

\begin{srem}\label{lb638}
There is indeed a stronger criterion: 
\begin{align*}
f\in l^2(Y)\qquad\Longleftrightarrow\qquad\sup_{B\in\fin(2^Y)}\bk{f\chi_B|g}<+\infty\text{ for all }g\in l^2(Y)
\end{align*}
You can think about how to prove it if you know Baire's category theorem. (If you want to know the answer directly, search for "Banach-Steinhaus theorem".)
\end{srem}


Now, we consider the general case.

\begin{prob}\label{lb619}
Let $K:X\times Y\rightarrow\Cbb$. For each $A\in\fin(2^X),B\in\fin(2^Y)$, define
\begin{gather}
M_{A,B}=\sup_{
\begin{subarray}{c}
\psi\in\ovl B_{l^2(X)}(0,1)\\
\xi\in\ovl B_{l^2(Y)}(0,1)
\end{subarray}
}~\bigg|\sum_{x\in A,y\in B}K(x,y)\xi(y)\ovl{\psi(x)} \bigg|
\end{gather}
Assume that $M<+\infty$ where
\begin{align}\label{eq259}
M=\sup_{A\in\fin(2^X),B\in\fin(2^Y)} M_{A,B}
\end{align}
In the following, $\dps\lim_{A,B}$ means $\dps\lim_{A\in\fin(2^X),B\in\fin(2^Y)}$.
\begin{enumerate}
\item For each $A\in\fin(2^X),B\in\fin(2^Y)$, define
\begin{align}\label{eq267}
T_{A,B}:l^2(Y)\rightarrow l^2(X)\qquad \xi\mapsto \sum_{x\in A,y\in B}K(x,y)\xi(y)\cdot\chi_{\{x\}}
\end{align}
Prove that $\Vert T_{A,B}\Vert=M_{A,B}$. 
\item For each $y\in Y$, prove that $\dps\lim_{A,B} T_{A,B}\chi_{\{y\}}$ converges in $l^2(X)$. 
\item Prove that $\dps\lim_{A,B}T_{A,B}$ converges pointwise on $l^2(Y)$ to some bounded linear
\begin{align*}
T:l^2(Y)\rightarrow l^2(X)
\end{align*}
satisfying $\Vert T\Vert=M$.
\item Prove that $K$ is the matrix representation of $T$, i.e., \eqref{eq260} is satisfied.
\end{enumerate}
\end{prob}


\begin{proof}[Hint]
Part 2: Use Exe. \ref{lb632}. Part 3: Use Prop. \ref{lb520} and the fact that $\{\chi_{y}:y\in Y\}$ spans a dense subspace of $l^2(Y)$.
\end{proof}





\begin{rem}\label{lb622}
If $T:l^2(Y)\rightarrow l^2(X)$ is a bounded linear map, then its matrix representation $K$ clearly satisfies $\eqref{eq259}<+\infty$ by Prop. \ref{lb624}. Therefore, Pb. \ref{lb619} gives a description of bounded linear maps $l^2(Y)\rightarrow l^2(X)$ in terms of an explicit analytic condition on the matrix representations, and \eqref{eq259} gives an equivalent definition of operator norms.

Indeed, the above definitions of bounded ``linear maps" and their ``operator norms" \eqref{eq259} were introduced by Hilbert in an influential paper published in 1906, i.e., the fourth part (vierter Abschnitt) of  \cite{Hil12} (cf. also \cite[Sec. 5.2]{Die-H}). If you compare them with the modern definitions of bounded linear maps and operator norms in Sec. \ref{lb620} (which are due to F. Riesz), you will notice two features.

First, of course, Hilbert's definition is more explicit and easier to calculate. Through learning this definition, we know that mathematicians never invent abstract definitions (such as those of Riesz) out of thin air.

The second and more important aspect is that Hilbert's definitions are not really about linear operators (i.e. maps from $l^2(Y)$ to $l^2(X)$), but are about sesquilinear forms. To illustrate this point, let's start with some preparation. \hfill\qedsymbol
\end{rem}

\begin{df}
For each linear map $T:\mc H\rightarrow\mc K$, define a sesquilinear map
\begin{align}
\omega_T:\mc H\times\mc K\rightarrow\Cbb \qquad \omega_T(\xi,\eta)=\bk{T\xi|\eta}
\end{align}
\end{df}

\begin{prob}\label{lb627}
Let $\omega(\cdot|\cdot):\mc H\times\mc K\rightarrow\Cbb$ be sesquilinear. Define its norm
\begin{align}
\Vert\omega\Vert=\sup_{
\begin{subarray}{c}
\xi\in\ovl B_{\mc H}(0,1)\\
\eta\in\ovl B_{\mc K}(0,1)
\end{subarray}
}|\omega(\xi|\eta)|
\end{align}
Clearly $\Vert\omega\Vert$ is the smallest element in $[0,+\infty]$ satisfying
\begin{align*}
|\omega(\xi|\eta)|\leq\Vert\omega\Vert\cdot\Vert\xi\Vert\cdot\Vert\eta\Vert\qquad(\forall \xi\in\mc H,\forall \eta\in\mc K)
\end{align*}
We say that $\omega$ is \textbf{bounded} \index{00@Bounded sesquilinear form} if $\Vert\omega\Vert<+\infty$. Prove that the following are equivalent. 
\begin{enumerate}[label=(\arabic*)]
\item $\omega$ is bounded.
\item There exists a (necessarily unique) $T\in\fk L(\mc H,\mc K)$ such that $\omega=\omega_T$.
\item $\omega$ is continuous.
\item $\omega$ is continuous at $(0,0)$.
\end{enumerate}
\end{prob}

\begin{proof}[Hint]
(1)$\Rightarrow$(2): Mimic the proof of Cor. \ref{lb631}. (4)$\Rightarrow$(1): Mimic the proof of Prop. \ref{lb313}.
\end{proof}



\begin{rem}\label{lb666}
Pb. \ref{lb627} suggests that Riesz's language of bounded linear maps can be translated into Hilbert's language of bounded sesquilinear forms, and vice versa. Let us now translate Pb. \ref{lb619} into the language of sesquilinear forms. 

Hilbert's goal is to find a general condition ensuring the existence of a bounded form $\omega:l^2(Y)\times l^2(X)\rightarrow\Cbb$ satisfying $\omega(\chi_{y}|\chi_{x})=K(x,y)$ for all $x\in X,y\in Y$. For each $A\in\fin(2^X),B\in\fin(2^Y)$, one defines the truncated form $\omega_{A,B}:l^2(Y)\times l^2(X)\rightarrow\Cbb$ by
\begin{align*}
\omega_{A,B}(\xi|\psi)=\sum_{x\in A,y\in B}K(x,y)\xi(y)\ovl{\psi(x)}
\end{align*}
(Namely, $\omega_{A,B}$ is defined to be $\omega_{T_{A,B}}$ where $T_{A,B}=\eqref{eq267}$.) One wants to find a condition so that $\lim_{A,B}\omega_{A,B}$ converges pointwise to some continuous function $\omega$ (which is clearly sesquilinear).

The condition found by Hilbert, namely $M<+\infty$ in Pb. \ref{lb619}, \uwave{is a condition of equicontinuity}\footnote{Recall that operator norms are also related to equicontinuity!}: Clearly $M_{A,B}=\Vert\omega_{A,B}\Vert$. Thus, $M<+\infty$ means that $\sup_{A,B}\Vert\omega_{A,B}\Vert<+\infty$. This implies that $(\omega_{A,B})_{A\in\fin(2^X),B\in\fin(2^Y)}$ is an equicontinuous family of functions when restricted to $\Delta_R=\ovl B_{l^2(Y)}(0,R)\times\ovl B_{l^2(X)}(0,R)$ for every $R>0$. Moreover, this family clearly converges pointwise on the dense subset $\Delta_R\cap E$ where $E=\Span\{\chi_{\{y\}}:y\in Y\}\times \Span\{\chi_{\{x\}}:x\in X\}$. Therefore, by Prop. \ref{lb511}, $\lim_{A,B}\omega_{A,B}$ converges pointwise on each $\Delta_R$ to some continuous function.\footnote{This part is similar to the proof of Pb. \ref{lb619}-3, which uses Prop. \ref{lb520}, a special case of Prop. \ref{lb511}.} By considering all $R$, we get the desired function $\omega$ on $l^2(Y)\times l^2(X)$.  \hfill\qedsymbol
\end{rem}

\begin{srem}\label{lb639}
Soon after Hilbert's work, in 1906, Hellinger and Toeplitz simplified Hilbert's boundedness condition by proving that $\sup_{A,B}\Vert\omega_{A,B}\Vert<+\infty$ iff $\sup_{A,B}|\omega_{A,B}(\xi|\psi)|<+\infty$ for all $\xi\in l^2(Y),\psi\in l^2(X)$. Similar to Rem. \ref{lb638}, this result is nowadays proved using Baire's category theorem (or its consequence, the Banach-Steinhaus theorem). To my knowledge, this \textbf{Hellinger-Toeplitz theorem} (in its original form) is one of the very few theorems before Riesz's work that made full use of the \textit{completeness} of $l^2(X)$ without adopting the viewpoint of linear operators (cf. \cite[Sec. 6.4]{Die-H}).
\end{srem}




\begin{comment}
 Moreover, one wants $\omega$ to be continuous, which means (by the continuity of each $\omega_{A,B}$) that
\begin{align}\label{eq268}
\lim_{\xi\rightarrow\xi_0,\eta\rightarrow\eta_0}\lim_{A,B}\omega_{A,B}(\xi|\eta)=\lim_{A,B}\lim_{\xi\rightarrow\xi_0,\eta\rightarrow\eta_0}\omega_{A,B}(\xi|\eta)
\end{align}
\end{comment}

\subsubsection{Direct sums of Hilbert spaces}


A tip for solving the problems in this subsection: Use cleverly Prop. \ref{lb500} or Prop. \ref{lb520} or other ``reduction to densely-spanning subsets" tricks to simplify the proof. (We have already used such tricks in the solution of Pb. \ref{lb619}.)


\begin{prob}
Let $(\mc H_i)_{i\in I}$ be a family of Hilbert spaces. Recall that elements of $\dps\prod_{i\in I}\mc H_i$ are of the form $\xi_\blt=(\xi_i)_{i\in I}$ where $\xi_i\in\mc H_i$. Then $\dps\prod_{i\in I}\mc H_i$ is a vector space whose linear structure is defined componentwise. Define 
\begin{align}
\bigoplus_{i\in I}\mc H_i=\Big\{(\xi_i)_{i\in I}\in\prod_{i\in I}\mc H_i:\sum_{i\in I}\Vert\xi_i\Vert^2<+\infty  \Big\}
\end{align}
equipped with the inner product
\begin{align}\label{eq270}
\bk{\xi_\blt|\eta_\blt}=\bk{(\xi_i)_{i\in I}|(\eta_i)_{i\in I}}=\sum_{i\in I}\bk{\xi_i|\eta_i}
\end{align}
Prove that the rightmost term of \eqref{eq270} converges (absolutely). Prove that the inner product space $\bigoplus_{i\in I}\mc H_i$ is complete. (So $\bigoplus_{i\in I}\mc H_i$ is a Hilbert space, called the \textbf{(Hilbert space) direct sum}\index{00@Hilbert space direct sums} of $(\mc H_i)_{i\in I}$.) An element $(x_\blt)$ in $\bigoplus_{i\in I}\mc H_i$ is also written as $\oplus_{i\in I}\xi_i$.
\end{prob}






\begin{prob}
Let $(\mc H_i)_{i\in I}$ be a family of Hilbert spaces. For each $i\in I$, choose $T_i\in\fk L(\mc H_i)$. Assume that
\begin{align}
\sup_{i\in I}\Vert T_i\Vert<+\infty
\end{align} 
Prove that there is a unique bounded linear map $T$ on $\mc H:=\bigoplus_{i\in I}\mc H_i$ such that for each $\oplus_i\xi_i\in\mc H$,
\begin{align}
T(\oplus_i\xi_i)=\oplus_i (T_i\xi_i)
\end{align}
We write $T=\oplus_{i\in I}T_i$ and call it the \textbf{direct sum} \index{00@Direct sum of bounded linear operators} of $(T_i)_{i\in I}$. Prove that
\begin{align}
(\oplus_i T_i)^*=\oplus_i(T_i^*)
\end{align}
\end{prob}

\begin{prob}\label{lb633}
Let $(\mc H_i)_{i\in I}$ be a mutually orthogonal family of closed linear subspaces of $\mc H$. Assume that this family span a dense subspace of $\mc H$. Prove that there is a unitary map
\begin{gather}
\begin{gathered}
\Phi:\bigoplus_{i\in I}\mc H_i\xlongrightarrow{\simeq}\mc H\qquad \oplus_{i\in I}\xi_i\mapsto\sum_{i\in I}\xi_i
\end{gathered}
\end{gather}
\end{prob}
\begin{proof}[Hint]
Once you have proved that $\Phi$ is a linear isometry, to prove that $\Phi$ is surjective, you only need to show that $\Phi$ has dense range. (Why?) 
\end{proof}


\begin{prob}\label{lb659}
In Pb. \ref{lb633}, choose $T\in\fk L(\mc H)$ such that each $\mc H_i$ is \textbf{$T$-invariant}, i.e., $T\mc H_i\subset\mc H_i$. Thus, the restriction of $T$ to each $\mc H_i$ gives $T_i\in\fk L(\mc H_i)$. Clearly $\sup_i\Vert T_i\Vert\leq \Vert T\Vert<+\infty$. Prove that the following diagram commutes:
\begin{equation}
\begin{tikzcd}[column sep=large]
\bigoplus_{i\in I}\mc H_i\arrow[r,"\oplus_i T_i"] \arrow[d,"\simeq","\Phi"'] & \bigoplus_{i\in I}\mc H_i \arrow[d,"\Phi","\simeq"'] \\
 \mc H \arrow[r,"T"]           &  \mc H          
\end{tikzcd}
\end{equation}
In other words, $\Phi$ implements a \textbf{unitary equivalence} of $\oplus_i T_i$ and $T$. \index{00@Unitary equivalence of operators}
\end{prob}



\newpage


\section{The birth of Hilbert spaces}\label{lb672}

\subsection{From Dirichlet problems to integral equations}\label{lb668}

Around 1906, the Hilbert space $l^2(\Zbb)$ was introduced by Hilbert and Schmidt to the study of integral equations. As mentioned in the previous chapter, the main analytic properties of $l^2(\Zbb)$ attracting Hilbert and Schmidt are the two key properties studied in Sec. \ref{lb640} and \ref{lb641} respectively: 1. The norm convergence (equivalently, the weak convergence, by Prop. \ref{lb635}) of $\sum_i a_ie_i$ when $\sum_i|a_i|^2<+\infty$ and $(e_i)_{i\in I}$ is orthonormal. 2. The weak compactness of the closed unit ball. 

\subsubsection{Dirichlet problems}


The goal of this chapter is to learn how Hilbert and Schmidt used $l^2(\Zbb)$ and the above mentioned properties to study integral equations. A main source of integral equations comes from solving the \textbf{Poisson equation with Dirichlet boundary condition}:
\begin{itemize}
\item Let $\ovl\Omega$ be a compact region in $\Rbb^N$ with interior $\Omega$ and smooth boundary $\partial\Omega$.\footnote{Technically speaking, $\ovl\Omega$ is a compact smooth $n$-dimensional submanifold of $\Rbb^N$ with boundary} Given good enough (say $C^r$, or $C^\infty$) functions $g$ on $\partial\Omega$ and $\varphi$ on $\Omega$, find good enough $u$ on $\Omega$ satisfying
\begin{gather}\label{eq271}
-\Delta u|_\Omega=\varphi\qquad u|_{\partial\Omega}=g
\end{gather}
Here, $\Delta$ is the Laplacian $\partial_1^2+\cdots+\partial_N^2$.
\end{itemize}
In the following, we abbreviate $-\Delta u|_\Omega$ to $-\Delta u$.

The Tietze type extension gives a good $\wtd g$ on $\ovl\Omega$ with $\wtd g|_{\partial\Omega}=g$. Then \eqref{eq271} becomes $-\Delta(u-\wtd g)=\Delta\wtd g$ with $(u-\wtd g)|_{\partial\Omega}=0$. Therefore, \eqref{eq271} can be reduced to the special case
\begin{align}\label{eq272}
-\Delta u=\varphi\qquad u|_{\partial\Omega}=0
\end{align}
By replacing $\ovl\Omega$ with a more general smooth compact manifold with (or without) boundary, this problem, and also the \textbf{Helmholtz equation}
\begin{align}\label{eq273}
-\Delta u=\lambda u\qquad u|_{\partial\Omega}=0
\end{align}
have wide applications in differential geometry and in other types of differential equations. (For example, let $v=v(t,x_1,\dots,x_N)$, then the \textbf{heat equation} $\partial_t v=\Delta v$ and the \textbf{wave equation} $\partial_t^2v=\Delta v$ can be solved by $v=\sum_je^{-\lambda_jt}u_j$ and $v=\sum_j e^{\pm\im\sqrt{\lambda_j}t}u_j$ where $-\Delta u_j=\lambda_ju_j$.)


\subsubsection{Compact operators (i.e. completely continuous operators)}\label{lb667}

The following is roughly the modern treatment of the \textbf{Dirichlet problem} \eqref{eq272}: Let $L^2(\ovl\Omega)$ be the Hilbert space of Lebesgue integrable functions $u:\ovl\Omega\rightarrow\Cbb$ satisfying $\int_{\ovl\Omega} |u|^2<+\infty$. The inner product is given by $\bk{u|v}=\int_{\ovl\Omega} uv^*$. One shows that $-\Delta$, defined on a suitable dense linear subspace of $L^2(\ovl\Omega)$, has a bounded inverse $T\in\fk L(L^2(\ovl\Omega))$ satisfying $\bk{T\xi|\xi}\geq0$ for all $\xi\in L^2(\ovl\Omega)$.\footnote{In the case that ${\ovl\Omega}$ is a compact manifold without boundary, $-\Delta$ should be replaced by $C-\Delta$ where $C>0$.} (Namely, $T$ is a bounded \textbf{positive operator}.) Thus, the problem \eqref{eq272} has solution $u=T\varphi$ in $L^2(\ovl\Omega)$. An analysis of regularity (with the help of Sobolev spaces) then shows that $u$ is good enough.


Moreover, one shows that $T$ is a compact operator (equivalently, a completely continuous operator, cf. Def. \ref{lb653}):
\begin{df}
A linear map $T:V\rightarrow W$ of Banach spaces is called \textbf{compact} if $T(\ovl B_V(0,1))$ is precompact in $W$ (under the norm topology).
\end{df}
According to the \textbf{Hilbert-Schmidt theorem} (to be learned in this chapter), $T$ has countably many eigenvalues $\lambda_1\geq\lambda_2\geq\cdots\geq0$ satisfying $\lim_n\lambda_n=0$, and the (normalized) eigenvectors of $T$ form an orthonormal basis of $L^2(\ovl\Omega)$. The regularity analysis also shows that the eigenvectors of $T$ are smooth. Thus, the eigenvalue problem \eqref{eq273} can be fully solved.

We refer the interested readers to \cite[Ch. 6]{Eva}, \cite{Tay} (especially Sec. 5.1), \cite{Yu} (especially Sec. 78-80) for a detailed study of this topic.

\subsubsection{From Dirichlet problems to integral equations}


In the days of Hilbert and Schmidt, the Dirichlet problem \eqref{eq271} was understood in a different way by transforming it to a problem about integral equations. It was in the process of solving these integral equations that the Hilbert space $l^2(\Zbb)$ was discovered. In the following, I will sketch how to transform Dirichlet problems to integral equations. Details can be found in \cite[Sec. 3.3]{Sim-O} (from which I learned this topic) or \cite[Sec. 81]{RN}.

Set $x=(x_1,\dots,x_N)$. Without imposing the boundary condition $u|_{\partial\Omega}$, and assuming that the $\varphi$ in $-\Delta u=\varphi$ can be extended to a good function on $\Rbb^N$ with compact support, then $-\Delta u=\varphi$ has a solution
\begin{align*}
u(x)=(\Phi*\varphi)(x)=\int_{\Rbb^N}\Phi(x-y)\varphi(y)dy
\end{align*} 
where $\Phi$, called the \textbf{fundamental solution}, is defined by
\begin{align}
\Phi(x)=\left\{
\begin{array}{ll}
\dps-\frac 1{2\pi}\log\Vert x\Vert&\text{ if }N=2\\
\dps\frac 1{(N-2)\sigma_{N-1}\cdot\Vert x\Vert^{N-2}}&\text{ if }N\geq3
\end{array}
\right.
\end{align}
where $\sigma_{N-1}=2\pi^{\frac N2}\Gamma(N/2)^{-1}$ is the volume of $\Sbb^{N-1}=\{x\in\Rbb^N:\Vert x\Vert=1\}$. $\Delta\Phi$ is the delta function at $0$, in particular, it is zero outside $0$. Cf. \cite[Sec. 2.2.1]{Eva}.

By replacing $u$ with $u-\Phi*\varphi$, \eqref{eq271} is transformed to the \textbf{harmonic equation} (with Dirichlet boundary condition)
\begin{align}\label{eq274}
-\Delta u=0\qquad u|_{\partial\Omega}=g
\end{align}
Write $\nabla v=(\partial_1v,\dots,\partial_Nv)$ for each differentiable function $v$ on subsets of $\Rbb^N$. For each $y\in\partial\Omega$ we let $\mbf n_{y}$ be the outward pointing unit vector at $y$ orthogonal to (the tangent space of) $\partial\Omega$ at $y$. Define $G(x,y)$ for each $x\in\Rbb^N,y\in \partial\Omega$ by
\begin{align}\label{eq277}
G(x,y)=\bigbk{(\nabla\Phi)(x-y),\mbf n_{y}} =\frac{\bk{y-x,\mbf n_{y}}}{\sigma_{N-1}\Vert x-y\Vert^N} 
\end{align}
For each $x\in\Rbb^N$ and continuous function $f$ on $\partial\Omega$, define $(\mc Df)(x)=\int_{\partial\Omega}f(y)(\nabla\Phi)(x-y)\cdot d\Sbf$ where the RHS is a ``surface integral of second type", i.e.
\begin{align}
(\mc D f)(x)=\int_{\partial\Omega}G(x,y)f(y)dy
\end{align}
When $x\in\partial\Omega$, this is an improper integral, which converges because
\begin{align}\label{eq276}
G(x,y)\sim \Vert x-y\Vert^{2-N}
\end{align}
when $x\in\partial\Omega$ approaches $y$. (Note that $\int_U\Vert y\Vert^{2-N}dy$ converges if $U\subset\Rbb^{N-1}$ is a bounded neighborhood of $0$.)

$\mc Df$ is not continuous at the points of $\partial\Omega$. However, if we define $u:{\ovl\Omega}\rightarrow\Cbb$ by
\begin{align}\label{eq275}
u(x)=\left\{
\begin{array}{ll}
(\mc Df)(x)&\text{ if }x\in\Omega\\
f(x)+(\mc Df)(x)&\text{ if }x\in\partial\Omega
\end{array}
\right.
\end{align}
then $u$ is continuous on ${\ovl\Omega}$. Moreover, $\Delta u=0$ on $\Omega$ since  $\Delta G(x,y)=0$ when $x\neq y$. Therefore, the Dirichlet problem \eqref{eq274} can be solved if there exists a good function $f$ on $\partial\Omega$ such that $f+\mc Df|_{\partial\Omega}=g$.

To summarize, define an integral operator $T$ on the space $L^2(\partial\Omega)$ of Lebesgue square integrable functions by
\begin{align}
(Tf)(x)=\int_{\partial\Omega}G(x,y)f(y)dy
\end{align}
The equation \eqref{eq274} has solution \eqref{eq275} if there is an (at least continuous) $f:\partial\Omega\rightarrow\Cbb$ satisfying $f+Tf=g$. The problem is then reduced to finding such $f$ for a given $g$.


\subsubsection{Summary of the problem of integral equations}\label{lb642}


Given a function $g$ on $\partial\Omega$, we want to find a function $f$ on $\partial\Omega$ satisfying
\begin{subequations}\label{eq282}
\begin{align}
f+Tf=g
\end{align}
Moreover, we shall consider the case that $N=2$ and hence $\dim\partial\Omega=1$. Let us assume that $\partial\Omega$ has only one connected component so that $\partial\Omega\simeq\Sbb^1$. So we view $f,g$ also as $2\pi$-periodic functions with $L^2$-norms $\Vert f\Vert=\sqrt{\frac 1{2\pi}\int_{-\pi}^\pi |f|^2}$ and $\Vert g\Vert=\sqrt{\frac 1{2\pi}\int_{-\pi}^\pi |g|^2}$. Then
\begin{align}
(Tf)(x)=\frac 1{2\pi}\int_{-\pi}^\pi K(x,y)f(y)dy
\end{align}
\end{subequations}
where $K(x,y)=G(x,y)\cdot \lambda(y)\lambda(x)$ for some $\lambda\in C^\infty(\Sbb^1,\Rbb)$ defined by the change of variable from $\partial\Omega$ to $\Sbb^1$ (similar to the function $\Phi'$ in \eqref{eq278}).  


Since $G$ takes real values, so does $K$. \eqref{eq276} suggests that $K$ is uniformly bounded on $\Sbb^1\times\Sbb^1$. Indeed, $K$ is continuous (cf. \cite[Sec. 81]{RN}). However, we will only need the weaker fact that $K\in L^2(\Sbb^1\times\Sbb^1)$, i.e.
\begin{align}
\Vert K\Vert_{L^2}=\frac 1{(2\pi)^2}\int_{-\pi}^\pi\int_{-\pi}^\pi|K(x,y)|^2dydx<+\infty
\end{align}


In finite-dimensional linear algebra, we know that a linear operator is surjective iff it is injective. If this is the case for $1+T$, then solving $(1+T)f=g$ can be reduced to the easier task of proving that $-1$ is not an eigenvalue of $T$. However, there are bounded linear operators on infinite dimensional function spaces that are injective but not surjective: Consider the right translation operator on $l^2(\Nbb)$, which is the bounded linear operator sending each $\chi_{\{n\}}$ to $\chi_{\{n+1\}}$.

%In the present case, it is not hard to check (using a general argument for Hilbert spaces) that if $\Ker(1+T)=0$ then $1+T$ has dense range in $L^2(\Sbb^1)$. However, it is not clear why $1+T$ must be surjective and, in particular, why $1+T$ must have closed range. %(Indeed, many bounded linear operators on Hilbert spaces are injective but do not have closed ranges.)

To understand the behavior of $T$, and to solve other problems leading to integral equations, it turns out that we must have a good understanding of the eigenvalues and eigenvectors of $T$. As soon as we understand the eigenvalue problem of $T$ very well, we can prove the theorem of \textbf{Fredholm alternative}, which says that $\Ker(1+T)=0$ iff $1+T$ is surjective. 




\subsection{Discretizing the integral equations}\label{lb669}

Fix $K\in L^2(\Sbb^1\times\Sbb^1)$. (The readers can assume for simplicity that $K$ is continuous, which is often the case when $K$ is defined by the 2d Dirichlet problem as in Subsec. \ref{lb642}.) Define the integral operator $T$ sending each $2\pi$-periodic function $f$ to $Tf$, where 
\begin{align}
(Tf)(x)=\frac 1{2\pi}\int_{-\pi}^\pi K(x,y)f(y)dy
\end{align}
$K$ is called the \textbf{kernel} of $T$. Our goal is to understand the eigenvalues and eigenvectors of $T$.

Hilbert's idea is to use Fourier series. Functions on $\Sbb^1\times\Sbb^1$ have Fourier series of the form $\sum_{m,n}a_{m,n}e^{\im(nx+my)}$ in a similar way as functions on $\Sbb^1$. Therefore, for each $m,n\in\Zbb$, define the Fourier coefficient
\begin{align}\label{eq283}
\wht K(m,n)=\bk{Te_n|e_m}=\frac 1{(2\pi)^2}\int_{-\pi}^\pi\int_{-\pi}^\pi K(x,y)e^{\im (nx-my)}dxdy
\end{align}
Then, by Thm. \ref{lb595}, we get $f=\sum_n \wht f_ne_n$ and $g=\sum_mn \wht g_me_m$, and hence
\begin{align*}
\bk{Tf|g}=\sum_{m,n}\wht K(m,n)\wht f(n)\ovl{\wht g(m)}=\bk{\wht T\wht f|\wht g}
\end{align*}
if we view $\wht f,\wht g$ as in $l^2(\Zbb)$ and define a linear map $\wht T:l^2(\Zbb)\rightarrow l^2(\Zbb)$ by
\begin{align}\label{eq284}
\tcboxmath{(\wht T\wht f)(m)=\sum_{n\in\Zbb} \wht K(m,n)\wht f(n)}
\end{align}
By applying Parseval's identity to $K$, we obtain $\sum_{m,n}|\wht K(m,n)|^2=(2\pi)^{-2}\int\int K(x,y)dxdy$, and hence, by $K\in L^2(\Sbb^1\times\Sbb^1)$, we have
\begin{align}
\wht K\in l^2(\Zbb\times\Zbb)
\end{align}
Thus, to understand the original integral equation, one must first understand the equation
\begin{align}
\wht f+\wht K\wht f=\wht g
\end{align}
%where $\wht K\in l^2(\Zbb\times\Zbb)$ and $\wht g\in l^2(\Zbb)$ are given.


In fact, at this point, Hilbert did not have the notion of $l^2(\Zbb)$ and $L^2$ yet. However, the idea of transforming the eigenvalue problem about $T$ to that about the \textit{matrix} $\wht K$ without introducing $l^2(\Zbb)$ is conceivable. After all, almost every mathematical progress leaves some questions about rigor until the end.

\begin{rem}
Parseval's identity $\frac 1{2\pi}\int_{-\pi}^\pi |f|^2=\sum_n |\wht f(n)|^2$ is well-known by the time Hilbert studied integral equations around 1906. However, Parseval's identity is far from enough to motivate the notion of $l^2(\Zbb)$: Unlike the completeness of $l^2(\Zbb)$ or the weak compactness of $\ovl B_{l^2(\Zbb)}(0,1)$, Parseval's identity is a property about individual functions, not about the set of all functions \textit{and their interactions}. 
%Knowing that \textit{some} functions on $\Zbb$ are square summable does not mean that the set of \textit{all} square summable functions should be studied. To have the motivation to move from some functions satisfying certain properties to the \emph{space} of all functions satisfying these properties, there must be a desire to use some algebraic/geometric/analytic properties about the entire space but not about the particular functions. In the case of the space $l^2(\Zbb)$, the weak compactness of the closed unit ball is about the set of all functions (and their interaction), but Parseval's identity is only stated for individual functions.
\end{rem}


In the remaining sections, we will forget the original function $K$ on $\Sbb^1\times\Sbb^1$, as Hilbert did, and focus on the matrix $\wht K\in l^2(\Zbb\times\Zbb)$. Thus, we will let $K$ denote $\wht K$, or denote a general function $X\times Y\rightarrow\Cbb$ where $X,Y$ are sets. 



\subsection{Hilbert's complete continuity}

We fix Hilbert spaces $\mc H$ and $\mc K$. Let $X,Y$ be sets. For each linear map $T:\mc H\rightarrow\mc K$ we let \index{zz@$\omega_T$, the sesquilinear map for $T$}
\begin{gather}\label{eq280}
\omega_T:\mc H\times\mc K\rightarrow\Cbb\qquad \omega_T(\xi|\eta)=\bk{T\xi|\eta}
\end{gather}

In his important 1906 paper, Hilbert introduced two crucial analytic properties about sesquilinear forms. The first one is boundedness (i.e., the condition $M<+\infty$ in Pb. \ref{lb619}), which is equivalent to the boundedness of linear maps between Hilbert spaces. See Subsec. \ref{lb643} for details. In particular, we have the obvious fact
\begin{align}
T\in\fk L(\mc H,\mc K)\qquad\Rightarrow\qquad \omega_T\text{ is continuous}
\end{align}

The second condition is stronger than the first one (cf. \cite[p.147]{Hil12}): A sesquilinear $\omega:\mc H\times\mc K\rightarrow\Cbb$ is called \textbf{completely continuous} if $\omega$ is weakly continuous when restricted to $\ovl B_{\mc H}(0,1)\times\ovl B_{\mc K}(0,1)$. It is easy to adapt this definition to linear maps:

\begin{df}\label{lb653}
A linear map $T:\mc H\rightarrow\mc K$ is called \textbf{completely continuous}\index{00@Completely continuous} if $\omega_T$ is continuous on $\ovl B_{\mc H}(0,1)\times\ovl B_{\mc K}(0,1)$ where $\ovl B_{\mc H}(0,1)$ and $\ovl B_{\mc K}(0,1)$ are equipped with their weak topologies.
\end{df}

Thus, complete continuity means that if $(\xi_\alpha)$ converges weakly in $\ovl B_{\mc H}(0,1)$ to $\xi$ and  $(\eta_\alpha)$ converges weakly in $\ovl B_{\mc K}(0,1)$ to $\eta$, then $\lim_\alpha \bk{T\xi_\alpha|\eta_\alpha}=\bk{T\xi|\eta}$. 




Completely continuous sesquilinear forms are clearly continuous (i.e., bounded, cf. Pb. \ref{lb627}). Thus, it is not hard see:

\begin{lm}\label{lb645}
A completely continuous linear map $T:\mc H\rightarrow\mc K$ is bounded. 
\end{lm}
\begin{proof}
$\omega_T$ is weakly continuous and hence continuous on $\ovl B_{\mc H}(0,1)\times\ovl B_{\mc K}(0,1)$. In particular, $\omega_T$ is continuous on $(0,0)$. Thus, there exists $\eps>0$ such that $|\omega_T(\xi|\eta)|\leq 1$ whenever $\Vert\xi\Vert\leq\eps,\Vert\eta\Vert\leq \eps$. Thus, whenever $\Vert\xi\Vert\leq 1,\Vert\eta\Vert\leq 1$, we have
\begin{align*}
|\omega_T(\xi|\eta)|=\eps^{-2}|\omega_T(\eps\xi|\eps\eta)|\leq \eps^{-2}
\end{align*}
By Prop. \ref{lb624}, we have $\Vert T\Vert\leq\eps^{-2}<+\infty$.
\end{proof}

%\begin{rem}
%Though Lem. \ref{lb645} looks elementary, we will not need this lemma. In fact, all the completely continuous operators we will encounter are easily verified to be bounded. Thus, we do not lose much if we \textit{define} a completely continuous map $T$ to be a \textit{bounded} linear map $T$ such that $\omega_T$ is weakly continuous on $\ovl B_{\mc H}(0,1)\times\ovl B_{\mc K}(0,1)$.
%\end{rem}


\begin{exe}
Show that a finite linear combination of completely continuous operators is continuous. Show that the adjoint of a completely continuous operator is completely continuous. Show that if $T\in\fk L(\mc H_1,\mc H_2)$ and $S\in\fk L(\mc H_2,\mc H_3)$ where each $\mc H_i$ is a Hilbert space, and if one of $T$ and $S$ is completely continuous, then $ST:\mc H_1\rightarrow\mc H_3$ is completely continuous. In particular, the set of completely continuous operators on $\mc H$ is a two-sided ideal of $\fk L(\mc H)$.
\end{exe}


The easiest examples of completely continuous maps are finite-rank operators:

\begin{df}
A linear map of vector spaces is said to have \textbf{finite rank} if its range is finite-dimensional. \index{00@Finite rank operator}
\end{df}

\begin{pp}\label{lb644}
Let $T\in\fk L(\mc H,\mc K)$. The following are equivalent:
\begin{enumerate}
\item[(1)] $T$ has finite rank.
\item[(2)] There exist finitely many vectors $\mu_1,\dots,\mu_n\in\mc H$ and $\eta_1,\dots,\eta_n\in\mc K$ such that for all $\eta\in\mc H$,
\begin{align}
T\xi=\sum_{i=1}^n \bk{\xi|\mu_i}\eta_i
\end{align} 
\end{enumerate} 
\end{pp}

Note that without assume that $T$ is bounded, (1) does not imply (2).

\begin{proof}
``(2)$\Rightarrow$(1)" is obvious. Assume (1). Restrict the inner product of $\mc K$ to the finite-dimensional $V=T(\mc H)$. By Gram-Schmidt, $V$ is spanned by an orthonormal set of vectors $\eta_1,\dots,\eta_n$. For each $1\leq i\leq n$, the linear map $\xi\in\mc H\mapsto \bk{T\xi|\eta_i}\in\Cbb$ is bounded. Thus, by Riesz-Fr\'echet, there exists $\mu_i\in\mc H$ such that $\bk{T\xi|\eta_i}=\bk{\xi|\mu_i}$ for all $\xi\in\mc H$. By Thm. \ref{lb595}, we have
\begin{align*}
T\xi=\sum_i \bk{T\xi|\eta_i}\eta_i=\sum_i\bk{\xi|\mu_i}\eta_i
\end{align*}
\end{proof}


\begin{eg}
If $T\in\fk L(\mc H,\mc K)$ has finite rank, then $T$ is completely continuous.
\end{eg}

\begin{proof}
By Prop. \ref{lb644}, and by linearity, it suffices to assume that $T$ takes the form $T\xi=\bk{\xi|\mu}\eta$ for some $\mu\in\mc H,\eta\in\mc K$. Then $\omega_T(\xi|\psi)=\bk{\xi|\mu}\bk{\eta|\psi}$ is clearly weakly continuous with respect to $\xi$ and $\psi$.
\end{proof}

In order to have nontrivial examples of completely continuous operators, we need the following simple fact:

\begin{thm}\label{lb647}
Let $(T_\alpha)$ be a net of completely continuous maps $\mc H\rightarrow\mc K$. Assume that $T:\mc H\rightarrow\mc K$ is linear and $\lim_\alpha\Vert T-T_\alpha\Vert=0$. Then $T$ is completely continuous.
\end{thm}

\begin{proof}
By Cor. \ref{lb646}, $\omega_{T_\alpha}$ converges uniformly on $\ovl B_{\mc H}(0,1)\times\ovl B_{\mc K}(0,1)$ to $\omega_T$. Since the uniform limit of a net of continuous functions is continuous, we conclude that $\omega_T$ is weakly continuous on $\ovl B_{\mc H}(0,1)\times\ovl B_{\mc K}(0,1)$.
\end{proof}


Motivated by the above theorem, we make the following definition:
\begin{df}
We say that a linear map $T:\mc H\rightarrow\mc K$ is \textbf{approximable} \index{00@Approximable} if there exists a net (equivalently, a sequence) of finite-rank operators $(T_\alpha)$ in $\fk L(\mc H,\mc K)$ such that $\lim_\alpha \Vert T-T_\alpha\Vert=0$. Approximable operators are clearly bounded.
\end{df}


\begin{thm}\label{lb648}
Let $T:\mc H\rightarrow\mc K$. Then $T$ is approximable iff $T$ is completely continuous.
\end{thm}



\begin{proof}[$\star$ Proof]
Since finite-rank bounded linear operators are completely continuous, by Thm. \ref{lb647}, approximability implies complete continuity. Conversely, assume that $T$ is completely continuous. Assume WLOG that $\mc H=l^2(Y)$ and $\mc K=l^2(X)$. For each $A\in\fin(2^X)$ and $B\in\fin(2^Y)$ we set
\begin{align}\label{eq281}
T_{A,B}:l^2(Y)\rightarrow l^2(X)\qquad T_{A,B}\xi=\chi_A\cdot T(\chi_B\xi)
\end{align}
We claim that $\lim_{A,B}\Vert T-T_{A,B}\Vert=0$.

By Cor. \ref{lb646}, we need to show that $(\omega_{T_{A,B}})_{A\in\fin(2^X),B\in\fin(2^Y)}$ converges uniformly on $\Omega=\ovl B_{l^2(Y)}(0,1)\times\ovl B_{l^2(X)}(0,1)$ to $\omega_T$. We equip $\Omega$ with the product weak topology, which is compact by Cor. \ref{lb636}. Since each $\omega_{T_{A,B}}$ is (weakly) continuous, by Thm. \ref{lb277} and Prop. \ref{lb281}, it suffices to prove that
\begin{align*}
\lim_{A,B,\xi',\eta'} |\omega_T(\xi'|\eta')-\omega_{T_{A,B}}(\xi'|\eta')|=|\omega_T(\xi|\eta)-\omega_T(\xi|\eta)|=0
\end{align*}
where $\xi'$ converges weakly to $\xi$ and $\eta'$ converges weakly to $\eta$. Equivalently, it suffices to prove that for each net $(\xi_i,\eta_i)_{i\in I}$ in $\Omega$ converging to $(\xi,\eta)\in\Omega$, we have
\begin{align*}
\lim_{A,B,i} |\bk{(T-T_{A,B})\xi_i|\eta_i}|=0
\end{align*}
Clearly $\lim \bk{T\xi_i|\eta_i}=\bk{T\xi|\eta}$ since $T$ is completely continuous. Note that $\bk{T_{A,B}\xi_i|\eta_i}=\bk{T\chi_B\xi_i|\chi_A\eta_i}$. It remains to show that this expression also converges to $\bk{T\xi|\eta}$. Since $T$ is completely continuous, it suffices to prove that $\lim_{B,i}{\chi_B\xi_i}$ converges weakly to $\xi$ and $\lim_{A,i}{\chi_A\eta_i}$ converges weakly to $\eta$.

By Thm. \ref{lb530}, we know for each $y\in Y$ that  $\lim_i\xi_i(y)=\xi(y)$ and hence $\lim_{B,i}\chi_B(y)\xi_i(y)=\xi(y)$. By Thm. \ref{lb530} again, we conclude that $\lim_{B,i}{\chi_B\xi_i}$ converges weakly to $\xi$. The second (weak) limit can be proved in the same way.
\end{proof}

\begin{rem}
Since the proof of the direction ``$\Leftarrow$" in Thm. \ref{lb648} is slightly more complicated (although the main idea is clear), we will not use this direction in the future. Note that the direction ``$\Leftarrow$" relies on the weak compactness of the unit balls, whereas the easy direction ``$\Rightarrow$" does not.
\end{rem}


\begin{rem}
In the proof of Thm. \ref{lb648} we have shown that if $T:l^2(Y)\rightarrow l^2(X)$ is completely continuous, then
\begin{align}
\lim_{A\in\fin(2^X),B\in\fin(2^Y)}\Vert T-T_{A,B}\Vert=0
\end{align}
In contrast, if $T$ is only bounded, then it is easy to see that $\lim_{A,B}T_{A,B}$ converges pointwise to $T$.
\end{rem}



\subsection{Hilbert-Schmidt operators}


Let $X$ and $Y$ be sets. Recall Pb. \ref{lb649} for the basic facts about matrix representations.

\begin{thm}\label{lb651}
Let $K\in l^2(X\times Y)$. Then $K$ is the matrix representation of a (necessarily unique) completely continuous $T:l^2(Y)\rightarrow l^2(X)$. Moreover, we have
\begin{align}
\Vert T\Vert\leq \Vert K\Vert_{l^2}
\end{align}
\end{thm}

Such $T$ is called a \textbf{Hilbert-Schmidt operator}. \index{00@Hilbert-Schmidt operator} See Pb. \ref{lb650} for the general definition of Hilbert-Schmidt operators.




\begin{lm}\label{lb652}
Thm. \ref{lb651} is true when $X,Y$ are finite sets.
\end{lm}

\begin{proof}
The only nontrivial part is $\Vert T\Vert\leq \Vert K\Vert_{l^2}$. By Prop. \ref{lb624}, it suffices to prove for all $f\in \ovl B_{l^2(Y)}(0,1)$ and $g\in\ovl B_{l^2(X)}(0,1)$ that $|\bk{Tf|g}|\leq\Vert K\Vert_2$. But
\begin{align*}
|\bk{Tf|g}|=\Big|\sum_{x,y}K(x,y)f(y)\ovl{g(x)}\Big|=|\bk{K|\Gamma}|\leq \Vert K\Vert_2\cdot\Vert\Gamma\Vert_2
\end{align*}
where $\Gamma:X\times Y\rightarrow\Cbb$ is defined by $\Gamma(x,y)=\ovl{f(y)}g(x)$. Since
\begin{align*}
\Vert\Gamma\Vert_2^2=\sum_{x,y}|f(y)^2g(x)^2|=\sum_y|f(y)|^2\cdot \sum_x|g(x)|^2\leq 1
\end{align*}
we have $|\bk{Tf|g}|\leq\Vert K\Vert_2$.
\end{proof}







\begin{proof}[\textbf{Proof of Thm. \ref{lb651}}]
For each $\Omega\in\fin(2^{X\times Y})$, let $K_\Omega:X\times Y\rightarrow\Cbb$ be $K_\Omega=K\chi_\Omega$. Let $T_\Omega:l^2(Y)\rightarrow l^2(X)$ be the finite rank operator whose matrix representation is $K_\Omega$, namely, 
\begin{align}
T_\Omega\xi=\sum_{(x,y)\in\Omega} K(x,y)\xi(y)\cdot\chi_{\{y\}}
\end{align}
Since $K\in L^2(X\times Y)$, we know that $\lim_\Omega \Vert K-K_\Omega\Vert_2=0$, and hence $(K_\Omega)_{\Omega\in\fin(2^{X\times Y})}$ is a Cauchy net in $l^2(X\times Y)$. For each $\Omega,\Gamma\in\fin(2^{X\times Y})$, since $T_\Omega-T_\Gamma$ has matrix representation $K_\Omega-K_\Gamma$,  by Lem. \ref{lb652} we have
\begin{align*}
\Vert T_\Omega-T_\Gamma\Vert\leq \Vert K_\Omega-K_\Gamma\Vert_{l^2}
\end{align*}
Therefore $\lim_{\Omega,\Gamma}\Vert T_\Omega-T_\Gamma\Vert=0$, i.e., $(T_\Omega)$ is a Cauchy net in $\fk L(l^2(Y),l^2(X))$. By Thm. \ref{lb540}, $\fk L(l^2(Y),l^2(X))$ is complete. So $\lim_\Omega T_\Omega$ converges under the operator norm to some $T\in\fk L(l^2(Y),l^2(X))$.

Clearly $T$ has matrix representation $K$. Since each $T_\Omega$ has finite rank, by Thm. \ref{lb648}, $T$ is completely continuous. Finally, by Lem. \ref{lb652}, $\Vert T_\Omega\Vert\leq \Vert K_\Omega\Vert_2\leq \Vert K\Vert_2$ for all $\Omega$. Taking $\lim_\Omega$, we get $\Vert T\Vert\leq\Vert K\Vert_2$.
\end{proof}

\begin{rem}
In the above proof, we have used the completeness of $\fk L(l^2(Y),l^2(X))$, which relies on the completeness of $l^2(X)$. This seems contradictory to our earlier statement that Hilbert and Schmidt did not rely on the completeness of $l^2$ spaces to study integral equations. In fact, there is no contradiction since Hilbert took the sesquilinear form perspective: One can modify the above proof by showing that for every $R>0$, $\omega_{T_\Omega}$ converges uniformly on $\ovl B_{l^2(X)}(0,R)\times \ovl B_{l^2(X)}(0,R)$ to some function $\omega$. This gives the desired completely continuous sesquilinear form for $T$ without using the completeness of $l^2$ spaces.
\end{rem}



\subsection{The triumph of weak(-*) compactness: Hilbert-Schmidt theorem}


Fix a Hilbert space $\mc H$. For each $T\in\fk L(\mc H)$, let $\omega_T:\mc H\times\mc H\rightarrow\Cbb$ be $\omega_T(\xi|\eta)=\bk{T\xi|\eta}$ as usual. 

Recall from linear algebra that $\lambda\in\Cbb$ is called an \textbf{eigenvalue} \index{00@Eigenvalue} of $T$ if there is a nonzero $\xi\in\mc H$ such that $T\xi=\lambda\xi$. In this case, $\xi$ is called the \textbf{$\lambda$-eigenvector} \index{00@Eigenvector} of $T$.


\subsubsection{Self-adjoint operators and positive operators}


\begin{df}
Let $T\in\fk L(\mc H)$. We say that $T$ is \textbf{self-adjoint} \index{00@Self-adjoint operator} if one of the following conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item $T=T^*$.
\item $\bk{T\xi|\eta}=\bk{\xi|T\eta}$ for all $\xi,\eta\in\mc H$.
\item The sesquilinear form $\omega_T$ is a Hermitian.
\item $\bk{T\xi|\xi}\in\Rbb$ for all $\xi\in\mc H$.
\end{enumerate}
\end{df}

\begin{proof}[Proof of equivalence]
Since we always have $\bk{T\xi|\eta}=\bk{\xi|T^*\eta}$, the equivalence (1)$\Leftrightarrow$(2) is obvious. Recall that $\omega_T$ is Hermitian iff $\omega_T(\xi|\eta)=\ovl{\omega_T(\eta|\xi)}$. But $\ovl{\omega_T(\eta|\xi)}=\ovl{\bk{T\eta|\xi}}=\bk{\xi|T\eta}$, which equals $\bk{T\xi|\eta}$ for all $\xi,\eta$ iff (2) is true. This proves (2)$\Rightarrow$(3). By Prop. \ref{lb588}, we have (3)$\Leftrightarrow$(4).
\end{proof}


\begin{rem}
Let $T\in\fk L(\mc H)$. Assume that $E\subset\mc H$ spans a dense subspace of $\mc H$. By sesquilinearity, and by the continuity of $\bk{\cdot|\cdot}$, $T$ is self-adjoint iff $\bk{T\xi|\eta}=\bk{\xi|T\eta}$ for all $\xi,\eta\in E$. Therefore:
\end{rem}

\begin{eg}\label{lb678}
Let $X$ be a set. Let $T\in\fk(\mc H)$. Let $K:X\times X\rightarrow\Cbb$ be the matrix representation of $T$, i.e., $K(x,y)=\bk{T\chi_{\{y\}}|\chi_{\{x\}}}$ for all $x,y\in Y$.  Then $T$ is self-ajoint iff $K(x,y)=\ovl{K(y,x)}$ for all $x,y\in Y$.
\end{eg}


\begin{proof}
This is because $\{\chi_{\{x\}}:x\in X\}$ spans a dense subspace, and $\ovl{K(y,x)}=\ovl{\bk{T\chi_{\{y\}}|\chi_{\{x\}}}}=\bk{ \chi_{\{x\}}|T\chi_{\{y\}} }$.
\end{proof}


\begin{df}
Let $T\in\fk L(\mc H)$. We say that $T$ is \textbf{positive} \index{00@Positive operator} and write $T\geq0$ if $\bk{T\xi|\xi}\geq0$ for all $\xi\in\mc H$. Positive operators are clearly self-adjoint. More generally, if $S,T\in\fk L(\mc H)$, we write
\begin{align}
S\leq T\qquad\Longleftrightarrow\qquad T-S\geq0
\end{align}
In other words, $S\leq T$ means $\bk{S\xi|\xi}\leq \bk{T\xi|\xi}$ for all $\xi\in\mc H$. Clearly ``$\leq$" is a partial order on $\fk L(\mc H)$.\footnote{Note that if $T\leq S$ and $S\leq T$, then $\omega_{T-S}(\xi|\xi)=0$ for all $\xi$. By the polarization identity \eqref{eq238}, we get $\omega_{T-S}=0$ and hence $T=S$.}
\end{df}

Self-adjoint operators and positive operators are analogous to real-valued functions and positive functions. We will make this analogy precise in the future. At this point, let us see an example of the analogy:



\begin{eg}\label{lb656}
Suppose that $T\in\fk L(\mc H)$ is self-adjoint, and $\lambda\geq\Vert T\Vert$. Then $\lambda+T$ and $\lambda-T$ are positive, i.e., $-\lambda\leq T\leq\lambda$.
\end{eg}

\begin{proof}
Since $\bk{T\xi|\xi}\leq\Vert T\Vert\cdot\bk{\xi|\xi}\leq\lambda\bk{\xi|\xi}$, we get $T\leq\lambda$. Similarly, $-T\leq\lambda$.
\end{proof}




The following two basic facts will be used in the proof of the Hilbert-Schmidt theorem:

\begin{lm}\label{lb655}
Suppose that $T\in\fk L(\mc H)$ is positive. Assume that $\xi\in\mc H$ satisfies $\bk{T\xi|\xi}=0$. Then $T\xi=0$.
\end{lm}

\begin{proof}
Our assumption is $\omega_T(\xi|\xi)=0$. Since the sesquilinear form $\omega_T$ is positive, by Cauchy-Schwarz (cf. Rem. \ref{lb654}), for each $\eta\in\mc H$ we have $|\omega_T(\xi|\eta)|^2\leq \omega_T(\xi|\xi)\omega_T(\eta|\eta)=0$ and hence $\bk{T\xi|\eta}=0$. (Alternatively, one can use \eqref{eq240} to show $\omega_T(\xi|\eta)=0$.) Therefore $T\xi=0$.
\end{proof}


\begin{comment}
Lem. \ref{lb655} gives a simple criterion for eigenvalues of self-adjoint operators.

\begin{eg}
Suppose that $T\in\fk L(\mc H)$ is self-adjoint and $\lambda\geq\Vert T\Vert$. If $\xi\in\mc H$ satisfies $\bk{T\xi|\xi}=\lambda\bk{\xi|\xi}$, then $T\xi=\lambda\xi$. 
\end{eg}
\begin{proof}
Apply Lem. \ref{lb655} to $\lambda-T$, which is positive by Exp. \ref{lb656}.
\end{proof}
\end{comment}


\begin{df}
Let $T\in\fk L(\mc H)$. We say that a linear subspace $\mc K\subset\mc H$ is \textbf{invariant under $T$} \index{00@Invariant subspace} (or simply $T$-invariant) if $T\mc H\subset\mc H$.
\end{df}

\begin{pp}\label{lb658}
Let $T\in\fk L(\mc H)$. Let $\mc K$ be a closed linear subspace of $\mc H$. Suppose that $\mc K$ is invariant under $T$ and $T^*$. Then so is $\mc K^\perp$.
\end{pp}

In particular, if $T$ is self-adjoint and $\mc K$ is $T$-invariant, then $\mc K^\perp$ is $T$-invariant.

\begin{proof}
Let $\eta\in\mc K^\perp$. Then $\bk{T\eta|\mc K}=\bk{\eta|T^*\mc K}\subset \bk{\eta|0}=0$ and $\bk{T^*\eta|\mc K}=\bk{\eta|T\mc K}\subset \bk{\eta|0}=0$. So $T\eta,T^*\eta\in\mc K^\perp$.
\end{proof}


\begin{rem}
Prop. \ref{lb658} gives a simple method of decomposing the action of $T$: If $\mc K$ is invariant under $T,T^*$, then by Pb. \ref{lb659}, $T$ is unitarily equivalent to the ``block diagonal operator" $T|_{\mc K}\oplus T|_{\mc K^\perp}$.
\end{rem}







\subsubsection{The Hilbert-Schmidt theorem}


We first prove the Hilbert-Schmidt theorem for positive operators.


\begin{thm}[\textbf{Hilbert-Schmidt theorem}]\index{00@Hilbert-Schmidt theorem}\label{lb657}
Assume that $T\in\fk L(\mc H)$ is positive and completely continuous. Then $\mc H$ has an orthonormal basis $(e_1,e_2,\dots)\cup(f_j)_{j\in J}$, where the countable family $(e_1,e_2,\dots)$ is possibly finite, such that:
\begin{enumerate}[label=(\alph*)]
\item $Te_n=\lambda_ne_n$ for some $\lambda_n\in\Rbb$, and $Tf_j=0$.
\item $\lambda_1\geq \lambda_2\geq\cdots >0$.
\item If $(e_1,e_2,\dots)$ is infinite, then $\lim_n\lambda_n=0$.
\end{enumerate}  
\end{thm}

Before proving this theorem, we give an interpretation:
\begin{rem}\label{lb663}
In Thm. \ref{lb657}, write $(e_1,e_2,\dots)$ as $(e_i)_{i\in I}$ where $I=\Zbb_+$ or $I=\{1,2,\dots,N\}$ for some $N\in\Zbb_+$. Let $X=I\sqcup J$. Let $\Phi:l^2(X)\rightarrow \mc H$ be the unitary map sending each $\varphi$ to $\sum_{x\in X}\varphi(x)\chi_{\{x\}}$. Then we have a commutative diagram
\begin{equation}
\begin{tikzcd}[column sep=large]
l^2(X)\arrow[r,"\wht T"] \arrow[d,"\simeq","\Phi"'] & l^2(X) \arrow[d,"\Phi","\simeq"'] \\
 \mc H \arrow[r,"T"]           &  \mc H          
\end{tikzcd}
\end{equation}
where $\wht T$ has matrix representation
\begin{align}
\diag(\lambda_1,\lambda_2,\dots,(0)_{j\in J})
\end{align} 
i.e., for each $i\in I,j\in J$, we have $\wht T\chi_{\{i\}}=\lambda_i\chi_{\{i\}}$ and $T\chi_{\{j\}}=0$. A similar description holds in Thm. \ref{lb662}.
\end{rem}

Thm. \ref{lb657} will be proved by finding $e_1,e_2,\dots$ inductively. $(f_j)_{j\in J}$ will be an arbitrary orthonormal basis of $\Ker(T)$. 


\begin{proof}[\textbf{Proof of Thm. \ref{lb657}}]
We make our first simplification by noting that (b) can be weakened to
\begin{enumerate}
\item[(b')]  $\lambda_1\geq\lambda_2\geq\cdots\geq0$.
\end{enumerate}
Then, by moving those of $e_1,e_2,\dots$ with $0$-eigenvalue to the list $(f_j)_{j,\in J}$, the theorem is proved. Moreover, we assume for simplicity that $\mc H$ is infinite dimensional; the finite dimensional case will follow from a similar but easier proof. \\[-1ex]

Step 1. We first explain how to find $\lambda_1,e_1$. Let $\Omega=\ovl B_{\mc H}(0,1)$. Since $T$ is completely continuous, the function $g:\Omega\rightarrow\Rbb_{\geq0}$ defined by $g(\xi)=\omega_T(\xi|\xi)=\bk{T\xi|\xi}$ is weakly continuous. By Cor. \ref{lb636}, $\Omega$ is weakly compact. Therefore, $g$ attains its maximum $\lambda_1=g(e_1)\geq0$ at some $e_1\in\Omega$. It suffices to assume $e_1\neq0$. So $\Vert e_1\Vert=1$. (Otherwise, we have $0<\Vert e_1\Vert<1$ and hence $g$ has a larger value at $e_1/\Vert e_1\Vert$, impossible.)

Since $\bk{T\xi|\xi}\leq\lambda_1$ for any unit vector $\xi\in\mc H$, by (sesqui)linearity, we get
\begin{align}
\bk{T\xi|\xi}\leq\lambda_1\bk{\xi|\xi}\qquad\text{ for all }\xi\in\mc H
\end{align}
This proves $0\leq T\leq\lambda_1$.

Since $\bk{(\lambda_1-T)e_1|e_1}=0$ and $\lambda_1-T\geq0$, by Lem. \ref{lb655}, we get $(\lambda_1-T)e_1=0$. This finishes the construction of $\lambda_1$ and $e_1$.\\[-1ex]

Step 2. Suppose that we have found $\lambda_1\geq\cdots\geq\lambda_n>0$ and orthonormal $e_1,\dots,e_n$ such that $Te_i=\lambda_ie_i$ for all $1\leq i\leq n$, and that
\begin{align}\label{eq279}
\bk{T\xi|\xi}\leq\lambda_n\bk{\xi|\xi}\qquad\text{ for all }\xi\in V_n^\perp
\end{align}
Here, $V_n=\Span\{e_1,\dots,e_n\}$, which is a finite-dimensional Hilbert subspace of $\mc H$. 

Clearly $V_n$ is $T$-invariant. Therefore, by Prop. \ref{lb658}, $V_n^\perp$ is $T$-invariant. (Here we have used $T=T^*$.) Clearly $T|_{V_n^\perp}\geq0$. By the process in Step 1, there exists a unit vector $e_{n+1}\in V_n^\perp$ such $Te_{n+1}=\lambda_{n+1}e_{n+1}$ for some $\lambda_{n+1}\in [0,\lambda_n]$.\\[-1ex]

Step 3. By the inductive process in Step 2, we obtain an (infinite) orthonormal sequence $(e_n)_{n\in\Zbb_+}$ satisfying that $Te_n=\lambda_ne_n$, that $\lambda_1\geq\lambda_2\geq\cdots\geq0$, and that \eqref{eq279} holds for each $n$. We claim that $\lim_n\lambda_n=0$. Suppose this is true. Let $\mc K=\ovl{\Span\{e_1,e_2,\dots\}}$. By \eqref{eq279}, for any vector $\xi\in\mc K^\perp$ we have $\bk{T\xi|\xi}=0$, and hence $T\xi=0$ by Lem. \ref{lb655}. \footnote{One can also use the polarization identity \eqref{eq238} instead of Lem. \ref{lb655} to conclude $T\xi=0$.} So $T|_{\mc K^\perp}=0$. Thus, the proof is finished by choosing $(f_j)$ to be an orthonormal basis of $\mc K^\perp$: By Thm. \ref{lb617}, an orthonormal basis of $\mc H$ can be obtained by taking the union of one of $\mc K$ and one of $\mc K^\perp$.

Suppose that $\lambda=\lim_n\lambda_n$ is $>0$. Then $\lim_n\bk{Te_n|e_n}=\lambda>0$. However, $\lim_n e_n$ converges weakly to $0$ (cf. Exp. \ref{lb661}). Since $T$ is completely continuous, we have $\lim_n\bk{Te_n|e_n}=0$, impossible.
\end{proof}



Thm. \ref{lb657} can be easily generalized to self-adjoint competely continuous operators by slightly weakening condition (b):



\begin{thm}[\textbf{Hilbert-Schmidt theorem}]\index{00@Hilbert-Schmidt theorem}\label{lb662}
Assume that $T\in\fk L(\mc H)$ is self-adjoint and completely continuous. Then $\mc H$ has an orthonormal basis $(e_1,e_2,\dots)\cup(f_j)_{j\in J}$, where the countable family $(e_1,e_2,\dots)$ is possibly finite, such that:
\begin{enumerate}[label=(\alph*)]
\item $Te_n=\lambda_ne_n$ for some $\lambda_n\in\Rbb$, and $Tf_j=0$.
\item $|\lambda_1|\geq |\lambda_2|\geq\cdots >0$.
\item If $(e_1,e_2,\dots)$ is infinite, then $\lim_n\lambda_n=0$.
%\item For each $\lambda\in\Rbb$, the set $\{n:\lambda_n=\lambda\}$ is finite.
\end{enumerate}  
\end{thm}

%Note that (d) is automatic in Thm. \ref{lb657}. However, in the case that the eigenvalues might have different signs, (d) is needed in order to avoid the cases such as $1,-1,1,-1/2,1,-1/3,\dots$. 


\begin{proof}
%Assume for simplicity that $\mc H$ is infinite dimensional. 
As in the proof of Thm. \ref{lb657}, (b) can be weakened to $|\lambda_1|\geq|\lambda_2|\geq\cdots \geq0$. One then produces orthonormal $e_1,e_1',e_2,e_2',\dots$ such that $Te_n=\mu_ne_n$ and $Te_k'=\mu_k' e_k'$ for each $n,k$, that $\mu_1\geq\mu_2\geq\cdots\geq0$ and $\mu_1'\leq\mu_2'\leq\cdots\leq0$, and that
\begin{gather*}
\bk{T\xi|\xi}\leq\mu_n\bk{\xi|\xi}\qquad\text{ if }\xi\perp\{e_1,\dots,e_n\}\\
\mu_k'\bk{\xi|\xi}\leq\bk{T\xi|\xi}\qquad\text{ if }\xi\perp\{e_1',\dots,e_k'\}
\end{gather*}
(To see this, one first finds $e_1$ as in the proof of Thm. \ref{lb657}. Restricting $-T$ to $\{e_1\}^\perp$, one finds $e_1'$. Restricting $T$ to $\{e_1,e_1'\}^\perp$, one finds $e_2$. Restricting $-T$ to $\{e_1,e_1',e_2\}^\perp$, one finds $e_2'$. Repeat this procedure.)
\end{proof}


\begin{rem}
The converse of Thm. \ref{lb662} is also true: If $T\in\fk L(\mc H)$ has an orthonormal basis $(e_1,e_2,\dots)\cup(f_j:j\in J)$ satisfying the description in Thm. \ref{lb662}, then $T$ is self-adjoint and completely continuous. In fact, by Rem. \ref{lb663}, it suffices to prove:
\end{rem}

\begin{exe}\label{lb664}
Let $I=\Zbb_+$ or $\{1,\dots,N\}$. Let $J$ be a set. Let $X=I\sqcup J$. Choose $(\lambda_i)_{i\in I}$ in $\Cbb\setminus\{0\}$ satisfying $\lim_i\lambda_i=0$ if $I=\Zbb_+$. Prove that there is a (necessarily unique) completely continuous $T\in\fk L(l^2(X))$ satisfying $T\chi_{\{i\}}=\lambda_i\chi_{\{i\}}$ for all $i\in I$, and $T\chi_{\{j\}}=0$ for all $j\in J$. Prove that $T=T^*$ iff $\lambda_i\in\Rbb$ for all $i\in I$.
\end{exe}

\begin{proof}[Hint]
Define $T$ to be the limit (under the operator norm) of a sequence of finite-rank operators. Then the complete continuity follows from Thm. \ref{lb648}.
\end{proof}


\begin{rem}
Not all Hilbert-Schmidt operators are completely continuous: According to Exe. \ref{lb664}, we have a completely continuous operator on $l^2(\Zbb_+)$ whose matrix representation is $\diag(1,1/{\sqrt2},1/{\sqrt 3},\dots)$. It is not Hilbert-Schmidt, because $\sum_n n^{-1}=+\infty$.
\end{rem}



\begin{co}[\textbf{Fredholm alternative}]\index{00@Fredholm alternative}\label{lb670}
Let $T\in\fk L(\mc H)$ be self-adjoint. Let $\lambda\in\Rbb\setminus\{0\}$. Then one of the following two, and only one of them, is true:
\begin{enumerate}
\item[(1)] $\lambda$ is an eigenvalue of $T$, i.e., $T\xi=\lambda\xi$ for some nonzero $\xi\in\mc H$.
\item[(2)] $\lambda-T$ is surjective. 
\end{enumerate}
\end{co}
In other words, $\lambda-T$ is surjective iff $\lambda$ is not an eigenvalue.

\begin{proof}
Assume for simplicity that $\mc H$ is infinite dimensional. By Hilbert-Schmidt Thm. \ref{lb662}, we may assume that $\mc H=l^2(X)$ where $X=\Zbb_+\sqcup J$ and $J$ is a set. We assume that there is a sequence $(\lambda_n)_{n\in\Zbb_+}$ in $\Rbb$ such that $|\lambda_n|$ decreases to $0$, and that $T$ has matrix representation $\diag(\lambda_1,\lambda_2,\dots,(0)_{j\in J})$.

If $\lambda=\lambda_n$ for some $n$, then clearly $\chi_{\{n\}}$ is not in the range of $\lambda-T$. So $\lambda-T$ is not surjective. Conversely, suppose that $\lambda\neq\lambda_n$ for all $n$. For each $\eta\in l^2(X)$, let $\xi:X\rightarrow\Rbb$ be defined by $\xi(n)=(\lambda-\lambda_n)^{-1}\eta(n)$ if $n\in\Zbb_+$, and $\xi(j)=\lambda^{-1}\eta(j)$ if $j\in J$. Then clearly $\xi\in l^2(X)$, and $(\lambda-T)\xi=\eta$.
\end{proof}










\subsection{Concluding remarks}

\subsubsection{On the proof of the Hilbert-Schmidt theorem}

The proof of the Hilbert-Schmidt theorem in the last section, despite written in the modern language, is very close to the proof in Hilbert's 1906 paper, the fourth part of his work \cite{Hil12}. (Hilbert's proof is located in p.148-150 of \cite{Hil12}.)


The two key properties in Ch. \ref{lb665} are the only analytic properties\footnote{Here, by ``analytic property" I mean any property about inner product spaces that is equivalent to completeness, such as those described in Thm. \ref{lb612}.} used in the proof of the Hilbert-Schmidt theorem. The key property 2, the weak compactness of $\ovl B_{\mc H}(0,1)$, is the most crucial one. In the proof of Thm. \ref{lb617}, we have used this property to find the unit vector $e_1$ maximizing the function $g(\xi)=\bk{T\xi|\xi}$ defined on $\ovl B_{\mc H}(0,1)$. This method is heavily influenced by the method of variation, and is compatible with Hilbert's sesquilinear form viewpoint (rather than Riesz's operator viewpoint). The subsequent vectors $e_2,e_3,\dots$ are constructed in the same way by restricting $T$ to orthogonal complements of finite dimensional subspaces. Hilbert used exactly the same method in his work! \uwave{The weak compactness of $\ovl B_{l^2(\Zbb)}(0,1)$ is the single most important reason that Hilbert spaces were introduced in history.}

%(By the way, Hilbert also treated positive forms first.)

The key property 1, the convergence of summing orthogonal vectors, is used to ensure the existence of the orthogonal decompositions (cf. the proof of Thm. \ref{lb617}). In the proof of Thm. \ref{lb657}, this property is used (and only used) to show that $e_1,e_2,\dots$ and an orthonormal basis $(f_j)_{j\in J}$ of $\mc K^\perp=\{e_1,e_2,\dots\}^\perp$ form an orthonormal basis of $\mc H$. (Let us quickly recall the key point: It is obvious that $e_1,e_2,\dots$ and $(f_j)$ form an orthonormal family. To show that they are densely-spanning, for each $\xi\in\mc H$, one sets $\eta=\sum_n \bk{\xi|e_n}e_n$, which converges by key property 1. Then $\xi-\eta\in\mc K^\perp$. Apply Thm. \ref{lb595} to $\xi-\eta$. (This does not rely on any analytic property). Then we have $\xi=\eta+\sum_j\bk{\xi-\eta|f_j}f_j$. So $\xi$ is approximated by linear combinations of $e_1,e_2,\dots$ and $(f_j)$.) 

%By the way, when $\mc H$ is separable (which is the main interesting case), the existence of an orthonormal basis $(f_j)$ does not rely on any analytic property, but follows directly from Gram-Schmidt: see Exp. \ref{lb613}. Even if one does not assume that $\mc H$ is separable, the existence of $(f_j)$ also follows from key property 1 (cf. the proof of (2)$\Rightarrow$(3) of Thm. \ref{lb612}).

Indeed, Hilbert did not use key property 1 and orthogonal decomposition in his proof. He did not need the vectors $(f_j)_{j\in J}$ since his ``Hilbert-Schmidt" theorem is stated in the following form (cf. \cite[Satz 35]{Hil12}): There exist orthonormal vectors $e_1,e_2,\dots$ and $\lambda_1\geq\lambda_2\geq\cdots\geq0$ satisfying $\lim_n\lambda_n=0$ and
\begin{align}
\omega_T(\xi|\xi)=\sum_n \lambda_n \bk{\xi|e_n}\bk{e_n|\xi}
\end{align}
for all vectors $\xi$. That each $e_n$ is a $\lambda_n$-eigenvector of $T$ is a mere consequence of this formula.


\subsubsection{On the applicability of the Hilbert-Schmidt theorem}


The Hilbert-Schmidt theorem lies at the heart of modern partial differential equations. As mentioned in Subsec. \ref{lb667}, the inverse of $-\Delta$ (with boundary condition $u|_{\partial\Omega}=0$) is a completely continuous \textit{positive} operator on $L^2(\ovl\Omega)$. Therefore, by the Hilbert-Schmidt theorem, the unbounded operator $-\Delta$ have eigenvalues $0<\lambda_1\leq \lambda_2\leq\cdots$ converging to $+\infty$, and the eigenvectors (which can be proved to be ``good enough" such as $C^r$ or $C^\infty$) form an orthonormal basis of $L^2(\ovl\Omega)$. With the spectral analysis of $-\Delta$, the Dirichlet problem \eqref{eq271} can be understood very well.

However, this modern theory was also developed with the help of many other theories that were not yet available at the time of Hilbert-Schmidt: distributions and Sobolev spaces, unbounded closed operators, etc..  As we have mentioned in Sec. \ref{lb668}, in the early days, the Dirichlet problem was studied in terms of their associated integral equations of functions on $\partial\Omega$. However, the Hilbert-Schmidt theorem has very limited application to these integral equations. Let me explain this in the following. 


We have mentioned that the operator $T$ in the integral equation \eqref{eq282}, after discretization by taking Fourier series, gives a bounded linear operator $\wht T$ on $l^2(\Zbb)$ whose matrix representation $\wht K:\Zbb\times\Zbb\rightarrow\Cbb$ is $l^2$-finite, i.e. $\sum_{m,n}|\wht K(m,n)|^2<+\infty$. (See Subsec. \ref{lb669}.) Therefore, $\wht T$ is a Hilbert-Schmidt operator, and hence is completely continuous by Thm. \ref{lb651}. Moreover, some elementary calculations show that $-1$ is not an eigenvalue of $T$, cf. \cite[Thm. 3.3.9]{Sim-O} or \cite[Sec. 81]{RN}. Therefore, if $T$ is self-adjoint, the  Fredholm alternative (Cor. \ref{lb670}) shows that for each $g\in L^2(\partial\Omega)$ there exists a (necessarily unique) $f\in L^2(\partial\Omega)$ satisfying \eqref{eq282}. More precisely, one finds the Fourier series $\wht f\in l^2(\Zbb)$ solving $\wht f+\wht T\wht f=\wht g$.


Unfortunately, in many cases $T$ is not self-adjoint (since the real-valued function $G$ in \eqref{eq227} does not satisfy $G(x,y)=G(y,x)$). Therefore, although the Hilbert-Schmidt theorem would later be proved to be an effective tool for studying the Dirichlet problem, at the time of its inception, its relevance to the Dirichlet problem was not significant. Hilbert restricted his study to self-adjoint operators (more precisely, Hermitian forms), perhaps in view of the many other uses of integral equations, such as the Sturm-Liouville problem.


Assuming self-adjointness, the original problem about integral equation can be fully solved: Let $K\in C^r(\Sbb^1\times\Sbb^1)$ where $r\geq0$. Then we have a linear operator $T:C^r(\Sbb^1)\rightarrow C^r(\Sbb^1)$ defined by $(Tf)(x)=\frac 1{2\pi}\int_{-\pi}^\pi K(x,y)f(y)dy$. Assume that $K$ is \textbf{symmetric}, i.e. $K(x,y)=\ovl{K(y,x)}$. In the following, we prove the $C^r$-Fredholm alternative: If there is no non-zero $f\in C^r(\Sbb^1)$ satisfying $f+Tf=0$, then for each $g\in C^r(\Sbb^1)$ there exists $f\in C^r(\Sbb^1)$ such that $f+Tf=g$. 



\subsubsection{$\star$ Regularity of the solutions of integral equations}








Define $\wht K$ by \eqref{eq283}, which is an element of $l^2(\Zbb\times\Zbb)$. Equip $C^r(\Sbb^1)$ with the inner product $\bk{f|g}=\frac 1{2\pi}\int_{-\pi}^\pi fg^*$. Then we have a commutative diagram
\begin{equation}\label{eq286}
\begin{tikzcd}
C^r(\Sbb^1) \arrow[r,"\Psi"] \arrow[d,"T"'] & l^2(\Zbb) \arrow[d,"\wht T"] \\
C^r(\Sbb^1) \arrow[r,"\Psi"]           & l^2(\Zbb)          
\end{tikzcd} 
\end{equation}
where $\Psi$ is the map $f\mapsto \wht f$ (which is a linear isometry with dense range, cf. Cor. \ref{lb603}), and $\wht T$ is defined by \eqref{eq284}. Then $K(x,y)=\ovl{K(y,x)}$ implies that $\wht K(m,n)=\ovl{\wht K(n,m)}$, and hence that $\wht T$ is self-adjoint (cf. Exp. \ref{lb678}). Briefly speaking, \eqref{eq286} asserts $\wht{Tf}=\wht T\wht f$ where $f\in C^r(\Sbb^1)$.

It can be proved that
\begin{align}
\wht T\big(l^2(\Zbb)\big)\subset\Psi\big(C^r(\Sbb^1) \big)  \label{eq285}
\end{align}
To see this, choose $\varphi\in l^2(\Zbb)$, and let $f_n=\sum_{k=-n}^n \varphi(k)e_k$. Then it is easy to check that $\lim_n Tf_n$ converges uniformly on $\Sbb^1$ to some $g\in C(\Sbb^1)$. Moreover, one checks that $\lim_n (Tf_n)^{(k)}$ converges uniformly for all $k\leq r$. Thus, by Thm. \ref{lb336}, we see that $g\in C^r(\Sbb^1)$. Using Parseval's identity for continuous functions on $\Sbb^1\times\Sbb^1$, one checks that $\wht g=\wht T\varphi$, i.e., $\Psi(g)=\wht T\varphi$. This proves \eqref{eq285}.

We now show that if $\lambda\in\Cbb\setminus\{0\}$, a $\lambda$-eigenvector $\varphi$ of $\wht T$ corresponds to a $\lambda$-eigenvector $f\in C^r(\Sbb^1)$ of $T$ satisfying $\wht f=\varphi$. Proof: Let $\varphi\in l^2(\Zbb)$ and $\wht T\varphi=\lambda\varphi$. By \eqref{eq285}, there exists $f\in C^r(\Sbb^1)$ such that $\wht T\varphi=\lambda \wht f$. So $\wht f=\varphi$, and hence  $\wht {Tf}=\wht T\wht f=\wht T\varphi=\lambda\varphi=\lambda\wht f$, and hence $Tf=\lambda f$ (since $\Psi$ is injective). QED.

We now finish the proof. Suppose that there is no non-zero $f\in C^r(\Sbb^1)$ satisfying $f+Tf=0$. By the above paragraph, there is no non-zero $\varphi\in l^2(\Zbb)$ satisfying $\varphi+\wht T\varphi=0$. Choose any $g\in C^r(\Sbb^1)$.  By Fredholm alternative (Cor, \ref{lb670}), there exists $\varphi\in l^2(\Zbb)$ such that $\varphi+\wht T\varphi=\wht g$. By \eqref{eq285}, there is $f\in C^r(\Sbb^1)$ such that $\wht T\varphi=\wht{g-f}=\wht g-\wht f$. Thus $\wht f=\varphi$, and hence $\wht f+\wht{Tf}=\wht f+\wht T\wht f=\wht f+\wht T\varphi=\wht g$. So $f+Tf=g$.



\subsubsection{Toward measure theory}


We have seen that the problem of finding an $C^r$-solution $f$ of an integral equation $f+Tf=g$ is solved by first finding its Fourier series $\wht f$, which is an element of $l^2(\Zbb)$ solving the equation $\wht f+\wht T\wht f=\wht g$. To solve the later equation, one uses only the fact that $\wht K\in l^2(\Zbb\times\Zbb)$, or more precisely, that $\wht K$ is the matrix representation of a completely continuous (self-adjoint) operator on $l^2(\Zbb)$. 

The lesson here is that even if we are ultimately interested in the Fourier series of functions in $C^r(\Sbb^1)$, i.e., the elements of $\Psi(C^r(\Sbb^1))$, at the outset, we must consider all possible elements of $l^2(\Zbb)$: If you remember, the proof of the Hilbert-Schmidt theorem uses the weak-compactness of the closed unit ball of $l^2(\Zbb)$. However, $\Psi(C(\Sbb^1))$ is a dense proper subspace of $l^2(\Zbb)$, and its closed unit ball is therefore not weakly compact (by Thm. \ref{lb637}).

This lesson naturally leads to the question: What is the function-theoretic meaning of an element $\varphi\in l^2(\Zbb)$ that is not necessarily the Fourier series of a continuous (or even Riemann-integrable) $2\pi$-periodic function? 


Lebesgue's integral theory was invented in 1902. And it was F. Riesz (and then Fischer) who realized that Lebesgue integral gives a satisfying answer to the above question. They showed in 1907 (a year after Hilbert's seminal work \cite{Hil12} introducing the $l^2$-method to integral equations) that the linear isometry $\Psi:C(\Sbb^1)\rightarrow l^2(\Zbb)$ can be extended to a unitary map
\begin{align*}
\Psi:L^2(\Sbb^1)\rightarrow l^2(\Zbb)
\end{align*}
where $L^2(\Sbb^1)$ is the space of $2\pi$-periodic Lebesgue measurable functions $f$ satisfying $\int_{-\pi}^\pi |f|^2<+\infty$, and
\begin{align*}
(\Psi f)(n)=\wht f(n):=\frac 1{2\pi}\int_{-\pi}^\pi fe_{-n}
\end{align*}
where $e_n(x)=e^{\im nx}$, and the integral on the RHS is the Lebesgue integral. This result is called the \textbf{Riesz-Fischer theorem}, as already mentioned in Sec. \ref{lb549}. 

With this result, the proof of \eqref{eq285} becomes more straightforward: Let $\varphi\in l^2(\Zbb)$. By Riesz-Fischer, $\varphi=\wht f$ for some $f\in L^2(\Sbb^1)$. Using the commutativity of limits and integrals (under certain assumptions) one sees that $g\in C^r(\Sbb^1)$ if $g$ is defined by the Lebesgue integral $g(x)=\frac 1{2\pi}\int_{-\pi}^\pi K(x,y)f(y)$. Parseval's identity implies $\wht g=\wht T\wht f$. 

It will be our task in the next few chapters to study Lebesgue integrals, the Riesz-Fischer theorem, and some of their generalizations.















\begin{comment}
The second issue is that even if $T$ is self-adjoint, and therefore the Hilbert-Schmidt theorem is available, the function-theoretic meaning of the solution $\wht f$ of $\wht f+\wht T\wht f=\wht g$ was not clear when Hilbert gave his proof using $l^2$-method (in 1906). More precisely, it was not clear why $\wht f$, a general element in $l^2(\Zbb)$, is indeed the Fourier coefficients of a ``function" $f$ on $\Sbb^1$, let alone a continuous function or a ``good enough function". I have already mentioned this problem in Sec. \ref{lb549}.

Luckily, soon after Hilbert's proof, in 1907, Riesz and Fischer proved a theorem saying that elements of $l^2(\Zbb)$ are exactly the Fourier series of functions in $L^2(\Sbb^1)$ (i.e., Lebesgue measurable functions $f$ satisfying $\int |f|^2<+\infty$). But we shouldn't take that luck for granted. Hilbert's research method of slightly deviating from the context of the original function-theoretic problems in exchange for a more algebraic way of understanding is very worthwhile. History has proved that Hilbert spaces are one of the greatest inventions in the early 20th century. The context that was temporarily ignored in Hilbert's $l^2$-theorey was able to be dealt with systematically at a later date.\footnote{Actually, Hilbert has given several different proofs of the diagonalization theorem for integral operators, many of which are more function-theoretic than the $l^2$-method. However, the $l^2$-method is the only one that is still widely used today.}

\end{comment}






\subsection{$\star$ Epilogue: Riesz's compact operators}\label{lb677}

Let $\mc H,\mc K$ be Hilbert spaces. Recall \eqref{eq280} for the meaning of $\omega_T$.

In the previous section, we have mentioned that many integral equations do not have self-adjoint integral operators. When a completely continuous operator $T$ on a $\mc H$ is not self-adjoint, the Hilbert-Schmidt theorem does not hold. However, one can still expect that the Fredholm alternative is true. In fact, in Fredholm's  1900 paper there was no ``self-adjointness". However, Fredholm's original approach is quite complicated: Assuming that the function $K(x,y)$ in the integral operator $T=\int_{-\pi}^\pi K(x,y)f(y)dy$ is continuous, Fredholm used Riemann sums to approximate the Riemann integral, studied the determinants of these summation operators, and then passed to the integral operator $T$ by taking limit. (See \cite[Sec. 5.1]{Die-H} for a brief discussion. See \cite[3.11]{Sim-O} for a detailed account of Fredholm's approach in a modern language.)



Fredholm clearly did not have the idea of Hilbert spaces and completely continuous operators. Now, to study completely continuous operators that are not necessarily self-adjoint, one needs to give an equivalent but different characterization of complete continuity. 

Hilbert's original definition of complete continuity (cf. Def. \ref{lb653}) is essentially about \textit{sesquilinear forms}, not about \textit{linear operators}. Recall that $T$ is completely continuous iff $\omega_T:\ovl B_{\mc H}(0,1)\times\ovl B_{\mc K}(0,1)\rightarrow\Cbb$ is weakly continuous. To describe this property in terms of the linear map $T$,  the weak continuity on the second variable $\ovl B_{\mc K}(0,1)$, which is a property about the source, should be raised to the target. This is achieved with the help of the following variant of Prop. \ref{lb635}.

\begin{lm}\label{lb673}
Let $(\xi_\alpha)_{\alpha\in I}$ be a net in $\ovl B_{\mc H}(0,1)$, and let $\xi\in\ovl B_{\mc H}(0,1)$. Then the following are equivalent.
\begin{enumerate}
\item[(1)] $\lim_\alpha\xi_\alpha=\xi$.
\item[(2)] $(\xi_\alpha)$ converges weakly to $\xi$. Moreover, for every net $(\eta_\beta)_{\beta\in J}$ in $\ovl B_{\mc H}(0,1)$ converging weakly to $\eta\in\ovl B_{\mc H}(0,1)$ we have
\begin{align}
\lim_{\alpha\in I,\beta\in J}\bk{\xi_\alpha|\eta_\beta}=\bk{\xi|\eta}
\end{align}
\end{enumerate}
\end{lm}

Similar to Prop. \ref{lb635}, this lemma is also true when $\mc H$ is only an inner product space, as you can see from the following proof.

\begin{proof}
Assume (1). Let $(\eta_\beta)$ converge weakly in $\ovl B_{\mc H}(0,1)$ to $\eta$. For each $\beta\in J$, define a continuous function $g_\beta:\mc H\rightarrow\Cbb$ by $g_\beta(\psi)=\bk{\psi|\eta_\beta}$. Then $g_\beta$ converges pointwise to $g:\psi\in \mc H\mapsto \bk{\psi|\eta}\in\Cbb$. Since $\sup_\beta\Vert\eta_\beta\Vert<+\infty$, $(g_\beta)_{\beta\in J}$ is equicontinuous. Therefore, by Thm. \ref{lb277} (and noting Prop. \ref{lb281}), we get $\lim_{\alpha,\beta} g_\beta(\xi_\alpha)=g(\xi)$. This proves (2).\footnote{It is not hard to prove (1)$\Rightarrow$(2) directly. We leave such a direct proof to the readers as an exercise.}

Assume (2). Then (1) follows directly from Prop. \ref{lb635} by choosing $J=I$ and $\eta_\alpha=\xi_\alpha$ for each $\alpha\in I$.
\end{proof}


\begin{pp}
Let $T\in\fk L(\mc H,\mc K)$. The following are equivalent.
\begin{enumerate}
\item[(1)] $T$ is completely continuous, i.e., $\omega_T$ is weakly continuous on $\ovl B_{\mc H}(0,1)\times\ovl B_{\mc K}(0,1)$.
\item[(2)] $T:\ovl B_{\mc H}(0,1)\rightarrow\mc K$ is continuous, if $\ovl B_{\mc H}(0,1)$ is equipped with the weak topology, and if $\mc K$ is equipped with the norm topology. 
\end{enumerate}
\end{pp}

\begin{proof}
(2) means that if $(x_\alpha)$ is a net converging weakly in $\ovl B_{\mc H}(0,1)$ to $\xi$, then $T\xi_\alpha$ converges in norm to $T\xi$. By Lem. \ref{lb673} (applied to $\Vert T\Vert^{-1}T\xi_\alpha$), we see that $\lim_\alpha T\xi_\alpha=T\xi$ is equivalent to that $\lim_{\alpha,\beta}\bk{T\xi_\alpha|\eta_\beta}=\bk{T\xi|\eta}$ for every net $(\eta_\beta)_{\beta\in J}$ converging weakly in $\ovl B_{\mc K}(0,1)$ to $\eta$. This proves the equivalence.
\end{proof}


\begin{df}\label{lb674}
A linear map of normed vector spaces $T:V\rightarrow W$ is called \textbf{completely continuous} \index{00@Completely continuous} if $T$ is bounded, and if the restriction $T:\ovl B_V(0,1)\rightarrow W$ is continuous, where $\ovl B_V(0,1)$ is given the weak topology, and $W$ is given the norm topology.
\end{df}

Def. \ref{lb674} was introduced by F. Riesz in 1913 and applied by him to $l^p$ spaces. However, this is not a good definition for $C[0,1]$ because, unlike $l^p(X)$ (where $1<p<+\infty$), $C[0,1]$ is not reflexive and (hence) its closed unit ball is not weakly compact (cf. Thm. \ref{lb658}). In order to study integral equations on non-necessarily reflexive function spaces such as $C[0,1]$, in 1918, Riesz introduced the modern definition of compact operators (which he still called completely continuous operators). Let us recall the definition: 

\begin{df}\label{lb675}
Let $T:V\rightarrow W$ be a linear map of normed vector spaces. We say that $T$ is a \textbf{compact operator} \index{00@Compact operator} if $T(\ovl B_V(0,1))$ is precompact in $W$ (under the norm topology).
\end{df}

Note that if $T$ is compact, then $T$ is bounded. This is because every compact set in a metric space, in particular $\ovl{T(\ovl B_V(0,1))}$, is bounded.

\begin{thm}
Let $T:\mc H\rightarrow\mc K$ be linear. Then $T$ is completely continuous iff $T$ is compact.
\end{thm}

The following proof shows that this theorem is also true when $\mc H,\mc K$ are only normed vector spaces and $\ovl B_{\mc H}(0,1)$ is weakly compact. (By Thm. \ref{lb568}, the latter condition is equivalent to that $\mc H$ is reflexive.)


\begin{proof}
Suppose that $T$ is completely continuous. Let $(\eta_\alpha)$ be a net in $T(\ovl B_{\mc H}(0,1))$, which is of the form $(T\xi_\alpha)$ where $(\xi_\alpha)$ is a net in $\ovl B_{\mc H}(0,1)$. Since $\ovl B_{\mc H}(0,1)$ is weakly compact, $(\xi_\alpha)$ has a subnet converging weakly to some $\xi\in\ovl B_{\mc H}(0,1)$. By the complete continuity of $T$, we have $\lim_\beta T\xi_\beta=T\xi$. Thus $(\eta_\alpha)$ has a convergent subnet $(T_\beta)$. Therefore, by Prop. \ref{lb505}, $T(\ovl B_{\mc H}(0,1))$ is precompact.

Conversely, assume that $T$ is compact. Let $(\xi_\alpha)$ be a net in $\ovl B_{\mc H}(0,1)$ converging weakly to $\xi\in\ovl B_{\mc H}(0,1)$. Since $T$ is compact, $\Gamma=T(\ovl B_{\mc H}(0,1))$ is (norm-)compact. Therefore, to show that $\lim_\alpha T\xi_\alpha=T\xi$, by Pb. \ref{lb237}, it suffices to prove that every cluster point of $(T\xi_\alpha)$ is $T\xi$. This is clear, since every cluster point of $(T\xi_\alpha)$ is a weak cluster point, which must be $T\xi$ by Exe. \ref{lb676}.
\end{proof}



Def. \ref{lb675} is the correct definition which allowed Riesz to prove the Fredholm alternative for compact operators on Banach spaces (including integral operators on $C[0,1]$) in 1918. A discussion of this history, as well as the main ideas in Riesz's proof of Fredholm alternative, can be found in \cite[Sec. 7.1]{Die-H}. Riesz's theory on compact operators can be found in many textbooks on functional analysis, such as  \cite[Ch. 21]{Lax}, \cite[Ch. 4]{Rud-F}, \cite[Sec. 3.3]{Sim-O}.


































\subsection{Problems and supplementary material}

Let $\mc H,\mc K$ be Hilbert spaces. Recall \eqref{eq280} for the meaning of $\omega_T$.

\begin{exe}\label{lb676}
Let $T\in\fk L(\mc H,\mc K)$. Prove that $T$ is continuous if both $\mc H$ and $\mc K$ are equipped with their weak topologies.
\end{exe}


\begin{comment}
\begin{sprob}
Let $T:l^2(Y)\rightarrow l^2(X)$ be completely continuous. For each $A\in\fin(2^X)$ and $B\in\fin(2^Y)$, let $T_{A,B}:l^2(Y)\rightarrow l^2(X)$ be defined by \eqref{eq281}, i.e., $T_{A,B}=\chi_A T\chi_B$. Let $\Omega=\ovl B_{l^2(Y)}(0,1)\times\ovl B_{l^2(Y)}(0,1)$, equipped with the product weak topology. Prove that $(\omega_{T_{A,B}})_{A\in\fin(2^X),B\in\fin(2^Y)}$ is an equicontinuous family of functions on $\Omega$.
\end{sprob}

\begin{proof}[Note]
Compare this with the fact that if $T\in\fk L(l^2(Y),l^2(X))$, then $(\omega_{T_{A,B}})_{A,B}$ is an equicontinuous family of functions on $\Omega$ where $\Omega$ is equipped with the product norm topology. (Compare Rem. \ref{lb666}.)
\end{proof}
\end{comment}





The following problem studies the one-variable analog of Thm. \ref{lb648}.

\begin{sprob}
Let $\varphi:\mc H\rightarrow\Cbb$ be a linear map. Prove that the following are equivalent:
\begin{enumerate}
\item[(1)] $\varphi$ belongs to $\mc H^*=\fk L(\mc H,\Cbb)$.
\item[(2)] $\varphi$ is weakly continuous.
\item[(3)] $\varphi$ is weakly continuous when restricted to $\ovl B_{\mc H}(0,1)$.
\end{enumerate}

\end{sprob}

\begin{proof}[Hint for (3)$\Rightarrow$(1)]
Assume WLOG that $\mc H=l^2(X)$. For each $A\in\fin(2^X)$, let $\varphi_A:\xi\in\mc H\mapsto \bk{\chi_A\xi,\varphi}$. Prove that $\lim_A\varphi_A$ converges uniformly on $\ovl B_{\mc H}(0,1)$ to $\varphi$ by (for example) mimicking the proof of Thm. \ref{lb648}.
\end{proof}



\begin{prob}\label{lb650}
Let $(e_i)$ and $(f_j)$ be orthonormal bases of $\mc H,\mc K$ respectively. Let $T\in\fk L(\mc H,\mc K)$. Define
\begin{align}
\Vert T\Vert_{\mathrm{HS}}=\Big(\sum_{i,j} |\bk{Te_i|f_j}|^2\Big)^{\frac 12}
\end{align}
which is in $[0,+\infty]$. Prove that $\Vert T\Vert_{\mathrm{HS}}$ is independent of the choice of bases. We say that $T$ is a \textbf{Hilbert-Schmidt operator} \index{00@Hilbert-Schmidt operator} if $\Vert T\Vert_{\mathrm{HS}}<+\infty$.
\end{prob}

\begin{prob}\label{lb660}
Let $T\in\fk L(\mc H)$ be positive. Prove that $\Vert T\Vert\leq 1$ iff $0\leq T\leq 1$.
\end{prob}

\begin{proof}[Hint]
To prove ``$\Leftarrow$", apply Cauchy-Schwarz to $\omega_T$.
\end{proof}


\begin{sprob}
Let $T\in\fk L(\mc H)$ satisfy $0\leq T\leq 1$. Prove that $T^2\leq T$.
\end{sprob}
\begin{proof}[Hint]
Use Cauchy-Schwarz to give an upper bound of $|\omega_T(\xi|T\xi)|$.
\end{proof}



\newpage


\section{Measure spaces}


\subsection{Introduction}\label{lb722}


Starting from this chapter, we will study Lebesgue integrals, and study measure theory in general. We will be able to define $\int_{\Rbb^N} f$ for a large class of functions $f:\Rbb^N\rightarrow\Rbb$ called \textbf{Lebesgue measurable functions}. It is worth noting that when Lebesgue introduced his integral theory in 1902, he was primarily concerned with bounded measurable functions on a compact interval $[a,b]\subset\Rbb$. For example, the dominated convergence theorem, one of the most important theorems in measure theory, was first proved by Lebesgue for bounded measurable functions on $[a,b]$. 

Only by knowing what were the earliest primary examples in history, and only by knowing how the theory can be developed and applied to these crucial examples, can one comprehend the essence of the theory. Therefore, I will sometimes give a more straightforward proof of an important special case of a theorem after proving this theorem in full generality. %The ideas in these straightforward proofs are closer to the original proofs of the theorem in history. 


In the following, we sketch Lebesgue's main idea of the construction of integrals. A detailed account of the history can be found in \cite[Sec. 5.1]{Haw} and \cite[Sec. 9.6]{Jah}.



Let $-\infty<a<b<+\infty$, and let $f:(a,b)\rightarrow\Rbb$ be bounded, say $-M\leq f\leq M$ for some $M>0$.\footnote{Lebesgue originally considered functions on $[a,b]$. For the simplicity of the following discussion, we consider functions on $(a,b)$ instead, which makes no essential difference to the development of the theory.} We know that Riemann integras are defined by partitioning the domain $(a,b)$. By contrast, the Lebesgue integra $\int_a^b fdm$ is defined by partitioning the codomain: Let $\{c_0<c_1<\dots<c_n\}$ be a partition of $[-M,M+1]$. Then it is expected that $\int_a^b fdm$ is approximated by
\begin{align}\label{eq287}
\sum_{i=1}^n c_{i-1}\cdot m(E_i)
\end{align}
where $E_i=f^{-1}[c_{i-1},c_i)$, and $m(E_i)$ is the ``measure" of $E_i$. 
Thus, one can attempt to define $\int_a^b fdm$ to be the limit of the ``Lebesgue sum" \eqref{eq287}. In fact, since $\int f$ should be the area of the region below the graph of $f$, it is expect that if $\sup_i |c_i-c_{i-1}|<\eps$, then 
\begin{align*}
\Big| \int_a^b fdm- \sum_{i=1}^n c_{i-1}\cdot m(E_i)  \Big|\leq (b-a)\eps
\end{align*}

Therefore, in order to define the Lebesgue integral, one must first define the \textbf{Lebesgue measure} $m(E_i)$. Let me temporarily denote this value by $m^*(E_i)$ to reflect the fact (to be explained shortly) that not every subset of $(a,b)$ can be assigned a measure. The construction is as follows. Suppose that $E\subset(a,b)$. Then
\begin{align}\label{eq294}
m^*(E)=\inf\Big\{\sum_{n\in\Zbb_+}|I_i|:I_1,I_2,\dots\text{ are intervals covering }E  \Big\}
\end{align}
is called the \textbf{outer Lebsgue measure} of $E$. Clearly $m^*(E)\leq m^*(F)$ if $E\subset F$. %It is an easy exercise that if $I$ is a subinterval of $(a,b)$ with endpoints $c\leq d$, then $m^*(I)=|I|:=d-c$.


Unfortunately, it is not necessarily true that $m^*(E\cup F)=m^*(E)+m^*(F)$ if $F\subset(a,b)$ is disjoint from $E$. Indeed, it is not necessarily true that $b-a=m^*(E)+m^*((a,b)\setminus E)$. Consequently, if we use the Lebesgue sum to define the integral, then it is not necessarily true that $\int_a^b 1=\int_a^b\chi_E+\int_a^b\chi_{(a,b)\setminus E}$. In general, we only have
\begin{align}\label{eq288}
m^*(E\cup F)\leq m^*(E)+m^*(F)
\end{align}
if $E,F\in(a,b)$ are disjoint. Thus, we must focus on a class $\fk M$ of subsets of $(a,b)$ satisfying certain nice properties. The elements in $\fk M$ will be called \textbf{Lebesgue measurable sets}. 

The most remarkable property about $\fk M$ is the \textbf{countable additivity}: If $E_1,E_2,\dots\in \fk M$ are mutually disjoint, then $\bigcup_n E_n\in\fk M$, and $m^*(\bigcup_n E_n)=\sum_nm^*(E_n)$. Then the Lebesgue integral will be defined for \textbf{Lebesgue measurable functions}, e.g., bounded functions $f:(a,b)\rightarrow\Rbb$ satisfying $f^{-1}[c,d)\in\fk M$ for all $c<d$. \uwave{The powerful properties concerning $\lim_n\int f_n=\int\lim_nf_n$ will be proved as easy consequences of the countable additivity.} Therefore, the construction of Lebesgue measure satisfying countable additivity is the most central and difficult part of the whole theory of Lebesgue's integrals.


\begin{comment}


Let $-\infty<a<b<+\infty$, and let $m^*:2^{(a,b)}\rightarrow\Rbb_{\geq0}$ be defined by \eqref{eq294}. In this section, we sketch the construction of the set $\fk M$ of Lebesgue measurable sets in $(a,b)$. The restriction of $m^*$ to $\fk M$ will be denoted by $m$ and called the \textbf{Lebesgue measure}. There are many equivalent definitions of $\fk M$. Our definition will follow the original one of Lebesgue (cf. \cite[Sec. 5.1]{Haw} and \cite[Sec. 9.6]{Jah}): a measurable set is a set whose ``outer measure is equal to the inner measure". (This definition needs to be modified when applied to unbounded subsets of $\Rbb$ and $\Rbb^N$. We will discuss this in Sec. \ref{lb688} and \ref{lb726}.)

It is not required that the reader first read this section before reading Sec. \ref{lb688} and \ref{lb726} for a formal discussion of the construction of the Lebesgue measure on $\Rbb^N$. The readers may return to this section when they feel the need for motivations when reading future sections (especially Sec. \ref{lb726}). The construction of the Lebesgue measure is usually viewed as the most complicated and technical part of the whole Lebesgue theory. One of the purposes of this section is to give the readers a general idea of at least how much effort is required to build the most central part of Lebesgue's theory of integrals. 


\subsubsection{Basic properties of the outer Lebsgue measure $m^*$}\label{lb715}





\begin{pp}\label{lb686}
Let $E_1,E_2,\dots\subset (a,b)$. Then 
\begin{align}
m^*(\bigcup_n E_n)\leq\sum_n m^*(E_n)\label{eq293}
\end{align}
\end{pp}

\begin{proof}
Let $\eps>0$. For each $E_n$, let $I_{n,1},I_{n,2},\dots$ be intervals covering $E_n$ whose total length is $\leq m^*(E_m)+\eps/2^n$. Then $\bigcup_nE_n$ is covered by $\{I_{n,k}\}$ whose total length is $\leq \sum_n m^*(E_n)+\eps$. 
\end{proof}

In some simple cases, the inequality in \eqref{eq293} becomes equality:

\begin{exe}\label{lb682}
Let $I_1,I_2,\dots$ be disjoint subintervals of $(a,b)$. Prove
\begin{align}\label{eq289}
m^*(\bigcup_n I_n)=\sum_n |I_n|
\end{align}
\end{exe}
\begin{proof}[Hint]
``$\leq$" is obvious from the definition of $m^*$. To prove ``$\geq$", it suffices to prove $m^*(\bigcup_n I_n)\geq|I_1|+\cdots+|I_N|$ for each $N$. Shrink each $I_n$ to a slightly smaller compact interval $J_n$. Use the compactness of $J_n$ to prove $m^*(J_1\cup\cdots\cup J_N)\geq|J_1|+\cdots+|J_N|$.
\end{proof}


Since each open subset of $(a,b)$ is a countable union of disjoint intervals (cf. Pb. \ref{lb679}), by Exe. \ref{lb682}, for each $E\subset (a,b)$ we have
\begin{align}\label{eq291}
m^*(E)=\inf\big\{m^*(O):E\subset O\subset(a,b)\text{ and $O$ is open}  \big\}
\end{align}
This gives a more elegant description of $m^*$.









\begin{lm}\label{lb687}
Suppose that $K_1,K_2,\dots$ are disjoint compact subsets of $(a,b)$. Then $m^*(\bigcup_n K_n)=\sum_n m^*(K_n)$.
\end{lm}
\begin{proof}
By Prop. \ref{lb686}, it suffices to prove ``$\geq$". Thus, it suffices to prove $m^*(\bigcup_nK_n)\geq \sum_{n=1}^Nm^*(K_n)$ and apply $\lim_{N\rightarrow\infty}$. Therefore, it suffices to prove $m^*(\bigcup_{n=1}^N K_n)\geq  \sum_{n=1}^Nm^*(K_n)$. By induction, it suffices to assume $N=2$. So we take disjoint compact $K_1,K_2$, and we shall prove $m^*(K_1)+m^*(K_2)\leq m^*(K)$ where $K=K_1\cup K_2$.

By \eqref{eq291}, for each $\eps>0$, there is an open $O$ covering $K$ such that $m^*(O)\leq m^*(K)+\eps$. Since $d(K_1,K_2)>0$, one can easily find open disjoint subsets $O_1,O_2$ of $O$ covering $K_1,K_2$ respectively. By Exe. \ref{lb682} and the fact that $O_1,O_2$ are disjoint unions of intervals, we get $m^*(O_1\cup O_2)=m^*(O_1)+m^*(O_2)$. Therefore $m^*(K_1)+m^*(K_2)\leq m^*(O_1)+m^*(O_2)=m^*(O_1\cup O_2)\leq m^*(O)\leq m^*(K)-\eps$.
\end{proof}


\begin{lm}\label{lb684}
Let $K\subset (a,b)$ be compact. Then for each open $\Omega\subset(a,b)$ containing $K$ we have $m^*(\Omega)=m^*(K)+m^*(\Omega\setminus K)$.
\end{lm}
\begin{proof}
It suffices to prove ``$\geq$". $\Omega\setminus K$ is open, and hence is a disjoint union of intervals. Thus, by Exe. \ref{lb682}, $m^*(\Omega\setminus K)$ can be approximated by $m^*(I)$ where $I$ is a finite disjoint union of closed subintervals of $\Omega\setminus K$. In particular,
\begin{align}
m^*(\Omega\setminus K)=\sup\big\{m^*(L):L\text{ is a compact subset of }\Omega\setminus K   \big\}
\end{align}
By Lem. \ref{lb687}, for each compact $L\subset \Omega\setminus K$ we have $m^*(K)+m^*(L)=m^*(K\cup L)\leq m^*(\Omega)$. Taking $\sup_L$ finishes the proof.
\end{proof}



\subsubsection{Lebesgue measurable sets}\label{lb722}

%To ensure that the integral operator $\int$ is linear, we must restrict it to the (so called) measurable functions on $(a,b)$. To describe measurable functions, we must first describe measurable sets. 

For each $E\subset (a,b)$, the \textbf{inner measure} $m_*(E)$ is defined to be
\begin{align}\label{eq290}
m_*(E)=\sup\big\{m^*(K):K\subset E\text{ and $K$ is compact}  \big\}
\end{align}
Clearly $m_*(E)\leq m^*(E)$. A set $E\subset (a,b)$ is called \textbf{Lebesgue measurable} if
\begin{align}
m^*(E)=m_*(E)
\end{align}
If $E$ is (Lebesgue) measurable, we call $m(E):=m^*(E)$ the \textbf{Lebesgue measure} of $E$. We let $\fk M$ denote the set of Lebesgue measurable subsets of $(a,b)$. 

\begin{rem}\label{lb685}
Let $E\subset(a,b)$. Then $E\in\fk M$ iff for every $\eps>0$ there exist an open $O\subset(a,b)$ containing $E$ and a compact $K$ inside $E$ such that $m^*(O\setminus K)<\eps$.
\end{rem}

\begin{proof}
We have $m_*(E)=m^*(E)$ iff (by \eqref{eq291} and \eqref{eq290}) for every $\eps>0$ there exist an open $O\subset(a,b)$ containing $E$ and a compact $K$ inside $E$ such that $m^*(O)-m^*(K)<\eps$. But $m^*(O)-m^*(K)=m^*(O\setminus K)$ by Lem. \ref{lb684}.
\end{proof}



\begin{eg}\label{lb680}
If $E$ is an open or compact subset of $(a,b)$, then $E\in\fk M$.
\end{eg}

\begin{proof}
This is obvious when $E$ is compact. When $E$ is open, this follows from Exe. \ref{lb682}.
\end{proof}

\begin{pp}\label{lb683}
For each $E\subset (a,b)$ we have
\begin{align}\label{eq292}
m_*(E)=b-a-m^*((a,b)\setminus E)
\end{align}
\end{pp}

In fact, \eqref{eq292} is Lebesgue's original definition of inner measure. As an immediate consequence of \eqref{eq292}, we see that $E\in\fk M$ iff $(a,b)\setminus E\in\fk M$.

\begin{proof}
Let $F=(a,b)\setminus E$. We want to prove $m_*(E)+m^*(F)=b-a$. Choose any compact $K\subset E$. Then $m^*(K)+m^*((a,b)\setminus E)\leq m^*(K)+m^*((a,b)\setminus K)$ where the RHS equals $b-a$ by Lem. \ref{lb684}. This proves $m_*(E)+m^*(F)\leq b-a$. 

To prove ``$\geq$", by \eqref{eq291}, it suffices to pick any open $O\subset (a,b)$ containing $F$ and prove $m_*(E)+m^*(O)\geq b-a$. Note that $K_0=(a,b)\setminus O$ is closed in $(a,b)$ but not necessarily compact. But we still have $m_*(K_0)+m^*(O)\geq b-a$ by Prop. \ref{lb686}. To get compact sets, for each $\eps>0$ we choose a compact interval $I\subset (a,b)$ such that $|I|>b-a-\eps$. Thus $m^*(K_0\setminus I)\leq m^*((a,b)\setminus I)<\eps$. Let $K=K_0\cap I$, which is a closed subset of $I$ and hence compact. Therefore, by Prop. \ref{lb686}, we have $m^*(K_0)<m^*(K)+\eps$, and hence $m^*(K)+\eps+m^*(O)\geq b-a$. This proves $m_*(E)+\eps+m^*(F)\geq b-a$ for all $\eps$.
\end{proof}




















\subsubsection{Measures and $\sigma$-algebras}\label{lb717}


\begin{thm}
$\fk M$ is a \textbf{$\sigma$-algebra}, which means that $\emptyset\in\fk M$, that $E\in\fk M$ iff its complement $(a,b)\setminus E$ is in $\fk M$, and that $\bigcup_n E_n\in \fk M$ if $E_1,E_2,\dots\in\fk M$.
\end{thm}


\begin{proof}
The only nontrivial property is that $\fk M$ is closed under taking countable union. We first prove that $\fk M$ is closed under taking finite unions. By induction, it suffices to take $E_1,E_2\in\fk M$ and show $E_1\cup E_2\in\fk M$. By Rem. \ref{lb685}, for each $\eps>0$, there exist compact $K_i\subset E_i$ and open $O_i\supset E_i$ such that $m^*(O_i\setminus K_i)<\eps$ (where $i=1,2$). Let $K=K_1\cup K_2$ and $O=O_1\cup O_2$. Then $O\setminus K\subset (O_1\setminus K_1)\cup (O_2\setminus K_2)$, and hence $m^*(O\setminus K)\leq m^*((O_1\setminus K_1)\cup (O_2\setminus K_2))$, which is $<2\eps$ by Lem. \ref{lb686}. This proves $E_1\cup E_2\in\fk M$.


Now let $E_1,E_2,\dots\in\fk M$. Let $F_1=E_1$ and $F_n=E_n\setminus\{E_1\cup\cdots\cup E_{n-1}\}$ so that $E=\bigcup_n E_n$ is the disjoint union $\bigsqcup_n F_n$. Since $\fk M$ is closed under taking complement and finite union, we conclude that $F_n\in\fk M$ for each $n\in\Zbb_+$. Choose any $\eps>0$. By Rem. \ref{lb685}, for each $n$, there exist compact $K_n\subset F_n$ and open $O_n\supset F_n$ such that $m^*(O_n\setminus K_n)<\eps/2^n$. Let $O=\bigcup_n O_n$ and $K=\bigcup K_n$. Then $O\setminus K\subset\bigcup_n O_n\setminus K_n$ and hence, by Prop. \ref{lb686}, $m^*(O\setminus K)\leq\sum_n m^*(O_n\setminus K_n)<\eps$.

Note that $K$ is not necessarily compact. However, by Lem. \ref{lb687}, there exists $N\in\Zbb_+$ such that the compact set $\wtd K:=K_1\sqcup\cdots\sqcup K_N$ satisfies $m^*(K)\leq m^*(\wtd K)+\eps$. By Lem. \ref{lb687}, we get $m^*(K\setminus \wtd K)\leq \eps$. Thus $m^*(O\setminus\wtd K)\leq m^*(O\setminus K)+m^*(K\setminus\wtd K)<2\eps$.
\end{proof}


\begin{thm}
$m^*$ is a \textbf{measure} on $\fk M$ and is denoted by $m$. Namely, it satisfies $m(\emptyset)=0$, and $m(\bigcup_n E_n)=\sum_n m(E_n)$ if $E_1,E_2,\dots\in \fk M$ are mutually disjoint.
\end{thm}

\begin{proof}
By Prop. \ref{lb686}, it suffices to prove $m(\bigcup_n E_n)\geq\sum_n m(E_n)$. Similar to the proof of Lem. \ref{lb687}, it suffices to prove $m(E_1)+m(E_2)\leq m(E_1\cup E_2)$ if $E_1,E_2\in\fk M$ are mutually disjoint. For each $\eps>0$, choose compact $K_i\subset E_i$ and open $O_i\supset E_i$ such that $m(O_i\setminus K_i)<\eps$. So $m(E_i\setminus K_i)<\eps$. By Prop. \ref{lb686}, we get $m(E_i)< m(K_i)+\eps$. Thus, by Lem. \ref{lb687}, we have
\begin{align*}
m(E_1\cup E_2)\geq m(K_1\cup K_2)=m(K_1)+m(K_2)>m(E_1)+m(E_2)-2\eps
\end{align*}
\end{proof}
\end{comment}







\subsection{Measurable spaces and measurable functions}


\begin{df}\label{lb689}
Let $X$ be a set. A subset $\fk M$ of $2^X$ is called a \textbf{$\sigma$-algebra} \index{00@$\sigma$-algebra} if it satisfies the following conditions:
\begin{itemize}
\item $\emptyset\in\fk M$.
\item If $E\in\fk M$ then $X\setminus E\in\fk M$.
\item If we have countably many elements $E_1,E_2,\dots\in\fk M$, then $\bigcup_n E_n\in\fk M$.
\end{itemize}
If $\fk M$ is a $\sigma$-algebra, we say that $(X,\fk M)$ is a \textbf{measurable space}. \index{00@Measurable space}
\end{df}
The second condition means that $\fk M$ is closed under complements. The third condition means that $\fk M$ is closed under \textit{countable} unions. Since $(\bigcup_n E_n)^c=\bigcap_n E_n^c$, we see that a $\sigma$-algebra is also closed under countable intersections. 

\begin{rem}
In Def. \ref{lb689}-(3) it suffices to assume that $\fk M$ is closed under countably \textit{infinite} unions. This is because any finite union could be enlarged to a countably infinite union by including $\emptyset$.
\end{rem}

\begin{exe}\label{lb690}
If $(\fk M_i)_{i\in I}$ is a family of $\sigma$-algebra on $X$, then $\bigcap_{i\in I}\fk M_i$ is a $\sigma$-algebra on $X$.
\end{exe}

\begin{df}
Let $\fk E\subset 2^X$. By Exp. \ref{lb690},
\begin{align}
\sigma(\fk E):=\bigcap_{\fk M\text{ is a $\sigma$-algebra containing $\fk E$}}\fk M 
\end{align}
is a $\sigma$-algebra on $X$. We call $\sigma(\fk E)$ the \textbf{$\sigma$-algebra generated by $\fk E$}. \index{00@$\sigma$-algebra generated by $\fk E$} \index{zz@$\sigma(\fk E)$} It is the smallest $\sigma$-algebra containing $\fk E$.
\end{df}

The most important $\sigma$-algebras in this course are Borel $\sigma$-algebras:

\begin{df}
Let $(X,\mc T_X)$ be a topological space. Recall that $\mc T_X$ is the set of open subsets of $X$. We let \index{BX@$\fk B_X$}
\begin{align}
\fk B_X:=\sigma(\mc T_X)
\end{align}
and call $\fk B_X$ the \textbf{Borel $\sigma$-algebra} \index{00@Borel $\sigma$-algebra} of $X$. Elements of $\fk B_X$ are called \textbf{Borel sets}. \index{00@Borel set}
\end{df}

\begin{eg}
Let $X$ be a topological space. Every closed subset of $X$ is Borel since it is the complement of an open set. $[a,b)$ is Borel set of $\Rbb$ since it equals $(-\infty,b)\cap[a,+\infty)$
\end{eg}


\begin{df}
Let $(X,\fk M)$ and $(Y,\fk N)$ be measurable sets. Let $f:X\rightarrow Y$ be a function. Then 
\begin{align}
f^{-1}(\fk N)=\{f^{-1}(E):E\in\fk N\}
\end{align}
is clearly a $\sigma$-algebra on $X$. We say that $f$ is \textbf{measurable} \index{00@Measurable function/map} if $f^{-1}(\fk N)\subset\fk M$, i.e., if $f^{-1}(E)\in\fk M$ for each $F\in\fk N$.
\end{df}

\begin{rem}\label{lb696}
If $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ are measurable, then clearly $g\circ f:X\rightarrow Z$ is measurable.
\end{rem}


\begin{df}
Let $(X,\fk M)$ be a measurable space, and let $(Y,\mc T_Y)$ be a topological space. A map $f:X\rightarrow Y$ is called \textbf{measurable} if $f$ is measurable as a map $(X,\fk M)\rightarrow (Y,\fk B_Y)$, i.e., $f^{-1}(E)\in\fk M$ for each Borel set $E\subset Y$.
\end{df}

\begin{rem}
The most important measurable functions $X\rightarrow Y$, where $Y$ is a topological space, are those such that $Y=\Rbb,\ovl\Rbb_{\geq 0},\Cbb$. Note that $\Rbb,\Cbb$ are equipped with the Euclidean topologies, and $\ovl\Rbb_{\geq0}=[0,+\infty]$ is equipped with its standard topology (cf. Exp. \ref{lb690}), i.e., the unit topology such that any increasing bijection $\ovl\Rbb_{\geq0}\rightarrow[0,1]$ is a homeomorphism.
\end{rem}



\begin{df}
Let $f:X\rightarrow Y$ be a map of topological spaces. We say that $f$ is \textbf{Borel measurable} (or simply \textbf{Borel}) \index{00@Borel function/map} if $f$ is measurable as a map of measurable spaces $f:(X,\fk B_X)\rightarrow (Y,\fk B_Y)$.
\end{df}




Checking that a map $f$ is Borel using the original definition is difficult, since Borel sets of the codomain could be very complicated. In the following, we will see a very useful method of showing that a map is measurable.


\begin{pp}\label{lb692}
Let $(X,\fk M)$ and $(Y,\fk N)$ be measurable sets where $\fk N=\sigma(\fk E)$ for some $\fk E\subset 2^Y$. Then the following are equivalent.
\begin{enumerate}
\item[(1)] $f$ is measurable, i.e., $f^{-1}(\sigma(\fk E))\subset\fk M$.
\item[(2)] $f^{-1}(\fk E)\subset\fk M$.
\end{enumerate}
\end{pp}

\begin{proof}
Clearly (1) implies (2). Assume (2). Then
\begin{align*}
\fk N=\{E\in 2^Y:f^{-1}(E)\in\fk M\}
\end{align*}
is a $\sigma$-algebra containing $\fk E$. So $\fk N$ contains $\sigma(\fk E)$. Thus, for each $E\in\sigma(\fk E)$ we have $E\in\fk N$, i.e., $f^{-1}(E)\in\fk M$. This proves (1).
\end{proof}




\begin{co}\label{lb695}
Let $f:X\rightarrow Y$ be a map of sets. Let $\fk E\subset 2^Y$. Then
\begin{align}
\sigma(f^{-1}(\fk E))=f^{-1}(\sigma(\fk E))
\end{align}
\end{co}


\begin{proof}
$f^{-1}(\sigma(\fk E))$ is a $\sigma$-algebra on $X$, and it contains $f^{-1}(\fk E)$. Therefore $f^{-1}(\sigma(\fk E))$ contain the smallest $\sigma$-algebra containing $f^{-1}(\fk E)$. This prove ``$\subset$". Since $f^{-1}(\fk E)$ is contained in $\fk M:=f^{-1}(\sigma(\fk E))$, by Prop. \ref{lb692}, $f^{-1}(\sigma(\fk E))$ is contained in $\fk M$. This proves ``$\supset$".
\end{proof}

\begin{co}\label{lb693}
Let $(X,\fk M)$ be a measurable space. Let $(Y,\mc T_Y)$ be a topological space. Let $f:X\rightarrow Y$ be a map. The following are equivalent:
\begin{enumerate}
\item[(1)] $f$ is measurable, i.e., $f^{-1}(\fk B_Y)\subset\fk M$.
\item[(2)] $f^{-1}(\mc T_Y)\subset \fk M$.
\item[(3)] $f^{-1}\{\text{closed subsets of $Y$}\}\subset \fk M$.
\end{enumerate}
\end{co}

\begin{proof}
This follows from Prop. \ref{lb692} and the fact that $\fk B_Y$ is generated by $\mc T_Y$ and also by the set of closed subsets of $Y$.
\end{proof}

\begin{eg}
Every continuous map of topological spaces is Borel.
\end{eg}

\begin{proof}
Immediate from Cor. \ref{lb693}.
\end{proof}

Checking $f^{-1}(\mc T_Y)\subset\fk M$ is still not very easy. To make further simplification we observe:

\begin{pp}\label{lb694}
Let $(Y,\mc T_Y)$ be a second countable topological space. Let $\mc U$ be a basis for the topology $\mc T_Y$. Then $\sigma(\mc U)=\fk B_Y$. Consequently, a function $f:(X,\fk M)\rightarrow Y$ is measurable iff $f^{-1}(\mc U)\subset \fk M$.
\end{pp} 



\begin{proof}
Since $\mc U\subset\mc T_Y$, we clearly have $\sigma(\mc U)\subset\fk B_Y$. If we can prove that $\sigma(U)\supset\mc T_Y$, then $\sigma(U)\supset\sigma(\mc T_Y)$, finishing the proof.

Choose any open set $O\subset Y$. Then for each $y\in O$ there is a neighborhood $U_y\in \mc U$ of $y$ contained inside $O$. Thus $O=\bigcup_{y\in O}U_y$. So $U$ is a union of elements of $\mc U$. Since $Y$ is second countable, the subset $O$ is also second countable and hence (by Cor. \ref{lb265}) Lindel\"of. Therefore, $O$ is a countable union of elements of $\mc U$. This proves $O\in\sigma(\mc U)$. 
\end{proof}

\begin{eg}\label{lb703}
Let $(X,\fk M)$ be a measurable space. Let $Y$ be a separable (equivalently, second countable) metric space. Then a map $f:X\rightarrow Y$ is measurable iff $f^{-1}(B_Y(y,r))\in\fk M$ for each $y\in Y,r>0$. This is because the open balls of $Y$ form a basis for the topology of $Y$.
\end{eg}


\begin{eg}\label{lb697}
We have
\begin{align*}
&\fk B_{\ovl\Rbb}=\sigma\{(a,+\infty]:a\in\ovl\Rbb\}=\sigma\{[a,+\infty]:a\in\ovl\Rbb\}\\
=&\sigma\{[-\infty,a):a\in\ovl\Rbb\}=\sigma\{[-\infty,a]:a\in\ovl\Rbb\}
\end{align*}
\end{eg}

Thus, for example, if $f:X\rightarrow\ovl\Rbb$ where $X$ is a measurable space, then $f$ is measurable iff $f^{-1}(a,+\infty]$ is measurable for all $a\in\ovl\Rbb$.

\begin{proof}
Let $\fk M=\sigma\{(a,+\infty]:a\in\ovl\Rbb\}$, which is clearly a subset of $\fk B_{\ovl R}$. Then $[a,+\infty]\in\fk M$ since $[a,+\infty]=\bigcap_{n\in\Zbb_+}(a-1/n,+\infty]$. Taking complements and intersections, we see $[-\infty,b)\in\fk M$ and $(a,b)\in\fk M$. Therefore, $\fk M$ contains a basis for $\mc T_{\ovl\Rbb}$, and hence contains $\fk B_{\ovl\Rbb}$ by Prop. \ref{lb694}. This prove the first relation. The other relations can be proved in the same way.
\end{proof}





\begin{rem}
Let $f:(X,\fk M)\rightarrow (Y,\mc T_Y)$. Suppose that $Z$ is a subspace of $Y$ containing $f(X)$. Equip $Z$ with the subspace topology $\mc T_Z$. Then $f:X\rightarrow Y$ is measurable iff $f:X\rightarrow Z$ is measurable. 
\end{rem}

\begin{proof}
$f:X\rightarrow Y$ is measurable iff $f^{-1}(U)$ is measurable for each $U\in\mc T_Y$. But $f^{-1}(U)$ equals $f^{-1}(U\cap Z)$, and the open subset of $Z$ are precisely of the form $U\cap Z$.
\end{proof}


\begin{eg}
Let $(X,\fk M)$ be a measurable space. Let $A\subset X$. Then $\chi_A:X\rightarrow\Rbb$ is measurable iff $\chi_A:X\rightarrow\{0,1\}$ is measurable iff $\chi_A^{-1}(1)=A$ and $\chi_A^{-1}(0)=X\setminus A$ are measurable iff $A\in\fk M$.
\end{eg}



\begin{pp}\label{lb699}
Let $(X,\fk M)$ be a measurable space. Let $Y_1,Y_2$ be second countable topological spaces. Let $f_1:X\rightarrow Y_1$ and $f_2:X\rightarrow Y_2$ be functions. Then $f_1\vee f_2:X\rightarrow Y_1\times Y_2$ is measurable iff $f_1$ and $f_2$ are both measurable. 
\end{pp}

\begin{proof}
Let $f=f_1\vee f_2$. Let $\pi_i:Y_1\times Y_2\rightarrow Y_i$ be the projection. Then $f_i=\pi_i\circ f$. Thus, if $f$ is measurable, since $\pi_i$ is continuous and hence Borel measurable, we conclude that $f_i$ is measurable (cf. Rem. \ref{lb696}).

Conversely, assume that $f_1,f_2$ are measurable. If $V_1$ and $V_2$ are open subsets of $Y_1,Y_2$ respectively, then $f_i^{-1}(V_i)$ belongs to $\fk M$, and hence $f^{-1}(V_1\times V_2)=f_1^{-1}(V_1)\cap f_2^{-1}(V_2)$ belongs to $\fk M$. Since subsets of the form $V_1\times V_2$ form a basis for the topology of $Y_1\times Y_2$, by Prop. \ref{lb694}, $f$ is measurable.
\end{proof}


\begin{co}
Let $(X,\fk M)$ be a measurable space. Let $f,g:X\rightarrow\Cbb$ or $f,g:X\rightarrow\ovl\Rbb_{\geq0}$ be measurable. Then $f+g$ and $fg$ are measurable functions from $X$ to $\Cbb$ or $\ovl\Rbb_{\geq0}$.
\end{co}

Recall Def. \ref{lb114} for the definition of additions and multiplications in $\ovl\Rbb_{\geq0}$.


\begin{proof}
The multiplication map $\Cbb\times\Cbb\rightarrow\Cbb$ is continuous, and the multiplication map $\ovl\Rbb_{\geq0}\times\ovl\Rbb_{\geq0}\rightarrow\ovl\Rbb_{\geq0}$ is lower semicontinuous and hence Borel measurable (cf. Exp. \ref{lb698}). By Prop. \ref{lb699}, $f\vee g$ is measurable. So its composition with the multiplication map (i.e., $fg$) is measurable. The same argument shows that $f+g$ is measurable.
\end{proof}



The pointwise limit of a sequence of Riemann integrable functions is not necessarily Riemann integrable.  However, the following theorem shows that the pointwise limit of a sequences of measurable functions is measurable. Thus, for example, the Dirichlet function, which is the limit of $f_n(x)=\chi_{A_n}$ where $A_n=\{a_1,\dots,a_n\}$ and $\Qbb\cap[0,1]=\{a_1,a_2,\dots\}$, is not Riemann integrable (cf. Exp. \ref{lb704}). But it is Borel measurable.

\begin{thm}\label{lb700}
Let $X$ be a measurable space. Let $(f_n)_{n\in\Zbb_+}$ be a sequence of functions $X\rightarrow\ovl\Rbb$. Then $\sup_n f_n$, $\inf_n f_n$, $\limsup_n f_n$, and $\liminf_n f_n$ are measurable.  
\end{thm}

\begin{proof}
Let $F=\sup_n f_n$. Then for each $a\in\ovl\Rbb$, we have
\begin{align*}
F^{-1}[-\infty,a]=\bigcap_n f_n^{-1}[-\infty,a]
\end{align*}
where the RHS is measurable. Therefore, by Exp. \ref{lb697}, $\sup_n f_n$ is measurable. Similarly, $\inf_n f_n$ is measurable.

For each $n$, let $g_n:X\rightarrow\ovl\Rbb$ be defined by $g_n(x)=\sup_{k\geq n}f_k(x)$. The first paragraph shows that $g_n$ is measurable for each $n$, and hence $\limsup_nf_n=\inf_n g_n$ is measurable. Similarly, $\liminf_nf_n$ is measurable.
\end{proof}


\begin{co}\label{lb701}
Let $X$ be a measurable space. Let $Y$ be $\ovl\Rbb$ or $\Rbb^N$ or $\Cbb^N$. Let $(f_n)$ be a sequence of measurable functions $X\rightarrow Y$ converging pointwise to $f:X\rightarrow Y$. Then $f$ is measurable.
\end{co}
\begin{proof}
This is immediate from Thm. \ref{lb700} and Prop. \ref{lb699}.
\end{proof}

The pointwise limit of a \textit{net} of measurable functions is not necessarily measurable:

\begin{eg}
Let $(X,\fk M)$ be a measurable space such that $\fk M\neq 2^X$, and that $\fk M$ contains all finite subsets of $X$. (For example, let $X$ be an uncountable set, and let $\fk M$ be the set of all $E\subset X$ such that either $E$ or $X\setminus E$ is countable.) Let $E\in 2^X\setminus\fk M$. Then $(\chi_A)_{A\in\fin(2^E)}$ is a net of measurable functions $X\rightarrow \Rbb$. However, its pointwise limit $\chi_E$ is not measurable.
\end{eg}



\subsection{Measures and measure spaces}




\begin{df}
Let $(X,\fk M)$ be a measurable space. A function $\mu:\fk M\rightarrow[0,+\infty]$ is called a \textbf{measure} \index{00@Measure} if it satisfies the following conditions:
\begin{itemize}
\item $\mu(\emptyset)=0$.
\item (\textbf{Countable additivity}) \index{00@Countable additivity} If we have countably many $E_1,E_2,\dots\in\fk M$ that are pairwise disjoint, then $\mu\big(\bigcup_n E_n\big)=\sum_n\mu(E_n)$.
\end{itemize}
We call $(X,\fk M,\mu)$ (or simply call $(X,\mu)$) a \textbf{measure space}. \index{00@Measure space} If $X$ is a topological space and $\mu$ is defined on $\fk M=\fk B_X$, we call $\mu$ a \textbf{Borel measure}. \index{00@Borel measure}
\end{df}



\begin{df}
Let $X$ be a set. For each $E\subset X$, let $\mu(E)=\sum_{x\in E}1$. Namely, $\mu(E)$ is the cardinality of $E$ when $E$ is a finite set, and $\mu(E)=+\infty$ when $E$ is not finite. Then $(X,2^X,\mu)$ is a measure space. $\mu$ is called the \textbf{counting measure}. \index{00@Counting measure}
\end{df}


\begin{pp}
Let $(X,\fk M,\mu)$ be a measure space. The following are true.
\begin{enumerate}[label=(\alph*)]
\item (\textbf{Monotonicity}) If $E,F\in\fk M$ and $E\subset F$, then $\mu(E)\leq \mu(F)$.
\item If $(E_n)_{n\in\Zbb_+}$ is an increasing sequence of elements of $\fk M$, then $\mu\big(\bigcup_n E_n\big)=\lim_{n\rightarrow\infty}\mu(E_n)$.
\item If $(E_n)_{n\in\Zbb_+}$ is an increasing sequence of elements of $\fk M$, and if $\mu(E_1)<+\infty$, then $\mu\big(\bigcap_n E_n\big)=\lim_{n\rightarrow\infty}\mu(E_n)$.
\item (\textbf{Countable subadditivity}) \index{00@Countable subadditivity} If $E_1,E_2,\dots\in\fk M$, then $\mu\big(\bigcup_n E_n\big)\leq\sum_n\mu(E_n)$
\end{enumerate}
\end{pp}


\begin{proof}
(a) $\mu(F)=\mu(E\sqcup(F\setminus E))=\mu(E)+\mu(F\setminus E)\geq\mu(E)$.

(b) Let $F_1=E_2$, and $F_n=E_n\setminus E_{n-1}$ if $n>1$. Then
\begin{align*}
&\mu\big(\bigcup_n E_n \big)=\mu\big(\bigsqcup_n F_n \big)=\sum_n\mu(F_n)=\lim_n \big(\mu(F_1)+\cdots+\mu(F_n) \big)\\
=&\lim_n\mu(F_1\cup\cdots\cup F_n)=\lim_n\mu(E_n)
\end{align*}

(c) Let $F_n=E_1\setminus E_n$. Then $E_1$ is the disjoint union of $E\bigcap_n E_n$ and $F=\bigcup_n F_n$ By (b), we have $\mu(F)=\lim_n \mu(F_n)$. Equivalently, $\mu(E_1)-\mu(E)=\lim_n\big(\mu(E_1)-(E_n)\big)$.

(d) We have $\mu(E_1\cup E_2)=\mu(E_1)+\mu(E_2\setminus E_1)\leq \mu(E_1)+\mu(E_2)$. By induction, we get $\mu(E_1\cup\cdots\cup E_n)\leq \mu(E_1)+\cdots+\mu(E_n)$. Thus
\begin{align*}
\mu(E_1\cup\cdots\cup E_k)\leq\sum_n \mu(E_n)
\end{align*}
for each $k$. Apply $\lim_k$ to the LHS. Then (b) implies $\mu(\bigcup E_n)\leq\sum_n \mu(E_n)$.
\end{proof}


\begin{df}
Let $(X,\fk M,\mu)$ be a measure space. A subset $E\subset X$ is called a \textbf{$\mu$-null set} \index{00@Null set, $\mu$-null set} (or simply a null set) if $E\in\fk M$ and $\mu(E)=0$. If $P:X\rightarrow\{\text{true}, \text{false}\}$ is a property on $X$, we say that $P$ is true \textbf{$\mu$-almost everywhere} \index{00@almost everywhere=a.e.} (or simply that $P$ is true \textbf{$\mu$-a.e.}) if $P$ is true outside a $\mu$-null set.
\end{df}

\begin{rem}\label{lb709}
By the countable subadditivity, a countable union of null sets is null.
\end{rem}

\begin{df}
A measure space $(X,\mu)$ is called \textbf{complete} \index{00@Complete measure space} if every subset of a null set is measurable (and hence is null by the monotonicity).
\end{df}

The main advantage of completeness is the following property:

\begin{pp}
Let $(X,\fk M,\mu)$ be a complete measure space. Let $(Y,\fk N)$ be a measurable space. The following are true.
\begin{enumerate}
\item[(a)] If $f,g:X\rightarrow Y$ are equal a.e. (namely, there is a null set outside of which $f$ and $g$ are equal), and if $f$ is measurable, then $g$ is measurable.
\item[(b)] Let $X_0\in\fk M$ such that $X\setminus X_0$ is null. Suppose that $f:(X_0,\fk M|_{X_0})\rightarrow Y$ is measurable. (Namely, $f^{-1}(E)\in\fk M$ for each $E\in\fk N$. Cf. Def. \ref{lb707}.) Let $\wtd f:X\rightarrow Y$ be any function extending $f$. Then $\wtd f$ is measurable. 
\end{enumerate}
\end{pp}

\begin{proof}
(a) Let $X_0\in\fk M$ with null complement such that $f|_{X_0}=g|_{X_0}$. Then for each $E\in\fk N$, we have $f^{-1}(E)\cap X_0=g^{-1}(E)\cap X_0$. Since $g^{-1}(E)\setminus X_0$ is a subset of $X_0^c$, by the completeness, $g^{-1}(E)\setminus X_0$ is measurable. So $g^{-1}(E)$ is measurable.

(b) This can be proved in a similar way as (a). 
\end{proof}

\begin{df}
Let $(X,\fk M,\mu)$ be a measure space. If $\nu$ is a measure on a $\sigma$-algebra $\fk N\subset 2^X$, we say that $(\fk N,\nu)$ is an \textbf{extension of $(\fk M,\mu)$} \index{00@Extension of measure} and write $(\fk M,\mu)\subset(\fk N,\nu)$, if $\fk M\subset \fk N$ and $\nu|_{\fk M}=\mu$. 
\end{df}

\begin{thm}\label{lb708}
Let $(X,\fk M,\mu)$ be a measure space. Then there is a (necessarily unique) smallest complete extension $(\ovl{\fk M},\ovl\mu)$ of $(\fk M,\mu)$. Moreover, the elements of $\ovl{\fk M}$ are precisely of the form $E\cup F$ where $E\in\fk M$ and $F$ is a subset of a $\mu$-null set, and $\ovl\mu(E\cup F)=\mu(E)$. We call $(\ovl{\fk M},\ovl\mu)$ the \textbf{completion} of $(\fk M,\mu)$. \index{00@Completion of measure}
\end{thm}
The phrase ``smallest complete extension" means that $(\ovl{\fk M},\ovl\mu)$ is a complete measure on $X$ extending $(\fk M,\mu)$, and that every complete measure extending $(\fk M,\mu)$ also extends $(\ovl {\fk M},\ovl\mu)$. 



\begin{proof}
Define $\ovl{\fk M}$ to be the set of subsets of $X$ of the form $E\cup F$ where $E\in\fk M$ and $F$ is a subset of a $\mu$-null set, and let $\ovl\mu(E\cup F)=\mu(E)$. This is well-defined: Suppose that $E\cup F=E'\cup F'$ where $E,F\in\fk M$, and $F,F'$ are subsets of null sets $A,A'$ respectively. Then $\mu(E\cup F)\leq \mu(E'\cup A')$. Since $A'\setminus E'$ is null, we have $\mu(E'\cup A')=\mu(E')$. So $\mu(E)\leq\mu(E')$. Similarly, $\mu(E')\leq\mu(E)$.

Given sets $E_1\cup F_1,E_2\cup F_2,\dots$ where $E_n\in \fk M$ and $F_n$ is inside a null set, then $\bigcup_n E_n\in\fk M$, and $\bigcup F_n$ is inside a null set (Rem. \ref{lb709}). So $\ovl{\fk M}$ is closed under countable unions. Let $E\in\fk M$ and $F$ be inside a null set $A$. Then $(E\cup F)^c=E^c\setminus F$ can be written as $E^c\setminus A$ union a subset of $A$. So $\ovl{\fk M}$ is closed under complements. This proves that $\ovl{\fk M}$ is a $\sigma$-algebra. Using the countable additivity of $\mu$, one checks easily that $\ovl\mu$ is a measure.

If $(\fk N,\nu)$ is a complete measure on $X$ extending $(\fk M,\mu)$, then for each $E\in\fk M$ and $F$ inside a null set, we have $F\in\fk N$ by the completeness of $\nu$. So $E\cup F\in\fk N$. This proves $\ovl{\fk M}\subset\fk N$. Moreover, $\nu(E\cup F)=\nu(E)+\nu(F\setminus E)=\nu(E)$ since $F\setminus E$ is $\nu$-null. So $\nu(E\cup F)=\nu(E)=\mu(E)=\ovl\mu(E)$. This proves that $\nu$ extends $\ovl\mu$.
\end{proof}
















\subsection{The Lebesgue measure $m$ on $\Rbb^N$}\label{lb688}


Let $N\in\Zbb_+$. In this section, we construct the Lebesgue measure $m$ on $\Rbb^N$, which is the completion of a Borel measure.




Recall Subsec. \ref{lb494} for the definition of the Lebesgue measure $m(U)$ of an open set $U\subset\Rbb^N$: It is the supremum of the multiple Riemann integral $\int_{\Rbb^N}f$ (cf. Def. \ref{lb497}) where $f$ ranges over all elements of $C_c(U,[0,1])$. (Recall Def. \ref{lb465} for the meaning of $C_c(U,[0,1])$.)  It clearly satisfies the monotonicity: If $U_1\subset U_2$ are open, then $m(U_1)\leq m(U_2)$.
\begin{df}
For each $E\subset\Rbb^N$, define the \textbf{outer Lebesgue measure} \index{00@Outer Lebesgue measure $m^*$} to be
\begin{subequations}
\begin{align}
m^*(E)=\inf\big\{m(U):U\text{ is an open set containing }E \big\}
\end{align}
Clearly $m^*(U)=m(U)$ when $U$ is open. Clearly $m^*(E)\leq m^*(F)$ if $E\subset F\subset\Rbb^N$. Define the \textbf{inner Lebesgue measure} \index{00@Inner Lebesgue measure $m_*$} to be
\begin{align}
m_*(E)=\sup\big\{m^*(K):K\text{ is a compact subset of }E\big\}
\end{align}
\end{subequations}
Clearly $m_*(E)\leq m^*(E)$. A set $E\subset \Rbb^N$ is called \textbf{$m$-regular} if $m^*(E)=m_*(E)$.
\end{df}

From the definition, it is clear that compact sets are regular.

When Lebesgue introduced his integral theory in 1902, he focused on the measures of \textit{bounded subsets} of $\Rbb$. He defined a bounded measurable set to be a bounded $m$-regular set. (Cf. \cite[Sec. 5.1]{Haw} and \cite[Sec. 9.6]{Jah}.) This definition should be modified for unbounded sets. At this moment, let use show that open sets are regular.

\begin{lm}\label{lb714}
Any open subset $U\subset\Rbb^N$ is $m$-regular.
\end{lm}

\begin{proof}
Let $U$ be open. Then $m_*(U)\leq m^*(U)=m(U)$. We want to show $m_*(U)\geq m(U)$. Since $m(U)=\sup\{\int f:f\in C_c(U,[0,1])\}$, it suffices to show for each $f\in C_c(U,[0,1])$ that $\int f\leq m_*(U)$. By the definition of $m_*(U)$, it suffices to find a compact $K\subset U$ such that $\int f\leq m^*(K)$.

Let $K=\Supp(f)$, which is a compact subset of $U$. Then $m^*(K)$ is the infinimum of $m(V)$ where $V$ is open and contains $K$. So we shall prove that $\int f\leq m(V)$ for each open $V\subset\Rbb^N$ containing $K$. But this is obvious from the definition of $m(V)$.
\end{proof}

\begin{comment}
By Urysohn's lemma (Thm. \ref{lb711}), there exists $g\in C_c(V,[0,1])$ satisfying $g|_K=1$. Then $f\leq g$, and hence $\int f\leq \int g$ (because $g-f\geq0$ implies $\int(g-f)\geq0$). By the definition of $m(V)$, we have $\int g\leq m(V)$. This proves $\int f\leq m(V)$.

Note that the only property about $\int$ used in this proof is that $\int:C_c(\Rbb^N,\Rbb_{\geq0})\rightarrow\Rbb_{\geq0}$ is an $\Rbb_{\geq0}$-linear map. 
\end{comment}


\begin{thm}
$m^*$ is a measure on $\fk B_{\Rbb^N}$. Its completion will be denoted by $(\fk M,m)$ and called the \textbf{Lebesgue measure} \index{00@Lebesgue measure} on $\Rbb^N$. Elements in $\fk M$ are called \textbf{Lebesgue measurable sets}. \index{00@Lebesgue measurable set} Moreover, for each bounded  set $E\in\fk M$ we have $m(E)<+\infty$.
\end{thm}

\begin{proof}
That $m^*$ is a measure on $\fk B_{\Rbb^N}$ is an easy consequence of Thm. \ref{lb724} (which can be applied to the current situation, cf. Exp. \ref{lb725}), to be proved in the next section. Suppose that $E\in\fk M$ is bounded. Then $K$ is contained in a bounded open rectangle $U=(a_1,b_1)\times\cdots\times(a_N,b_N)$. For each $f\in C_c(\Rbb^N,[0,1])$, we have $\int f\leq (b_1-a_1)\cdots(b_N-a_N)<+\infty$. This proves $m(U)<+\infty$, and hence $m(E)<+\infty$.
\end{proof}


\subsection{A general method for constructing measures}\label{lb726}


Let $(X,\mc T_X)$ be a Hausdorff topological space. Let $\mu:\mc T_X\rightarrow[0,+\infty]$. Throughout this section, we assume the following assumption.

\begin{ass}\label{lb712}
For each $E\subset X$, define the \textbf{outer measure} \index{00@Outer measure} $\mu^*(E)$ and the \textbf{inner measure} \index{00@Inner measure} $\mu_*(E)$ to be 
\begin{subequations}\label{eq292}
\begin{gather}
\mu^*(E)=\inf\big\{\mu(U):U\text{ is an open subset of $X$ containing }E \big\}\\
\mu_*(E)=\sup\big\{\mu^*(K):K\text{ is a compact subset of }E \big\}\label{eq292b}
\end{gather}
\end{subequations}
(Clearly $\mu(U)=\mu^*(U)$ if $U$ is open.) Then the following conditions are satisfied:
\begin{enumerate}[label=(\alph*)]
\item $\mu(\emptyset)=0$.
\item (Monotonicity) If $U\subset V\subset X$ are open, then $\mu(U)\leq\mu(V)$.
\item (Countable subadditivity) For countably many open subsets $U_1,U_2,\dots$ of $X$ we have $\mu\big(\bigcup_n U_n\big)\leq\sum_n\mu(U_n)$.
\item (Additivity) If $U_1,U_2$ are disjoint open subsets of $X$, then $\mu(U_1\cup U_2)=\mu(U_1)+\mu(U_2)$.
\item (Regularity on open sets) If $U\subset X$ is open, then $\mu(U)=\mu_*(U)$.
\end{enumerate}
\end{ass}


\begin{df}
We say that $E$ is \textbf{$\mu$-regular} if $\mu_*(E)=\mu^*(E)$. Note that compact sets and open sets are clearly $\mu$-regular. If $E$ is $\mu$-regular, we write $\mu_*(E)$ and $\mu^*(E)$ as $\mu(E)$.
\end{df}

We abbreviate ``$\mu$-regular" to ``regular" when no confusion arises. (It has nothing to do with regular topological spaces (defined in Def. \ref{lb504}).)

\begin{eg}\label{lb725}
Let $X=\Rbb^N$ and $\mu=m$. Then $m$ satisfies Asmp. \ref{lb712}: (a) and (b) are obvious. (c) and (d) were discussed in Pb. \ref{lb713}. (e) was proved in Lem. \ref{lb714}.
\end{eg}


Our goal is to show that $\mu^*$ is countably additive on a large class of sets, including all Borel sets of $X$. We shall show that the countable additivity holds at least for regular sets based on the following simple idea:
\begin{align}\label{eq290}
\tcboxmath{
\begin{array}{c}
\text{$\mu^*$ satisfies countable subadditivity}\\
\text{$\mu_*$ satisfies countable superadditivity}\\
\text{If $\mu^*=\mu_*$, then the countable additivity is true}
\end{array}
}
\end{align} 





\subsubsection{Countable subadditivity and countable superadditivity}



In this subsection, we prove that $\mu^*$ satisfies the countable subadditivity, and $\mu_*$ satisfies the countable superadditivity.

\begin{pp}\label{lb718}
$\mu^*:2^X\rightarrow[0,+\infty]$ satisfies the following conditions:
\begin{enumerate}[label=(\alph*)]
\item $\mu^*(\emptyset)=0$.
\item (Monotonicity) If $E\subset F\subset X$, then $\mu^*(E)\leq\mu^*(F)$.
\item (Countable subadditivity)\index{00@Countable subadditivity} For countably many subsets $E_1,E_2,\dots$ of $X$ we have $\mu^*\big(\bigcup_n E_n\big)\leq\sum_n\mu^*(E_n)$.
\end{enumerate}
\end{pp}



\begin{proof}
The first two conditions are obvious. Let us check the countable subadditivity. Let $E_1,E_2,\dots\subset X$. Assume WLOG that each $\mu^*(E_n)$ is finite; otherwise the countable subadditivity is obvious. Let $\eps>0$. By the definition of $\mu^*$, each $E_n$ is contained in an open set $U_n$ such that $\mu(U_n)\leq \mu^*(E_n)+\eps/2^n$. Then $E=\bigcup_n E_n$ is contained in $\bigcup_n U_n$. By the countable subadditivity for open sets, we have $\mu(\bigcup_n U_n)\leq \sum_n\mu(U_n)\leq\sum_n\mu^*(E_n)+\eps$. By the monotonicity, we have $\mu(E)\leq\sum_n\mu^*(E_n)+\eps$. This finishes the proof, since $\eps$ can be arbitrary. 
\end{proof}

%We want to show that $\mu^*$ is countably additive on measurable sets. Since measurable sets with finite measures are approximated by their compact subsets, the first step of proving countable additivity is to prove it for compact sets:





To establish the countable superadditivity for $\mu_*$, we first need to prove the additivity for compact sets:

\begin{lm}\label{lb716}
Suppose that $K_1,K_2$ are disjoint compact subsets of $X$. Then $\mu(K_1\cup K_2)=\mu(K_1)+\mu(K_2)$.
\end{lm}

\begin{proof}
By the (countable) subadditivity of $\mu^*$, it remains to prove $\mu^*(K_1\cup K_2)\geq\mu^*(K_1)+\mu^*(K_2)$. By the definition of $\mu^*$, it suffices to prove that for each open $U$ containing $K_1\cup K_2$ we have $\mu(U)\geq\mu^*(K_1)+\mu^*(K_2)$. We claim that there exist disjoint open $U_1,U_2\subset U$ containing $K_1,K_2$ respectively. Then by condition (d) of Asmp. \ref{lb712}, we get $\mu(U)\geq\mu(U_1\cup U_2)=\mu(U_1)+\mu(U_2)\geq\mu^*(K_1)+\mu^*(K_2)$.

The proof of the claim is a routine argument in point-set topology. Choose any $x\in K_1,y\in K_2$. Since $U$ is Hausdorff, there exist disjoint $V_{xy}\in\Nbh_U(x)$ and $W_{xy}\in\Nbh_U(y)$. Since $K_2$ is compact, there exit $y_1,\dots,y_n\in K_2$ such that $K_2$ is contained in $W_x=W_{xy_1}\cup\cdots \cup W_{xy_n}$. Let $V_x=V_{xy_1}\cap\cdots\cap V_{xy_n}$. Then $V_x$ and $W_x$ are disjoint open subsets of $U$ containing respectively $x$ and $K_2$. By the compactness of $K_1$, there are $x_1,\dots,x_m\in K_1$ such that $K_1$ is covered by $U_1=V_{x_1}\cup\cdots\cup V_{x_m}$. Then $U_2=W_{x_1}\cap\cdots\cap W_{x_m}$ contains $K_2$ and is disjoint from $U_1$. 
\end{proof}


%Condition (e) of Asmp. \ref{lb712} will be used (and only used) in the following way.



\begin{pp}\label{lb719}
$\mu_*:2^X\rightarrow[0,+\infty]$ satisfies $\mu_*(E)\leq\mu^*(E)$ for all $E\subset X$. Moreover, the following are true.
\begin{enumerate}[label=(\alph*)]
\item $\mu_*(\emptyset)=0$.
\item (Monotonicity) If $E\subset F\subset X$, then $\mu_*(E)\leq\mu_*(F)$.
\item (\textbf{Countable superadditivity})\index{00@Countable superadditivity} For countably many \uwave{disjoint} subsets $E_1,E_2,\dots$ of $X$ we have $\mu_*\big(\bigcup_n E_n\big)\geq\sum_n\mu_*(E_n)$.
\end{enumerate}
\end{pp}

\begin{proof}
The monotonicity of $\mu^*$ implies $\mu_*(E)\leq\mu^*(E)$. (a) and (b) are obvious. To prove (c), it suffices to prove $\mu_*\big(\bigcup_k E_k\big)\geq\mu_*(E_1)+\cdots+\mu_*(E_n)$ for all $n$, and hence to prove $\mu_*(E_1\cup\cdots\cup E_n)\geq\mu_*(E_1)+\cdots+\mu_*(E_n)$. By induction on $n$, it suffices to prove $\mu_*(E_1\cup E_2)\geq\mu_*(E_1)+\mu_*(E_2)$. By the definition of $\mu_*$, it suffices to prove that for every compact $K_1\subset E_1$ and $K_2\subset E_2$ we have $\mu_*(E_1\cup E_2)\geq\mu^*(K_1)+\mu^*(K_2)$. By Lem. \ref{lb716}, it suffices to prove $\mu_*(E_1\cup E_2)\geq\mu^*(K_1\cup K_2)$. But this is obvious from the definition of $\mu_*$.
\end{proof}


\subsubsection{Criteria for $\mu$-regularity}\label{lb735}

We now use the idea \eqref{eq290} to derive many criteria for $\mu$-regularity. In fact, the results in this subsection will imply that if $F\subset X$ satisfies $\mu^*(F)<+\infty$, then the set of $\mu$-regular subsets of $F$ is closed under countable unions and relative complements. (Such set is called a \textbf{$\sigma$-ring}.)

\begin{co}\label{lb727}
Let $E_1,E_2,\dots$ be pairwise disjoint $\mu$-regular subsets of $X$. Then $\bigcup_n E_n$ is $\mu$-regular, and 
\begin{align}\label{eq296}
\mu\Big(\bigcup_n E_n\Big)=\sum_n\mu(E_n)
\end{align}
\end{co}

\begin{proof}
Let $E=\bigcup_n E_n$. Then $\mu_*(E)\leq\mu^*(E)$. Since $\mu_*(E_n)=\mu^*(E_n)$, by Prop. \ref{lb718} and \ref{lb719} we have
\begin{align*}
\mu^*(E)\leq\sum_n\mu^*(E_n)=\sum_n\mu_*(E_n)\leq\mu_*(E)
\end{align*}
This proves that $\mu_*(E)=\mu^*(E)$, and that \eqref{eq296} is true.
\end{proof}




\begin{rem}\label{lb728}
Note that the subadditivity often plays a similar role as that of triangle inequality: Suppose that $E\subset F$ and $\mu^*(F\setminus E)$ is small. Then $\mu^*(E)$ is close to $\mu^*(F)$ since $\mu^*(F)-\mu^*(E)\leq m^*(F\setminus E)$. The following two corollaries are applications of this idea.
\end{rem}





\begin{co}\label{lb721}
Let $E\subset X$ satisfy $\mu^*(E)<+\infty$. Then $E$ is $\mu$-regular iff for every $\eps>0$ there exists a compact $K\subset E$ such that $\mu^*(E\setminus K)<\eps$.
\end{co}

\begin{proof}
Proof of ``$\Leftarrow$": $\mu^*(E)\leq\mu^*(K)+\mu^*(E\setminus K)<\mu(K)+\eps\leq \mu_*(E)+\eps$.




Proof of ``$\Rightarrow$": Suppose that $\mu_*(E)=\mu^*(E)$. By the definition of $\mu_*$ and $\mu^*$, there exist open $U\supset E$ and compact $K\subset E$ such that $\mu(U)-\mu^*(E)<\eps/2$ and $\mu_*(E)-\mu(K)<\eps/2$. By Cor. \ref{lb727}, we have
\begin{align}\label{eq289}
\mu(U\setminus K)=\mu(U)-\mu(K)<\eps
\end{align}
Thus $\mu^*(E\setminus K)<\eps$ by the monotonicity of $\mu^*$. 
\end{proof}


\begin{co}\label{lb731}
Let $E\subset X$ satisfy $\mu^*(E)<+\infty$. Then $E$ is $\mu$-regular iff for every $\eps>0$ there exist a compact $K\subset E$ and an open $U\supset E$ such that $\mu(U\setminus K)<\eps$.
\end{co}



\begin{proof}
``$\Leftarrow$": By Cor. \ref{lb721} and $\mu^*(E\setminus K)\leq \mu^*(U\setminus K)<\eps$. ``$\Rightarrow$": By \eqref{eq289}.
\end{proof}

The following lemma is a special case of the main Thm. \ref{lb724}. But it can now be proved easily using Cor. \ref{lb721} and \ref{lb731}.

\begin{lm}\label{lb730}
Let $E_1,E_2$ be $\mu$-regular subsets of $X$ satisfying $\mu(E_1)<+\infty$ and $\mu(E_2)<+\infty$. Then $E_1\cup E_2,E_1\cap E_2,E_2\setminus E_1$ are $\mu$-regular.
\end{lm}

\begin{proof}
We first note that all sets involved have finite outer measure: The largest set is $E_1\cup E_2$, and $\mu^*(E_1\cup E_2)\leq \mu^*(E_1)+\mu^*(E_2)<+\infty$.

Choose any $\eps>0$. By Cor. \ref{lb721}, there exist compact $K_1\subset E_1$ and $K_2\subset E_2$ such that $\mu^*(E_i\setminus K_i)<\eps/2$. It is clear from Venn diagrams that 
\begin{subequations}
\begin{gather}
(E_1\cup E_2)\setminus(K_1\cup K_2)\subset (E_1\setminus K_1)\cup (E_2\setminus K_2)\\
(E_1\cap E_2)\setminus(K_1\cap K_2)\subset (E_1\setminus K_1)\cup (E_2\setminus K_2)
\end{gather}
\footnote{$(E_1\cap E_2)\setminus(K_1\cap K_2)=(E_1\cap E_2)\cap (K_1^c\cup K_2^c)=(E_1\cap E_2\cap K_1^c)\cup (E_1\cap E_2\cap K_2^c)\subset (E_1\setminus K_1)\cup (E_2\setminus K_2)$} By the subadditivity of $\mu^*$, both $\mu^*((E_1\cup E_2)\setminus(K_1\cup K_2))$ and $\mu^*((E_1\cap E_2)\setminus(K_1\cap K_2))$ are $<\eps$. Therefore, by Cor. \ref{lb721}, $E_1\cup E_2$ and $E_1\cap E_2$ are regular.

To show that $E_2\setminus E_1$ is regular, by replacing $E_1$ with $E_1\cap E_2$, we assume $E_1\subset E_2$. By Cor. \ref{lb731}, there exists an open $U_1\supset E_1$ such that $\mu^*(U_1\setminus E_1)<\eps/2$. By Cor. \ref{lb721}, there is a compact $K_2\subset E_2$ such that $\mu^*(E_2\setminus K_2)<\eps/2$. Another Venn diagram shows
\begin{align}
(E_2\setminus E_1)\setminus (K_2\setminus U_1)\subset (E_2\setminus K_2)\cup (U_1\setminus E_1)
\end{align}
\footnote{$(E_2\setminus E_1)\setminus (K_2\setminus U_1)=E_2\cap E_1^c\cap (K_2\cap U_1^c)^c=(E_2\cap E_1^c\cap K_2^c)\cup (E_2\cap E_1^c\cap U_1)\subset (E_2\setminus K_2)\cup (U_1\setminus E_1)$} By the subadditivity of $\mu^*$, we have $\mu^*((E_2\setminus E_1)\setminus (K_2\setminus U_1))<\eps$. Note that $K_2\setminus U_1$ is a compact subset of $E_2\backslash E_1$. Therefore, by Cor. \ref{lb721}, $E_2\setminus E_1$ is $\mu$-regular.
\end{subequations}
\end{proof}


\begin{exe}
Prove Lem. \ref{lb730} using only Cor. \ref{lb731} and not Cor. \ref{lb721}. 
\end{exe}







\subsubsection{Locally $\mu$-regular sets}\label{lb736}


When $\mu(X)<+\infty$, the results in Subsec. \ref{lb735} imply easily that the set of $\mu$-regular sets form a $\sigma$-algebra $\fk M_\mu$ containing all open sets (and hence containing $\fk B_X$), and that $\mu$ is a measure on $\fk M_\mu$. However, when $\mu(X)=+\infty$,  $\mu$-regular sets are not good enough. In fact, given a $\mu$-regular set $E$ with $\mu(E)=+\infty$, the regularity only implies that $E$ contains a compact subset $K$ with large enough $\mu(K)$. But it does not imply that $\mu^*(E\setminus K)$ can be small enough. In other words, Cor. \ref{lb721} and \ref{lb731} may fail for regular sets with infinite $\mu$-measures. Therefore, one cannot prove that the $\mu$-regular sets form a $\sigma$-algebra. To overcome this difficulty, we consider the better notion of local $\mu$-regularity:
\begin{df}
Let $E\subset X$. We say that $E$ is \textbf{locally $\mu$-regular} \index{00@Locally $\mu$-regular} if $\mu^*(E\cap\Omega)=\mu_*(E\cap\Omega)$ for every open set $\Omega\subset X$ satisfying $\mu(\Omega)<+\infty$.
\end{df}

%We shall discuss the relationship between regular sets and locally regular sets. First, we need a lemma:



The following proposition shows that when $\mu(X)<+\infty$, the $\mu$-regularity and the local $\mu$-regularity are equivalent.

\begin{pp}\label{lb723}
Assume that $E\subset X$ satisfies $\mu^*(E)<+\infty$. Then $E$ is $\mu$-regular iff $E$ is locally $\mu$-regular. 
\end{pp}




\begin{proof}
Assume that $E$ is locally regular. Since $\mu^*(E)<+\infty$, there exists an open $U\supset E$ such that $\mu(U)<+\infty$. So $E\cap U$ is regular. Hence $E$ is regular. Conversely, assume that $E$ is regular. Let $\Omega\subset X$ be an open set satisfying $\mu(\Omega)<\infty$. Since $U$ is $\mu$-regular, by Lem. \ref{lb730}, $E\cap\Omega$ is regular. So $E$ is locally regular.
\end{proof}
























\subsubsection{The main theorem}

\begin{thm}\label{lb724}
Let $\mu:2^X\rightarrow[0,+\infty]$ satisfy Asmp. \ref{lb712}. Let $\fk M_\mu$ be the set of locally $\mu$-regular subsets of $X$. Then $\fk M_\mu$ is a $\sigma$-algebra containing $\fk B_X$, and $\mu^*$ is a complete measure on $\fk M_\mu$. 
\end{thm}



%We divide the proof into two steps. In the first step, we prove that $\fk M_\mu$ is a $\sigma$-algebra. In the second step, we prove that $\fk M_\mu$ contains $\fk B_X$, and that $(\fk M_\mu,\mu)$ is complete.

After proving this theorem, we will write $\mu^*(E)$ as $\mu(E)$ if $E\in\fk M_\mu$. The restriction $(\fk B_X,\mu)$ will be called a \textbf{Radon measure} if $X$ is LCH and $\mu$ is finite on compact sets. We will discuss Radon measures in detail in a future chapter.

\begin{proof}
Step 1. We show that $\fk M_\mu$ is a $\sigma$-algebra. Clearly $\emptyset\in\fk M_\mu$.  Let $\Omega\in\mc T_X$ such that $\mu(\Omega)<+\infty$. We want to show that if $E\cap\Omega$ is regular then $E^c\cap\Omega$ is also regular. We want to show that if $E_1,E_2,\dots\subset 2^X$ are such that $E_1\cap\Omega,E_2\cap\Omega,\dots$ are regular, then $(\bigcup_n E_n)\cap\Omega$ is regular. By replacing $E$ with $E\cap\Omega$ and $E_n$ with $E_n\cap\Omega$, it suffices to prove:
\begin{enumerate}
\item[(a)] If $E\subset\Omega$ is regular, then $\Omega\setminus E$ is regular.
\item[(b)] If $E_1,E_2,\dots$ are regular subsets of $\Omega$, then $\bigcup_n E_n$ is regular. 
\end{enumerate}
By Lem. \ref{lb730}, (a) is true. Let $E_1,E_2,\dots\subset\Omega$ be regular. Let $F_1=E_1$ and $F_n=E_n\setminus(E_1\cup\cdots\cup E_{n-1})$ if $n>1$. Then each $F_n$ is regular by Lem. \ref{lb730}. Therefore, by Cor. \ref{lb727}, $\bigcup E_n=\bigsqcup_n F_n$ is regular.\\[-1ex]

Step 2. By Asmp. \ref{lb712}-(e), every open set is regular. Thus every open set is also locally regular. Therefore, $\fk M_\mu$ contains $\mc T_X$, and hence contains $\fk B_X=\sigma(\mc T_X)$.

Clearly $\mu^*(\emptyset)=0$. To show that $\mu^*$ is a measure on $\fk M_\mu$, we take mutually disjoint $E_1,E_2,\dots\in\fk M_\mu$, and we shall show that $\mu^*(E)=\sum_n\mu^*(E_n)$ where $E=\bigcup_n E_n$. If $\mu^*(E)=+\infty$, by the countable subadditivity, we have $+\infty=\mu^*(E)\leq\sum_n\mu^*(E_n)$, and hence $\mu^*(E)=\sum_n\mu^*(E_n)$. So it suffices to assume $\mu^*(E)<+\infty$. Therefore $\mu^*(E_n)<+\infty$ for each $n$. By Prop. \ref{lb723}, $E_n$ is regular. Therefore $\mu^*(E)=\sum_n\mu^*(E_n)$ by Cor. \ref{lb727}.

Finally, we need to prove that $\mu^*$ is complete on $\fk M_\mu$. Let $E\in\fk M_\mu$ such that $\mu^*(E)=0$. Let $F\subset E$. Then $\mu^*(F)\leq\mu^*(E)=0$ and hence $\mu^*(F)=0$. So $\mu_*(F)=\mu^*(F)=0$. Therefore $F$ is regular and has finite $\mu^*$-value. So $F\in\fk M_\mu$ by Prop. \ref{lb723}.
\end{proof}



\subsubsection{A relationship between $\fk M_\mu$ and the completion of $\fk B_X$}





By Thm. \ref{lb708}, $(\fk M_\mu,\mu)$ extends the completion of $(\fk B_X,\mu)$. Very often, we only care about the completion of $(\fk B_X,\mu)$ but not about the larger set $\fk M_\mu$. However, the following proposition shows that in many cases (e.g. when $X=\Rbb^N$ and $\mu=m$), $(\fk M_\mu,\mu)$ is equal to the completion.


\begin{spp}\label{lb737}
Assume that $(\fk B_X,\mu)$ is \textbf{$\sigma$-finite}, i.e., $X$ is a countable union of elements of $\fk B_X$ with finite $\mu$-measures. Then $(\fk M_\mu,\mu)$ is the completion of $(\fk B_X,\mu)$.
\end{spp}

\begin{proof}[$\star$ Proof]
$(\fk M_\mu,\mu)$ extends the completion $(\fk M,\mu)$. We want to show that $\fk M_\mu\subset\fk M$. Let $E\in\fk M_\mu$. Since $X$ is $\sigma$-finite, we have $X=\bigcup_n A_n$ where $A_n\in\fk B_X$ and $\mu(A_n)<+\infty$. It suffices to prove that $E_n:=E\cap A_n$ belongs to $\fk M$ for each $n$. Note that $E_n\in\fk M_\mu$ and $\mu(E_n)<+\infty$. Thus, by Cor. \ref{lb721}, for each $k\in\Zbb_+$ there exists a compact $K_k\subset E_n$ such that $\mu(U_k\setminus K_k)<1/k$. By replacing $K_k$ with $K_1\cup\cdots\cup K_k$, we assume $(K_k)$ is decreasing. Let $F=\bigcup_k K_k$, which is a Borel set. Then $F\subset E_n$ and $E_n\setminus F$ is $\mu$-null. Since $(\fk M,\mu)$ is complete, we see that $E_n\setminus F$ belongs to $\fk M$. So $E_n\in\fk M$.
\end{proof}


\subsection{A discussion of measurable sets: from Jordan to Lebesgue to Carath\'eodory}\label{lb733}


Let $(X,\mc T_X)$ be a topological space, and assume that $\mu:\mc T_X\rightarrow[0,+\infty]$ satisfies Asmp. \ref{lb712}. 

\subsubsection{Lebesgue's outer measure and inner measure}

Here, I will make some comparisons between our approach in Sec. \ref{lb726} and Lebesgue's original method. We refer the readers to \cite[Sec. 5.1]{Haw} and \cite[Sec. 9.6]{Jah} for detailed discussions of Lebesgue's approach.


In the 1902 paper where Lebesgue introduced his integral theory, he focused on measurable subsets of $[a,b]$. For the convenience of the following discussion, I will consider a bounded open interval $(a,b)$ instead. For each $E\subset(a,b)$, Lebesgue define the outer measure $m^*(E)$ by \eqref{eq294}, i.e., the infinimum of the total sizes of intervals covering $E$. It is not hard to see that his definition agrees with ours, since any open subset of $\Rbb$ is a countable disjoint union of open intervals (cf. Pb. \ref{lb679}). 

On the other hand, Lebesgue's definition of inner measure is
\begin{align}\label{eq291}
m_*(E)=b-a-m^*((a,b)\setminus E)
\end{align}
If we generalize \eqref{eq291} to the setting of Sec. \ref{lb726} where $E$ is a subset of an open $\Omega\subset X$ satisfying $\mu(\Omega)<+\infty$, the inner measure of $E$ should be defined by $\mu(\Omega)-\mu^*(\Omega\setminus E)$. We now show that this definition is equal to our definition of $\mu_*(E)$ (cf. \eqref{eq292b}): 

\begin{pp}\label{lb720}
Let $\Omega$ be an open subset of $X$ such that $\mu(\Omega)<+\infty$. Let $E\subset\Omega$. Then
\begin{align}\label{eq295}
\mu_*(E)=\mu(\Omega)-\mu^*(\Omega\setminus E)
\end{align}
\end{pp}



\begin{proof}[$\star$ Proof]
Let $F=\Omega\setminus E$. By the definition of $\mu^*$, we have
\begin{align*}
\mu^*(F)=\inf\big\{\mu(V): V\text{ is open in $\Omega$ and contains $F$} \big\}
\end{align*}
By Lem. \ref{lb730}, $\Gamma:=\Omega\setminus V$ is $\mu$-regular, and $\mu(\Omega)=\mu(V)+\mu(\Gamma)$. Therefore, $\mu(\Omega)-\mu^*(\Omega\setminus E)$ equals the supremum of $\mu(\Gamma)$ where $\Gamma$ is a closed subset of $\Omega$ contained in $E$. Thus,  proving \eqref{eq295} means proving
\begin{align*}
\sup\big\{\mu(K):K\subset E\text{ is compact}\big\}=\sup\big\{\mu(\Gamma):\Gamma\text{ is closed in $\Omega$, and }\Gamma\subset E\big\}
\end{align*}
We clearly have ``$\leq$". Since $\mu(\Gamma)=\mu_*(\Gamma)$, for each $\eps>0$ there exists a compact $K\subset \Gamma$ such that $\mu(K)>\mu(\Gamma)-\eps$. So ``$\geq$" is also true.
\end{proof}


\subsubsection{Subadditivity and superadditivity}

As mentioned in the slogan \eqref{eq290}, the countable additivity of $\mu$ on regular sets follows directly from the fact that the outer measure $\mu^*$ is countably subadditive, and the inner measure $\mu_*$ is countably superadditive. This simple and intuitive idea is not new. It already appeared in Darboux integrals: Given a bounded real-valued function $f$ defined on $R=[a_1,b_1]\times\cdots[a_N,b_N]$, the upper Darboux integral $\ovl\int f$ and the lower Darboux integral $\udl\int f$ are defined in a similar way as in Thm. \ref{lb444}, where the Darboux sums are defined by partitioning the box $R$ into smaller boxes. It is not hard to check that $\ovl\int$ and $\udl\int$ satisfy respectively the subadditivity and the superadditivity:
\begin{align*}
\ovl\int(f+g)\leq\ovl\int f+\ovl\int g\qquad \udl\int(f+g)\geq\udl\int f+\udl\int g
\end{align*}
(See also Pb. \ref{lb732}.) Thus, since $f$ is Riemann integrable iff $\ovl\int f=\udl\int f$, $\ovl\int$ and $\udl\int$ must be linear on Riemann integrable functions.



In fact, before Lebesgue,  Jordan had utilized this idea (subadditivity $+$ superadditivity $\Rightarrow$ additivity) to study measurable sets in 1902 (cf. \cite[Sec. 9.4]{Jah} and \cite[4.1]{Haw}): For each bounded $E\subset\Rbb^N$, the \textbf{outer content}  $c^*(E)$ is defined to be the infinimum of the total sizes of \textit{finitely many boxes covering $E$}, and the \textbf{inner content}  $c_*(E)$ is the supremum of the total sizes of finitely many disjoint boxes contained in $E$.  Jordan defined $E$ to be measurable if $c^*(E)=c_*(E)$. Using our familiar language, 
\begin{align}
c^*(E)=\ovl\int\chi_E\qquad c_*(E)=\udl\int\chi_E
\end{align}
and $E$ is Jordan-measurable iff $\chi_E$ is Riemann integrable. $c^*$ and $c_*$ satisfy subadditivity and superadditivity respectively, just as $\ovl\int$ and $\udl\int$ do. So $c^*$ satisfies additivity on Jordan-measurable sets. 

Compared to Jordan, an important improvement Lebesgue made is that he extended the finite subadditivity and superadditivity to countable ones. He achieved this goal by giving the better definition of outer measure. Recall that the outer content $c^*(E)$ is defined by the infinimum of the sizes of \textbf{simple regions} (i.e., finite unions of boxes) covering $E$. The outer Lebesgue measure $m^*(E)$ is defined in a similar way, except that one allows for \textit{countable} unions of boxes to cover $E$. From the modern viewpoint (i.e., the viewpoint in Sec. \ref{lb688} and \ref{lb726}), $m^*(E)$ is defined by covering $E$ by arbitrary open sets, not just by simple regions. The modern viewpoint is equivalent to the classical one due to the following observation:

\begin{exe}
Let $\Omega$ be an open subset of $\Rbb^N$. Show that $\Omega$ is a countable union of boxes $R_1\cup R_2\cup\cdots$ where $\Int(R_i)\cap\Int(R_j)=\emptyset$ if $i\neq j$. Here, a box denotes a set $I_1\times\cdots\times I_N$ where each $I_i$ is a bounded interval in $\Rbb$. 
\end{exe}

\begin{proof}[Hint]
First treat the case that $\Omega$ is inside $R=[0,1]^N$. In step $k$, partition $R$ equally into $2^{kN}$ pieces, and take all subboxes inside $\Omega$ but not inside the subboxes taken from step $1$ to step $k-1$. In the general case, consider $\Omega\cap R$ where $R=[n_1,n_1+1]\times\cdots\times[n_N,n_N+1]$ and $n_1,\dots,n_N\in\Zbb$.
\end{proof}


\subsubsection{The dilemma of $\mu$-regular sets with possibly infinite measures.}


We know that when $\mu(X)<+\infty$, the $\mu$-regular sets form a $\sigma$-algebra containing $\fk B_X$ (Thm. \ref{lb724} and Prop. \ref{lb723}). When $\mu(X)=+\infty$, this statement cannot be proved, so we must find alternatives to $\mu$-regular sets. 

The alternatives we gave in Sec. \ref{lb726} are locally $\mu$-regular sets, i.e., the sets $E$ such that $E\cap\Omega$ is $\mu$-regular for any open $\mu$-finite set $\Omega$. Our treatment is similar to that in Rudin's book \cite{Rud-R}, except that Rudin considered those $E$ such that $E\cap K$ is regular for any compact $K\subset X$. (Rudin assumed that $X$ is LCH, and $\mu$ is finite on compact sets.) See the proof of Thm. 2.14 in \cite{Rud-R}. In particular, Step IV of the proof that theorem is similar to Cor. \ref{lb727}, Step V is similar to Cor. \ref{lb731}, Step VI is similar to Lem. \ref{lb730}, Step VII and IX are similar to Thm. \ref{lb724}, and Step VIII is similar to Prop. \ref{lb723}.














\hypertarget{beforeindex}{}






















\subsection{Problems and supplementary material}

\subsubsection{Basic properties}

\begin{df}\label{lb707}
Let $(X,\fk M)$ be a measurable space. Let $Y$ be a subset of $X$. Let
\begin{align}
\fk M|_Y=\iota^{-1}(\fk M)
\end{align}
where $\iota:Y\rightarrow X$ is the inclusion map. In other words, $\fk M|_Y=\{Y\cap E:E\in\fk M\}$. Then $\fk M|_Y$ is clearly a $\sigma$-algebra on $Y$, called the \textbf{restriction} \index{00@Restriction of $\sigma$-algebra} of $\fk M$ to $Y$.
\end{df}

\begin{exe}
Let $(X,\fk M)$ and $(Y,\fk N)$ be measurable spaces. Let $f:X\rightarrow Y$ be a map. Let $Z\subset Y$. Let $\fk N|_Z$ be the restriction of $\fk N$ to $Z$. Prove that $f:(X,\fk M)\rightarrow(Y,\fk N)$ is measurable iff $f:(X,\fk M)\rightarrow (Z,\fk N|_Z)$ is measurable.
\end{exe}

\begin{prob}
Let $Y$ be a topological space, and let $Z$ be a subspace of $Y$ (equipped with the subspace topology). Prove $\fk B_Y|_Z=\fk B_Z$.
\end{prob}

\begin{proof}[Hint]
Apply Cor. \ref{lb695} to the inclusion map $\iota:Z\rightarrow Y$.
\end{proof}


\begin{df}
Let $X$ be a topological space. A function $f:X\rightarrow\ovl\Rbb$ is called \textbf{lower semicontinuous} \index{00@Lower semicontinuous} if $f^{-1}(a,+\infty]$ is open for each $a\in\ovl\Rbb$. We say that $f$ is  \textbf{upper semicontinuous} \index{00@Upper semicontinuous} if $f^{-1}[-\infty,a)$ is open for each $a\in\ovl\Rbb$. By Exp. \ref{lb697}, semicontinuous functions are Borel functions.
\end{df}

\begin{eg}\label{lb698}
Recall Def. \ref{lb114} for the meaning of summations and multiplications in $\ovl\Rbb_{\geq0}$. Then the addition map $(a,b)\in\ovl\Rbb_{\geq0}\times\ovl\Rbb_{\geq0}\mapsto a+b\in\ovl\Rbb_{\geq0}$ is continuous. The the multiplication map $(a,b)\in\ovl\Rbb_{\geq0}\times\ovl\Rbb_{\geq0}\mapsto ab\in\ovl\Rbb_{\geq0}$ is lower semicontinuous. (It is continuous when restricted to $\Rbb_{\geq0}\times\Rbb_{\geq0}$.)
\end{eg}


\begin{prob}
Let $X$ be a topological space.
\begin{enumerate}
\item Let $A\subset X$. Prove that $\chi_A$ is lower semicontinuous iff $A$ is open. 
\item Let $(f_i)_{i\in I}$ be a family of lower semicontinuous functions $X\rightarrow\ovl\Rbb$. Let $f(x)=\sup_{i\in I}f_i(x)$. Prove that $f:X\rightarrow\ovl\Rbb$ is lower semicontinuous.
\end{enumerate}
\end{prob}

Recall Pb. \ref{lb346} for the basic properties of $\limsup$ and $\liminf$.
\begin{prob}
Let $X$ be a topological space. Let $f:X\rightarrow\ovl\Rbb$. Prove that the following are equivalent.
\begin{enumerate}
\item[(1)] $f$ is lower semicontinuous.
\item[(2)] For each $x\in X$ and each net $(x_\alpha)$ in $X$ converging to $x$, we have $\limsup_\alpha f(x_\alpha)\geq f(x)$.
\item[(3)] For each $x\in X$ and each net $(x_\alpha)$ in $X$ converging to $x$, we have $\liminf_\alpha f(x_\alpha)\geq f(x)$.
\end{enumerate}
\end{prob}


\subsubsection{$\star$ Weakly measurable functions}

Let $(X,\fk M)$ be a measurable space. 

\begin{df}
Let $\mc V$ be a normed vector space over $\Fbb\in\{\Rbb,\Cbb\}$, equipped with the norm topology and the corresponding Borel $\sigma$-algebra $\fk B_{\mc V}$. We say that a map $f:X\rightarrow\mc V$ is \textbf{weakly measurable} \index{00@Weakly measurable} if for every $\varphi\in\mc V^*=\fk L(\mc V,\Fbb)$ the function $\varphi\circ f:X\rightarrow\Fbb$ is measurable. 
\end{df}

It is clear (from Rem. \ref{lb696}) that if $f$ is measurable, then $f$ is weakly measurable.

\begin{exe}\label{lb706}
Let $\mc V$ be a normed vector space. Let $f:X\rightarrow\mc V$ be a function. Suppose that $\mc W$ is a linear subspace of $\mc V$ containing $f(X)$. Prove that $f:X\rightarrow\mc V$ is weakly measurable iff $f:X\rightarrow\mc W$ is weakly measurable.
\end{exe}
\begin{proof}[Hint]
Use Hahn-Banach Thm. \ref{lb499}.
\end{proof}


\begin{prob}\label{lb702}
Let $\mc V$ be a normed vector space over $\Fbb\in\{\Rbb,\Cbb\}$. Let $f:X\rightarrow\mc V$. Suppose that $f(X)$ is separable (as a metric subspace of $\mc V$). Prove that $f$ is measurable iff $f$ is weakly measurable.
\end{prob}

\begin{proof}[Hint]
Let $f:X\rightarrow\mc V$ be weakly measurable. Use Exe. \ref{lb706} to show that one can assume WLOG that $\mc V$ is separable.  Then, by Thm. \ref{lb523}, there exist countably many elements $\varphi_1,\varphi_2,\dots$ forming a weak-* dense subset of $\ovl B_{\mc V^*}(0,1)$. Prove that $\Vert v\Vert=\sup_n |\bk{\varphi,v}|$ for each $v\in\mc V$. (Hint: Show that $v$ can be viewed as a \textit{continuous} function on $\ovl B_{\mc V^*}(0,1)$.) Conclude that for each $v\in\mc V$, the function $x\in X\mapsto \Vert f(x)-v\Vert$ is measurable. Use this fact to show that $f$ is measurable.
\end{proof}



\begin{prob}\label{lb705}
Let $Y$ be a metric space. Let $(f_n)$ be a sequence of measurable functions $X\rightarrow Y$ converging pointwise to $f:X\rightarrow Y$. Assume that $f(X)$ is separable. Prove that $f$ is measurable.
\end{prob}

\begin{proof}[Hint]
First method: By Thm. \ref{lb521}, $Y$ can be viewed as a metric subspace of a real normed vector space $\mc V$. Show that $f$ is weakly measurable.

Second method: First prove that for each $y\in Y$, the function $x\in X\mapsto d(f(x),y)$ is measurable. Conclude that the map $f:X\rightarrow f(X)$ is measurable.
\end{proof}

\begin{comment}
\begin{proof}
By Thm. \ref{lb521}, $Y$ can be isometrically embedded into a real Banach space $\mc V$. So we view $Y$ as a metric subspace of $\mc V$. Since $Y$ is separable, $Y$ is contained in a separable linear subspace $\mc W$ of $\mc V$. (Proof: Let $E$ be a countable dense subset of $Y$. Then $\Span_\Qbb E$ is a countable countable dense subset of $\mc W=\Span_\Rbb E$.) So we can view $f_n$ and $f$ as functions $X\rightarrow\mc W$. By Cor. \ref{lb701}, $f$ is weakly measurable. Therefore, by Pb. \ref{lb702}, $f$ is measurable.
\end{proof}
\end{comment}


In fact, without assuming that $f(X)$ is separable, the statement in Pb. \ref{lb705} is still true. (See the end of Sec. 38 in \cite{Yu}.) However, measurable functions whose ranges are non-separable metric spaces are not very useful because Exp. \ref{lb703} does not hold for such spaces.



\begin{prob}
Let $f:X\rightarrow l^2(\Zbb_+)$ be a map. For each $x\in X$, write $f(x)=(f_1(x),f_2(x),\dots)$. Prove that $f$ is measurable iff $f_n:X\rightarrow\Cbb$ is measurable for each $n\in\Zbb_+$.
\end{prob}


\subsubsection{$\star$ Construction of measures}

Let $(X,\mc T_X)$ be a topological space. Let $\mu:\mc T_X\rightarrow[0,+\infty]$ and its associated $\mu^*,\mu_*:2^X\rightarrow[0,+\infty]$ be as in Asmp. \ref{lb712}. Recall from Thm. \ref{lb724} that $\mu_*$ is a measure on the Borel $\sigma$-algebra $\fk B_X$, and $\mu_*$ is denoted by $\mu$ when restricted to $\fk B_X$.

\begin{prob}\label{lb734}
Let $E\subset X$. Prove that 
\begin{subequations}
\begin{gather}
\mu_*(E)=\sup\big\{\mu(A):A\in\fk B_X\text{ and }A\subset E \big\}\qquad\text{if }\mu^*(E)<+\infty\\
\mu^*(E)=\inf\big\{\mu(B):B\in\fk B_X\text{ and }E\subset B \big\}
\end{gather}
\end{subequations}
Conclude that if $\Omega\in\fk B_X$ satisfies $\mu(\Omega)<+\infty$, then
\begin{align}
\mu_*(\Omega\cap E)=\mu(\Omega)-\mu^*(\Omega\setminus E)
\end{align}
This formula generalizes Prop. \ref{lb720}.
\end{prob}

\begin{rem}
In a similar way, it can be proved that the statements in Pb. \ref{lb734} would still be valid if $\fk B_X$ is replaced by $\fk M_\mu$.
\end{rem}

The following problem is parallel to Cor. \ref{lb721} and \ref{lb731}.

\begin{prob}\label{lb738}
Let $E\subset X$. Assume that $\mu^*(E)<+\infty$. Prove that $E$ is $\mu$-regular iff for every $\eps>0$ there is an open $U\subset X$ such that $\mu^*(U\setminus E)<\eps$.
\end{prob}

\begin{proof}[Hint for $\Leftarrow$]
Show that $E=G\setminus \Delta$ where $G\in\fk B_X$ and $\mu^*(\Delta)=0$.
\end{proof}









\newpage

\printindex	






	\begin{thebibliography}{999999}
		\footnotesize	

\bibitem[AK]{AK}
Albiac, F., \& Kalton, N. J. (2006). Topics in Banach space theory (Vol. 233, pp. xii+-373). New York: Springer.

\bibitem[Axl]{Axl}
Axler, S. (2015). Linear algebra done right. 3rd ed.

\bibitem[BK84]{BK84}
Birkhoff, G., \& Kreyszig, E. (1984). The establishment of functional analysis. Historia Mathematica, 11(3), 258-321.

%\bibitem[BS]{BS}
%B\"uhler, T., \& Salamon, D. A. (2018). Functional analysis (Vol. 191). American Mathematical Soc..


\bibitem[Che92]{Che92}
Chernoff, P. R. (1992). A simple proof of Tychonoff's theorem via nets. The American mathematical monthly, 99(10), 932-934.


\bibitem[Die-H]{Die-H}
Dieudonn\'e, J. (1983). History of Functional Analysis. North Holland, 1st edition.

\bibitem[Eva]{Eva}
Evans, L. C. (2022). Partial differential equations (Vol. 19). American Mathematical Society.

\bibitem[Fol]{Fol}
Folland, G. B. (1999). Real analysis: modern techniques and their applications (Vol. 40). John Wiley \& Sons.

\bibitem[Gud74]{Gud74}
Gudder, S. (1974). Inner product spaces. The American Mathematical Monthly, 81(1), 29-36.

\bibitem[Hau14]{Hau14}
Hausdorff, F. (1914) Grundz\"uge der Mengenlehre


\bibitem[Haw]{Haw}
Hawkins, T. (1979) Lebesgue's theory of integration: Its origins and development. Corrected reprint of the 2nd edition. 


\bibitem[Hil12]{Hil12}
Hilbert, D. (1912) Grundz\"uge einer allgemeinen Theorie der linearen Integralgleichungen.

\bibitem[Jah]{Jah}
Jahnke, H. N. (2003). A history of analysis (No. 24). American Mathematical Soc..


\bibitem[Kel]{Kel}
Kelley, J. L., General topology. 

\bibitem[Kli]{Kli}
Kline, M. (1990). Mathematical Thought from Ancient to Modern Times.

\bibitem[Lax]{Lax}
Lax, P. D., (2002) Functional analysis. 


\bibitem[Lee]{Lee}
Lee, J. M., (2013). Introduction to Smooth manifolds, 2nd ed. Springer.

\bibitem[MS22]{MS22}
Moore, E. H., \& Smith, H. L. (1922). A general theory of limits. American journal of Mathematics, 44(2), 102-121.

\bibitem[Mun]{Mun}
Munkres, J. (2000). Topology. Second Edition.

\bibitem[NB97]{NB97}
Narici, L., \& Beckenstein, E. (1997). The Hahn-Banach theorem: the life and times. Topology and its Applications, 77(2), 193-211.


\bibitem[vNeu30]{vNeu30}
von Neumann, J. (1930). Allgemeine eigenwerttheorie hermitescher funktionaloperatoren. Mathematische Annalen, 102(1), 49-131.

\bibitem[Nie]{Nie}
Nietzsche, F. W. (1887). The Gay Science: with a Prelude in Rhymes and an Appendix of Songs. Translated, with Commentary, by Walter Kaufmann (1974).

\bibitem[RN]{RN}
F. Riesz and B. Sz.-Nagy, Functional analysis. Transl. from the 2nd French ed. by Leo F. Boron. (1956)

\bibitem[RS]{RS}
Reed, M., \& Simon, B. (1972). Methods of Modern Mathematical Physics I: Functional analysis, Academic Press, New York.



\bibitem[Rud-P]{Rud-P}
Rudin, W. (1976). Principles of Mathematical Analysis. 3rd ed.

\bibitem[Rud-R]{Rud-R}
Rudin, W. (1987). Real and complex analysis. 3rd ed. 


\bibitem[Rud-F]{Rud-F}
Rudin, W. (1991). Functional analysis. 2nd ed.

\bibitem[Sim-R]{Sim-R}
Simon, B. (2015). Real analysis. A comprehensive course in analysis, part 1. Providence, RI: American Mathematical Society (AMS)

\bibitem[Sim-O]{Sim-O}
Simon, B. (2015). Operator theory. A comprehensive course in analysis, part 4. Providence, RI: American Mathematical Society (AMS) 

\bibitem[Sto37]{Sto37}
Stone, M. H. (1937). Applications of the theory of Boolean rings to general topology. Transactions of the American Mathematical Society, 41(3), 375-481.

\bibitem[Sto48]{Sto48}
Stone, M. H. (1948). The generalized Weierstrass approximation theorem. Mathematics Magazine, 21(5), 167-184, 237-254.


\bibitem[Tay]{Tay}
M. E. Taylor, Partial differential equations. I: Basic theory. 2nd ed. New York, NY: Springer (2011)

\bibitem[Wil]{Wil}
Willard, S. (1970). General topology. 

\bibitem[Yu]{Yu}
Pin Yu (2020). Lecture notes of mathematical analysis (in Chinese). Version of 2020.12.27


\bibitem[Zor-2]{Zor-2}
Zorich, V. A.(2016). Mathematical analysis II, 2nd ed.

		
\end{thebibliography}

\noindent {\small \sc Yau Mathematical Sciences Center, Tsinghua University, Beijing, China.}

\noindent {\textit{E-mail}}: binguimath@gmail.com\qquad bingui@tsinghua.edu.cn
\end{document}









