% !TeX spellcheck = en_US
% !TEX program = pdflatex
\documentclass[12pt,b5paper,notitlepage]{article}
\usepackage[b5paper, margin={0.5in,0.65in}]{geometry}
%\usepackage{fullpage}
\usepackage{amsmath,amscd,amssymb,amsthm,mathrsfs,amsfonts,layout,indentfirst,graphicx,caption,mathabx, stmaryrd,appendix,calc,imakeidx,upgreek} % mathabx for \wtidecheck
%\usepackage{ulem} %wave underline
\usepackage[dvipsnames]{xcolor}
\usepackage{palatino}  %template
\usepackage{slashed} % Dirac operator
\usepackage{mathrsfs} % Enable using \mathscr
%\usepackage{eufrak}  another template/font
\usepackage{extarrows} % long equal sign, \xlongequal{blablabla}
\usepackage{enumitem} % enumerate label change e.g. [label=(\alph*)]  shows (a) (b) 


\usepackage{csquotes} % \begin{displayquote}   \begin{displaycquote}  for quotation
\usepackage{epigraph}   %\epigraph{}{}  for quotation
%\pmb  mandatory math bold 

\usepackage{fancyhdr} % date in footer

%\usepackage{soul}  %\ul underline break line automatically

\usepackage{ulem}  % \uline  underline break line   also    \uwave

\usepackage{relsize} % use \mathlarger \larger \text{\larger[2]$...$} to enlarge the size of math symbols

\usepackage{verbatim}  % comment environment


\usepackage{halloweenmath} % Interesting halloween math symbols

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{tcolorbox}
\tcbuselibrary{theorems}
% box around equations   \tcboxmath
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% circled colon and thick colon \hcolondel and \colondel

\usepackage{pdfrender}

\newcommand*{\hollowcolon}{%
	\textpdfrender{
		TextRenderingMode=Stroke,
		LineWidth=.1bp,
	}{:}%
}

\newcommand{\hcolondel}[1]{%
	\mathopen{\hollowcolon}#1\mathclose{\hollowcolon}%
}
\newcommand{\colondel}[1]{%
	\mathopen{:}#1\mathclose{:}%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\usepackage{tikz}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

\usepackage{tikz-cd}
\usepackage[nottoc]{tocbibind}   % Add  reference to ToC


\makeindex


% The following set up the line spaces between items in \thebibliography
\usepackage{lipsum}  
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
	\OLDthebibliography{#1}
	\setlength{\parskip}{0pt}
	\setlength{\itemsep}{2pt} 
}


\allowdisplaybreaks  %allow aligns to break between pages
\usepackage{latexsym}
\usepackage{chngcntr}
\usepackage[colorlinks,linkcolor=blue,anchorcolor=blue, linktocpage,
%pagebackref
]{hyperref}
\hypersetup{ urlcolor=cyan,
	citecolor=[rgb]{0,0.5,0}}


\setcounter{tocdepth}{2}	 %hide subsections in the content


\counterwithin{figure}{section}

\pagestyle{plain}

\captionsetup[figure]
{
	labelsep=none	
}













\theoremstyle{definition}
\newtheorem{df}{Definition}[section]
\newtheorem{eg}[df]{Example}
\newtheorem{exe}[df]{Exercise}
\newtheorem{rem}[df]{Remark}
\newtheorem{obs}[df]{Observation}
\newtheorem{ass}[df]{Assumption}
\newtheorem{cv}[df]{Convention}
\newtheorem{prin}[df]{Principle}
\newtheorem{nota}[df]{Notation}
\newtheorem*{axiom}{Axiom}
\newtheorem{coa}[df]{Theorem}
\newtheorem{srem}[df]{$\star$ Remark}
\newtheorem{seg}[df]{$\star$ Example}
\newtheorem{sexe}[df]{$\star$ Exercise}
\newtheorem{sdf}[df]{$\star$ Definition}




\newtheorem{prob}{\color{red}Problem}[section]
%\renewcommand*{\theprob}{{\color{red}\arabic{section}.\arabic{prob}}}
\newtheorem{sprob}[prob]{\color{red}$\star$ Problem}
%\renewcommand*{\thesprob}{{\color{red}\arabic{section}.\arabic{sprob}}}
% \newtheorem{ssprob}[prob]{$\star\star$ Problem}



\theoremstyle{plain}
\newtheorem{thm}[df]{Theorem}
\newtheorem{ccl}[df]{Conclusion}
\newtheorem{thd}[df]{Theorem-Definition}
\newtheorem{pp}[df]{Proposition}
\newtheorem{co}[df]{Corollary}
\newtheorem{lm}[df]{Lemma}
\newtheorem{sthm}[df]{$\star$ Theorem}

\newtheorem{cond}{Condition}
\newtheorem{Mthm}{Main Theorem}
\renewcommand{\thecond}{\Alph{cond}} % "letter-numbered" theorems
\renewcommand{\theMthm}{\Alph{Mthm}} % "letter-numbered" theorems


%\substack   multiple lines under sum
%\underset{b}{a}   b is under a


% Remind: \overline{L_0}



\usepackage{calligra}
\DeclareMathOperator{\shom}{\mathscr{H}\text{\kern -3pt {\calligra\large om}}\,}
\DeclareMathOperator{\sext}{\mathscr{E}\text{\kern -3pt {\calligra\large xt}}\,}
\DeclareMathOperator{\Rel}{\mathscr{R}\text{\kern -3pt {\calligra\large el}~}\,}
\DeclareMathOperator{\sann}{\mathscr{A}\text{\kern -3pt {\calligra\large nn}}\,}
\DeclareMathOperator{\send}{\mathscr{E}\text{\kern -3pt {\calligra\large nd}}\,}
\DeclareMathOperator{\stor}{\mathscr{T}\text{\kern -3pt {\calligra\large or}}\,}

\usepackage{aurical}
\DeclareMathOperator{\VVir}{\text{\Fontlukas V}\text{\kern -0pt {\Fontlukas\large ir}}\,}






\newcommand{\fk}{\mathfrak}
\newcommand{\mc}{\mathcal}
\newcommand{\wtd}{\widetilde}
\newcommand{\wht}{\widehat}
\newcommand{\wch}{\widecheck}
\newcommand{\ovl}{\overline}
\newcommand{\udl}{\underline}
\newcommand{\tr}{\mathrm{t}} %transpose
\newcommand{\Tr}{\mathrm{Tr}}
\newcommand{\End}{\mathrm{End}} %endomorphism
\newcommand{\idt}{\mathbf{1}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\Conf}{\mathrm{Conf}}
\newcommand{\Res}{\mathrm{Res}}
\newcommand{\res}{\mathrm{res}}
\newcommand{\KZ}{\mathrm{KZ}}
\newcommand{\ev}{\mathrm{ev}}
\newcommand{\coev}{\mathrm{coev}}
\newcommand{\opp}{\mathrm{opp}}
\newcommand{\Rep}{\mathrm{Rep}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Dom}{\scr D}
\newcommand{\loc}{\mathrm{loc}}
\newcommand{\con}{\mathrm{c}}
\newcommand{\uni}{\mathrm{u}}
\newcommand{\ssp}{\mathrm{ss}}
\newcommand{\di}{\slashed d}
\newcommand{\Diffp}{\mathrm{Diff}^+}
\newcommand{\Diff}{\mathrm{Diff}}
\newcommand{\PSU}{\mathrm{PSU}(1,1)}
\newcommand{\Vir}{\mathrm{Vir}}
\newcommand{\Witt}{\mathscr W}
\newcommand{\Span}{\mathrm{Span}}
\newcommand{\pri}{\mathrm{p}}
\newcommand{\ER}{E^1(V)_{\mathbb R}}
\newcommand{\prth}[1]{( {#1})}
\newcommand{\bk}[1]{\langle {#1}\rangle}
\newcommand{\bigbk}[1]{\big\langle {#1}\big\rangle}
\newcommand{\Bigbk}[1]{\Big\langle {#1}\Big\rangle}
\newcommand{\biggbk}[1]{\bigg\langle {#1}\bigg\rangle}
\newcommand{\Biggbk}[1]{\Bigg\langle {#1}\Bigg\rangle}
\newcommand{\GA}{\mathscr G_{\mathcal A}}
\newcommand{\vs}{\varsigma}
\newcommand{\Vect}{\mathrm{Vec}}
\newcommand{\Vectc}{\mathrm{Vec}^{\mathbb C}}
\newcommand{\scr}{\mathscr}
\newcommand{\sjs}{\subset\joinrel\subset}
\newcommand{\Jtd}{\widetilde{\mathcal J}}
\newcommand{\gk}{\mathfrak g}
\newcommand{\hk}{\mathfrak h}
\newcommand{\xk}{\mathfrak x}
\newcommand{\yk}{\mathfrak y}
\newcommand{\zk}{\mathfrak z}
\newcommand{\pk}{\mathfrak p}
\newcommand{\hr}{\mathfrak h_{\mathbb R}}
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\DHR}{\mathrm{DHR}_{I_0}}
\newcommand{\Repi}{\mathrm{Rep}_{\wtd I_0}}
\newcommand{\im}{\mathbf{i}}
\newcommand{\Co}{\complement}
%\newcommand{\Cu}{\mathcal C^{\mathrm u}}
\newcommand{\RepV}{\mathrm{Rep}^\uni(V)}
\newcommand{\RepA}{\mathrm{Rep}(\mathcal A)}
\newcommand{\RepN}{\mathrm{Rep}(\mathcal N)}
\newcommand{\RepfA}{\mathrm{Rep}^{\mathrm f}(\mathcal A)}
\newcommand{\RepAU}{\mathrm{Rep}^\uni(A_U)}
\newcommand{\RepU}{\mathrm{Rep}^\uni(U)}
\newcommand{\RepL}{\mathrm{Rep}^{\mathrm{L}}}
\newcommand{\HomL}{\mathrm{Hom}^{\mathrm{L}}}
\newcommand{\EndL}{\mathrm{End}^{\mathrm{L}}}
\newcommand{\Bim}{\mathrm{Bim}}
\newcommand{\BimA}{\mathrm{Bim}^\uni(A)}
%\newcommand{\shom}{\scr Hom}
\newcommand{\divi}{\mathrm{div}}
\newcommand{\sgm}{\varsigma}
\newcommand{\SX}{{S_{\fk X}}}
\newcommand{\DX}{D_{\fk X}}
\newcommand{\mbb}{\mathbb}
\newcommand{\mbf}{\mathbf}
\newcommand{\bsb}{\boldsymbol}
\newcommand{\blt}{\bullet}
\newcommand{\Vbb}{\mathbb V}
\newcommand{\Ubb}{\mathbb U}
\newcommand{\Xbb}{\mathbb X}
\newcommand{\Kbb}{\mathbb K}
\newcommand{\Abb}{\mathbb A}
\newcommand{\Wbb}{\mathbb W}
\newcommand{\Mbb}{\mathbb M}
\newcommand{\Gbb}{\mathbb G}
\newcommand{\Cbb}{\mathbb C}
\newcommand{\Nbb}{\mathbb N}
\newcommand{\Zbb}{\mathbb Z}
\newcommand{\Qbb}{\mathbb Q}
\newcommand{\Pbb}{\mathbb P}
\newcommand{\Rbb}{\mathbb R}
\newcommand{\Ebb}{\mathbb E}
\newcommand{\Dbb}{\mathbb D}
\newcommand{\Hbb}{\mathbb H}
\newcommand{\cbf}{\mathbf c}
\newcommand{\Rbf}{\mathbf R}
\newcommand{\wt}{\mathrm{wt}}
\newcommand{\Lie}{\mathrm{Lie}}
\newcommand{\btl}{\blacktriangleleft}
\newcommand{\btr}{\blacktriangleright}
\newcommand{\svir}{\mathcal V\!\mathit{ir}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cok}{\mathrm{Coker}}
\newcommand{\Sbf}{\mathbf{S}}
\newcommand{\low}{\mathrm{low}}
\newcommand{\Sp}{\mathrm{Sp}}
\newcommand{\Rng}{\mathrm{Rng}}
\newcommand{\vN}{\mathrm{vN}}
\newcommand{\Ebf}{\mathbf E}
\newcommand{\Nbf}{\mathbf N}
\newcommand{\Stb}{\mathrm {Stb}}
\newcommand{\SXb}{{S_{\fk X_b}}}
\newcommand{\pr}{\mathrm {pr}}
\newcommand{\SXtd}{S_{\wtd{\fk X}}}
\newcommand{\univ}{\mathrm {univ}}
\newcommand{\vbf}{\mathbf v}
\newcommand{\ubf}{\mathbf u}
\newcommand{\wbf}{\mathbf w}
\newcommand{\CB}{\mathrm{CB}}
\newcommand{\Perm}{\mathrm{Perm}}
\newcommand{\Orb}{\mathrm{Orb}}
\newcommand{\Lss}{{L_{0,\mathrm{s}}}}
\newcommand{\Lni}{{L_{0,\mathrm{n}}}}
\newcommand{\UPSU}{\widetilde{\mathrm{PSU}}(1,1)}
\newcommand{\Sbb}{{\mathbb S}}
\newcommand{\Gc}{\mathscr G_c}
\newcommand{\Obj}{\mathrm{Obj}}
\newcommand{\bpr}{{}^\backprime}
\newcommand{\fin}{\mathrm{fin}}
\newcommand{\Ann}{\mathrm{Ann}}
\newcommand{\Real}{\mathrm{Re}}
\newcommand{\Imag}{\mathrm{Im}}
%\newcommand{\cl}{\mathrm{cl}}
\newcommand{\Ind}{\mathrm{Ind}}
\newcommand{\Supp}{\mathrm{Supp}}
\newcommand{\Specan}{\mathrm{Specan}}
\newcommand{\red}{\mathrm{red}}
\newcommand{\uph}{\upharpoonright}
\newcommand{\Mor}{\mathrm{Mor}}
\newcommand{\pre}{\mathrm{pre}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Jac}{\mathrm{Jac}}
\newcommand{\emb}{\mathrm{emb}}
\newcommand{\Sg}{\mathrm{Sg}}
\newcommand{\Nzd}{\mathrm{Nzd}}
\newcommand{\Owht}{\widehat{\scr O}}
\newcommand{\Ext}{\mathrm{Ext}}
\newcommand{\Tor}{\mathrm{Tor}}
\newcommand{\Com}{\mathrm{Com}}
\newcommand{\Mod}{\mathrm{Mod}}
\newcommand{\nk}{\mathfrak n}
\newcommand{\mk}{\mathfrak m}
\newcommand{\Ass}{\mathrm{Ass}}
\newcommand{\depth}{\mathrm{depth}}
\newcommand{\Coh}{\mathrm{Coh}}
\newcommand{\Gode}{\mathrm{Gode}}
\newcommand{\Fbb}{\mathbb F}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Modf}{\mathrm{Mod}^{\mathrm f}}
\newcommand{\codim}{\mathrm{codim}}
\newcommand{\card}{\mathrm{card}}
\newcommand{\dps}{\displaystyle}
\newcommand{\Int}{\mathrm{Int}}
\newcommand{\Nbh}{\mathrm{Nbh}}
\newcommand{\Pnbh}{\mathrm{PNbh}}
\newcommand{\Cl}{\mathrm{Cl}}





\usepackage{tipa} % wierd symboles e.g. \textturnh
\newcommand{\tipar}{\text{\textrtailr}}
\newcommand{\tipaz}{\text{\textctyogh}}
\newcommand{\tipaomega}{\text{\textcloseomega}}
\newcommand{\tipae}{\text{\textrhookschwa}}
\newcommand{\tipaee}{\text{\textreve}}
\newcommand{\tipak}{\text{\texthtk}}
\newcommand{\eps}{\varepsilon}




\usepackage{tipx}
\newcommand{\tipxgamma}{\text{\textfrtailgamma}}
\newcommand{\tipxcc}{\text{\textctstretchc}}
\newcommand{\tipxphi}{\text{\textqplig}}















\numberwithin{equation}{section}




\title{Qiuzhen Lectures on Analysis}
\author{{\sc Bin Gui}
	%\\
	%{\small Department of Mathematics, Rutgers university}\\
	%{\small bin.gui@rutgers.edu}
}
%\date{}
\begin{document}\sloppy % avoid stretch into margins
	\pagenumbering{arabic}
	%\pagenumbering{gobble}
	\setcounter{page}{1}
%	\setcounter{section}{-1}
	%\setcounter{equation}{6}
	



	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



	
	\maketitle
%\thispagestyle{empty}	 %remove page number of this page


%Contents hyperlinks: \hyperlink{page.2}{Page 2}, \hyperlink{page.3}{Page 3}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-0.5cm}
\makeatletter
\newcommand*{\toccontents}{\@starttoc{toc}}
\makeatother
\toccontents



	
% title and table of contents same page, no content title

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\section{Basic set theory and numbers}


In this chapter, we discuss informally some of the basic notions in set theory and basic properties about numbers. A more thorough treatment can be found in \cite[Ch. 1]{Mun} (for set theory) and \cite[Ch. 1]{Rud-P} (for numbers). 

Let me first list the notations and conventions that will be used throughout the notes. We use frequently the abbreviations:
\begin{gather*}
\text{iff=if and only if}\\
\text{LHS=left hand side}\qquad
\text{RHS=reft hand side}\\
\text{$\exists$=there exists}\qquad \text{$\forall$=for all}\\
\text{i.e.=id est=that is=namely}\qquad\text{e.g.=for example}\\
\text{cf.=compare/check/see/you are referred to}\\
\text{resp.=respectively}\qquad 
\text{WLOG=without loss of generality}\\
\end{gather*}
If $P,Q$ are properties, then
\begin{align*}
P\land Q=P\text{ and }Q\qquad P\lor Q=P\text{ or }Q\qquad \neg P=\text{ Not }P
\end{align*}
When we write $A:=B$ or $A\xlongequal{\mathrm{def}}B$, we mean that $A$ is defined by the expression $B$. When we write $A\equiv B$, we mean that $A$ are $B$ are different symbols of the same object.

If $\Fbb$ is any field (e.g. $\Qbb,\Rbb,\Cbb$), we let $\Fbb^\times=\Fbb\setminus\{0\}$. \index{F@$\Fbb^\times=\Fbb\setminus\{0\}$} If $\alpha$ is a complex number and $n\in\Nbb$, we define the \textbf{binomial coefficient}\index{zz@$\alpha\choose n$}  \index{00@Binomial coefficient}
\begin{align}
{\alpha\choose n}=\left\{
\begin{array}{ll}
\dps\frac{\alpha\cdot(\alpha-1)\cdots (\alpha-n+1)}{n!} &\text{ if }n\geq 1\\[1ex]
1&\text{ if }n=0
\end{array}
\right.
\end{align}
where $n!=n(n-1)(n-2)\cdots 2\cdot 1$ and $0!=1$. The bold letter $\im$ means \index{i@$\im=\sqrt{-1}$}
\begin{align*}
\im=\sqrt{-1}
\end{align*}
If $z=a+b\im$ where $a,b\in\Rbb$, we let
\begin{align*}
\Real(z)=a\qquad \Imag(z)=b
\end{align*}


Topics marked with $\star\star$ are technical and/or their methods are rarely used in later studies. Topics marked with $\star$ are interesting, but not necessarily technical or difficult. They are not essential for understanding the rest of the notes. You can skim or skip the starred topics on first reading. When a chapter/section/subsection is starred, it means that all of the material in that chapter/section/subsection is starred.

%Topics in the ``Problems" section (or the whole ``Problems" sections) marked with $\heartsuit$ are important problems that will be used later in this course.



\subsection{Basic operations and axioms}
Intuitively, a set denotes a collection of elements. For instance:\index{N@$\Nbb=\{0,1,2,\dots\}$} \index{Z@$\Zbb_+=\{1,2,\dots\}$}
\begin{gather*}
\Zbb=\{\text{all integers}\}\qquad \Nbb=\Zbb_{\geq0}=\{n\in\Zbb:n\geq0\}\qquad \Zbb_+=\{n\in\Zbb:n>0\}
\end{gather*}
have infinitely many elements. (In this course, we will not be concerned with the rigorous construction of natural numbers and integers from Peano axioms.) We also let
\begin{align*}
\Qbb=\{\text{all rational numbers}\}\qquad\Rbb=\{\text{all real numbers}\}
\end{align*}
if we that rational and real numbers exist and satisfy the properties we are familiar with in high school mathematics. (We will construct $\Qbb$ and $\Rbb$ rigorously, by the way.)


Set theory is the foundation of modern mathematics. It consists of several Axioms telling us what we can do about the sets. For example, the following way of describing sets
\begin{align}
\{x: x\text{ satisfies property...}\}  \label{eq1}
\end{align}
is illegal, since it gives \textbf{Russell's paradox}: Consider
\begin{align}
S=\{A: A\text{ is a set and }A\notin A\}\label{eq12}
\end{align}
If $S$ were a set, then $S\in S\Rightarrow S\notin S$ and $S\notin S\Rightarrow S\in S$. This is something every mathematician doesn't want to happen.

Instead, the following way of defining sets is legitimate:
\begin{align}
\{x\in X:x\text{ satisfies property}\dots\}  \label{eq2}
\end{align}
where \textit{$X$ is a given set}.  For instance, we can define the \textbf{difference} of two sets:\index{AB@$A\backslash B$}
\begin{align*}
A\setminus B=A-B=\{x\in A:x\notin B\}
\end{align*}




So let us figure out the legal way of defining unions and intersections of sets. The crucial point is that we assume the following axiom:
\begin{axiom}
If $\scr A$ is a set of sets, then there exists a set $X$ such that $A\subset X$ for all $A\in\scr A$.
\end{axiom}

Thus, if $\scr A$ is a set of sets, let $X$ satisfy $A\subset X$ for all $A\in\scr A$, then we can define the \textbf{union} and the \textbf{intersection} 
\begin{subequations}\label{eq3}
\begin{gather}
\bigcup_{A\in\scr A}A=\{x\in X:\text{there exists $A\in\scr A$ such that $x\in A$}\}\\
\bigcap_{A\in\scr A}A=\{x\in X:\text{for all $A\in\scr A$ we have $x\in A$}\}
\end{gather}
\end{subequations}
It is clear that this definition does not rely on the particular choice of $X$.

\begin{rem}
In many textbooks, it is not uncommon that sets are defined as in \eqref{eq1}. You should interpret such definition as \eqref{eq2}, where the set $X$ is omitted because it is clear from the context. For instance, if the context is clear, the set $\{x\in\Rbb:x\geq 0\}$ could be simply written as $\{x:x\geq0\}$ or even $\{x\geq0\}$. By the same token, the phrase ``$\in X$" in \eqref{eq3} could be omitted. So we can also write
\begin{gather*}
A\cup B=\{x: x\in A\text{ or }x\in B\} \qquad  A\cap B=\{x: x\in A\text{ and }x\in B\}
\end{gather*}
which are special cases of \eqref{eq3}.
\end{rem}


\begin{rem}
In the same spirit, when discussing subsets of a given ``large" set $X$, and if $X$ is clear from the context, we shall write $X\setminus A$ (where $A\subset X$) as $A^c$ \index{Ax@$A^c$, the complement of $A$} and call it the \textbf{complement} of $A$.
\end{rem}


\begin{eg}
We have
\begin{gather*}
\bigcup_{x\in(1,+\infty)}[0,x)=[0,+\infty)\qquad\bigcap_{n\in\Zbb_+}(0,1+1/n)=(0,1]\qquad \bigcup_{n\in\Nbb}(0,1-1/n]=(0,1)
\end{gather*}
The readers may notice that these examples are not exactly in the form \eqref{eq3}. They are actually unions and intersections of indexed families of sets. (See Def. \ref{lb1}.) We need some preparation before discussing this notion.
\end{eg}




\begin{axiom}
If $A_1,\dots,A_n$ are sets, their \textbf{Cartesian product} exists:
\begin{align*}
A_1\times\cdots\times A_n=\{(a_1,\dots,a_n): a_i\in A_i\text{ for all }1\leq i\leq n\}
\end{align*}
where two elements $(a_1,\dots,a_n)$ and $(b_1,\dots,b_n)$ of the Cartesian product are regarded equal iff $a_1=b_1,\dots,a_n=b_n$. We also write
\begin{align*}
(a_1,\dots,a_n)=a_1\times\cdots\times a_n
\end{align*}
especially when $a,b$ are real numbers and $(a,b)$ can mean an open interval. We understand $A_1\times\cdots\times A_n$ as $\emptyset$ if some $A_i$ is $\emptyset$.

If $A_1=\cdots=A_n=A$, we write the Cartesian product as $A^n$. \hfill\qedsymbol
\end{axiom}

\begin{eg}
Assume that the set of real numbers $\Rbb$ exists. Then the set of complex numbers $\Cbb$ \index{C@$\Cbb$, the set of complex numbers} is defined to be $\Rbb^2=\Rbb\times\Rbb$ as a set. We write $(a,b)\in\Cbb$ as $a+b\im$ where $a,b\in\Rbb$. Define
\begin{gather*}
(a+b\im)+(c+d\im)=(a+c)+(b+d)\im\\
(a+b\im)\cdot (c+d\im)=(ac-bd)+(ad+bc)\im
\end{gather*}
Define the zero element $0$ of $\Cbb$ to be $0+0\im$. More generally, we consider $\Rbb$ as a subset of $\Cbb$ by viewing $a\in\Rbb$ as $a+0\im\in\Cbb$. This defines the usual arithmetic of complex numbers. 

If $z=a+b\im$, we define its \textbf{absolute value} $|z|=\sqrt{a^2+b^2}$. Then $z=0$ iff $|z|=0$. We define the \textbf{(complex) conjugate} of $z$ to be $\ovl z=a-b\im$. Then $|z|^2=z\ovl z$.

If $z\neq 0$, then there clearly exists a unique $z^{-1}\in\Cbb$ such that $zz^{-1}=z^{-1}z=1$:  $z^{-1}=|z|^{-2}\cdot \ovl z$. Thus, using the language of modern algebra, $\Cbb$ is a  \textbf{field}.\footnote{See Rem. \ref{lb166} or  \cite[Def. 1.12]{Rud-P} for the definition of fields. Rather than memorizing the full definition of fields, it is more important to keep in mind some typical (counter)examples: $\Qbb,\Rbb,\Cbb$ are fields. $\Zbb$ is not a field, because not every non-zero element of $\Zbb$ has an inverse. The set of quaternions $\{a+b\im+c\mathbf{j}+d\mathbf{k}: a,b,c,d\in\Rbb\}$ is not a field because it is not commutative ($\im\mathbf{j}=-\mathbf{j}\im=\mathbf{k}$). The set of rational functions $P(x)/Q(x)$, where $P,Q$ are polynomials with coefficients in $\Rbb$ and $Q\neq 0$, is a field.}  \hfill\qedsymbol
\end{eg}


The axiom of Cartesian product allows us to define relations and functions:

\begin{df}
If $A,B$ are sets, a subset $R$ of $A\times B$ is called a \textbf{relation}. For $(a,b)\in A\times B$, we write $aRb$ iff $(x,y)\in R$. We understand ``$aRb$" as ``$a$ is related to $b$ through the relation $R$".
\end{df}


\begin{df}\label{lb39}
A relation $f$ of $A,B$  is called a \textbf{function} or a \textbf{map} (or a \textbf{mapping}), if for every $a\in A$ there is a unique $b\in B$ such that $afb$. In this case, we write $b=f(a)$. 

When we write $f:A\rightarrow B$, we always mean that $A,B$ are sets and $f$ is a function from $A$ to $B$. $A$ and $B$ are called respectively the \textbf{domain} and the \textbf{codomain} of $f$. (Sometimes people also use the words ``source" and ``target" to denote $A$ and $B$.) 

If $E\subset A$ and $F\subset B$, we define the \textbf{image under $f$} of $E$  and the \textbf{preimage under $f$} of $F$ to be
\begin{gather*}
f(E)=\{b\in B:\exists a\in E\text{ such that }b=f(a)\}\\
f^{-1}(F)=\{a\in A: f(a)\in F\}.
\end{gather*}
$f(A)$ is simply called the \textbf{image} of $f$, or the \textbf{range} of $f$. If $b\in B$, $f^{-1}(\{b\})$ is often abbreviated to $f^{-1}(b)$. The function \index{f@$f\lvert_E$, the restriction of $f$ to $E$}
\begin{align*}
f|_E:E\rightarrow B\qquad x\mapsto f(x)
\end{align*}
is called the \textbf{restriction}  of $f$ to $E$. \hfill\qedsymbol
\end{df}


The intuition behind the definition of functions is clear: we understand functions as the same as their graphs. So a subset $f$ of the ``coordinate plane" $A\times B$ is the graph of a function iff it ``intersects every vertical line precisely once".


\begin{rem}\label{lb40}
According to our definition, $\emptyset$ (as a subset of $\emptyset\times B$) is the only function from $\emptyset$ to $B$. (A false assumption implies any statement.) If $A\neq\emptyset$, there are no functions $A\rightarrow\emptyset$.
\end{rem}


\begin{df}\label{lb13}
A function $x:\Zbb_+\rightarrow A$ is called a \textbf{sequence in $A$}. We write $x(n)$ as $x_n$, and write this sequence as $(x_n)_{n\in\Zbb_+}$ (or simply $(x_n)_n$ or $(x_n)$).
\end{df}

Many people write such a sequence as $\{x_n\}_{n\in\Zbb_+}$. We do not use this notation, since it can be confused with $\{x_n: n\in\Zbb_+\}$ (the range of the function $x$).



\begin{axiom}
If $X$ is a set, then the \textbf{power set} \index{00@Power set $2^X$} $2^X$ exists, where
\begin{align*}
2^X=\{\text{Subsets of }X\}
\end{align*}
\end{axiom}

\begin{eg}
The set $2^{\{1,2,3\}}$ has $8$ elements: $\emptyset$, $\{1\}$, $\{2\}$, $\{3\}$, $\{1,2\}$, $\{1,3\}$, $\{2,3\}$, $\{1,2,3\}$. Surprisingly, $8=2^3$. As we shall see in Exp. \ref{lb11} and Cor. \ref{lb12}, this relationship holds more generally, which explains the terminology $2^X$.  
\end{eg}

Now we are ready to define indexed families of sets.
\begin{df}\label{lb1}
An \textbf{indexed family of sets} \index{00@Indexed family of sets}  $(S_i)_{i\in I}$ is defined to be a function $S:I\rightarrow 2^X$ for some sets $I,X$. We write $S(i)$ as $S_i$. (So $S_i$ is a subset of $X$.) $I$ is called the \textbf{index set}. Define
\begin{align*}
\bigcup_{i\in I}S_i= \bigcup_{T\in S(I)}T\qquad \bigcap_{i\in I}S_i= \bigcap_{T\in S(I)}T
\end{align*}
Note that $S(I)$ is the image of the function $S$.
\end{df}


\begin{eg}
In the union $\bigcup_{x\in(1,+\infty)}[0,x)$, the index set is $I=(1,+\infty)$, and $X$ can be the set of real numbers $\Rbb$. Then $S:I\rightarrow 2^X$ is defined to be $S_i=S(i)=[0,i)$.
\end{eg}




\begin{exe}\label{lb5}
Let $f:A\rightarrow B$ be a function. We say that $f$ is \textbf{injective} if for all $a_1,a_2\in A$ satisfying $a_1\neq a_2$ we have $f(a_1)\neq f(a_2)$. We say that $f$ is \textbf{surjective} if for each $b\in B$ we have $f^{-1}(b)\neq\emptyset$. $f$ is called \textbf{bijective} if it is both surjective and bijective. Define the \textbf{identity maps} $\id_A:A\rightarrow A,a\mapsto a$ \index{id@$\id_A$} and $\id_B$ in a similar way. Prove that
\begin{subequations}
\begin{gather}
\text{$f$ is injective $\Longleftrightarrow$ there is $g:B\rightarrow A$ such that $g\circ f=\id_A$}\label{eq4}\\
\text{$f$ is surjective $\Longleftrightarrow$ there is $g:B\rightarrow A$ such that $f\circ g=\id_B$}\label{eq5}\\
\text{$f$ is bijective $\Longleftrightarrow$ there is $g:B\rightarrow A$ such that $g\circ f=\id_A$ and $f\circ g=\id_B$}\label{eq6}
\end{gather}
\end{subequations}
Show that the $g$ in \eqref{eq4} (resp. \eqref{eq5}, \eqref{eq6}) is surjective (resp. injective, bijective).
\end{exe}


The equivalence \eqref{eq5} is subtler, since its proof requires Axiom of Choice.


\begin{axiom}
Let $(S_i)_{i\in I}$ be an indexed family of sets. The \textbf{Axiom of Choice} asserts that if $S_i\neq\emptyset$ for all $i\in I$, then there exists a function (the \textbf{choice function})
\begin{align*}
f:I\rightarrow \bigcup_{i\in I}S_i
\end{align*}
such that $f(i)\in S_i$ for each $i\in I$.
\end{axiom}


Intuitively, the axiom of choice says that for each $i\in I$ we can choose an element $f(i)$ of $S_i$. And such choice gives a function $f$.


\begin{eg}
Let $f:A\rightarrow B$ be surjective. Then each member of the family $(f^{-1}(b))_{b\in B}$ is nonempty. Thus, by axiom of choice, there is a choice function $g$ defined on the index set $B$ such that $g(b)\in f^{-1}(b)$ for each $b$. Clearly $f\circ g=\id_B$.
\end{eg}



\begin{rem}
Suppose that each member $S_i$ of the family $(S_i)_{i\in I}$ has exactly one element. Then the existence of a choice function does not require Axiom of Choice: Let $X=\bigcup_{i\in I}S_i$ and define relation
\begin{align*}
f=\{(i,x)\in I\times X: x\in S_i\}
\end{align*}
Then one checks easily that this relation between $I$ and $X$ is a function, and that it is the (necessarily unique) choice function of $(S_i)_{i\in I}$.
\end{rem}

According to the above remark, one does not need Axiom of Choice to prove \eqref{eq4} and \eqref{eq6}. Can you see why?


\subsection{Partial and total orders, equivalence relations}\label{lb299}

\begin{df}
Let $A$ be a set. A \textbf{partial order} (or simply an \textbf{order}) $\leq$ on $A$ is a relation on $A\times A$ satisfying for all $a,b,c\in A$ that:
\begin{itemize}
\item (Reflexivity) $a\leq a$.
\item (Antisymmetry) If $a\leq b$ and $b\leq a$ then $a=b$.
\item (Transitivity) If $a\leq b$ and $b\leq c$ then $a\leq c$.
\end{itemize}
We write $b\geq a$ iff $a\leq b$. Write $a>b$ iff $a\geq b$ and $a\neq b$. Write $a<b$ iff $b>a$. So $\geq$ is also an order on $A$. The pair $(A,\leq)$ is called a \textbf{partially ordered set}, or simply a \textbf{poset}. \index{00@Poset=partially ordered set} A partial order $\leq$ on $A$ is called a \textbf{total order}, if for every $a,b\in A$ we have either $a\leq b$ or $b\leq a$.
\end{df}


\begin{eg}
The following are examples of orders.
\begin{itemize}
\item Assume that $\Rbb$ exists. Then $\Rbb$ has the canonical total order, which restricts to the total order of $\Zbb$. This is the total order that everyone is familiar with.
\item Let $X$ be a set. Then $(2^X,\subset)$ is a poset.
\item $\Rbb^2$ is a poset, if we define $(a,b)\leq (c,d)$ to be $a\leq c$ and $b\leq d$. 
\end{itemize}
\end{eg}


\begin{df}\label{lb156}
A relation $\sim$ on a set $A$ is called an \textbf{equivalence relation}, \index{00@Equivalence relation} if for all $a,b,c\in A$ we have
\begin{itemize}
\item (Reflexivity) $a\sim a$.
\item (Symmetry) $a\sim b$ iff $b\sim a$.
\item (Transitivity) If $a\sim b$ and $b\sim c$ then $a\sim c$.
\end{itemize}
\end{df}

Later, we will use the notions of partial orders and equivalence relation not just for a set, but for a collection of objects ``larger" than a set. See Sec. \ref{lb4}.

\begin{df}\label{lb157}
Let $A$ be a set, together with an equivalence relation $\sim$. Define a new set
\begin{align*}
{A/\sim}=\{[a]: a\in A\}
\end{align*}
where the notion $[a]$ can be understood in the following two equivalent ways (we leave it to the readers to check the equivalence):
\begin{itemize}
\item[(1)] $[a]$ is a new symbol. We understand $[a]$ and $[b]$ as equal iff $a\sim b$.
\item[(2)] $[a]=\{x\in A: x\sim a \}$
\end{itemize}
We call $[a]$ the \textbf{equivalence class} (or the \textbf{residue class}) of $a$, and call $A/\sim$ the \textbf{quotient set} \index{00@Quotient sets} of $A$ under $\sim$. The surjective map $\pi:a\in A\mapsto [a]\in {A/\sim}$ is called the \textbf{quotient map}.
\end{df}


\begin{exe}
Prove that every surjective map  is equivalent to a quotient map. More precisely, prove that for every surjection $f:A\rightarrow B$, there is an equivalence relation $\sim$ on $A$ and a bijective map $\Phi:{A/\sim}\rightarrow B$ such that the following diagram commutes (i.e. $f=\Phi\circ\pi$):
\begin{equation}\label{eq7}
\begin{tikzcd}[column sep=small]
                          & A \arrow[rd, "f"] \arrow[ld, "\pi"'] &   \\
{A/\sim} \arrow[rr, "\Phi"] &                                      & B
\end{tikzcd}
\end{equation}
\end{exe}


This is the first time we see commutative diagrams. Commutative diagrams are very useful for indicating that certain maps between sets are ``equivalent" or are satisfying some more general relations. For example, \eqref{eq7} shows that the maps $f$ and $\pi$ are equivalent, and that this equivalence is implemented by the bijection $\Phi$. The formal definition of commutative diagrams is the following:


\begin{df}
A diagram (i.e. some sets denoted by symbols, and some maps denoted by arrows) is called a \textbf{commutative diagram}, \index{00@Commutative diagram} if all directed paths in the diagram with the same start and endpoints lead to the same result.
\end{df}


Here is an example of commutative diagram in linear algebra. This example assumes some familiarity with the basic properties of vector spaces \index{00@Vector spaces} and linear maps.\footnote{Again, we refer the readers to Internet or any Linear Algebra textbook (e.g. \cite{Axl}) for the definition of vector spaces and linear maps.}


\begin{eg}\label{lb67}
Let $V,W$ be vector spaces over a field $\Fbb$ with finite dimensions $m,n$ respectively. Let $e_1,\dots,e_m$ be a basis of $V$, and let $\eps_1,\dots,\eps_n$ be a basis of $W$. We know that there are unique linear isomorphisms $\Phi:\Fbb^m\xrightarrow{\simeq} V$ and $\Psi:\Fbb^n\xrightarrow{\simeq} W$ such that
\begin{align*}
\Phi(a_1,\dots,a_m)=a_1e_1+\cdots+a_me_m\qquad \Psi(b_1,\dots,b_n)=b_1\eps_1+\cdots+b_n\eps_n
\end{align*}
Let $T:V\rightarrow W$ be a \index{00@Linear maps} \textbf{linear map}, i.e., a map satisfying $T(a\xi+b\eta)=aT\xi+bT\eta$ for all $a,b\in\Fbb,\xi,\eta\in V$. Then there is a unique $n\times m$ matrix $A\in\Fbb^{n\times m}$ \index{Fnm@$\Fbb^{n\times m}$, the set of $n\times m$ matrices} (viewed as a linear map $\Fbb^m\rightarrow\Fbb^n$ defined by matrix multiplication) such that the following diagram commutes:
\begin{equation}
\begin{tikzcd}
\Fbb^m \arrow[r,"\Phi","\simeq"'] \arrow[d,"A"'] & V \arrow[d,"T"] \\
\Fbb^n \arrow[r,"\Psi","\simeq"']           & W          
\end{tikzcd}
\end{equation} 
namely, $T\Phi=\Psi A$. This commutative diagram tells us that $T$ is equivalent to its \textbf{matrix representation} \index{00@Matrix representation} $A$ under the bases $e_\blt,\eps_\star$, and that this equivalence is implemented by the linear isomorphisms $\Phi$ (on the sources) and $\Psi$ (on the targets). 
\end{eg}

Commutative diagrams are ubiquitous in mathematics. You should learn how to read commutative diagrams and understand their intuitive meanings. We will see more examples in the future of this course.



\subsection{$\Qbb$, $\Rbb$, and $\overline{\mathbb R}=[-\infty,+\infty]$}



Using equivalence classes, one can construct rational numbers from integers, and real numbers from rationals. We leave the latter construction to the future, and discuss the construction of rationals here.

\begin{eg}[Construction of $\Qbb$ from $\Zbb$]\label{lb17}
Define a relation on $\Zbb\times\Zbb^\times$ (where $\Zbb^\times=\Zbb\setminus\{0\}$) as follows. If $(a,b),(a',b')\in\Zbb\times\Zbb^\times$, we say $(a,b)\sim(a',b')$ iff $ab'=a'b$. It is a routine check that $\sim$ is an equivalence relation. Let \index{Q@$\Qbb$, the field of rational numbers}  $\Qbb=(\Zbb\times\Zbb^\times)/\sim$, and write the equivalence class of $(a,b)$ as $a/b$ or $\frac ab$. Define additions and multiplications in $\Qbb$ to be
\begin{align*}
\frac ab+\frac cd=\frac{ad+bc}{bd}\qquad \frac ab\cdot\frac cd=\frac{ac}{bd}
\end{align*}
We leave it to the readers to check that this definition is \index{00@Well defined} \textbf{well-defined}: If $(a,b)\sim(a',b')$ and $(c,d)\sim(c',d')$ then $(ad+bc,bd)\sim(a'd'+b'c',b'd')$ and $(ac,bd)\sim(a'c',b'd')$.

We regard $\Zbb$ as a subset of $\Qbb$ by identifying $n\in\Zbb$ with $\frac n1$. (This is possible since the map $n\in\Zbb\mapsto \frac n1\in\Qbb$ is injective.) Each $a/b\in\Qbb$ has additive inverse $\frac{-a}b$. If $a/b\in\Qbb$ is not zero (i.e. $(a,b)\nsim (0,1)$), then $a/b$ has multiplicative inverse $b/a$. This makes $\Qbb$ a field: the field of \textbf{rational numbers}.

If $a/b\in\Qbb$, we say $a/b\geq 0$ if $ab\geq0$. Check that this is well-defined (i.e., if $(a,b)\sim(a',b')$, then $ab\geq0$ iff $a'b'\geq0$). More generally, if $a/b,c/d\in\Qbb$, we say $\frac ab\geq \frac cd$ if $\frac ab-\frac cd\geq0$. Check that $\geq$ is a total order on $\Qbb$.  Check that $\Qbb$ is an Archimedean ordered field, defined below.\hfill\qedsymbol
\end{eg}


\begin{df}\label{lb161}
A field $\Fbb$, together with a total order $\leq$, is called an \index{00@Ordered field} \textbf{ordered field}, if for every $a,b,c\in\Fbb$ we have
\begin{itemize}
\item (Addition preserves $\leq$) If $a\leq b$ then $a+c\leq b+c$.
\item (Multiplication by $\Fbb_{\geq0}$ preserves $\geq0$) If $a,b\geq 0$ then $ab\geq0$.
\end{itemize}
These two properties relate $\leq$ to $+$ and $\cdot$ respectively.
\end{df}

\begin{rem}
Many familiar properties about inequalities in $\Qbb$ hold for an ordered field. For instance: 
\begin{gather*}
a\geq b~\wedge~ c\geq d \qquad\Longrightarrow\qquad a+c\geq b+d\\
a\geq0\qquad\Longleftrightarrow\qquad -a\leq0\\
a\geq0~\wedge~b\geq c\qquad\Longleftrightarrow\qquad ab\geq ac\\
a\leq0~\wedge~b\geq c\qquad\Longleftrightarrow\qquad ab\leq a\\
a^2\geq0\\
0<a\leq b\qquad\Longrightarrow\qquad 0< b^{-1}\leq a^{-1}
\end{gather*}
Check them yourself, or see \cite[Prop. 1.18]{Rud-P}.
\end{rem}


\begin{df}
We say that an ordered field $\Fbb$ satisfies \index{00@Archimedean property} \textbf{Archimedean property} if for each $a,b\in\Fbb$ we have
\begin{align*}
a> 0\qquad\Longrightarrow \qquad\exists n\in\Nbb\text{ such that }na>b
\end{align*}
where $na$ means $\underbrace{a+\cdots+a}_{n}$.
\end{df}

\begin{eg}
$\Qbb$ satisfies Archimedean property. Indeed, let $a,b\in\Qbb$ and $a>0$. Then $a=p/q$ and $b=r/s$ where $p,q,s\in\Zbb_+$ and $r\in\Zbb$. So $na>b$ where $n=q|r|+q$.
\end{eg}



Prop. \ref{lb2} gives an important application of Archimedian property. We will use this in the construction of $\Rbb$ from $\Qbb$, and in the proof that $\Qbb$ is dense in $\Rbb$. 

\begin{df}
Let $\Fbb$ be a field. A subset $\Ebb\subset\Fbb$ is called a  \textbf{subfield} \index{00@Subfield} of $\Fbb$, if $\Ebb$ contains the $1$ of $\Fbb$, and if $\Ebb$ is closed under the operations of addition, multiplication, taking negative, and taking inverse in $\Fbb$ (i.e. if $a,b\in\Ebb$ then $a+b,ab,-a\in\Ebb$, and $a^{-1}\in\Ebb$ whenever $a\neq 0$). We also call $\Fbb$ a \index{00@Field extension} \textbf{field extension} of $\Ebb$, since $\Ebb$ is clearly a field.
\end{df}

Note that if $\Ebb$ is a subfield of $\Fbb$, the $0$ of $\Fbb$ is in $\Ebb$ since $0=1+(-1)\in\Ebb$.

\begin{df}
Let $\Ebb$ be an ordered field. A field extension $\Fbb$ of $\Ebb$ is called an \index{00@Ordered field extension=ordered subfield} \textbf{ordered field extension}, if $\Fbb$ is equipped with a total order $\leq$ such that $\Fbb$ is an ordered field, and if the order $\leq$ of $\Fbb$ restricts to that of $\Ebb$. We also call $\Ebb$ an \textbf{ordered subfield} of $\Fbb$.
\end{df}

Our typical example of ordered field extension will be $\Qbb\subset\Rbb$.

\begin{pp}\label{lb2}
Let $\Fbb$ be an ordered field extension  of $\Qbb$. Assume that $\Fbb$ is Archimedean. Then for every $x,y\in\Fbb$ satisfying $x<y$, there exists $p\in\Qbb$ such that $x<p<y$.
\end{pp}

\begin{proof}
Assume $x,y\in\Fbb$ and $x<y$. Then $y-x>0$ (since $y-x\neq 0$ and $-x+x\leq -x+y$). By Archimedean property, there exists $n\in\Zbb_+$ such that $n(y-x)>1$. So $\displaystyle y-x>\frac 1n$ and hence $\displaystyle x+\frac 1n<y$.

Let us prove that the subset
\begin{equation*}
A=\big\{k\in\Zbb: \frac {~k~}{n}\leq x\big\}
\end{equation*}
is nonempty and bounded from above in $\Zbb$. By Archimedean property, there is $m\in\Zbb_+$ such that $m>nx$, i.e. $\displaystyle \frac mn>x$. So for each $k\in\Zbb_+$ satisfying $k\geq m$, we have $\displaystyle \frac kn=\frac mn+\frac{k-m}n>x$. Therefore, for each $k\in A$ we have $k<m$. So $A$ is bounded above. By Archimedean property again, there is $l\in\Zbb_+$ such that $\displaystyle \frac ln>-x$. So $\displaystyle -\frac ln<x$, and hence $A$ is nonempty.

We now use the fact that \textit{every nonempty subset of $\Zbb$ bounded above has a maximal element}. Let $k=\max A$. Since $k+1\notin A$, we have $\displaystyle x<\frac{k+1}n$. Since $\displaystyle \frac kn\leq x$, we have
\begin{align*}
\frac{k+1}n=\frac kn+\frac 1n\leq x+\frac 1n<y
\end{align*}
This proves $x<p<y$ with $\displaystyle p=\frac{k+1}n$.
\end{proof}

To introduce $\Rbb$ formally, we need more definitions:

\begin{df}
Let $(X,\leq)$ be a poset and $E\subset X$. An \textbf{upper bound of $E$ in $X$} \index{00@Upper bound} is an element $x\in X$ satisfying $e\leq x$ for all $e\in E$. An upper bound $x\in X$ of $E$ is called a \textbf{least upper bound} or a \textbf{supremum} \index{00@Supremum}  if $x\leq y$ for every upper bound $y\in Y$ of $E$. In this case, we write the supremum as \index{sup@$\sup E$} $\sup E$. It is not hard to check that supremums are unique if they exist.

We leave it to the readers to define \textbf{lower bounds} and the \textbf{greatest lower bound} (if exists) of $E$, also called the \textbf{infinimum} \index{00@Infinimum} and is denoted by \index{inf@$\inf E$} $\inf E$. \hfill\qedsymbol
\end{df}


\begin{df}
Let $(X,\leq)$ be a poset. We say that $X$ satisfies the \textbf{least-upper-bound property}, if every every nonempty subset $E\subset X$ which is bounded above (i.e. $E$ has an upper bound) has a supremum in $X$. The \textbf{greatest-lower-bound property} is defined in the opposite way.
\end{df}

\begin{eg}
$\Zbb$ satisfies the least-upper-bound and the greatest-lower-bound property: Let $A\subset \Zbb$. If $A$ is bounded above (resp. below), then the maximum $\max A$ (resp. minimum $\min A$) exists and is the supremum (resp. infinimum) of $A$.
\end{eg}

\begin{eg}
Let $X$ be a set. Then $(2^X,\subset)$ satisfies the least-upper-bound and the greatest-lower-bound property: Let $\scr A\subset 2^X$, i.e., $\scr A$ is a set of subsets of $X$. Then $\scr A$ is bounded from above by $X$, and is bounded from below by $\emptyset$. Moreover,
\begin{align*}
\sup\scr A=\bigcup_{A\in\scr A}A\qquad \inf\scr A=\bigcap_{A\in\scr A}A
\end{align*}
\end{eg}





\begin{thm}\label{lb3}
There is an ordered field extension of $\Qbb$ which is Archimedian and satisfies the least-upper-bound property. This field is denoted by  $\Rbb$. Its elements are called \index{00@Real number} \textbf{real numbers}.
\end{thm}

Thm. \ref{lb3} will be proved in Ch. \ref{lb167}. Note that by taking negative, we see that $\Rbb$ also satisfies the greatest-lower-bound property.


\begin{rem}
The ordered field extensions satisfying the conditions in Thm. \ref{lb3} are unique ``up to isomorphisms". (The words ``\textbf{isomorphism}"\index{00@Isomorphism}  and ``equivalence" are often interchangeable, though ``isomorphism" is more often used in the algebraic setting, whereas ``equivalence" can be used in almost every context. For example, in point-set topology, ``equivalence" means ``homeomorphism".) We leave it to the readers to give the precise statement. We will not use this uniqueness in this course. 

Note that to compare two extensions $\Fbb,\Rbb$ of $\Qbb$, it is very confusing to regard $\Qbb$ as a subset of both $\Fbb$ and $\Rbb$. You'd better consider two different injective maps $\tau:\Qbb\rightarrow \Fbb$ and $\iota:\Qbb\rightarrow\Rbb$ preserving the algebraic operations and the order of $\Qbb$, and use a commutative diagram to indicate that $\tau$ and $\iota$ are equivalent. (Thus, what's happening here is that we have an equivalence of maps, not just an equivalence of the fields $\Fbb$ and $\Rbb$.) \hfill\qedsymbol
\end{rem}


\begin{df}\label{lb114}
Let $-\infty,+\infty$ be two different symbols, and extend the total order $\leq$ of $\Rbb$ to the \textbf{extended real line}\index{R@$\ovl\Rbb=[-\infty,+\infty]=\Rbb\cup\{-\infty,+\infty\}$}
\begin{align*}
\ovl\Rbb=\Rbb\cup\{-\infty,+\infty\}
\end{align*}
by letting $-\infty<x<+\infty$ for all $x\in\Rbb$. Define for each $x\in\Rbb$ that
\begin{gather*}
x\pm\infty=\pm\infty+x=\pm\infty\qquad +\infty-(-\infty)=+\infty\\
x\cdot(\pm\infty)=\pm\infty\cdot x=\left\{
\begin{array}{cc}
\pm\infty&\text{if }x>0\\
\mp\infty&\text{if }x<0
\end{array}
\right.\\
\frac x{\pm\infty}=0\\
\frac{\pm\infty}{x}=x^{-1}\cdot(\pm\infty)\qquad \text{if }x\neq0
\end{gather*}
If $a,b\in\ovl\Rbb$ and $a\leq b$, we define \textbf{intervals} \index{00@Interval} with endpoints \index{00@Endpoints of an interval} $a,b$:
\begin{gather}\label{eq8}
\begin{gathered}
[a,b]=\{x\in\ovl\Rbb:a\leq x\leq b\}\qquad (a,b)=\{x\in\ovl\Rbb:a< x< b\}\\
(a,b]=\{x\in\ovl\Rbb:a< x\leq b\}\qquad [a,b)=\{x\in\ovl\Rbb:a\leq x< b\}
\end{gathered}
\end{gather}
So $\Rbb=(-\infty,+\infty)$ and $\ovl\Rbb=[-\infty,+\infty]$. If $a,b$ are in $\Rbb$, we say that the corresponding interval is \textbf{bounded}. \index{00@Bounded interval}
\end{df}

In this course, unless otherwise stated, an interval always means one of the four sets in \eqref{eq8}. The first two intervals are called respectively a \textbf{closed interval} and an \textbf{open interval}.


\begin{rem}
Clearly, every subset $E$ of $\ovl\Rbb$ is bounded and has a supremum and an infinimum. We have that $\sup E=+\infty$ iff $E$ is not bounded above in $\Rbb$, and that $\inf E=-\infty$ iff $E$ is not bounded below in $\Rbb$. 
\end{rem}


\subsection{Cardinalities, countable sets, and product spaces $Y^X$}\label{lb4}


\begin{df}
Let $A$ and $B$ be sets. We say that $A$ and $B$ have the same \textbf{cardinality} \index{00@Cardinality $\card(A)$} and write $\card(A)=\card(B)$ (or simply $A\approx B$), if there is a bijection $f:A\rightarrow B$. We write $\card(A)\leq\card(B)$ (or simply $A\precsim B$) if $A$ and a subset of $B$ have the same cardinality. 
\end{df}



\begin{exe}\label{lb9}
Show that $\card(A)\leq\card(B)$ iff there is an injection $f:A\rightarrow B$, iff there is a surjection $g:B\rightarrow A$. (You need either Axiom of Choice or its consequence \eqref{eq5} to prove the last equivalence.)
\end{exe}

It is clear that $\approx$ is an equivalence relation on the collection of sets. It is also true that $\precsim$ is a partial order: Reflexivity and transitivity are easy to show. The proof of antisymmetry is more involved:



\begin{thm}[Schr\"oder-Bernstein]\label{lb8}\index{00@Schr\"oder-Bernstein theorem}
Let $A,B$ be two sets. If $A\precsim B$ and $B\precsim A$, then $A\approx B$.
\end{thm}

\begin{proof}[$\star\star$ Proof]
Assume WLOG that $A\subset B$. Let $f:B\rightarrow A$ be an injection. Let $A_n=f^n(A)$ defined inductively by $f^0(A)=A$, $f^n(A)=f(f^{n-1}(A))$. Let $B_n=f^n(B)$. Then
\begin{align*}
B_0\supset A_0\supset \cdots\supset B_n\supset A_n\supset B_{n+1}\supset\cdots
\end{align*}
In particular, $C=\bigcap_{n\in\Nbb}A_n$ equals $\bigcap_{n\in\Nbb}B_n$. Note that $f$ gives a bijection $B_n\setminus A_n\rightarrow B_{n+1}\setminus A_{n+1}$ (since $f$ gives bijections $B_n\rightarrow B_{n+1}$ and $A_n\rightarrow A_{n+1}$). Therefore, we have a bijection $g:B\rightarrow A$ defined by
\begin{gather*}
g(x)=\left\{
{\begin{array}{ll}
f(x)&\text{if $x\in B_n\setminus A_n$ for some $n\in\Nbb$}\\[0.5ex]
x&\text{otherwise}
\end{array}}
\right.
\end{gather*}
where ``otherwise" means either $x\in C$ or $x\in A_n\setminus B_{n+1}$ for some $n$.
\end{proof}

Intuition about the above proof: View $B$ as an onion. The layers of $B$ are $B_n\setminus A_n$ (the odd layers) and $A_n\setminus B_{n+1}$ (the even layers). The bijection $g$ maps each odder layer to the subsequent odd one, and fixes the even layers and the core $C$.


\begin{eg}\label{lb6}
If $-\infty<a<b<+\infty$, then $(0,1)\approx (a,b)$.
\end{eg}
\begin{proof}
$f:(0,1)\rightarrow (a,b)$ sending $x$ to $(b-a)x+a$ is a bijection.
\end{proof}

\begin{eg}\label{lb7}
If $-\infty<a<b<+\infty$, then $\Rbb\approx (a,b)$
\end{eg}

\begin{proof}
By the previous example, it suffices to prove $\Rbb\approx(-1,1)$. The function
\begin{gather}\label{eq20}
f:\Rbb\rightarrow(-1,1)\qquad f(x)=\left\{
\begin{array}{ll}
\frac x{1+x}&\text{ if $x\geq0$}\\[0.5ex]
-f(-x)&\text{ if $x<0$}
\end{array}
\right.
\end{gather}
is bijective.
\end{proof}


Alternatively, one may use the tangent function to give a bijection between $(-\pi/2,\pi/2)$ and $\Rbb$. I have avoided this method, since \eqref{eq20} is more elementary than trigonometric functions. The mathematically rigorous definition of trigonometric functions and the verification of their well-known properties are far from easy tasks. 



\begin{pp}
Let $I$ be an interval with endpoints $a<b$ in $\ovl\Rbb$. Then $I\approx\Rbb$.
\end{pp}

\begin{proof}
Let $A=(0,1)\cup\{-\infty,+\infty\}$. By Exp.  \ref{lb7}, we have
\begin{align*}
(a,b)\subset I\precsim \ovl\Rbb\approx A\approx[0,1]\subset (-2,2)\approx (a,b)
\end{align*}
So $I\approx\ovl\Rbb$ by Schr\"oder-Bernstein Thm. \ref{lb8}. In particular, $\Rbb=(-\infty,+\infty)\approx\ovl\Rbb$.
\end{proof}


\begin{df}
A set $A$ is called \textbf{finite} if $A\precsim\{1,\dots,n\}$ for some $n\in\Zbb_+$. $A$ is called  \textbf{countable} if $A\precsim\Nbb$. \index{00@Countable}
\end{df}

Clearly, a set $A$ is finite iff either $A\approx\emptyset$ or $A\approx\{1,\dots,n\}$ for some $n\in\Zbb_+$.

\begin{rem}
Let $A\subset\Nbb$. If $A$ is bounded above, then $A\subset\{0,\dots,n\}$ and hence $A$ is finite. If $A$ is not bounded above, then we can construct a strictly increasing sequence $(x_n)_{n\in\Nbb}$ in $A$. (Pick any $x_0\in A$. Suppose we have $x_n\in A$. Since $x_n$ is not an upper bounded of $A$, there is $x_{n+1}\in A$ larger than $x_n$. So $(x_n)_{n\in\Nbb}$ can be constructed inductively.) This gives an injection $\Nbb\rightarrow A$. Therefore $A\succsim \Nbb$, and hence $A\approx \Nbb$ by Schr\"oder-Bernstein.

It follows that if $B\precsim\Nbb$, then either $B$ is a finite set, or $B\approx\Nbb$. Therefore, ``a set $B$ is \textbf{countably infinite}" \index{00@Countably infinite} means the same as ``$B\approx\Nbb$".  \hfill\qedsymbol 
\end{rem}


\begin{thm}\label{lb15}
A countable union of countable sets is countable. In particular, $\Nbb\times\Nbb\approx\Nbb$.
\end{thm}

\begin{proof}
Recall Exe. \ref{lb9}. Let $A_1,A_2,\dots$ be countable sets. Since each $A_i$ is countable, there is a surjection $f_i:\Nbb\rightarrow A_i$. Thus, the map $f:\Nbb\times\Nbb\rightarrow \bigcup_i A_i$ defined by $f(i,j)=f_i(j)$ is surjective. Therefore, it suffices to show that there is a surjection $\Nbb\rightarrow\Nbb\times\Nbb$. This is true, since we have a bijection $g:\Nbb\rightarrow\Nbb\times\Nbb$ where $g(0),g(1),g(2),\dots$ are $(0,0)$, $(1,0)$, $(0,1)$, $(2,0)$, $(1,1)$, $(0,2)$, $(3,0)$, $(2,1)$, $(1,2)$, $(0,3)$, etc., as shown by the figure
\begin{align*}
\vcenter{\hbox{{
			\includegraphics[width=3.5cm]{fig1.png}}}}
\end{align*}
\end{proof}

As an application, we prove the extremely important fact that $\Qbb$ is countable.
\begin{co}
We have $\Nbb\approx\Zbb_+\approx\Zbb\approx \Qbb$.
\end{co}



\begin{proof}
Clearly $\Zbb_{<0}\approx\Nbb\approx \Zbb_+$. By Thm. \ref{lb15}, $\Zbb=\Zbb_{<0}\cup\Nbb$ is countably infinite, and hence $\Zbb\approx\Nbb$. It remains to prove $\Zbb\approx\Qbb$. By Schr\"oder-Bernstein, it suffices to prove $\Qbb\precsim\Zbb$.  By Thm. \ref{lb15} again, $\Zbb\times\Zbb\approx\Zbb$. By Exp. \ref{lb17}, there is a surjection from a subset of $\Zbb\times\Zbb$ to $\Qbb$. So $\Qbb\precsim\Zbb\times\Zbb\approx\Zbb$.
\end{proof}



Later, when we have learned Zorn's Lemma (an equivalent form of Axiom of Choice), we will be able to prove the following generalization of $\Nbb\times\Nbb\approx\Nbb$. So we defer the proof of the following theorem to a later section.

\begin{thm}\label{lb16}
Let $X$ be a infinite set. Then $X\times\Nbb\approx X$.
\end{thm}





Our next goal is to prove an exponential law $a^{b+c}=a^b\cdot a^c$ for cardinalities. For that purpose, we first need to define the set-theoretic operations that correspond to the summation $b+c$ and the exponential $a^b$.


\begin{df}
We write $X=\bigsqcup_{\alpha\in\scr A}A_\alpha$ \index{A@$\bigsqcup_{\alpha\in\scr A}A_\alpha$, the disjoint union} and call $X$ the \textbf{disjoint union} \index{00@Disjoint union} of $(A_\alpha)_{\alpha\in\scr A}$,  if $X=\bigcup_{\alpha\in\scr A}A_\alpha$ and $(A_\alpha)_{\alpha\in\scr A}$ is a family of pairwise disjoint sets (i.e. $A_\alpha\cap A_\beta=\emptyset$ if $\alpha\neq\beta$). If moreover $\scr A=\{1,\dots,n\}$, we write $X=A_1\sqcup\cdots\sqcup A_n$.
\end{df}

\begin{df}
Let $X,Y$ be sets. Then \index{YX@$Y^X$, the set of functions $X\rightarrow Y$}
\begin{align}
Y^X=\{\text{functions }f:X\rightarrow Y\}
\end{align}
A more precise definition of $Y^X$ (in the spirit of \eqref{eq2}) is $\{f\in X\times Y \mid f:X\rightarrow Y\text{ is a function}\}$. Note that by Rem. \ref{lb40},
\begin{align}
Y^\emptyset=\{\emptyset\}  \label{eq10}
\end{align}
\end{df}

This new notation is compatible with the old one $Y^n=Y\times\cdots\times Y$:
\begin{eg}
Let $n\in\Zbb_+$. We have $Y^{\{1,\dots,n\}}\approx Y^n$ due to the bijection
\begin{align*}
Y^{\{1,\dots,n\}}\rightarrow Y^n\qquad f\mapsto (f(1),\dots,f(n))
\end{align*}
\end{eg}

\begin{rem}\label{lb18}
The above example suggests that in the general case that $X$ is not necessarily finite, we can view each function $f:X\rightarrow Y$ as $(f(x))_{x\in X}$, an \textbf{indexed family of elements} of $Y$ with index set $X$. Thus, intuitively and hence not quite rigorously, 
\begin{align}
Y^X=\underbrace{Y\times Y\times\cdots}_{\card(X)\text{ pieces}} \label{eq11}
\end{align}
This generalizes the intuition in Def. \ref{lb13} that a function $f:\Zbb_+\rightarrow Y$ is equivalently a sequence $(f(1),f(2),f(3),\dots)$.

The viewpoint that $Y^X$ is a \textbf{product space} with index set $X$ is very important and will be adopted frequently in this course. More generally, we can define:\hfill\qedsymbol
\end{rem}

\begin{df}
Let $(X_i)_{i\in I}$ be a family of sets with index set $I$. Their \textbf{product space} \index{00@Product space} \index{X@$\prod_{i\in I}X_i$} is defined by
\begin{align*}
\prod_{i\in I}X_i =\{f\in \fk X^I:f(i)\in X_i\text{ for all }i\in I \}
\end{align*}
where $\fk X=\bigcup_{i\in I}X_i$. If each $X_i$ is nonempty, then $\prod_{i\in I}X_i$ is nonempty by Axiom of Choice. An element of $\prod_{i\in I}X_i$ is also written as $(f_i)_{i\in I}$ when the $i$-th component of it is $f_i\in X_i$.
\end{df}

In particular, if all $X_i$ are equal to $X$, then $X^I=\prod_{i\in I}X$.



\begin{eg}\label{lb11}
Let $X$ be a set. For each $A\subset X$, define the \textbf{characteristic function} \index{00@Characteristic function} \index{zz@$\chi_A$, the characteristic function of $A$} $\chi_A:X\rightarrow\{0,1\}$ to be
\begin{align*}
\chi_A(x)=\left\{
\begin{array}{ll}
1&\text{if }x\in A\\
0&\text{if }x\notin A
\end{array}
\right.
\end{align*}
Then we have
\begin{align*}
2^X\approx \{0,1\}^X
\end{align*}
since the following map is bijective:
\begin{gather*}
2^X\rightarrow\{0,1\}^X\qquad A\mapsto\chi_A
\end{gather*}
Its inverse is $f\in\{0,1\}^X\mapsto f^{-1}(1)\in 2^X$.
\end{eg}

\begin{pp}[Exponential Law]\label{lb10}
Suppose that $X=A_1\sqcup\cdots\sqcup A_n$. Then
\begin{align*}
Y^X\approx Y^{A_1}\times \cdots\times Y^{A_n}
\end{align*}
\end{pp}

\begin{proof}
We have a bijection
\begin{gather}\label{eq9}
\begin{gathered}
\Phi:Y^X\rightarrow Y^{A_1}\times \cdots\times Y^{A_n}\\
f\mapsto (f|_{A_1},\dots,f|_{A_n})
\end{gathered}
\end{gather}
where we recall that $f|_{A_i}$ is the restriction of $f$ to $A_i$. 
\end{proof}

\begin{exe}
Assume that $A_1,\dots,A_n$ are subsets of $X$. Define $\Phi$ by \eqref{eq9}. Prove that $\Phi$ is injective iff $X=A_1\cup\cdots\cup A_n$. Prove that $\Phi$ is surjective iff $A_1,\dots, A_n$ are pairwise disjoint. 
\end{exe}

\begin{co}\label{lb12}
Let $X,Y$ be finite sets with cardinalities $m,n\in\Nbb$ respectively. Assume that $Y\neq\emptyset$. Then $Y^X$ is a finite set with cardinality $n^m$.
\end{co}

\begin{proof}
The special case that $m=0$ (i.e. $X=\emptyset$, cf. \eqref{eq10}) and $m=1$ is clear. When $m>1$, assume WLOG that $X=\{1,\dots,m\}$. Then $X=\{1\}\sqcup\cdots\sqcup\{m\}$. Apply Prop. \ref{lb10} to this disjoint union. We see that $Y^X\simeq Y\times \cdots\times Y\simeq\{1,\dots,n\}^m$ has $n^m$ elements.
\end{proof}



We end this section with some (in)equalities about the cardinalities of product spaces. To begin with, we write $X\precnsim Y$ (or $\card(X)<\card(Y)$) if $X\precsim Y$ and $X\napprox Y$.

\begin{pp}\label{lb14}
Let $X,Y$ be sets with $\card(Y)\geq 2$ (i.e. $Y$ has at least two elements). Then $X\precnsim Y^X$. In particular, $X\precnsim 2^X$.
\end{pp}

\begin{proof}
The case $X=\emptyset$ is obvious since $0<1$. So we assume $Y\neq\emptyset$. Clearly $2^X\simeq\{0,1\}^X$ is $\precsim Y^X$. So it suffices to prove $X\precnsim 2^X$. Since the map $X\rightarrow 2^X$ sending $x$ to $\{x\}$ is injective, $X\precsim 2^X$. Let us prove $X\napprox 2^X$.

Assume that $X\approx 2^X$. So there is a bijection $\Phi:X\rightarrow 2^X$ sending each $x\in X$ to a subset $\Phi(x)$ of $X$. Motivated by Russell's Paradox \eqref{eq12}, we define
\begin{align*}
S=\{x\in X:x\notin \Phi(x)\}
\end{align*}
Since $\Phi$ is surjective, there exists $y\in X$ such that $S=\Phi(y)$. If $y\in\Phi(y)$, then $y\in S$, and hence $y\notin \Phi(y)$ by the definition of $S$. If $y\notin\Phi(y)$, then $y\notin S$, and hence $y\in\Phi(y)$ by the definition of $S$. This gives a contradiction.
\end{proof}


\begin{rem}
Write $\{1,\dots,n\}^X$ as $n^X$ for short. \index{nX@$n^X=\{1,\dots,n\}^X$} Assuming that real numbers have decimal, binary, or (more generally) base-$n$ representations where $n\in\Zbb_{\geq 2}$, then  $\Rbb\approx n^{\Nbb}$. So by Prop. \ref{lb14}, $\Nbb\precnsim\Rbb$, i.e. \textit{$\Rbb$ is uncountable}. The base-$n$ representations of real numbers suggest that $\card(n^\Nbb)$ is independent of $n$. This fact can be proved by elementary methods without  resorting to the analysis of real numbers:
\end{rem}

\begin{thm}
Let $X$ be an infinite set. Then
\begin{align*}
2^X\approx 3^X\approx 4^X\approx\cdots\approx \Nbb^X
\end{align*}
\end{thm}

\begin{proof}
First, we assume that $X=\Nbb$. Clearly, for each $n\in\Zbb_{\geq 2}$ we have $2^X\precsim n^X\precsim \Nbb^X$. Since elements of $\Nbb^X$ are subsets of $X\times\Nbb$ (i.e. elements of $2^{X\times\Nbb}$), we have
\begin{align*}
\Nbb^X\subset 2^{X\times\Nbb}\simeq 2^X
\end{align*}
since $X\times\Nbb\approx X$ by Thm. \ref{lb15}. So $2^X\approx n^X\approx \Nbb^X$ by Schr\"oder-Bernstein.

As pointed out earlier (cf. Thm. \ref{lb16}), it can be proved by Zorn's Lemma that $X\times\Nbb\approx X$ for every infinite set $X$. So the same conclusion holds for such $X$.
\end{proof}

\newpage

\section{Metric spaces}


We first give an informal introduction to metric spaces, hoping to motivate the readers from a (relatively) historical perspective. It is okay if you do not understand all of the concepts mentioned in the introduction on the first read. Simply return to this section when you feel unmotivated while formally studying these concepts in later sections. (The same suggestion applies to all the introductory sections and historical comments in our notes.)









\subsection{Introduction: what is point-set topology?}\label{lb55}


\begin{displayquote}
\small The method which has been used with success by Volterra and Hilbert consists in observing that a function (for instance a continuous one) can be replaced by a countable infinity of parameters. One treats the problem first as though one had only a finite number of parameters and then one goes to the limit... We believe that this method has played a useful role because it followed intuition, but that its time has passed... The most fruitful method in functional analysis seems to us to treat the element of which the functional depends directly as a variable and in the form in which it presents itself naturally.

\hfill ---- Fr\'echet, 1925 ~~(cf. \cite[Sec. 13.8]{Jah})
\end{displayquote}




In this chapter, we begin the study of point-set topology by learning one of its most important notions: metric spaces. Similar to \cite{Rud-P}, we prefer to introduce metric spaces and basic point-set topology at the early stage of our study. An obvious reason for doing so is that metric spaces provide a uniform language for the study of basic analysis problems in $\Rbb,\Rbb^n,\Cbb^n$, and more generally in function spaces such as the space of continuous functions $C([a,b])$ on the interval $[a,b]\subset\Rbb$. With the help of such a language, for example, many useful criteria for the convergence of series in $\Rbb$ and $\Cbb$ (e.g. root test, ratio test) are generalized straightforwardly to criteria for the \textit{uniform} convergence of series of functions in $C([a,b])$.

Point-set topology was born in 1906 when Fr\'echet defined metric spaces, motivated mainly by the study of function spaces in analysis (i.e. \textit{functional analysis}). Indeed, point-set topology and functional analysis are the two faces of the same coin: they both originated from the study of \textbf{functionals}, \index{00@Functionals} i.e., functions of functions. See for example \eqref{eq24}. The core ideas of point-set topology are as follows:
\begin{enumerate}[label=(\arabic*)]
\item Take $X$ to be a set of functions defined on a ``classical space" (e.g. the set of all continuous functions $f:[a,b]\rightarrow\Cbb$). Then a functional is a function  $S:X\rightarrow \Cbb$. This is a generalization of functions on $\Rbb,\Cbb,\Rbb^n,\Cbb^n$ or on their subsets.
\item Unlike $\Rbb^n$, a function space $X$ is usually ``infinite dimensional". Thus, one may think that a functional $S$ is a function with infinite variables. In point-set topology, this viewpoint is abandoned; the philosophy of point-set topology is diametrically opposed to that of multivariable calculus.\footnote{Very often, the formula of $S(f)$ involves an integral. See e.g. \eqref{eq24}. Mathematicians (e.g. Volterra, L\'evy, Fredholm, and early Hilbert) used to study $S(f)$ by discretizing $S(f)$, i.e., by approximating integrals by finite sums. Thus, $S$ is approximated by a sequence of functions with $n$ variables where $n\rightarrow\infty$. This viewpoint is abandoned in point-set topology.} Instead, \uline{one should view a functional $S$ as a function with one variable $x$}, where $x$ denotes a general point of the function space $X$.
\item Rather than looking at each variable/component and doing explicit muti-variable calculations, one uses geometric intuitions to study the analytic properties of functionals.\footnote{This is similar to linear algebra where one prefers vectors, linear subspaces, and linear operators to $n$-tuples, sets of solutions, and matrices.} \uline{These geometric intuitions (e.g. distances, open balls, convergence) are borrowed from  $\Rbb$ and $\Rbb^n$}  and are mostly irrelevant to dimensions or numbers of variables.
\end{enumerate}



(Sequential) \textbf{compactness}, \textbf{completeness}, and \textbf{separability} are prominent geometric properties that are useful in the study of the analytic properties of functionals. The importance of these three notions  was  already recognized by Fr\'echet by the time he defined metric spaces. The study of these three properties will be a main theme of our course.


Consider sequential compactness for example. The application of compactness to function spaces originated from the problems in calculus of variations. For instance, let $L(x,y,z)$ be a polynomial or (more generally) a continuous function in $3$ variables. We want to find a ``good" (e.g. differentiable) function $f:[0,1]\rightarrow \Rbb$ minimizing or maximizing the expression
\begin{subequations}\label{eq24}
\begin{align}
S(f)=\int_0^1 L(t,f(t),f'(t))dt
\end{align}
This is the general setting of Lagrangian mechanics. In the theory of integral equations, one considers the extreme values and points of the functional
\begin{align}
S(f)=\int_0^1\int_0^1 f(x)K(x,y)\ovl{f(y)}dxdy
\end{align}
\end{subequations}
where $K:[0,1]^2\rightarrow\Rbb$ is continuous and $f:[0,1]\rightarrow\Cbb$ is subject to the condition $\int_0^1 |f(x)|^2dx\leq 1$. Any $f$ maximizing (resp. minimizing) $S(f)$ is an eigenvector of the linear operator $g\mapsto \int_0^1 K(x,y)g(y)dy$ with maximal (resp. minimal) eigenvalue.


As we shall learn, (sequential) compactness is closely related to the problem of finding (or proving the existence of) maximal/minimal values of a continuous function and the points at which the function attains its maximum/minimum. So, in 19th century, when people were already familiar with sequential compactness in $\Rbb^n$ (e.g. Bolzano-Weierstrass theorem, Heine-Borel theorem), they applied compactness to function spaces and functionals. The idea is simple: Suppose we are given $X$, a set of functions (say continuous and differentiable) from $[a,b]$ to $\Rbb$. We want to find $f\in X$ maximizing $S(f)$. Here is an explicit process (see also the proof of Lem. \ref{lb56}):
\begin{itemize}
\item[(A)] Find a sequence $(f_n)_{n\in\Zbb_+}$ in $X$ such that $S(f_n)$ increases to $M=\sup S(X)$. 
\item[(B)] Define convergence in $X$ in a suitable way, and verify that $S:X\rightarrow\Rbb$ is continuous (i.e. if $f_n$ converges to $f$ in the way we define, then $S(f_n)\rightarrow S(f)$). 
\item[(C)] Suppose we can find a subsequence $(f_{n_k})_{k\in\Zbb_+}$ converging to some $f\in X$, then $S$ attains its maximum at $f$. In particular, $S(f)=M$ and hence $M<+\infty$. 
\end{itemize}


To carry out step (B), we need to define suitable geometric structures for a function space $X$ so that the convergence of sequences in $X$ and the continuity of functions $S:X\rightarrow\Rbb$ can be defined and studied in a similar pattern as that for $\Rbb^n$. \textbf{Metric} (of a metric space) and \textbf{topology} (of a topological space) are such geometric structures. As we shall learn, the topology of a metric space is uniquely determined by the convergence of sequences in this space. 

Step (C) can be carried out if every sequence in $X$ has a convergent subsequent, i.e., if $X$ is sequentially compact. Thus, we need a good criterion for sequential compactness for subsets of a function space.  Arzel\`a-Ascoli theorem, the  $C([a,b])$-version of Heine-Borel theorem, is such a criterion. This famous theorem was proved in late 19th century (and hence before the birth of point-set topology), and it gave an important motivation for Fr\'echet to consider  metric spaces in general. We will learn this theorem at the end of the first semester.


To summarize: Metric spaces are defined not just for fun. We introduce such geometric objects because we want to study the convergence of sequences and the analytic properties of continuous functions using geometric intuitions. And moreover, the examples we are interested in are not just subsets of $\Rbb^n$, but also subsets of function spaces. With this in mind, we now begin our journey into point-set topology.


\subsection{Basic definitions and examples}



\begin{df}
Let $X$ be a set. A function $d:X\times X\rightarrow\Rbb_{\geq0}$ is called a \textbf{metric} if for all $x,y,z\in X$ we have
\begin{enumerate}[label=(\arabic*)]
\item $d(x,y)=d(y,x)$.
\item $d(x,y)=0$ iff $x=y$.
\item (Triangle inequality) \index{00@Triangle inequality} $d(x,z)\leq d(x,y)+d(y,z)$.
\end{enumerate}
The pair $(X,d)$, or simply $X$, is called \index{00@Metric space} a \textbf{metric space}. If $x\in X$ and $r\in(0,+\infty]$,\footnote{We want open and closed balls to be nonempty. So we assume $r\neq0$ only for open balls.} the set \index{Br@$B_X(x,r)=B(x,r)$ and $\ovl B_X(x,r)=\ovl B(x,r)$}
\begin{align*}
B_X(x,r)=\{y\in X:d(x,y)<r\}
\end{align*}
often abbreviated to $B(x,r)$, is called the \textbf{open ball} with center $x$ and radius $r$. If $r\in[0,+\infty)$,
\begin{align*}
\ovl B_X(x,r)=\{y\in X:d(x,y)\leq r\}
\end{align*}
also abbreviated to $\ovl B(x,r)$, is called the \textbf{closed ball} with center $x$ and radius $r$.
\end{df}


We make some comments on this definition.


\begin{rem}
That ``$d(x,y)=0$ iff $x=y$" is very useful. Think about $X$ as a set of functions $[0,1]\rightarrow\Rbb$ and $d$ is a metric on $X$. To show that $f,g\in X$ are equal, instead of checking that infinitely many values are equal (i.e. $f(t)=g(t)$ for all $t\in\Rbb$), it suffices to check that one value (i.e. $d(f,g)$) is zero.
\end{rem}

\begin{rem}
Triangle inequality clearly implies ``polygon inequality":
\begin{align}
d(x_0,x_n)\leq\sum_{j=1}^n d(x_{j-1},x_j)
\end{align}
\end{rem}

\begin{rem}\label{lb20}
Choose distinct points $x,y\in X$. Then $x,y$ are separated by two open balls centered at them: there exists $r,\rho>0$ such that $B(x,r)\cap B(y,\rho)=\emptyset$. This is called the \textbf{Hausdorff property}. 

To see this fact, note that $d(x,y)>0$. Choose $r,\rho$ such that $r+\rho\leq d(x,y)$. If $z\in B(x,r)\cap B(y,\rho)$, then $d(x,z)+d(y,z)<r+\rho\leq d(x,y)$, contradicting triangle inequality.

We will see (cf. Prop. \ref{lb21}) that Hausdorff property guarantees that any sequence in a metric space cannot converge to two different points. Intuition: one cannot find a point which is very close to $x$ and $y$ at the same time.  \hfill\qedsymbol
\end{rem}



We give some examples, and leave it to the readers to check that they satisfy the definition of metric spaces. We assume that square roots of positive real numbers can be defined. (We will rigorously define square roots after we define $e^x$ using the series $\sum_{n\in\Nbb}x^n/n!$.)


\begin{eg}
$\Rbb$ is a metric space if we define $d(x,y)=|x-y|$
\end{eg}


\begin{eg}
On $\Rbb^n$, we can define \textbf{Euclidean metric} \index{00@Euclidean metric}
\begin{align*}
d(x,y)=\sqrt{(x_1-y_1)^2+\cdots+(x_n-y_n)^2}
\end{align*}
if $x_\blt,y_\blt$ are the components of $x,y$. The following are also metrics:
\begin{gather*}
d_1(x,y)=|x_1-y_1|+\cdot+|x_n-y_n|\\
d_\infty(x,y)=\max\{|x_1-y_1|,\dots,|x_n-y_n|\}
\end{gather*}
\end{eg}

\begin{eg}
The \textbf{Euclidean metric} on $\Cbb^n$ is
\begin{align*}
d(z,w)=\sqrt{|z_1-w_1|^2+\cdots+|z_n-w_n|^2}
\end{align*}
which agrees with the Euclidean metric on $\Rbb^{2n}$. The following are also metrics:
\begin{gather*}
d_1(z,w)=|z_1-w_1|+\cdot+|z_n-w_n|\\
d_\infty(z,w)=\max\{|z_1-w_1|,\dots,|z_n-w_n|\}
\end{gather*}
\end{eg}

\begin{cv}\label{lb33}
Unless otherwise stated, the metrics on $\Rbb^n$ and $\Cbb^n$ (and their subsets) are assumed to be the Euclidean metrics.
\end{cv}


\begin{rem}
One may wonder what the subscripts $1,\infty$ mean. This notation is actually due to the general fact that
\begin{equation*}
d_p(z,w)=\sqrt[p]{|z_1-w_1|^p+\cdots+|z_n-w_n|^p}
\end{equation*}
is a metric where $1\leq p< +\infty$, and $d_\infty=\lim_{q\rightarrow +\infty}d_q$. It is not easy to prove that $d_p$ satisfies triangle inequality: one needs Minkowski inequality. For now, we will not use such general $d_p$. And we will discuss Minkowski inequality in later sections.
\end{rem}


\begin{eg}\label{lb19}
Let $X=X_1\times\cdots\times X_N$ where each $X_i$ is a metric space with metric $d_i$. Write $x=(x_1,\dots,x_N)\in X$ and $y=(y_1,\dots,y_N)\in Y$. Then the following are metrics on $X$:
\begin{gather*}
d(x,y)=d_1(x_1,y_1)+\cdots+d_N(x_N,y_N)\\
\delta(x,y)=\max\{d_1(x_1,y_1),\dots,d_N(x_N,y_N)\}\\
\rho(x,y)=\sqrt{d_1(x_1,y_1)^2+\cdots+d_N(x_N,y_N)^2}
\end{gather*}
With respect to the metric $\delta$, the open balls of $X$ are ``polydisks"
\begin{align*}
B_X(x,r)=B_{X_1}(x_1,r)\times\cdots\times B_{X_N}(x_N,r)
\end{align*}
\end{eg}


There is no standard choice of metric on the product of metric spaces. $d,\delta,\rho$ are all good, and they are equivalent in the following sense:

\begin{df}
We say that two metrics $d_1,d_2$ on a set $X$ are \index{00@Equivalent metrics} \textbf{equivalent} and write $d_1\approx d_2$, if there exist $\alpha,\beta>0$ such that  for any $x,y\in X$ we have
\begin{gather*}
d_1(x,y)\leq\alpha d_2(x,y)\qquad d_2(x,y)\leq\beta d_1(x,y)
\end{gather*}  
This is an equivalence relation. More generally, we may write $d_1\precsim d_2$ if $d_1\leq \alpha d_2$ for some $\alpha>0$. Then $d_1\approx d_2$ iff $d_1\precsim d_2$ and $d_2\precsim d_1$.
\end{df}



\begin{eg}
In Exp. \ref{lb19}, we have $\delta\leq \rho\leq d\leq N\delta$. So $\delta\approx\rho\approx d$.
\end{eg}

\begin{cv}\label{lb32}
Given finitely many metric spaces $X_1,\dots,X_N$, the metric on their product space $X=X_1\times\cdots\times X_N$ is chosen to be any one that is equivalent to the ones defined in Exp. \ref{lb19}. In the case that each $X_i$ is a subset of $\Rbb$ or $\Cbb$, we follow Convention \ref{lb33} and choose the metric on $X$ to be the Euclidean metric (unless otherwise stated).
\end{cv}


\begin{df}\label{lb43}
Let $(X,d)$ be a metric space. Then a \textbf{metric subspace} \index{00@Metric subspace} is denotes an object $(Y,d|_Y)$ where $Y\subset X$ and $d|_Y$ is the restriction of $d$ to $Y$, namely, for all $y_1,y_2\in Y$ we set
\begin{align*}
d|_Y(y_1,y_2)=d(y_1,y_2)
\end{align*}
\end{df}

\begin{cv}\label{lb76}
Suppose $Y$ is a subset of a given metric space $(X,d)$. Unless otherwise stated, the metric of $Y$ is chosen to be $d|_Y$ whenever $Y$ is viewed as a metric space. For example, the metric of any subset of $\Rbb^n$ is assumed to be the Euclidean metric, unless otherwise stated.
\end{cv}



\subsection{Convergence of sequences} \label{lb73}

\begin{df}
Let $(x_n)_{n\in\Zbb_+}$ be a sequence in a metric space $X$. Let $x\in X$. We say that $x$ is the \textbf{limit} of $x_n$ and write $\displaystyle\lim_{n\rightarrow\infty}x_n=x$ (or $x_n\rightarrow x$), if:
\begin{itemize}
\item For every real number $\varepsilon>0$ there exists $N\in\Zbb_+$ such that for every $n\geq N$ we have $d(x_n,x)<\varepsilon$. 
\end{itemize}
Equivalently, this means that every (nonempty) open ball centered at $x$ contains \textbf{all but finitely many} \index{00@All but finitely many $x_n$} $x_n$.\footnote{``All but finitely many $x_n$ satisfies..." means ``for all but finitely many $n$, $x_n$ satisfies...". It does NOT mean that ``all but finitely many elements of the set $\{x_n:n\in\Zbb_+\}$ satisfies...".} 
\end{df}

\begin{rem}\label{lb100}
The negation of $x_n\rightarrow x$ is clear: 
\begin{itemize}
\item There exists $\eps>0$ such that for all $N\in\Zbb_+$ there exists $n\geq N$ such that $d(x_n,x)\geq\eps$.
\end{itemize}
Namely, one changes each ``for all" to ``there exists", changes each ``there exists" to ``for all", and negate the last sentence.
\end{rem}


\begin{exe}
Show that in the above definition of limits,  it suffices to consider rational numbers $\varepsilon>0$. (Note: You need to use Prop. \ref{lb2}.) 
\end{exe}
This exercise implies that the definition of limits does not require the existence of real numbers, i.e., does not assume Thm. \ref{lb3}. Indeed, we will use limits of sequences (and ``double sequences") to prove Thm. \ref{lb3}.


In many textbooks and research papers, you will see phrases such as \index{00@Sufficiently large}
\begin{gather}
\text{$x_n$ satisfies property $P$ for } \textbf{sufficiently large} \text{ $n$}
\end{gather}
This means that ``there exists $N\in\Zbb_+$ such that $P$ holds for all $n\geq N$". (We also say that $x_n$ \textbf{eventually} satisfies $P$.) Then $\lim_{n\rightarrow\infty} x_n=x$ means that ``for every $\varepsilon>0$, we have $d(x_n,x)<\varepsilon$ for sufficiently large $n$". 

\begin{pp}\label{lb21}
Any sequence $(x_n)_{n\in\Zbb_+}$ in a metric space $X$ has at most one limit.
\end{pp}

\begin{proof}
Suppose $(x_n)_{n\in\Zbb_+}$ converges to $x,y\in X$ where $x\neq y$. By Hausdorff property (Rem. \ref{lb20}), there exist $r,\rho>0$ such that $B(x,r)\cap B(y,\rho)=\emptyset$. By the definition of $x_n\rightarrow x$, there exists $N_1\in\Zbb_+$ such that $x_n\in B(x,r)$ for all $n\geq N_1$. Similarly, $x_n\rightarrow y$ means that there is $N_2\in\Zbb_+$ such that $x_n\in B(y,\rho)$ for all $n\geq N_2$. Choose any $n\geq N_1,N_2$ (e.g. $n=\max\{N_1,N_2\}$). Then $x_n\in B(x,r)\cap B(y,\rho)=\emptyset$, impossible.
\end{proof}


\subsubsection{Methods for proving convergence and computing limits}



\begin{eg}
$\dps \lim_{n\rightarrow\infty}\frac 1n=0$.
\end{eg}


\begin{proof}
Choose any $\varepsilon\in\Qbb_{>0}$. By Archimedean property, there exists $N\in\Zbb_+$ such that $N\varepsilon>1$, i.e. $1/N<\varepsilon$. Thus, for all $n\geq N$ we have $1/n<\varepsilon$.
\end{proof}

\begin{pp}
Let $\Fbb\in\{\Qbb,\Rbb\}$ and $(x_n),(y_n)$ be sequences in $\Fbb$ converging to $x,y\in\Rbb$. If $x_n\leq y_n$ for all $n$, then $x\leq y$.
\end{pp}

\begin{proof}
If $y<x$, let $\varepsilon=x-y$. Then all but finitely many members of $(x_n)$ are in $(x-\varepsilon/2,x+\varepsilon/2)$, and all but finitely many members of $(y_n)$ are in $(y-\varepsilon/2,y+\varepsilon/2)$. Since $y+\eps/2<x-\eps/2$, there must exist $n$ such that $y_n<x_n$.
\end{proof}


\begin{df}
If $A$ and $B$ are posets (or more generally, preordered sets, see Def. \ref{lb116}), we say a function $f:A\rightarrow B$ is \textbf{increasing} \index{00@Increasing and decreasing} \index{00@Strictly increasing and strictly decreasing} (resp. \textbf{strictly increasing}), if for each $x,y\in A$ we have
\begin{gather*}
x\leq y\qquad\Longrightarrow\qquad f(x)\leq f(y)\\
\text{resp.}\\
x<y\qquad\Longrightarrow \qquad f(x)<f(y)
\end{gather*}
We leave the definitions of \textbf{decreasing} and \textbf{strictly decreasing} to the readers. We say that $f$ is \index{00@Monotonic} \index{00@Strictly monotonic} \textbf{monotonic} (resp. \textbf{strictly monotonic}), if $f$ is either increasing or decreasing (resp. either strictly increasing or strictly decreasing).
\end{df}


\begin{pp}\label{lb57}
Let $(x_n)_{n\in\Zbb}$ be a sequence in $[a,b]\subset\Rbb$. If $(x_n)$ is increasing (resp. decreasing), then $\dps\lim_{n\rightarrow \infty}x_n$ equals $\dps\sup\{x_n:n\in\Zbb_+\}$ (resp. $\dps\inf\{x_n:n\in\Zbb_+\}$).
\end{pp}

\begin{proof}
Assume $(x_n)$ increases. (The case of decreasing is similar and hence its proof is omitted.) Let $A=\sup\{x_n:n\in\Zbb_+\}<+\infty$. Then for each $\eps>0$ there is $N$ such that $x_N>A-\eps$ (since $A-\eps$ is not an upper bound). Since $(x_n)$ is increasing, for all $n\in\Nbb$ we have $A-\eps<x_n\leq A$ and so $|x_n-A|<\eps$.
\end{proof}



\begin{eg}\label{lb27}
Let $(x_n)_{n\in\Zbb_+}$ be a sequence in a metric space $X$, and let $x\in X$. It is easy to see that
\begin{align*}
\lim_{n\rightarrow\infty} x_n=x\qquad\Longleftrightarrow \qquad \lim_{n\rightarrow\infty} d(x_n,x)=0
\end{align*}
\end{eg}

\begin{eg}\label{lb28}
Suppose that $(a_n)$ and $(b_n)$ are sequences in $\Rbb_{\geq 0}$, that $a_n\leq b_n$ for all $n$, and that $b_n\rightarrow 0$. Then  $a_n\rightarrow 0$. 
\end{eg}
\begin{proof}
For each $\varepsilon>0$, $[0,\varepsilon)$ contains all but finitely many $b_n$, and hence all but finitely many $a_n$.
\end{proof}


More generally, we have:

\begin{pp}[\textbf{Squeeze theorem}]\label{lb29}\index{00@Squeeze theorem}
Suppose that $(x_n)$ is a sequence in a metric space $X$. Let $x\in X$. Suppose that there is a sequence $(a_n)$ in $\Rbb_{\geq 0}$ such that $\dps\lim_{n\rightarrow\infty}a_n=0$ and that $d(x_n,x)\leq a_n$ for all $n$. Then $\dps\lim_{n\rightarrow\infty} x_n=x$.
\end{pp}

\begin{proof}
This follows immediately from Exp. \ref{lb27} and \ref{lb28}.
\end{proof}

The above proposition explains why many people say that ``analysis is the art of inequalities": It transforms the problem of convergence to the problem of finding a sequence $(a_n)\in\Rbb_{\geq 0}$ converging to $0$ such that the inequality $d(x_n,x)\leq a_n$ holds. And very often, a good (hard) analyst is one who knows how to find such good sequences!






\begin{pp}\label{lb38}
Let $X=X_1\times\cdots\times X_N$ be a product of metric spaces $(X_i,d_i)$. Let $d$ be any of the three metrics of $X$ in Exp. \ref{lb19}. Let $\mbf x_n=(x_{1,n},\dots,x_{N,n})$ be a sequence in $X$. Let $\mbf y=(y_1,\dots,y_N)$. Then 
\begin{align*}
\lim_{n\rightarrow\infty} \mbf x_n=\mbf y\qquad\Longleftrightarrow \qquad \lim_{n\rightarrow\infty} x_{i,n}=y_i~~(\forall 1\leq i\leq N)
\end{align*}
\end{pp}

\begin{proof}
We let $d$ be the metric $\delta$ in Exp. \ref{lb19}, i.e. defined by $\max_j d_j(x_j,y_j)$. Now choose a sequence $(\mbf x_n)$ and an element $\mbf y$ in $X$. Then
\begin{align}
\mbf x_n\rightarrow \mbf y~~\Longleftrightarrow~~ d_X(\mbf x_n,\mbf y)\rightarrow 0 ~~\Longleftrightarrow~~ \max_{1\leq j\leq N} d_j(x_{j,n},y_j)\rightarrow 0  \label{eq13}
\end{align}


Suppose that the RHS of \eqref{eq13} is true. Fix any $1\leq i\leq N$. Then $d_i(x_{i,n},y_i)\leq \max_j d_j(x_{j,n},y_j)$. So $x_{i,n}\rightarrow y_i$ by Prop. \ref{lb29}.

Conversely, assume that for every $i$ we have $x_{i,n}\rightarrow y_i$. Then for every $\eps>0$ there is $K_i\in\Zbb_+$ such that $d_i(x_{i,n},y_i)<\varepsilon$ for every $n\geq K_i$. Let $K=\max\{K_1,\dots,K_N\}$. Then for all $n\geq K$ we have $\max_j d_j(x_{j,n},y_j)<\eps$. This proves the RHS of \eqref{eq13}.

If $d$ is one of the other two metrics in Exp. \ref{lb19}, one can either use a similar argument, or use the following important (but easy) fact.
\end{proof}





\begin{pp}
Let $d,\delta$ be two equivalent metrics on a set $X$. Let $(x_n)_{n\in\Zbb_+}$ and $x$ be in $X$. Then $(x_n)$ converges to $x$ under the metric $d$ iff  $(x_n)$ converges to $x$ under $\delta$. Namely, $d(x_n,x)\rightarrow 0$ iff $\delta(x_n,x)\rightarrow 0$.
\end{pp}

\begin{proof}
Prove it yourself. (Or see Prop. \ref{lb48}.)
\end{proof}

More useful formulas about limit will be given in Exp. \ref{lb111}.







\subsubsection{Criteria for divergence}\label{lb119}

\begin{df}
A subset $E$ of a metric space $(X,d)$ is called \index{00@Bounded subset} \textbf{bounded} if either $E=\emptyset$ or there exist $p\in X$ and $R\in\Rbb_{>0}$ such that $E\subset B_X(p,R)$. If $X$ is bounded, we also say that $d$ is a \textbf{bounded metric}. \index{00@Bounded metric}
\end{df}

\begin{rem}
Note that if $E$ is bounded, then  for \textit{any} $q\in X$ there exists $\wtd R\in\Rbb_{>0}$ such that $E\subset B_X(q,\wtd R)$. (Indeed, choose $\wtd R=R+d(p,q)$, then by triangle inequality, $B(p,R)\subset B(q,\wtd R)$.)
\end{rem}

Some easy examples are as follows.
\begin{eg}
In a metric space $X$, if $x\in X$ and $0<r<+\infty$, then $B(x,r)$ is bounded. Hence $\ovl B(x,r)$ is bounded (since it is inside $B(x,2r)$).
\end{eg}


%% Record #1 2023/09/18 two lectures

Also, it is easy to see:
\begin{eg}\label{lb22}
A finite union of bounded subsets is bounded.
\end{eg}

\begin{pp}\label{lb24}
Let $(x_n)_{n\in\Zbb_+}$ be a convergent sequence in a metric space $X$. Then $(x_n)_{n\in\Zbb_+}$ is bounded.
\end{pp}

By saying that a sequence \index{00@Bounded sequence} $(x_n)_{n\in\Zbb_+}$ in $X$ is \textbf{bounded}, we mean that its range in $X$ (namely $\{x_n:n\in\Zbb_+\}$) is bounded.

\begin{proof}
Suppose that $x_n\rightarrow x$. Then for each $\varepsilon>0$, say $\varepsilon=1$, all but finitely many elements of $x_n$ (say $x_1,\dots,x_N$) are in $B(x,1)$. So this whole sequence is in $A=\{x_1\}\cup\cdots\{x_N\}\cup B(x,1)$. Since each $\{x_i\}$ is bounded, and since $B(x,1)$ is bounded, $A$ is bounded by Exp. \ref{lb22}.
\end{proof}


\begin{rem}\label{lb26}
Prop. \ref{lb24} gives our first criterion on divergence: If a sequence is unbounded (e.g. $x_n=n^2$), then it does not converge. But there are many bounded and divergent sequences. (See Exp. \ref{lb25}.) In this case, we need the second criterion: If a sequence has two subsequences converging to two different points, then this sequence diverge. (See Prop. \ref{lb23})
\end{rem}




\begin{df}
Let $(x_n)_{n\in\Zbb_+}$ be a sequence in a set $X$. If $(n_k)_{k\in\Zbb_+}$ is a strictly increasing sequence in $\Zbb_+$, we say that $(x_{n_k})_{k\in\Zbb_+}$ is a \textbf{subsequence} of $(x_n)_{n\in\Zbb_+}$. \index{00@Subsequence}
\end{df}



Thus, a subsequence of $(x_n)_{n\in\Zbb_+}$ is equivalently the restriction of the function $x:\Zbb_+\rightarrow X$ to an infinite subset of $\Zbb_+$.

\begin{pp}\label{lb23}
Let $(x_n)_{n\in\Zbb_+}$ be a sequence in a metric space $X$ converging to $x\in X$. Then every subsequence $(x_{n_k})_{k\in\Zbb_+}$ converges to $x$.
\end{pp}

\begin{proof}
For every $\varepsilon>0$, $B(x,\varepsilon)$ contains all but finitely many $\{x_n:n\in\Zbb_+\}$, and hence all but finitely many $\{x_{n_k}:k\in\Zbb_+\}$.
\end{proof}

\begin{eg}\label{lb25}
The sequence $x_n=(-1)^n$ in $\Rbb$ is divergent, because the subsequence $(x_{2k})_{k\in\Zbb_+}$ converges to $1$, whereas $(x_{2k-1})_{k\in\Zbb_+}$ converges to $-1$. 
\end{eg}




One may wonder if the two criteria in Rem. \ref{lb26} are complete in order to determine whether a sequence diverges. This is true for sequences in $\Rbb^n$. We will discuss this topic later. (See Cor. \ref{lb75}.)



\subsection{Continuous maps of metric spaces}\label{lb185}

Continuous maps are a powerful tool for showing that a sequence converges.


\begin{df}\label{lb31}
Let $f:X\rightarrow Y$ be a map of metric spaces. Let $x\in X$. We say that $f$ is \textbf{continuous at $x$} if one of the following equivalent conditions hold:
\begin{itemize}[align=left]
\item [(1)] For every sequence $(x_n)_{n\in\Zbb_+}$ in $X$, we have
\begin{align*}
\lim_{n\rightarrow\infty} x_n=x\qquad\Longrightarrow\qquad \lim_{n\rightarrow\infty} f(x_n)=f(x)
\end{align*}
\item[(2)] For every $\varepsilon>0$, there exists $\delta>0$ such that for every $p\in X$ satisfying $d(p,x)<\delta$, we have $d(f(p),f(x))<\varepsilon$.
\item[(2')] For every $\varepsilon>0$, there exists $\delta>0$ such that $B_X(x,\delta)\subset f^{-1}(B_Y(f(x),\varepsilon)))$.
\end{itemize}
We say that $f$ is a \textbf{continuous map/function}, if $f$ is continuous at every point of $X$.
\end{df}

\begin{proof}[Proof of the equivalence]
(2)$\Leftrightarrow$(2'): Obvious.

(2)$\Rightarrow$(1): Assume (2). Assume $x_n\rightarrow x$. For every $\varepsilon>0$, let $\delta>0$ be as in (2). Then since $x_n\rightarrow x$, there is $N\in\Zbb_+$ such that for all $n\geq N$ we have $d(x_n,x)<\delta$. By (2), we have $d(f(x_n),f(x))<\varepsilon$ for all $n\geq N$. This proves $f(x_n)\rightarrow f(x)$.

$\neg$(2)$\Rightarrow$ $\neg$(1): Assume that (2) is not true. Then there exists $\varepsilon>0$ such that for every $\delta>0$, there exists $p\in X$ such that $d(p,x)<\delta$ and $d(f(p),f(x))\geq\varepsilon$. Thus, for every $n\in\Zbb_+$, by taking $\delta=1/n$, we see that there exists $x_n\in X$ such that $d(x_n,x)<1/n$ and $d(f(x_n),f(x))\geq\varepsilon$. By Squeeze theorem (Prop. \ref{lb29}), $x_n\rightarrow x$. But $f(x_n)\nrightarrow f(x)$ (i.e. $d(f(x_n),f(x))\nrightarrow 0$). So (1) is not true.
\end{proof}

\begin{rem}
One can compare Def. \ref{lb31}-(1) to the definition of linear maps. A map is continuous iff it \textit{preserves the convergence of sequences}, i.e., iff it maps convergent sequences to convergent ones. A map (between vector spaces) is linear iff it perserves the addition and the scalar multiplication of vectors. In general, a good map between two sets with ``structures" is a map which preserves the structures. (Thus, linear combinations encode the linear structures of vector spaces. Similarly, the convergence of sequences remembers the ``topological" structures of metric spaces.) As another example, we will define an isometry of metric spaces to be one that preserves the metrics (the structures of metric spaces), see Exe. \ref{lb46}.
\end{rem}

\begin{rem}\label{lb115}
In this section, we mainly use Def. \ref{lb31}-(1) to study continuity. But in later sections we will also use Def. \ref{lb31}-(2'). An advantage of (2') is that it is more geometric. Indeed, if $X$ is a metric space and $E\subset X$, we say that $x\in E$ is an \textbf{interior point of $E$ in $X$} \index{00@Interior point} if there exists $\delta>0$ such that $B_X(x,\delta)\subset E$. For example, a point  $z\in\Cbb$ is an interior point of the closed unit disk $\ovl B_\Cbb(0,1)=\{w\in\Cbb:|w|\leq 1\}$ iff $|z|<1$. 

Thus, Def. \ref{lb31}-(2') says that for any map of metric spaces $f:X\rightarrow Y$ and $x\in X$, the following are equivalent:
\begin{itemize}
\item[(a)] $f$ is continuous at $x$.
\item[(b)] For each $\eps>0$, every $x\in X$ is an interior point of $f^{-1}\big(B_Y(f(x),\varepsilon)\big)$.
\end{itemize}
We say that a subset $U\subset X$ is \textbf{open} \index{00@Open set} if each point of $U$ is an interior point. For example, by triangle inequality, every open ball in a metric space is an open set. Thus, we have:
\begin{itemize}
\item A map of metric spaces $f:X\rightarrow Y$ is continuous iff the preimage under $f$ of every open ball of $Y$ is an open subset of $X$.
\end{itemize}

In the study of point-set topology, we will see that many properties can be studied in two approaches: using sequences (or using nets, the natural generalizations of sequences) and their convergence, and using open sets. The first example of such property is continuity, as we have seen in Def. \ref{lb31}. Another prominent example is sequential compactness vs. compactness. These two approaches represent two (very) different intuitions: one is dynamic, while the other is static and more geometric. (So it is surprising that these two very things are actually equivalent!) Sometimes both approaches work for a problem, but sometimes only one of them works, or one of them is much simpler. If you are a beginner in analysis and point-set topology, I suggest that whenever you see one approach applied to a problem, try to think about whether the other approach also works and which one is better.   \hfill\qedsymbol
\end{rem}


\subsubsection{Methods for proving continuity}


\begin{lm}\label{lb30}
Let $f:X\rightarrow Y$ be a map of metric spaces.  Let $(B_i)_{i\in I}$ be a collection of open balls in $X$ such that $X=\bigcup_{i\in I}B_i$. Suppose that for each $i$, the restriction $f|_{B_i}:B_i\rightarrow Y$ is continuous. Then $f$ is continuous. 
\end{lm}

This lemma shows that if we can prove that $f$ is ``locally" continuous, then $f$ is globally continuous. 

\begin{proof}
Choose $(x_n)$ in $X$ converging to $x\in X$. We shall show $f(x_n)\rightarrow f(x)$. Choose $i$ such that $x\in B_i$. Then one can find $\delta>0$ such that $B(x,\delta)\subset B_i$. (In the language of point-set topology: $x$ is an interior point of $B_i$.) To see this, write $B_i=B(y,r)$. Since $x\in B(y,r)$, we have $r-d(x,y)>0$. Choose $0<\delta\leq r-d(x,y)$. Then triangle inequality implies $B(x,\delta)\subset B(y,r)$. 

Since $x_n\rightarrow x$, there is $N\in\Zbb_+$ such that $x_n\in B(x,\delta)$ for all $n\geq N$. Thus, $(x_{k+N})_{k\in\Zbb_+}$ converges in $B_i$ to $x$. Since $f|_{B_i}$ is continuous, $\lim_{k\rightarrow\infty} f(x_{k+N})=f(x)$. So $f(x_n)\rightarrow f(x)$.
\end{proof}







\begin{df}
A map of metric spaces $f:X\rightarrow Y$ is called \index{00@Lipschitz continuous} \textbf{Lipschitz continuous} if there exists $L\in\Rbb_{>0}$ (called \textbf{Lipschitz constant}) \index{00@Lipschitz constant} such that for all $x_1,x_2\in X$,
\begin{align}
d_Y\big(f(x_1),f(x_2) \big)\leq L\cdot d_X(x_1,x_2) \label{eq14}
\end{align}
\end{df}

\begin{lm}\label{lb34}
Lipschitz continuous maps are continuous. 
\end{lm}

\begin{proof}
Suppose that $f:X\rightarrow Y$ is Lipschitz continuous with Lipschitz constant $L$. Suppose $x_n\rightarrow x$ in $X$. Then $L\cdot d(x_n,x)\rightarrow 0$. By \eqref{eq14} and Squeeze theorem (Prop. \ref{lb29}), $f(x_n)\rightarrow f(x)$. (You can also use Def. \ref{lb31}-(2) to prove this lemma.)
\end{proof}

\begin{eg}\label{lb35}
Let $\Fbb\in\{\Qbb,\Rbb,\Cbb\}$. The map $z\in\Fbb\setminus \{0\}\mapsto z^{-1}\in\Fbb$ is continuous.
\end{eg}



\begin{proof}
Let this map be $f$. Since $\Fbb$ is covered by open balls $B(z,\delta)$ where $z\in\Fbb\setminus\{0\}$ and $0<\delta<|z|$, by Lem. \ref{lb30}, it suffices to prove that $f$ is continuous when restricted to every such $B(z,\delta)$. Let $\eps=|z|-\delta>0$. Choose $x,y\in B(z,\delta)$. Then $|x|=|x-z+z|\geq |z|-|z-x|>\eps$ by triangle inequality. Similarly, $|y|>\eps$. So
\begin{align*}
|f(x)-f(y)|=|x^{-1}-y^{-1}|=|x-y|/|xy|\leq \eps^{-2}|x-y|
\end{align*}
So $f|_{B(z,\delta)}$ has Lipschitz constant $\eps^{-2}$, and hence is continuous.
\end{proof}

(Question: in the above proof, is the map $f:\Fbb\setminus\{0\}\rightarrow\Fbb$ Lipschitz continuous?)

We have given a fancy way of proving that  if $(z_n)$ is a sequence in $\Fbb\setminus\{0\}$ converging to $z\in\Fbb\setminus\{0\}$, then $z_n^{-1}\rightarrow z^{-1}$. You should think about how to prove this fact directly using $\eps-N$ language, and compare your proof with the above proof to find the similarities!




\begin{pp}\label{lb41}
Let $\Fbb\in\{\Qbb,\Rbb,\Cbb\}$. Then the following maps are continuous:
\begin{gather*}
+:\Fbb\times\Fbb\rightarrow\Fbb\qquad (x,y)\mapsto x+y\\
-:\Fbb\times\Fbb\rightarrow\Fbb\qquad  (x,y)\mapsto x-y\\
\times: \Fbb\times\Fbb\rightarrow\Fbb\qquad  (x,y)\mapsto xy\\
\div:\Fbb\times\Fbb^\times\rightarrow\Fbb\qquad  (x,y)\mapsto x/y
\end{gather*}
\end{pp}


Recall our Convention \ref{lb32} on the metrics of finite product spaces. 


\begin{proof}
We only prove that the last two are continuous: the first two can be treated in a similar (and easier) way.

Denote the multiplication map by $\mu$. We choose the metric on $\Fbb^2$ to be $d(\mbf x,\mbf x')=\max\{|x_1-x_1'|,|x_2-x_2'|\}$. Since $\Fbb\times\Fbb$ is covered by open balls of the form $B(0,r)=\{(x,y)\in\Fbb^2:|x|<r,|y|<r\}$ where $0<r<+\infty$, by Lem. \ref{lb30} and \ref{lb34}, it suffices to show that $\mu|_{B(0,r)}$ is Lipschitz continuous. This is true, since for each $(x,y),(x',y')\in B(0,r)$, we have
\begin{align}\label{eq22}
\begin{aligned}
&|\mu(x,y)-\mu(x',y')|=|xy-x'y'|\leq |(x-x')y|+|x'(y-y')|\\
<&2r\cdot \max\{|x-x'|,|y-y'|\}=2r \cdot d((x,y),(x',y'))  
\end{aligned}
\end{align}

By Exp. \ref{lb35} and Prop. \ref{lb37}, the map $(x,y)\in\Fbb\times\Fbb^\times\mapsto (x,y^{-1})\in\Fbb\times\Fbb$ is continuous. So its composition with the continuous map $\mu$ is continuous, thanks to Prop. \ref{lb36}. So $\div$ is continuous. 
\end{proof}

\begin{pp}\label{lb36}
Suppose that $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ are continuous maps of metric spaces. Then $g\circ f:X\rightarrow Z$ is continuous.
\end{pp}

\begin{pp}\label{lb37}
Suppose that $f_i:X_i\rightarrow Y_i$ is a map of metric spaces, where $1\leq i\leq N$. Then the product map \index{00@Product map}
\begin{gather*}
f_1\times\cdots\times f_N:X_1\times\cdots\times X_N\rightarrow Y_1\times\cdots\times Y_N\\
(x_1,\dots,x_N)\mapsto (f_1(x_1),\dots,f_N(x_N))
\end{gather*}
is continuous if and only if $f_1,\dots,f_N$ are continuous.
\end{pp}

\begin{proof}[Proof of Prop. \ref{lb36} and \ref{lb37}]
Immediate from Def. \ref{lb31}-(1) and Prop. \ref{lb38}.
\end{proof}


\begin{co}[\textbf{Squeeze theorem}]\index{00@Squeeze theorem}\label{lb61}
Let $\Fbb\in\{\Qbb,\Rbb\}$ and $(x_n),(y_n),(z_n)$ be sequences in $\Rbb$. Assume that $x_n\leq y_n\leq z_n$ for all $n$. Assume that $x_n$ and $z_n$ both converge to $A\in\Rbb$. Then $\lim_{n\rightarrow\infty}y_n=A$. 
\end{co}

\begin{proof}
Let $a_n=y_n-x_n$ and $b_n=z_n-x_n$. Then $0\leq a_n\leq b_n$, and $\lim_n b_n=\lim_n z_n-\lim_n x_n= 0$ because the subtraction map is continuous (Prop. \ref{lb41}). By Exp. \ref{lb28}, $a_n\rightarrow 0$. Since $x_n\rightarrow A$, $y_n=x_n+a_n$ converges to $A$, since the addition map is continuous by Prop. \ref{lb41}.
\end{proof}

Again, this is a fancy way of proving Squeeze theorem. The readers should know how to prove it directly from the definition of limits of sequences.

We give some more examples of continuous maps.

\begin{eg}
By Prop. \ref{lb41}, $f(x,y,z)=x^2y+5y^4z^7-3xyz^2$ is a continuous function on $\Cbb^3$. Clearly $z\in\Cbb\mapsto \ovl z\in\Cbb$ is continuous. So $g(x,y,z)=f(\ovl x,\ovl y,z)+2\ovl{f(z,x^2,xy^{-9})}-5xy^{-2}\ovl{z^{-3}}$ is a continuous function on $\Cbb\times\Cbb^\times\times\Cbb^\times$.
\end{eg}


\begin{eg}\label{lb44}
Let $f,g:X\rightarrow \Fbb$ be continuous functions where $\Fbb\in\{\Qbb,\Rbb,\Cbb\}$. Then by Prop. \ref{lb41} and \ref{lb36}, $f\pm g$ and $fg$ are continuous, and $f/g$ is continuous when $0\notin g(X)$. Here
\begin{align}
(f\pm g)(x)=f(x)\pm g(x)\qquad (fg)(x)=f(x)g(x)\qquad (f/g)(x)=f(x)/g(x)
\end{align} 
\end{eg}


\begin{eg}\label{lb54}
Let $f:X\rightarrow Y$ be a map of metric spaces. Let $E,F$ be subsets of $X,Y$ respectively. (Recall that the metrics of subsets are chosen as in Def. \ref{lb43}.) 
\begin{itemize}
\item The inclusion map $\iota_E:E\rightarrow X,x\mapsto x$ is clearly continuous. Thus, if $f$ is continuous, then $f|_E:E\rightarrow Y$ is continuous, since $f|_E=f\circ\iota_E$.
\item If $f(X)\subset F$, then we can restrict the codomain of $f$ from $Y$ to $F$: let $\wtd f:X\rightarrow F$ be $\wtd f(x)=f(x)$. Then it is clear that $f$ is continuous iff $\wtd f$ is continuous.
\end{itemize}
\end{eg}




\begin{pp}
Let $X_1,\dots,X_N$ be metric spaces. The following  projection is clearly continuous:
\begin{gather*}
\pi_{X_i}:X_1\times\cdots\times X_N\rightarrow X_i\qquad(x_1,\dots,x_N)\mapsto x_i
\end{gather*}
\end{pp}

\begin{pp}
Let $f_i:X\rightarrow Y_i$ be maps where $X,Y_1,\dots,Y_N$ are continuous. Then 
\begin{gather*}
f_1\vee \cdots\vee f_N:X\rightarrow Y_1\times\cdots\times Y_N\qquad x\mapsto (f_1(x),\dots,f_N(x))
\end{gather*}
is continuous iff $f_1,\dots,f_N$ are continuous.
\end{pp}


\begin{eg}
Let $X$ be a metric space. Then $d:X\times X\rightarrow\Rbb,(x,y)\mapsto d(x,y)$ is Lipschiz continuous with Lipschitz constant $1$ (by triangle inequality). So $d$ is continuous.
\end{eg}

\begin{pp}\label{lb42}
Let $X$ be a metric space and $E\subset X$ is nonempty. Define \textbf{distance function} \index{00@Distance function $d(\cdot,E)$} \index{dE@$d(x,E)$} 
\begin{gather*}
d(\cdot,E):X\rightarrow\Rbb_{\geq0}\qquad
d(x,E)=\inf_{e\in E}d(x,e)
\end{gather*}
Then $d(\cdot,E)$ is has Lipschitz constant $1$. So $d(\cdot,E)$ is continuous.
\end{pp}


\begin{proof}
Choose any $x,y\in X$. By triangle inequality, for each $e\in E$ we have $d(x,e)\leq d(x,y)+d(y,e)$. Since $d(x,E)\leq d(x,e)$, we get  $d(x,E)\leq d(x,y)+d(y,e)$. Applying $\inf_{e\in E}$ to the RHS gives $d(x,E)\leq d(x,y)+d(y,E)$. Hence $d(x,E)-d(y,E)\leq d(x,y)$. Exchanging $x$ and $y$ gives
\begin{align}
\big|d(x,E)-d(y,E)\big|\leq d(x,y)
\end{align}
This proves that $d(\cdot,E)$ has Lipschitz constant $1$.
\end{proof}

\begin{df}
More generally, if $E,F$ are subsets of a metric space $E$, we can define \index{DEF@$d(E,F)$}
\begin{align}
d(E,F)=\inf_{e\in E,f\in F}d(e,f)
\end{align}
to be the \textbf{distance between $E$ and $F$}.
\end{df}

\begin{exe}
Let $E,F\subset X$. Prove that
\begin{align}
d(E,F)=\inf_{f\in F}d(E,f)
\end{align}
\end{exe}

\begin{eg}\label{lb45}
If $X$ is a metric space and $p\in X$, then by Prop. \ref{lb42} (or simply by triangle inequality), the function $d_p:x\in X\mapsto d(x,p)\in\Rbb$ has Lipschitz constant $1$ and hence is continuous. In particular, if $\Fbb\in\{\Rbb,\Cbb\}$,  the function $z\in\Fbb\mapsto |z|$ is continuous (since $|z|=d_\Fbb(z,0)$). Thus, if $f:X\rightarrow\Fbb$ is continuous, then $|f|:X\rightarrow\Rbb_{\geq0}$ is continuous where
\begin{align}
|f|(x)=|f(x)|
\end{align}
\end{eg}


\begin{eg}
Let $N\in\Zbb_+$. Then the following function is continuous:
\begin{align*}
\max:\Rbb^N\rightarrow \Rbb\qquad (x_1,\dots,x_N)\mapsto\max\{x_1,\dots,x_N\}\in\Rbb
\end{align*}
Similarly, the minimum function is continuous.
\end{eg}

\begin{proof}
To avoid confusion, we write $\max$ as $\max_N$. The case $N=1$ is obvious. When $N=2$, we have
\begin{align}
\max(x_1,x_2)=\frac{x_1+x_2+|x_1-x_2|}2
\end{align}
So $\max_2$ is continuous by Exp. \ref{lb44} and \ref{lb45}.

We use induction. Suppose we have proved that $\max_N$ is continuous. Then $\max_N\times\id_\Rbb:\Rbb^N\times \Rbb\rightarrow\Rbb\times\Rbb$ is continuous. So $\max_{N+1}=\max_2\circ(\max_N\times\id_\Rbb)$ is continuous.
\end{proof}


\subsection{Homeomorphisms and isometric isomorphisms; convergence in $\ovl{\Rbb}$}

\subsubsection{General theory}



\begin{df}\label{lb189}
A bijection of metric spaces $f:X\rightarrow Y$ is called a \textbf{homeomorphism} if one of the following equivalent (cf. Def. \ref{lb31}) statements holds:
\begin{itemize}
\item[(1)] $f:X\rightarrow Y$ and its inverse map $f^{-1}:Y\rightarrow X$ are continuous.
\item[(2)] For each sequence $(x_n)$ in $X$ and each $x\in X$, we have $\dps \lim_{n\rightarrow\infty}x_n=x$ iff $\dps\lim_{n\rightarrow\infty}f(x_n)=f(x)$.
\end{itemize}
If such $f$ exists, we say that $X,Y$ are \textbf{homeomorphic}.
\end{df}

A special case of the above definition is:
\begin{df}\label{lb144}
Let $X$ be a set with metrics $d,\delta$. We say that $d$ and $\delta$ induce the \textbf{same topology} on $X$ (or that $d,\delta$ are \index{00@Topologically equivalent metrics} \textbf{topologically equivalent}) if one of the following clearly equivalent statements holds:
\begin{itemize}
\item[(1)] The map $(X,d)\rightarrow (X,\delta),x\mapsto x$ is a homeomorphism.\footnote{We prefer not to call this map the identity map, because the metrics on the source and on the target are different.}
\item[(2)] For each sequence $(x_n)$ in $X$ and each $x\in X$, $(x_n)$ converges to $x$ under the metric $d$ iff $(x_n)$ converges to $x$ under $\delta$.
\end{itemize}
\end{df}


\begin{pp}\label{lb48}
Suppose that $d,\delta$ are equivalent metrics on a set $X$. Then $d,\delta$ are topologically equivalent.
\end{pp}

\begin{proof}
Suppose $\delta\leq\alpha d$ and $d\leq\beta\delta$ for some $\alpha,\beta>0$. Then the map $f:(X,d)\rightarrow (X,\delta),x\mapsto x$ and its inverse $f^{-1}$ have Lipschitz constants $\alpha$ and $\beta$ respectively. So $f,f^{-1}$ are continuous.
\end{proof}






\begin{exe}\label{lb46}
Let $f:X\rightarrow Y$ be a map of metric spaces. We say that $f:X\rightarrow Y$ is an \textbf{isometry} (or is \textbf{isometric}) \index{00@Isometry and isometric isomorphism} if for all $x_1,x_2\in X$ we have
\begin{align}
d_Y(f(x_1),f(x_2))=d_X(x_1,x_2) \label{eq15}
\end{align}
Show that an isometry is injective and continuous.

We say that $f$ is an \textbf{isometric isomorphism} if $f$ is a surjective isometry. If an isometric isomorphism between two metric spaces $X,Y$ exists, we say that $X$ and $Y$ are \textbf{isometric metric spaces}.\index{00@Isometric metric spaces} Show that an isometric isomorphism is a homeomorphism.   \hfill\qedsymbol
\end{exe}

\begin{rem}
Isometric isomorphisms are important examples of homeomorphisms. That $f:X\rightarrow Y$ is an isometric isomorphism means that $X$ and $Y$ are equivalent as metric spaces, and that this equivalence can be implemented by the bijection $f$. 

We now look at isometric isomorphisms in a different direction. Suppose that $f:X\rightarrow Y$ is a bijection of sets. Suppose that $Y$ is a metric space. Then there is unique metric $d_X$ on $X$ such that $f$ is an isometric isomorphism: one defines $d_X$ using \eqref{eq15}. We write such $d_X$ as $f^*d_Y$, \index{fdY@$f^*d_Y$: pullback metric}  i.e.,
\begin{align*}
f^*d_Y(x_1,x_2)=d_Y(f(x_1),f(x_2))
\end{align*}
and call $f^*d_Y$ the \textbf{pullback metric} \index{00@Pullback metrics} of $d_Y$ by $f$. \hfill\qedsymbol
\end{rem}

Pullback metrics are a very useful way of constructing metrics on a set. We consider some examples below. 


\begin{exe}\label{lb49}
Two metrics inducing the same topology are not necessarily equivalent metrics. For example, let $f:[0,1]\rightarrow [0,1]$ be $f(x)=x^2$. Let $X=[0,1]$, and let $d_X$ be the  Euclidean metric: $d_X(x,y)=|x-y|$. So
\begin{align*}
f^*d_X(x,y)=|x^2-y^2|
\end{align*}
is a metric on $X$. It is not hard to check that $f:(X,d_X)\rightarrow (X,d_X)$ is a homeomorphism.  So $d_X$ and $f^*d_X$ give the same topology on $[0,1]$ (cf. Exe. \ref{lb50}). Show that $f^*d_X$ and $d_X$ are not equivalent metrics.
\end{exe}

\begin{exe}\label{lb50}
Let $f:X\rightarrow Y$ be a bijection of sets with metrics $d_X,d_Y$. Show that $d_X$ and $f^*d_Y$ give the same topology on $X$ iff $f:(X,d_X)\rightarrow(Y,d_Y)$ is a homeomorphism. 

In particular, if $f:X\rightarrow X$ is a bijection, and $d_X$ is a metric on $X$. Then $d_X$ and $f^*d_X$ give the same topology on $X$ iff $f:(X,d_X)\rightarrow (X,d_X)$ is a homeomorphism. 
\end{exe}


\subsubsection{Convergence in $\ovl\Rbb$}


Our second application of pullback metrics is the convergence in $\ovl\Rbb$.




\begin{df}\label{lb47}
We say that a sequence $(x_n)$ in $\ovl\Rbb$ \textbf{converges to} \index{00@Convergence in $\ovl{\Rbb}$} $+\infty$ (resp. $-\infty$), if for every $A\in\Rbb$ there is $N\in\Zbb_+$ such that for all $n\geq N$ we have $x_n>A$ (resp. $x_n<A$). 

Suppose $x\in\Rbb$. We say that a sequence $(x_n)$ in $\ovl\Rbb$ \textbf{converges to} $x$, if there is $N\in\Zbb_+$ such that $x_n\in\Rbb$ for all $n\geq N$, and that the subsequence $(x_{k+N})_{k\in\Zbb_+}$ converges in $\Rbb$ to $x$.  \hfill\qedsymbol
\end{df}



This notion of convergence is weird: it is not defined by a metric. So one wonders if there is a metric $d$ on $\ovl\Rbb$ such that convergence of sequences under $d$ agrees with that in Def. \ref{lb47}. We shall now find such a metric.

\begin{lm}\label{lb59}
Let $-\infty\leq a<b\leq +\infty$ and $-\infty\leq c<d\leq +\infty$. Then there is a strictly increasing bijective map $[a,b]\rightarrow[c,d]$.
\end{lm}
Note that this map clearly sends $a$ to $c$ and $b$ to $d$. So it restricts to strictly increasing bijections $(a,b)\rightarrow(c,d)$, $(a,b]\rightarrow(c,d]$, $[a,b)\rightarrow[c,d)$.

\begin{proof}
We have a strictly increasing bijection $f:\Rbb\rightarrow(-1,1)$ defined by \eqref{eq20}.  $f$ can be extended to a strictly increasing bijective map $\ovl\Rbb\rightarrow[-1,1]$ if we set $f(\pm\infty)=\pm1$. Thus, $f$ restricts to a strictly increasing bijection $[a,b]\rightarrow [f(a),f(b)]$. Choose a linear function $g(x)=\alpha x+\beta$ (where $\alpha>0$) giving an increasing bijection $[f(a),f(b)]\rightarrow[0,1]$. Then $h=g\circ f:[a,b]\rightarrow[0,1]$ is a strictly increasing bijection. Similarly, we have a strictly increasing bijection $k:[c,d]\rightarrow[0,1]$. Then $k^{-1}\circ h:[a,b]\rightarrow[c,d]$ is a strictly increasing bijection.
\end{proof}


\begin{thm}\label{lb51}
Let $\varphi:\ovl\Rbb\rightarrow[a,b]$ be a strictly increasing bijective map where $[a,b]\subset\Rbb$ is equipped with the Euclidean metric $d_{[a,b]}$. Then a sequence $(x_n)$ in $\ovl\Rbb$ converges to $x\in\ovl\Rbb$ in the sense of Def. \ref{lb47} iff $\varphi(x_n)$ converges to $\varphi(x)$ under the metric $d_{[a,b]}$. In other words, the convergence in $\ovl\Rbb$ is given by the metric $\varphi^*d_{[a,b]}$.
\end{thm}

\begin{proof}
Let $y=\varphi(x)$ and $y_n=\varphi(x_n)$. We need to prove that $x_n\rightarrow x$ (in the sense of Def. \ref{lb47}) iff $y_n\rightarrow y$ (under the Euclidean metric). Write $\psi=\varphi^{-1}$, which is a strictly increasing map $[a,b]\rightarrow\ovl\Rbb$. Note that $\varphi(+\infty)=b$ and $\varphi(-\infty)=a$. 

Case 1: $x\in\Rbb$. By discarding the first several terms, we may assume that $(x_n)$ is always in $\Rbb$. If $x_n\rightarrow x$, then for every $\eps>0$, all but finitely many $x_n$ are inside the open interval $(\psi(y-\varepsilon),\psi(y+\varepsilon))$. So all but finitely many $y_n$ are inside $(y-\varepsilon,y+\varepsilon)$. So $y_n\rightarrow y$. That $y_n\rightarrow y$ implies $x_n\rightarrow x$ is proved in a similar way.

Case 2: $x=\pm\infty$. We consider $x=+\infty$ only; the other case is similar. Note that if $0<\eps<b-a$, then $B_{[a,b]}(b,\varepsilon)=(b-\varepsilon,b]$.  If $x_n\rightarrow+\infty$, then for each  $0<\eps<b-a$, all but finitely many $x_n$ are $>\psi(b-\eps)$. So all but finitely many $y_n$ are inside $(b-\eps,b]$. This proves $y_n\rightarrow b$. Conversely, if $y_n\rightarrow b$, then for each $A\in\Rbb$, all but finitely many $y_n$ are inside $(\varphi(A),b]$ and hence $>\varphi(A)$. So all but finitely many $x_n$ are $>A$.
\end{proof}


\begin{cv}\label{lb77}
Unless otherwise stated, a metric on $\ovl\Rbb$ is one that makes Def. \ref{lb47} true, for instance $\varphi^*d_{[a,b]}$ in Thm. \ref{lb51}. Unless otherwise stated, we do NOT view $\Rbb$ (or any subset of $\Rbb$) as a metric subspace of $\ovl\Rbb$. Namely, we do not follow Convention \ref{lb76} for $\Rbb\subset\ovl\Rbb$, or more generally for $\Rbb^N\subset\ovl\Rbb^N$. Instead, we choose Euclidean metrics on $\Rbb^N$, following Convention \ref{lb33}. 
\end{cv}




The main reason for not following Convention \ref{lb76} here is that metrics on $\ovl\Rbb$ are all bounded (by Prop. \ref{lb71}). Thus, every subset of $\Rbb$ is bounded if we view $\Rbb$ as a metric subspace of $\ovl\Rbb$. However, we want a subset of $\Rbb$ to be bounded precisely when it is contained in $[a,b]$ for some $-\infty<a<b<+\infty$. (Recall also Def. \ref{lb114}.)



After learning topological spaces, we shall forget about the metrics on $\ovl\Rbb$ and only care about its topology. (See Conv. \ref{lb173}.)






\begin{rem}\label{lb58}
By Thm. \ref{lb51}, the properties of $[a,b]$ about convergence of sequences and inequalities can be transported to $\ovl\Rbb$, for example:
\begin{enumerate}
\item If $(x_n),(y_n)$ are sequences in $\ovl\Rbb$ converging to $A,B\in\ovl\Rbb$, and if $x_n\leq y_n$ for all $n$, then $A\leq B$.
\item \textbf{Squeeze theorem}: \index{00@Squeeze theorem} Suppose that $(x_n),(y_n),(z_n)$ are sequences in $\ovl\Rbb$, $x_n\leq y_n\leq z_n$ for all $n$, and $x_n$ and $z_n$ both converge to $A\in\ovl\Rbb$. Then $y_n\rightarrow A$.
\item Prop. \ref{lb57} also holds for $[-\infty,+\infty]=\ovl\Rbb$: if $(x_n)$ is an increasing resp. decreasing sequence in $\ovl\Rbb$, then $\lim_n x_n$ exists in $\ovl\Rbb$ and equals $\sup_n x_n$ resp. $\inf_n x_n$.
\end{enumerate}
We will see more examples when studying $\limsup$ and $\liminf$ in the future.
\end{rem}


We have shown that there is a metric on $\ovl\Rbb$ which defines the convergence in Def. \ref{lb47}. However, there is no standard choice of such a metric on $\ovl\Rbb$. Even worse, two possible choices of metrics might not be equivalent: Let $\varphi,\psi:\ovl\Rbb\rightarrow[0,1]$ be a strictly increasing bijections where $\psi\circ\varphi^{-1}:[0,1]\rightarrow[0,1]$ is $x\mapsto x^2$. Then by Exe. \ref{lb49}, $\varphi^*d_{[0,1]}$ and $\psi^*d_{[0,1]}$ are non-equivalent but topologically equivalent metrics on $\ovl\Rbb$. This is the first example that metrics are not convenient for the description of convergence. When studying the convergence in $\ovl\Rbb$, thinking about metrics is distracting. In the future, we will see a better notion for the study of convergence: the notion of topological spaces.

We end this section with a generalization of Thm. \ref{lb51}.

\begin{thm}\label{lb65}
Let $\varphi$ be a strictly increasing bijection in one of the following forms
\begin{gather*}
[a,b]\rightarrow [c,d]\qquad (a,b)\rightarrow(c,d)\\
(a,b]\rightarrow (c,d]\qquad [a,b)\rightarrow [c,d)
\end{gather*}
where $-\infty\leq a\leq b\leq +\infty$ and $-\infty\leq c\leq d\leq +\infty$. Then $\varphi$ is a homeomorphism, i.e., if $(x_n)$ and $x$ are in the domain, then $x_n\rightarrow x$ iff $\varphi(x_n)\rightarrow \varphi(x)$ (in the sense of Def. \ref{lb47}).
\end{thm}

\begin{proof}
The case $a=b$ is obvious. So we consider $a<b$, and hence $c<d$. We consider the left-open-right-closed case for example. The other cases are treated in a similar way. If the theorem can be proved for $(-\infty,+\infty]\rightarrow(c,d]$, then it can also be proved $(-\infty,+\infty]\rightarrow(a,b]$. By composing the inverse of the second map with the first map, we see that the theorem holds for $(a,b]\rightarrow (c,d]$. 

Let us consider $\varphi:(-\infty,+\infty]\rightarrow(c,d]$. $\varphi$ can be extended to a strictly increasing bijection $\varphi:\ovl\Rbb\rightarrow[c,d]$ by letting $\varphi(-\infty)=c$. It suffices to prove that this $\varphi$ is a homeomorphism. When $-\infty<c<d<+\infty$, then the theorem holds by Thm. \ref{lb51}. If one of $c,d$ is $\pm\infty$, the same argument as in the proof of Thm. \ref{lb51} proves that $\varphi$ is a homeomorphism. We leave it to the readers to fill in the details.
\end{proof}



%% Record  #2  2023/9/20   three lectures


\subsection{Problems and supplementary material}

\begin{df}
Let $A$ be a subset of $\Rbb$  satisfying $x+y\in A$ for all $x,y\in A$. (Or more generally, let $A$ be an abelian semigroup.) We say that a function $f:A\rightarrow \Rbb$ is \textbf{subadditive} \index{00@Subadditivity} if for every $x,y\in A$ we have $f(x+y)\leq f(x)+f(y)$.
\end{df}

\begin{prob}\label{lb52}
Consider the following increasing functions:
\begin{gather*}
f_1:\Rbb_{\geq 0}\rightarrow[0,1)\qquad f_1(x)=\frac{x}{1+x}\\
f_2:\Rbb_{\geq 0}\rightarrow[0,1]\qquad f_2(x)=\min\{x,1\}
\end{gather*}
Prove that they are subadditive functions. 
\end{prob}

\begin{prob}\label{lb53}
Let $f:\Rbb_{\geq0}\rightarrow \Rbb_{\geq0}$ be an increasing subadditive function satisfying the following conditions:
\begin{itemize}
\item[(1)] $f^{-1}(0)=\{0\}$.
\item[(2)] For any $(x_n)_{n\in\Zbb_+}$ in $\Rbb_{\geq0}$ we have $x_n\rightarrow 0$ iff $f(x_n)\rightarrow 0$.
\end{itemize}
Let $(X,d)$ be a metric space. Define
\begin{align*}
\delta:X\times X\rightarrow [0,A)\qquad \delta(x,y)=f\circ d(x,y)
\end{align*}  
Prove that $\delta$ is a metric, and that $\delta$ and $d$ are topologically equivalent.
\end{prob}


\begin{pp}\label{lb195}
Let $(X,d)$ be a metric space. Then there is a bounded metric $\delta$ on $X$ such that $d$ and $\delta$ are topologically equivalent.
\end{pp}
\begin{proof}
Let $f$ be either $f_1$ or $f_2$ defined in Pb. \ref{lb52}. Then $f$ satisfies the assumptions in Pb. \ref{lb53}. So $\delta=f\circ d$ is a desired metric due to Pb. \ref{lb53}. We write down the formulas explicitly:
\begin{align*}
\delta_1(x,y)=\frac{d(x,y)}{1+d(x,y)}\qquad \delta_2(x,y)=\min\{d(x,y),1\}
\end{align*}
\end{proof}

\begin{prob}\label{lb78}
Let $(X_i,d_i)_{i\in\Zbb_+}$ be a sequence of metric spaces. Assume that $d_i\leq 1$ for each $i$. Let $\dps S=\prod_{i\in\Zbb_+} X_i$. For each elements $f=(f(i))_{i\in\Zbb_+}$ and $g=(g(i))_{i\in\Zbb_+}$ of $S$, define
\begin{align}
d(f,g)=\sup_{i\in\Zbb_+} \frac {d_i(f(i),g(i))}{i}  \label{eq16}
\end{align} 
Prove that $d$ is a metric on $S$. Let $f_n=(f_n(i))_{i\in\Zbb_+}$ be a sequence in $S$. Let $g\in S$. Prove that the following are equivalent:
\begin{enumerate}[label=(\alph*)]
\item $\dps \lim_{n\rightarrow\infty} f_n=g$ under the metric $d$.
\item $f_n$ \textbf{converges pointwise} to $g$, namely, $\dps\lim_{n\rightarrow\infty} f_n(i)=g(i)$ for every $i\in\Zbb_+$.
\end{enumerate}
\end{prob}

\begin{rem}
The above problem gives our first non-trivial example of function spaces  as metric spaces, where the domain of functions is a countable set. After learning series, the readers can check that 
\begin{align}
\delta(f,g)=\sum_{i\in\Zbb_+}2^{-i} d_i(f(i),g(i))   \label{eq61}
\end{align}
also defines a metric, and that (a) (with $d$ replaced by $\delta$) and (b) are equivalent. So $\delta$ and $d$ (defined by \eqref{eq16}) induce the same topology on $X$, called the \textbf{pointwise convergence topology} or simply \textbf{product topology}. Unfortunately, if the index set $\Zbb_+$ is replaced by an uncountable set, there is in general no metric inducing the product topology. We will prove this in Pb. \ref{lb203}.
\end{rem}













\begin{sprob}\label{lb194}
Let $X=\bigsqcup_{\alpha\in \scr A}X_\alpha$ be a disjoint union of metric spaces $(X_\alpha,d_\alpha)$. Assume that $d_\alpha\leq 1$ for all $i$. For each $x,y\in X$, define
\begin{gather*}
d(x,y)=\left\{
\begin{array}{ll}
d_\alpha(x,y)&\text{ if $x,y\in X_\alpha$ for some $\alpha\in\scr A$}\\[0.5ex]
\frac 12&\text{ otherwise}
\end{array}
\right.
\end{gather*}
\begin{enumerate}
\item Prove that $d$ defines a metric on $X$. 
\item Choose $(x_n)_{n\in\Zbb_+}$ in $X$ and $x\in X$. What does $\dps\lim_{n\rightarrow\infty}x_n=x$ mean in terms of the convergence in each $X_\alpha$?
\end{enumerate}
\end{sprob}

Think about the question: Let $X$ be a set. For each $x,y\in X$ define $d(x,y)=0$ if $x=y$, and $d(x,y)=1$ if $x\neq y$. What does convergence in $(X,d)$ mean?



\newpage



\section{Sequential compactness and completeness}



\subsection{Sequential compactness}


\subsubsection{Basic properties of sequentially compact spaces}

\begin{df}
Let $X$ be a metric space. We say that $X$ is \textbf{sequentially compact} \index{00@Sequentially compact} if every sequence in $X$ has a subsequence converging to some point of $X$.
\end{df}





The notion of sequential compactness is extremely useful for finding solutions in an analysis problem. In general, suppose we want to find a point $x\in X$ which makes a property $P(x)$ to be true. Suppose that we can find an ``approximate solution", i.e. an $y\in X$ such that $P(y)$ is close to being true. Thus, we can find a sequence $(x_n)$ in $X$ such that $P(x_n)$ is closer and closer to being true when $n\rightarrow\infty$. Now, if $X$ is sequentially compact, then $(x_n)$ has a subsequence $(x_{n_k})$ converging to $x\in X$. Then $P(x)$ is true, and hence $x$ is a solution for the problem. (See also Sec. \ref{lb55}.) Let us see an explicit example:

\begin{lm}[\textbf{Extreme value theorem}]\label{lb56} \index{00@EVT=Extreme value theorem}
Let $X$ be a sequentially compact metric space. Let $f:X\rightarrow\Rbb$ be a continuous function. Then $f$ attains its maximum and minimum at points of $X$. In particular, $f(X)$ is a bounded subset of $\Rbb$.
\end{lm}

This extremely important result is the main reason for introducing sequentially compact spaces. We call this a lemma, since we will substantially generalize this result later. (See Exe. \ref{lb63}.)

Note that the \textbf{boundedness} of subsets of $\Rbb$ (or more generally, of $\Rbb^N$) is always understood under the Euclidean metric of $\Rbb$, not under any metric of $\ovl\Rbb$ or $\ovl\Rbb^N$. (Recall Convention \ref{lb77}.)

\begin{proof}
We show that $f$ attains its maximum on $X$. The proof for minimum is similar. Let $A=\sup f(X)$. Then $A\in (-\infty,+\infty]$. If $A<+\infty$, then for each $n\in\Zbb_+$ there is $x_n\in X$ such that $A-1/n<f(x_n)\leq A$ (since $A-1/n$ is not an upper bound of $f(X)$). If $A=+\infty$, then for each $n$ there is $x_n\in X$ such that $f(x_n)>n$. In either case, we have a sequence $(x_n)$ in $X$ such that $f(x_n)\rightarrow A$ in $\ovl \Rbb$.

Since $X$ is sequentially compact, $(x_n)$ has a subsequence $(x_{n_k})_{k\in\Zbb_+}$ converging to some $x\in X$. Now, consider $f$ as a map $f:X\rightarrow\ovl\Rbb$, which is continuous (cf. Exp. \ref{lb54}). Since $f(x_n)\rightarrow A$, its subsequence $f(x_{n_k})$ also converges to $A$. But since $x_{n_k}\rightarrow x$ and $f$ is continuous at $x$, we have $A=f(x)$. So $f$ attains its maximum at $x$. Since $f(X)\subset\Rbb$, we have $A\in\Rbb$.
\end{proof}

The following are some elementary examples of sequential compactness:


\begin{exe}
Show that finite unions of sequentially compact spaces is sequentially compact. (In particular, a finite set is sequentially compact.) 

More precisely, let $X$ be a metric space. Assume $X=A_1\cup\cdots\cup A_N$ where each metric subspace $A_i$ is sequentially compact. Show that $X$ is sequentially compact.  
\end{exe}

\begin{pp}\label{lb72}
Let $X_1,\dots,X_N$ be sequentially compact metric spaces. Then $X=X_1\times\cdots\times X_N$ is sequentially compact.
\end{pp}

\begin{proof}
Since $X=(X_1\times\cdots\times X_{N-1})\times X_N$, by induction, it suffices to assume $N=2$. So we write $X=A\times B$ where $A,B$ are sequentially compact. Let $(a_n,b_n)$ be a sequence in $X$. Since $A$ is sequentially compact, $(a_n)$ has a convergent subsequence $(a_{n_k})$. Since $B$ is sequentially compact, $(b_{n_k})$ has a convergent subsequence $(b_{n_{k_l}})$. So $(a_{n_{k_l}},b_{n_{k_l}})$ is a convergent subsequence of $(a_n,b_n)$.
\end{proof}


\begin{pp}\label{lb62}
Let $f:X\rightarrow Y$ be a continuous  map of metric spaces. Assume that $X$ is sequentially compact. Then $f(X)$, as a metric subspace of $Y$, is sequentially compact.
\end{pp}

\begin{proof}
Choose any sequence $(y_n)$ in $f(X)$. We can write $y_n=f(x_n)$ where $x_n\in X$. Since $X$ is sequentially compact, $(x_n)$ has a subsequence $(x_{n_k})$ converging to some $x\in X$. Since $f$ is continuous, $y_{n_k}=f(x_{n_k})$ converges to $f(x)$.
\end{proof}


\begin{exe}\label{lb63}
Prove that if $Y$ is a sequentially compact subset of $\Rbb$, then $\sup Y\in Y$ and $\inf Y\in Y$. Therefore, Prop. \ref{lb62} generalizes Lem. \ref{lb56}.
\end{exe}

\begin{pp}\label{lb71}
Let $X$ be a sequentially compact metric space. Then $X$ is bounded under its metric $d$.
\end{pp}

\begin{proof}
Choose any $p\in X$. The function $d_p:x\in X\mapsto d(x,p)\in\Rbb_{\geq 0}$ is continuous by Exp. \ref{lb45}. So, by Lem. \ref{lb56}, $d_p$ is bounded by some $0<R<+\infty$. So $X=\ovl B_X(p,R)\subset B_X(p,2R)$.
\end{proof} 






\subsubsection{Limits inferior and superior, and Bolzano-Weierstrass}\label{lb69}

The goal of this subsection is to prove that closed intervals are sequentially compact. 


\begin{df}\label{lb266}
Let $(x_n)$ be a sequence in a metric space $X$. We say that $x\in X$ is a \textbf{cluster point} \index{00@Cluster point of a sequence} of $(x_n)$, if $(x_n)$ has a subsequence $(x_{n_k})$ converging to $x$.
\end{df}

Warning: In a general topological space, the cluster points of a sequence will be defined in a different way. (See Pb. \ref{lb223} and Rem. \ref{lb267}.)





\begin{df}\label{lb60}
Let $(x_n)$ be a sequence in $\ovl\Rbb$. Define
\begin{gather}
\alpha_n=\inf\{x_k:k\geq n \}\qquad \beta_n=\sup\{x_k:k\geq  n \}
\end{gather}
It is clear that $\alpha_n\leq x_n\leq \beta_n$, that $(\alpha_n)$ is increasing and $(\beta_n)$ is decreasing. Define \index{liminfsup@$\liminf,\limsup$}
\begin{subequations}
\begin{gather}
\liminf_{n\rightarrow\infty}x_n=\sup\{\alpha_n:n\in\Zbb_+\}=\lim_{n\rightarrow\infty} \alpha_n \label{eq18}\\
\limsup_{n\rightarrow\infty}x_n=\inf\{\beta_n:n\in\Zbb_+\}=\lim_{n\rightarrow\infty} \beta_n\label{eq19}
\end{gather}
\end{subequations}
(cf. Rem. \ref{lb58}), called respectively the \textbf{limit inferior} and the \textbf{limit superior} \index{00@Limit inferior and superior} of $(x_n)$.
\end{df}

\begin{rem}
Let $(x_n),(y_n)$ be sequences in $\ovl\Rbb$. Suppose that $x_n\leq y_n$ for every $n$. It is clear that
\begin{gather*}
\liminf_{n\rightarrow\infty}x_n\leq\limsup_{n\rightarrow\infty} x_n\qquad \liminf_{n\rightarrow\infty}x_n\leq \liminf_{n\rightarrow\infty}y_n\qquad \limsup_{n\rightarrow\infty} x_n\leq \limsup_{n\rightarrow\infty} y_n
\end{gather*}
\end{rem}


\begin{thm}\label{lb68}
Let $(x_n)$ be a sequence in $\ovl\Rbb$, and let $S$ be the set of cluster points of $(x_n)$. Then $\dps\liminf_{n\rightarrow\infty}x_n$ and $\dps\limsup_{n\rightarrow\infty}x_n$ belong to $S$. They are respectively the minimum and the maximum of $S$.
\end{thm}

In particular, every sequence in $\ovl\Rbb$ has at least one cluster point.

\begin{proof}
We use the notations in Def. \ref{lb60}. Let $A=\eqref{eq18}$ and $B=\eqref{eq19}$. If $x\in S$, pick a subsequence $(x_{n_k})$ converging to $x$. Since $\alpha_{n_k}\leq x_{n_k}\leq \beta_{n_k}$, we have $A\leq x\leq B$ by Rem. \ref{lb58}. It remains to show that $A,B\in S$. We prove $B\in S$ by constructing a subsequence $(x_{n_k})$ converging to $B$; the proof of $A\in S$ is similar. 

Consider first of all the special case that $(x_n)$ is bounded, i.e., is inside $[a,b]\subset\Rbb$. Choose an arbitrary $n_1\in\Zbb_+$. Suppose $n_1<\dots<n_k$ have been constructed. By the definition of $\beta_{1+n_k}$, there is $n_{k+1}\geq 1+n_k$ such that $x_{n_{k+1}}$ is close to $\beta_{1+n_k}$, say
\begin{align}
\beta_{1+n_k}-\frac 1k<x_{n_{k+1}}\leq \beta_{1+n_k}  \label{eq17}
\end{align}
Since the left most and the right most of \eqref{eq17} both converge to $B$ as $k\rightarrow\infty$, by Squeeze theorem (Cor. \ref{lb61}) we conclude $\lim_k x_{n_k}=B$. 

In general, by Lem. \ref{lb59} and Thm. \ref{lb65}, there is an increasing (i.e. order-preserving) homeomorphism (i.e. topopogy-preserving map) $\varphi:\ovl\Rbb\rightarrow[0,1]$. Then $\varphi(\beta_n)=\sup\{\varphi(x_k):k\geq n\}$ (cf. Exe. \ref{lb66}) and $\varphi(B)=\lim_n\varphi(\beta_n)$. So $\varphi(B)=\limsup_n \varphi(x_n)$. By the above special case, $(\varphi(x_n))$ has a subsequence $(\varphi(x_{n_k}))$ converging to $\varphi(B)$. So $(x_{n_k})$ converges to $B$. 
\end{proof}

\begin{rem}
One can also prove the above general case directly using a similar idea as in the special case. And you are encouraged to do so! (Pay attention to the case $B=\pm\infty$.) 

The proof given above belongs to a classical proof pattern: To prove that a space $X$ satisfies some property, one first prove it in a convenient case. Then, in the general case, one finds an ``isomorphism" (i.e. ``equivalence" in a suitable sense) $\varphi:X\rightarrow Y$ where $Y$ is in the convenient case. Then the result on $Y$ can be translated via $\varphi^{-1}$ to $X$, finishing the proof. 

For example, to solve a linear algebra problem about linear maps between finite-dimensional vector spaces $V,W$, one first proves it in the special case that $V=\Fbb^m$ and $W=\Fbb^n$. Then, the general case can be translated to the special case via an equivalence as in Exp. \ref{lb67}.  \hfill\qedsymbol
\end{rem}


\begin{exe}\label{lb66}
Let $X,Y$ be posets. Let $\varphi:X\rightarrow Y$ be an increasing bijection whose inverse is also increasing. (Namely, $\varphi$ induces an equivalence of posets). Suppose $E\subset X$ has supremum $\sup E$. Explain why $\varphi(E)$ has supremum $\varphi(\sup E)$.
\end{exe}


It is now fairly easy to prove the famous

\begin{thm}[Bolzano-Weierstrass]\index{00@Bolzano-Weierstrass theorem}
Let $[a_1,b_1],\dots,[a_N,b_N]$ be closed intervals in $\ovl\Rbb$. Then $[a_1,b_1]\times\cdots\times [a_N,b_N]$ is sequentially compact. 
\end{thm}

\begin{proof}
By Prop. \ref{lb72}, it suffices to assume $N=1$. Write $a_1=a,b_1=b$. Let $(x_n)$ be a sequence in $[a,b]$. By Thm. \ref{lb68}, $(x_n)$ has a subsequence $(x_{n_k})$ converging to some $x\in\ovl\Rbb$. (E.g. $x=\limsup_n x_n$.) Since $a\leq x_{n_k}\leq b$, we have $a\leq x\leq b$ by Rem. \ref{lb58}.
\end{proof}

Bolzano-Weierstrass theorem illustrates why we sometimes prefer to work with $\ovl\Rbb$ instead of $\Rbb$: $\ovl\Rbb$ is sequentially compact, while $\Rbb$ is not. That every sequence has limits superior and inferior in $\ovl\Rbb$ but not necessarily in $\Rbb$ is closely related to this fact. In the language of point-set topology, $\ovl\Rbb$ is a \textbf{compactification} of $\Rbb$.


Bolzano-Weierstrass theorem (restricted to $\Rbb^N$) will be generalized to \textbf{Heine-Borel theorem}, which says that a subset of $\Rbb^N$ is sequentially compact iff it is bounded and closed (cf. Def. \ref{lb99} for the definition of closed subsets). See Thm. \ref{lb98}.





\subsubsection{A criterion for convergence in sequentially compact spaces}

At the end of Sec. \ref{lb73}, we have raised the following question: Suppose that $(x_n)$ is a bounded sequence in a metric space $X$ such that any two convergent subsequences converge to the same point. Does $(x_n)$ converge?

When $X$ is sequentially compact, $(x_n)$ is automatically bounded due to Prop. \ref{lb71}. The answer to the above question is yes:

\begin{thm}\label{lb74}
Let $X$ be a sequentially compact metric space. Let $(x_n)$ be a sequence in $X$. Then the following are equivalent.
\begin{itemize}
\item[(1)] The sequence $(x_n)$ converges in $X$.
\item[(2)] Any two convergent subsequences of $(x_n)$ converge to the same point. In other words, $(x_n)$ has only one cluster point. 
\end{itemize}
\end{thm}


\begin{proof}
(1)$\Rightarrow$(2): By Prop. \ref{lb23}.

(2)$\Rightarrow$(1): Assume that $(x_n)$ has at most one cluster point. Since $X$ is sequentially compact, $(x_n)$ has at least one cluster point $x\in X$. We want to prove $\lim_{n\rightarrow\infty} x_n=x$. Suppose not. Then there exists $\eps>0$ such that for every $N\in\Zbb_+$ there is $n\geq N$ such that $d(x_n,x)\geq \eps$. Thus, one can inductively construct a subsequence $(x_{n_k})$ of $(x_n)$ such that $d(x_{n_k},x)\geq\eps$ for all $k$. Since $X$ is sequentially compact, $(x_{n_k})$ has a subsequence $x'_n$ converging to $x'\in X$. So $d(x'_n,x)\geq\eps$ for all $n$. Since the function $y\in X\mapsto d(y,x)$ is continuous (Exp. \ref{lb45}), we have $\lim_{n\rightarrow\infty}d(x_n',x)=d(x',x)$. This proves that $d(x',x)\geq\eps>0$. However, $x',x$ are both cluster points of $(x_n)$, and so $x=x'$. This gives a contradiction. 
\end{proof}

\begin{rem}
Thm. \ref{lb74} can be used in the following way. Suppose that we want to prove that a given sequence $(x_n)$ in a sequentially compact space $X$ converges to $x$. Then it suffices to prove that if $(x_n')$ is a subsequence of $(x_n)$ converging to some $y\in X$, then $y=x$. This is sometimes easier to prove than directly proving the convergence of $(x_n)$. We will use this strategy in the proof of L'H\^opital's rule, for example. (See Subsec. \ref{lb349}.)
\end{rem}





\begin{co}\label{lb75}
Let $(x_n)$ be a sequence in $\Rbb^N$. The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item The sequence $(x_n)$ converges in $\Rbb^N$.
\item The sequence $(x_n)$ is bounded. Moreover, any two convergent subsequences of $(x_n)$ converge to the same point of $\Rbb^N$.
\end{enumerate}
\end{co}

\begin{proof}
(1)$\Rightarrow$(2): By Prop. \ref{lb24} and \ref{lb23}. 

(2)$\Rightarrow$(1): Assume (2). Since $(x_n)$ is bounded, it can be contained in $X=I_1\times\cdots\times I_N$ where each $I_i$ is a closed interval in $\Rbb$. Clearly, any two cluster points of $(x_n)$ are inside $X$, and are equal by (2). By Bolzano-Weierstrass, $X$ is sequentially compact. Thus, by Thm. \ref{lb74}, $(x_n)$ converges in $X$ and hence in $\Rbb^N$.
\end{proof}

\begin{co}\label{lb113}
The following are true.
\begin{itemize}
\item[1.] Let $(x_n)$ be a sequence in $\ovl\Rbb$. Then $(x_n)$ converges in $\ovl\Rbb$ iff $\dps\limsup_{n\rightarrow\infty} x_n$ equals $\dps\liminf_{n\rightarrow\infty} x_n$. 
\item[2.] Let $(x_n)$ be a sequence in $\Rbb$. Then $(x_n)$ converges in $\Rbb$ iff $\dps\limsup_{n\rightarrow\infty} x_n$ equals $\dps\liminf_{n\rightarrow\infty} x_n$ and $(x_n)$ is bounded.
\end{itemize}
\end{co}

Note that if $(x_n)$ converges in $\ovl\Rbb$, we must have $\lim x_n=\limsup x_n=\liminf x_n$ by Thm. \ref{lb68}.

\begin{proof}
1. Let $A=\liminf x_n$ and $B=\limsup x_n$. Let $S$ be the set of cluster points of $(x_n)$.  By Thm. \ref{lb68}, $A=\min S,B=\max S $. So $A=B$ iff $S$ has only one element. This is equivalent to the convergence of $(x_n)$ in $\ovl\Rbb$ due to Thm. \ref{lb74} (since $\ovl\Rbb$ is sequentially compact by Bolzano-Weierstrass.)

2. If $(x_n)$ converges, then $A=B$ by part 1. And $(x_n)$ is bounded due to Prop. \ref{lb24}. Conversely, if $A=B$ and if $(x_n)$ is bounded, say $\alpha\leq x_n\leq \beta$ for all $n$ where $-\infty<\alpha<\beta<+\infty$. Then $\alpha\leq A\leq B\leq\beta$. So $A,B\in\Rbb$. By part 1, $(x_n)$ converges to $A\in\Rbb$.
\end{proof}



\subsection{Outlook: sequentially compact function spaces}


In Sec. \ref{lb55}, we mentioned that metric spaces and (more generally) point-set topology were introduced by mathematicians in order to study (typically infinite dimensional) function spaces with the help of the geometric intuition of $\Rbb^N$.  Now we have learned a couple of important results about sequentially compact spaces. But we have not met any example arising from function spaces. So let me show one example to the curious readers: The product space $[0,1]^{\Zbb_+}$, equipped with the metric defined in Pb. \ref{lb78}, is sequentially compact. We will prove this result at the end of this chapter. (Indeed, we will prove a slightly more general version. See  Thm. \ref{lb89}.) This is a famous result, not only because it has many important applications (some of which will be hinted at in this section), but also because its proof uses the clever  ``diagonal method".  

Moreover, we will later prove an even more surprising fact: every sequentially compact metric space is homeomorphic to a closed subset of $[0,1]^{\Zbb_+}$. (See Thm. \ref{lb261}.) Thus, all sequentially compact metric spaces can be constructed explicitly, in some sense.

The readers may still complain that functions on $\Zbb_+$ are very different from those we often see and use in analysis and (especially) in differential equations: We are ultimately interested in functions on $\Rbb$ or on $[a,b]$, but not on countable sets. This is correct. But $[0,1]^{\Zbb_+}$ (and its closed subsets) are in fact very helpful for the study of spaces of functions on $\Rbb$ and on $[a,b]$. In this course, we shall learn two major examples that the sequential compactness of $[0,1]^{\Zbb_+}$ helps with:
\begin{enumerate}
\item $A=\Qbb\cap[a,b]$ is a countable dense subset of $[a,b]$. Thus, if we let $C([a,b])$ denote the set of continuous $\Rbb$-functions on $[a,b]$, then the restriction map $f\in C([a,b])\mapsto f|_A\in\Rbb^A$ is injective. In many applications, we are interested in a subset $\mc X\subset C([a,b])$ of uniformly bounded functions, say all $f\in\mc X$ take values in $[-1,1]$. Then we have an injective map
\begin{align*}
\Phi:\mc X\rightarrow [-1,1]^A\qquad f\mapsto f|_A
\end{align*}
If $\mc X$ satisfies a condition called ``\textbf{equicontinuous}", then  a sequence $f_n$ in $\mc X$ converges \textit{uniformly} to $f\in C([a,b])$ iff $f_n|_A$ converges \textit{pointwise} to $f|_A$. (See Rem. \ref{lb145}.) Thus, from the sequential compactness of $[-1,1]^A$ under pointwise convergence topology, one concludes that every sequence in $\mc X$ has a subsequence converging uniformly in $C([a,b])$. This remarkable sequential compactness result on (the closure of) $\mc X$ is called \textbf{Arzel\`a-Ascoli theorem}, and will be used to prove the fundamental Peano existence theorem in ordinary differential equations. We also see that the fact that $[a,b]$ has a countable dense subset $A$ plays a crucial role. This property of metric spaces is called ``\textbf{separable}" and will be studied later.

\item \textbf{Fourier series} are powerful for the study of partial differential equations. A continuous function $f:[-\pi,\pi]\rightarrow\Cbb$ satisfying $f(-\pi)=f(\pi)$ has Fourier series expansion $f(x)=\sum_{n\in\Zbb}a_n e^{\im nx}$ where $a_n\in\Cbb$. However, for the sake of studying differential equations, one needs to consider series $\sum_{n\in\Zbb}a_n e^{\im nx}$ converging to a function much worse than a continuous function. For example, in the study of integral equations (which are closely related to certain partial differential equations), Hilbert and Schmidt discovered that one has to consider all $f(x)=\sum_{n\in\Zbb}a_n e^{\im nx}$ satisfying $\sum_n |a_n|^2\leq 1$. Therefore, one lets $\ovl B=\{z\in\Cbb:|z|\leq 1\}$ and considers $\wht f:n\in\Zbb\mapsto a_n\in\Cbb$ as an element of $\ovl B^\Zbb$. The sequential compactness of $\ovl B^\Zbb$ helps one find the $\wht f$ such that the corresponding $f(x)=\sum_n \wht f(n)\cdot e^{\im nx}$ is a desired solution of the integral equation. 
\end{enumerate}




\subsection{Complete metric spaces and Banach spaces}

In this section, we let $\Fbb\in\{\Rbb,\Cbb\}$, and assume that all vector spaces are over $\Fbb$.


\subsubsection{Cauchy sequences and complete metric spaces}

\begin{df}
A sequence $(x_n)$ in a metric space $X$ is called a \textbf{Cauchy sequence}, \index{00@Cauchy sequence}if:
\begin{itemize}
\item For every $\eps>0$  there exists $N\in\Zbb_+$ such that for all $m,n\geq N$ we have $d(x_m,x_n)<\eps$.
\end{itemize}
\end{df}

Here, ``$\eps>0$" can mean either ``$\eps\in\Rbb_{>0}$" or ``$\eps\in\Qbb_{>0}$". The choice of this meaning does not affect the definition. The above definition can be abbreviated to ``for every $\eps>0$, we have $d(x_m,x_n)<\eps$ for sufficiently large $m,n$". 

\begin{rem}
It is an easy consequence of triangle inequality that $(x_n)$ is a Cauchy sequence iff
\begin{itemize}
\item For every $\eps>0$ there exists $N\in\Zbb_+$ such that for all $n\geq N$ we have $d(x_n,x_N)<\eps$.
\end{itemize}
Also, it is clear that every Cauchy sequence is bounded.
\end{rem}


\begin{pp}\label{lb83}
Every convergent sequence in a metric space $X$ is a Cauchy sequence.
\end{pp}

\begin{proof}
Assume $(x_n)$ converges to $x$ in $X$. Then for every $\eps>0$ there is $N\in\Zbb_+$ such that $d(x,x_n)<\eps/2$ for all $n\geq N$. Since this is true for every $m\geq N$, we have $d(x_m,x_n)\leq d(x,x_n)+d(x,x_m)<\eps/2+\eps/2=\eps$.
\end{proof}


\begin{df}
A metric space $X$ is called \textbf{complete} \index{00@Complete metric space} if every Cauchy sequence in $X$ converges.
\end{df}




We have many examples of complete metric spaces:

\begin{thm}\label{lb79}
If $(x_n)$ is a Cauchy sequence in a metric space $X$ with at least one cluster point, then $(x_n)$ converges in $X$. Consequently, every sequentially compact metric space is complete.
\end{thm}

\begin{proof}
Let $(x_n)$ be a Cauchy sequence in $X$ with subsequence $(x_{n_k})$ converging to $x\in X$. Let us show that $x_n\rightarrow x$. 

Since $(x_n)$ is Cauchy, for every $\eps>0$ there is $N\in\Zbb_+$ such that $d(x_n,x_m)<\eps/2$ for all $m,n\geq N$. Since $x_{n_k}\rightarrow x$, there is $k\geq N$ such that $d(x_{n_k},x)<\eps/2$. Since $n_k$ is strictly increasing over $k$, we have $n_k\geq k$. So $n_k\geq N$. So we can let $m=n_k$. This gives $d(x_n,x_{n_k})<\eps/2$. Therefore $d(x_n,x)\leq d(x_n,x_{n_k})+d(x_{n_k},x)<\eps$ for all $n\geq N$.
\end{proof}

\begin{eg}\label{lb84}
Let  $X=[a_1,b_1]\times\cdots\times [a_N,b_N]$  where each $[a_i,b_i]$ is a closed interval in $\Rbb$, then $X$ is  sequentially compact by Bolzano-Weierstrass. Thus, by Thm. \ref{lb79}. $X$ is complete.
\end{eg}


\begin{co}\label{lb80}
$\Rbb^N$ and $\Cbb^N$ are complete (under the Euclidean metrics).
\end{co}

\begin{proof}
Since $\Cbb^N$ is isometrically isomorphic to $\Rbb^{2N}$, it suffices to prove that $\Rbb^N$ is complete. Choose a Cauchy sequence $(x_n)$ in $\Rbb^N$. Since $(x_n)$ is bounded, $(x_n)$ is contained inside $X=I_1\times\cdots\times I_N$ where each $I_i=[a,b]$ is in $\Rbb$. By Exp. \ref{lb84}, $X$ is complete. So $(x_n)$ converges to some $x\in X$.
\end{proof}





\begin{df}\label{lb99}
We say that a subset $A$ of a metric space $X$ is \textbf{closed} \index{00@Closed subset} if the following condition is true: For every sequence $(x_n)$ in $A$ converging to a point $x\in X$, we have $x\in A$.
\end{df}


Thus, the word ``closed" here means ``closed under taking limits".


\begin{pp}\label{lb86}
Let $A$ be a metric subspace of a metric space $X$. Recall  that the metric of $A$ inherits from that of $X$ (cf. Def. \ref{lb43}). Consider the statements:
\begin{enumerate}[label=(\arabic*)]
\item $A$ is complete.
\item $A$ is a closed subset of $X$.
\end{enumerate}
Then (1)$\Rightarrow$(2). If $X$ is complete, then (2)$\Rightarrow$(1).
\end{pp}


\begin{proof}
First, assume that $X$ is complete and (2) is true. Let $(x_n)$ be a Cauchy sequence in $A$. Then it is a Cauchy sequence in $X$. So $x_n\rightarrow x\in X$ because $X$ is complete. So $x\in A$ by the definition of closedness. This proves (1).

Next, we assume (1). Choose a sequence $(x_n)$ in $A$ converging to a point $x\in X$. By Prop. \ref{lb83}, $(x_n)$ is a Cauchy sequence in $X$, and hence a Cauchy sequence in $A$. Since $A$ is complete, there is $a\in A$ such that $x_n\rightarrow a$. So we must have $x=a$ because any sequence has at most one limit in a metric space. This proves $x\in A$. So (2) is proved.
\end{proof}

A similar result holds for sequential compactness. See Pb. \ref{lb90}.

\begin{eg}
Let $-\infty<a<b<+\infty$. Then $(a,b)$ is not complete (under the Euclidean metric), because $(a,b)$ is not closed in the metric space $\Rbb$. (For sufficiently large $n$, $b-1/n$ is in $(a,b)$, but $\lim_{n\rightarrow\infty} (b-1/n)=b$ is not in $b$.) 
\end{eg}

\begin{eg}
By Prop. \ref{lb2}, for each $x\in\Rbb\setminus\Qbb$, we can choose an increasing sequence in $\Qbb$ converging to $x$. So $\Qbb$ is not closed in $\Rbb$. So $\Qbb$ is not complete under the Euclidean topology.
\end{eg}


\begin{eg}\label{lb97}
Let $X$ be a metric space. Let $p\in X$ and $0\leq R<+\infty$. Then $\ovl B_X(x,R)$ is a closed subset of $X$. Therefore, if $X$ is complete, then $\ovl B_X(p,R)$ is complete by Prop. \ref{lb86}.
\end{eg}

\begin{proof}[Proof of closedness]
Let $(x_n)$ be a sequence in $\ovl B(p,R)$ converging to $x\in X$. Then $d(p,x_n)\leq R$. Since the function $y\in X\mapsto d(p,y)\in\Rbb$ is continuous (Exp. \ref{lb45}),  we have $d(p,x)=\lim_{n\rightarrow\infty}d(p,x_n)\leq R$. So $x\in\ovl B(p,R)$.
\end{proof}



\begin{exe}
Let $d,\delta$ be two equivalent metrics on a set $X$. Show that a sequence $(x_n)$ in $X$ is Cauchy under $d$ iff $(x_n)$ is Cauchy under $\delta$. 

Note that if, instead of assuming $d,\delta$ are equivalent, we only assume that $d,\delta$ are topologically equivalent. Then the above conclusion is not necessarily true:
\end{exe}


\begin{exe}
Find a non-complete metric $\delta$ on $\Rbb$ topologically equivalent to the Euclidean metric.
\end{exe}



%% Record  #3  2023/9/25  two lectures





\subsubsection{Normed vector spaces and Banach spaces}



A major application of complete metric spaces is to show that many series converge without knowing to what exact values these series converge. A typical example is the convergence of $\sum_{n\in\Zbb_+}\sin(\sqrt 2n)/n^2$ in $\Rbb$. We are also interested in the convergence of series in function spaces, for instance: the uniform convergence of $f(x)=\sum_{n\in\Zbb_+}\sin(\sqrt 2nx^3)/n^2$ on $\Rbb$; a suitable convergence of the Fourier series $\sum_{n\in\Zbb}a_ne^{\im n x}$. But we cannot take sum in a general metric space since it has no vector space structures. Therefore, we need a notion which combines complete metric spaces with vector spaces. Banach spaces are such a notion. 



\begin{df}\label{lb91}
Let $V$ be a vector space over $\Fbb$ with zero vector $0_V$. A function $\lVert\cdot\lVert:V\rightarrow\Rbb_{\geq 0}$ is called a \textbf{norm} \index{00@Norm} if for every $u,v\in V$ and $\lambda\in\Fbb$, the following hold:
\begin{itemize}
\item (Subadditivity) $\lVert u+v\lVert\leq \lVert u\lVert+\lVert v\lVert$. \index{00@Subadditivity}
\item (Absolute homogeneity) $\lVert\lambda v\lVert=|\lambda|\cdot \lVert v\lVert$. In particular, (by taking $\lambda=0$) we have $\lVert 0_V\lVert=0$.
\item If $\lVert v\lVert=0$ then $v=0_V$.
\end{itemize}
We call $(V,\lVert\cdot\lVert)$ (often abbreviated to $V$) a \textbf{normed vector space}. \index{00@Normed vector space}
\end{df}

\begin{rem}
Let $V$ be a vector space. If $V$ is a normed vector space, then
\begin{align}
d(u,v)=\lVert u-v\lVert  \label{eq21}
\end{align} 
clearly defines a metric. (Note that triangle inequality follows from subadditivity.) Unless otherwise stated, we always assume that the metric of a normed vector space is defined by \eqref{eq21}.
\end{rem}




\begin{df}
Let $V$ be a normed vector space. We say that $V$ is a \textbf{Banach space} \index{00@Banach space} if $V$ is a complete metric space where the metric is the canonical one \eqref{eq21}. If $V$ is over the field $\Cbb$ (resp. $\Rbb$), we call $V$ a \textbf{complex} (resp. \textbf{real}) \textbf{Banach space}.
\end{df}




\begin{eg}
We always assume that the norm on $\Fbb^N$ is the \textbf{Euclidean norm} \index{00@Euclidean norm}
\begin{align}
\lVert (a_1,\dots,a_N)\lVert=\sqrt{|a_1|^2+\cdots+|a_N|^2}
\end{align}
The canonical metric it gives is the Euclidean metric. Thus, by Cor. \ref{lb80}, $\Fbb^N$ is a Banach space.
\end{eg}



If $(\lambda_n)$ is a sequence in $\Fbb$ converging to $\lambda$, and if $(x_n)$ is a sequence in $\Fbb^N$ converging to $x$, then one can show that $\lambda_nx_n$ converges to $\lambda x$ by checking that each component of $\lambda_nx_n$ converges to the corresponding component of $\lambda x$. This is due to Prop. \ref{lb38}. However, if $(x_n)$ is in general a sequence in a normed vector space, this method fails. So we need a different argument:

\begin{pp}\label{lb82}
Let $V$ be a normed vector space. The following maps are continuous
\begin{gather*}
+: V\times V\rightarrow V\qquad (u,v)\mapsto u+v\\
-: V\times V\rightarrow V\qquad (u,v)\mapsto u-v\\
\times_\Fbb: \Fbb\times V\rightarrow V\qquad (\lambda,v)\mapsto \lambda v\\
\lVert\cdot\lVert:V\rightarrow\Rbb_{\geq 0}\qquad v\mapsto \lVert v\lVert
\end{gather*}
\end{pp}

We didn't mention the continuity of the division map $(\lambda,v)\in\Fbb^\times\times V\mapsto\lambda^{-1}v$ since it follows from that of $\times_\Fbb$ and of the inversion map $\lambda\mapsto\lambda^{-1}$ by Exp. \ref{lb35}.

\begin{proof}
One can check that the addition map, the subtraction map, and the last map $\lVert\cdot\lVert$ are Lipschitz continuous. 

Define metric $d((\lambda,v),(\lambda',v'))=\max\{|\lambda-\lambda'|,\lVert v-v'\lVert \}$ on $\Fbb\times V$. Then $\Fbb\times V$ is covered by open balls of the form $B(0,r)=\{(\lambda,v)\in\Fbb\times V:|\lambda|<r,\lVert v\lVert<r\}$. Similar to the argument in \eqref{eq22}, one uses subadditivity (i.e. triangle inequality) and absolute homogeneity to show that $\times_\Fbb$ has Lipschitz constant $2r$ on $B(0,r)$. So $\times_\Fbb$ is continuous by Lem. \ref{lb30} and \ref{lb34}.
\end{proof}








\subsection{The Banach spaces $l^\infty(X,V)$ and $C(X,V)$}


In this section, we let $\Fbb\in\{\Rbb,\Cbb\}$ and assume that the vector spaces $V$ are over $\Fbb$. As the title suggests, in this section we shall introduce two important examples of Banach spaces: the space of bounded functions $l^\infty(X,V)$ and its  subspace of continuous functions $C(X,V)$ (when $X$ is a sequentially compact metric space).  In order for these two spaces to be Banach spaces, we must assume that $V$ is also Banach. 

In application, the main examples are $V=\Rbb,\Cbb,\Rbb^N,\Cbb^N$. Indeed, $C([a,b],\Rbb^N)$ is one of the main examples of function spaces considered by Fr\'echet when he defined metric spaces. Therefore, the readers can assume that $V$ is one of such spaces if they want to make life easier. Just keep in mind that we sometimes also consider the case where $V$ itself is a function space.

\begin{df}\label{lb150}
Let $X$ be a set and let $V$ be a vector space. The set $V^X$ \index{VX@$V^X$ as a vector space} is a vector space if we define for each $f,g\in V^X$ and $\lambda\in\Fbb$:
\begin{gather*}
f+g:X\rightarrow V\qquad (f+g)(x)=f(x)+g(x)\\
\lambda f:X\rightarrow V\qquad (\lambda f)(x)=\lambda f(x)
\end{gather*}
We also define the \textbf{absolute value function}\index{00@Absolute function $\lvert f\lvert$} \index{f@$\lvert f\lvert$}
\begin{align}
|f|:X\rightarrow\Rbb_{\geq 0}\qquad x\in X\mapsto \lVert f(x)\lVert
\end{align}
The symbol $|f|$ is sometimes also written as $\lVert f\lVert$ when it will not be confused with $\lVert f\lVert_{\infty}$ or other norms of $f$.
\end{df}



\begin{df}
Let $X$ be a set and let $V$ be a normed vector space. For each $f\in V^X$, define the \index{l@$\lVert\cdot\lVert_{l^\infty}=\lVert f\lVert_{l^\infty(X,V)}=\lVert\cdot\lVert_\infty$} \pmb{$l^\infty$}\textbf{-norm}
\begin{align}
\lVert f\lVert_{l^\infty(X,V)}\equiv\lVert f\lVert_{l^\infty}\equiv \lVert f\lVert_\infty=\sup_{x\in X}\lVert f(x)\lVert
\end{align}
where $\lVert f(x)\lVert$ is defined by the norm of $V$. Define the \pmb{$l^\infty$}\textbf{-space} \index{l@$l^\infty(X,V)$} 
\begin{align}
l^\infty(X,V)=\{f\in V^X:\lVert f\lVert_\infty<+\infty\}
\end{align}
which is a vector subspace of $V^X$. Then $l^\infty(X,V)$ is a normed vector space under the $l^\infty$-norm. A function $f:X\rightarrow V$ is called \textbf{bounded} \index{00@Bounded function} if $f\in l^\infty(X,V)$.
\end{df}





\begin{exe}\label{lb143}
Prove that for every $f,g\in V^X$ and $\lambda\in\Fbb$ we have
\begin{gather}\label{eq23}
\begin{gathered}
\lVert f+g\lVert_\infty\leq \lVert f\lVert_\infty+\lVert g\lVert_\infty\\
\lVert \lambda f\lVert_\infty=|\lambda|\cdot \lVert f\lVert_\infty
\end{gathered}
\end{gather}
(Note that clearly we have that $\lVert f\lVert_\infty=0$ implies $f=0$.) Here, we understand $0\cdot (+\infty)=0$. Use these relations to verify that $l^\infty(X,V)$ is a linear subspace of $V^X$ (i.e. it is closed under addition and scalar multiplication) and that $\lVert\cdot\lVert_\infty$ is a norm on $l^\infty(X,V)$.  \hfill\qedsymbol
\end{exe}

\begin{df}\label{lb148}
Let $V$ be a normed vector space. We say that a sequence $(f_n)$ in $V^X$ \textbf{converges uniformly} \index{00@Uniform convergence} to $f\in V^X$ if $\lim_{n\rightarrow\infty}\lVert f-f_n\lVert_\infty=0$. In this case, we write ${f_n}\rightrightarrows f$. \index{fnf@$f_n\rightrightarrows f$}

We say that $(f_n)$ \textbf{converges pointwise} \index{00@Pointwise convergence} to $f\in V^X$ if for every $x\in X$ we have $\lim_{n\rightarrow\infty} f_n(x)=f(x)$, i.e. $\lim_{n\rightarrow\infty} \lVert f_n(x)-f(x)\lVert=0$. 

The same definition will be applied to nets $(f_\alpha)_{\alpha\in I}$ in $V^X$ after learning net convergence in Sec. \ref{lb147}. \hfill\qedsymbol
\end{df}


In more details, the uniform convergence of $f_n$ to $f$ means that ``for every $\eps>0$ there is $N\in\Zbb_+$ such that for all $n\geq N$ and  \textit{for all $x\in X$}, we have $\lVert f_n(x)-f(x)\lVert<\eps$". If we place the words ``for all $x\in X$" at the very beginning of the sentence, we get pointwise convergence.


Uniform convergence implies pointwise convergence: If $\lVert f-f_n\lVert_\infty\rightarrow 0$, then for each $x\in X$ we have $\lVert f_n(x)-f(x)\lVert\rightarrow 0$ since $\lVert f(x)-f_n(x)\lVert\leq\lVert f-f_n\lVert_\infty$. 



\begin{eg}
Let $f_n:(0,1)\rightarrow\Rbb$ be $f_n(x)=x^n$. Then $f_n$ converges pointwise to $0$ (cf. Exp. \ref{lb110}). But $\sup_{x\in(0,1)}|x^n-0|=1$ does not converge to $0$. So $f_n$ does not converge uniformly to $0$.
\end{eg}



\begin{rem}
The uniform convergence of sequences in $l^\infty(X,V)$ is induced by the $l^\infty$-norm, and hence is induced by the metric $d(f,g)=\lVert f-g\lVert_\infty$. However, this formula cannot be extended to a metric on $V^X$, since for arbitrary $f,g\in V^X$, $\lVert f-g\lVert_\infty$ is possibly $+\infty$. 



In fact, it is true that the uniform convergence of sequences in $V^X$ is induced by a metric, see Pb. \ref{lb81}. When $X$ is countable, we have seen in Pb. \ref{lb78} that the pointwise convergence in $V^X$ is also given by a metric. \hfill\qedsymbol
\end{rem}












\begin{thm}\label{lb85}
Let $X$ be a set, and let $V$ be a Banach space (over $\Fbb$). Then $l^\infty(X,V)$ is a Banach space (over $\Fbb$).
\end{thm}


\begin{proof}
Let $(f_n)$ be a Cauchy sequence in $l^\infty(X,V)$. Then for every $\eps>0$ there is $N\in\Zbb_+$ such that for all $m,n\geq N$ we have that $\sup_{x\in X}\lVert f_n(x)-f_m(x)\lVert <\eps$, and hence $\lVert f_n(x)-f_m(x)\lVert <\eps$ for each $x\in X$. This shows that for each $x\in X$, $(f_n(x))$ is a Cauchy sequence in $V$, which converges to some element $f(x)\in V$ because $V$ is complete.

We come back to the statement that for each $\eps>0$, there exists $N\in\Zbb_+$ such that for all $n\geq N$ and all $x$,
\begin{align*}
\lVert f_n(x)-f_m(x)\lVert <\eps  
\end{align*}
for every $m\geq N$. Let $m\rightarrow\infty$. Then by the continuity of subtraction and taking norm (cf. Prop. \ref{lb82}.), we obtain $\lVert f_n(x)-f(x)\lVert\leq \eps$ for all $n\geq N$ and $x\in X$. In other words, $\lVert f_n-f\lVert_\infty\leq\eps$ for all $n\geq N$. In particular, $\lVert f\lVert_\infty\leq\lVert f_N\lVert_\infty +\lVert f_N-f\lVert_\infty<+\infty$ by \eqref{eq23}. This proves $f\in l^\infty(X,V)$ and $f_n\rightrightarrows f$.
\end{proof}


Mathematicians used to believe that ``if a sequence of continuous functions $f_n:[0,1]\rightarrow\Rbb$ converges pointwise to a function $f:[0,1]\rightarrow\Rbb$, then $f$ is continuous". Cauchy, one of the main figures in 19th century working on putting analysis on a rigorous ground, has given a problematic proof of this wrong statement. Counterexamples were later found in the study of Fourier series: Let $f:\Rbb\rightarrow\Rbb$ be a function with period $2\pi$ such that $f(x)=x$ when $-\pi<x<\pi$, and $f(x)=0$ when $x=\pm \pi$. Then the Fourier series  of this noncontinuous function $f$ converges pointwise to $f$, yet the partial sums of this series are clearly continuous functions. Later, it was realized that uniform convergence is needed to show the continuity of the limit function. (See Thm. \ref{lb87}.) This was the first time the importance of uniform convergence was realized.



The following discussions about (resp. sequentially compact) metric spaces also apply to general (resp. compact) topological spaces. The reader can come back and check the proofs for these more general spaces after studying them in the future.

\begin{df}
Let $X,Y$ be  metric spaces (resp. topological spaces). Then $C(X,Y)$ \index{CXY@$C(X,Y)$, the set of continuous functions $X\rightarrow Y$. See Conv. \ref{lb85} also} denotes the set of continuous functions from $X$ to $Y$. 
\end{df}


\begin{lm}
Let $X$ be a metric space (resp. a topological space), and let $V$ be a normed vector space. Then $C(X,V)$ is a linear subspace of $V^X$. If $X$ is sequentially compact (resp. compact), then $C(X,V)$ is a linear subspace of $l^\infty(X,V)$.
\end{lm}

\begin{proof}
Using Prop. \ref{lb82}, one checks easily that $C(X,V)$ is a linear subspace of $V^X$.  For any $f\in C(X,V)$, the absolute value function $|f|:x\in X\mapsto\lVert f(x)\lVert$ is continuous. Thus, assuming that $X$ is sequentially compact, then by Lem. \ref{lb56}, $|f|$ is bounded on $X$. This proves that $\lVert f\lVert_\infty<+\infty$. Thus $C(X,V)$ is a subset (and hence a linear subspace) of $l^\infty(X,V)$. 
\end{proof}



\begin{thm}\label{lb87}
Let $X$ be a metric space (resp. a topological space), and let $V$ be a normed vector space. Then $C(X,V)\cap l^\infty(X,V)$ is a closed linear subspace of $l^\infty(X,V)$. In particular, if $X$ is sequentially compact (resp. compact), then $C(X,V)$ is a closed linear subspace of $l^\infty(X,V)$.
\end{thm}



\begin{proof}
Choose a sequence $(f_n)$ in $C(X,V)\cap l^\infty(X,V)$ converging in $l^\infty(X,V)$ to $f$. Namely, $f_n\rightrightarrows f$. We want to prove that $f$ is continuous. We check that $f$ satisfies Def. \ref{lb31}-(2'). (One can also use Def. \ref{lb31}-(1). The proofs using these two definitions are not substantially different.)

Fix $p\in X$. Choose any $\eps>0$. Since $f_n\rightrightarrows f$, there exists $N\in\Zbb_+$ such that for all $n\geq N$ and we have $\lVert f-f_n\lVert_\infty<\eps$. Since $f_N$ is continuous, there exists $r>0$ such that for each $x\in B_X(p,r)$ we have $\lVert f_N(x)-f_N(p)\Vert<\eps$. Thus, for each $x\in B_X(p,r)$ we have
\begin{align*}
\lVert f(x)-f(p)\lVert\leq \lVert f(x)-f_N(x)\lVert +\lVert f_N(x)-f_N(p)\lVert+\lVert f_N(p)-f(p)\lVert<3\eps
\end{align*}
This finishes the proof.
\end{proof}





\begin{cv}\label{lb88}
Unless otherwise stated, if $X$ is sequentially compact metric space (or more generally, a compact topological space to be defined latter), and if $V$ is a normed vector space, the norm on $C(X,V)$ is chosen to be the $l^\infty$-norm.
\end{cv}


\begin{co}\label{lb101}
Let $X$ be a metric space  (resp. a topological space), and let $V$ be a Banach space. Then  $C(X,V)\cap l^\infty(X,V)$ is a Banach space under the $l^\infty$-norm. In particular, if $X$ is sequentially compact (resp. compact), then $C(X,V)$ is a Banach space.
\end{co}

\begin{proof}
This follows immediately from Prop. \ref{lb86}, Thm. \ref{lb87}, and the fact that $l^\infty(X,V)$ is complete (Thm. \ref{lb85}).
\end{proof}










\subsection{Problems and supplementary material}



\begin{prob}\label{lb64}
Let $(x_n)$ be a sequence in a metric space $X$. Let $x\in X$. Prove that the following are equivalent.
\begin{itemize}
\item[(1)] $x$ is a cluster point of $(x_n)$, i.e., the limit of a convergent subsequence of $(x_n)$.
\item[(2)] For each $\eps>0$ and each $N\in\Zbb_+$, there exists $n\geq N$ such that $d(x_n,x)<\eps$.
\end{itemize}
(Note: in a general topological space, these two statements are not equivalent.)
\end{prob}


\begin{rem}
Condition (2) is often abbreviated to ``for each $\eps>0$, the sequence $(x_n)$ is frequently in $B(x,\eps)$". In general, we say ``$(x_n)$ \textbf{frequently} satisfies P" if for each $N\in\Zbb_+$ there is $n\geq N$ such that $x_n$ satisfies P. We say that ``$(x_n)$ \textbf{eventually} satisfies P" if there exists $N\in\Zbb_+$ such that for every $n\geq N$, $x_n$ satisfies P. \index{00@Eventually} \index{00@Frequently}  

Thus ``$(x_n)$ eventually satisfies P" means the same as ``all but finitely many $x_n$ satisfies P". Its negation is ``$(x_n)$ frequently satisfies $\neg$P".   \hfill\qedsymbol
\end{rem}

\begin{rem}
Condition (2) of Pb. \ref{lb64} is sometimes easier to use than (1). For example, compared to the original definition of cluster points, it is much easier to find an explicit negation of (2) by using the rule suggested in Rem. \ref{lb100}: There exist $\eps>0$ and $N\in\Zbb_+$ such that $d(x_n,x)\geq\eps$ for all $n\geq N$. (Or simply: there exists $\eps>0$ such that $x_n$ is eventually not in $B(x,\eps)$.) 
\end{rem}



\begin{prob}\label{lb70}
Use Pb. \ref{lb64}-(2) to prove that if $(x_n)$ is a sequence in $\ovl\Rbb$, then $\dps\limsup_{n\rightarrow\infty} x_n$ is a cluster point of $(x_n)$.
\end{prob}

\begin{rem}
You will notice that your proof of Pb. \ref{lb70} is slightly simpler than the proof we gave for Thm. \ref{lb68}. This is because our construction of subsequence as in \eqref{eq17} has been incorporated into your proof of (2)$\Rightarrow$(1) in Pb. \ref{lb64}.
\end{rem}

\begin{prob}\label{lb235}
Let $f:X\rightarrow Y$ be a continuous map of metric spaces. Assume that $f$ is bijective and $X$ is sequentially compact.  Prove that $f$ is a homeomorphism using the following hint.
\end{prob}

\begin{proof}[Hint]
You need to prove that if $(y_n)$ is a sequence in $Y$ converging to $y\in Y$, then $x_n=f^{-1}(y_n)$ converges to $x=f^{-1}(y)$. Prove that $(x_n)$ has only one cluster point, and hence converges to some point $x'\in X$ (why?). Then prove $x'=x$. (In the future, we will use the language of open sets and closed sets to prove this result again. Do not use this language in your solution.)
\end{proof}




\begin{thm}[\textbf{Tychonoff theorem, countable version}]\index{00@Tychonoff theorem, countable version}  \label{lb89}
Let $(X_n)_{n\in\Zbb_+}$ be a sequence of sequentially compact metric spaces. Then the product space $\dps S=\prod_{n\in\Zbb_+} X_n$ is sequentially compact under the metric defined  as in Pb. \ref{lb78}.
\end{thm}

The method of choosing subsequence in the following proof is the reknowned \textbf{diagonal method}. \index{00@Diagonal method}

\begin{proof}
Let $(x_m)_{m\in\Zbb_+}$ be a sequence in $S$. Since $(x_m(1))_{m\in\Zbb_+}$ is a sequence in the sequentially compact space $X_1$, $(x_m)_{m\in\Zbb_+}$ has a subsequence $x_{1,1},x_{1,2},x_{1,3}\dots$ whose value at $n=1$ converges in $X_1$. Since $X_2$ is sequentially compact, we can choose a subsequence $x_{2,1},x_{2,2},x_{2,3},\dots$ of the previous subsequence such that its values at $n=2$ converge in $X_2$. Then pick a subsequence from the previous one whose values at $3$ converge in $X_3$. 

By repeating this process, we get an $\infty\times\infty$ matrix $(x_{i,j})_{i,j\in\Zbb_+}$:
\begin{equation}
\begin{tikzcd}[sep=0cm]
{x_{1,1}} & {x_{1,2}} & {x_{1,3}} & \cdots \\
{x_{2,1}} & {x_{2,2}} & {x_{2,3}} & \cdots \\
{x_{3,1}} & {x_{3,2}} & {x_{3,3}} & \cdots \\
\vdots    & \vdots    & \vdots    & \ddots
\end{tikzcd}
\end{equation}
such that the following hold:
\begin{itemize}
\item The $1$-st line is a subsequence of the original sequence $(x_m)_{m\in\Zbb_+}$.
\item The $(i+1)$-th line is a subsequence of the $i$-th line.
\item For each $n$, $\lim_{j\rightarrow\infty} x_{n,j}(n)$ converges in $X_n$.
\end{itemize}
Then the diagonal line $(x_{i,i})_{i\in\Zbb_+}$ is a subsequence of the original sequence $(x_m)_{m\in\Zbb_+}$. Moreover, for each $n$, $(x_{i,i})_{i\geq n}$ is a subsequence of the $n$-th line, whose value at $n$ therefore converges in $X_n$. Thus $\lim_{i\rightarrow\infty} x_{i,i}(n)$ converges in $X_n$. Thus, by Pb. \ref{lb78}, $(x_{i,i})_{i\in\Zbb_+}$ converges under any metric inducing the product topology.
\end{proof}




\begin{prob}\label{lb90}
Let $X$ be a sequentially compact metric space. Let $A\subset X$ be a metric subspace.  Consider the statements:
\begin{enumerate}[label=(\arabic*)]
\item $A$ is sequentially compact.
\item $A$ is a closed subset of $X$.
\end{enumerate}
Prove that (1)$\Rightarrow$(2). Prove that if $X$ is sequentially compact, then (2)$\Rightarrow$(1).
\end{prob}


The above problem implies immediately:

\begin{thm}[\textbf{Heine-Borel theorem}]\label{lb98}  \index{00@Heine-Borel theorem} 
Let $A$ be a subset of $\Rbb^N$. Then $A$ is sequentially compact iff $A$ is a bounded closed subset of $\Rbb^N$.
\end{thm}

\begin{proof}
Suppose that $A$ is sequentially compact. Then $A$ is bounded under the Euclidean metric by Prop. \ref{lb71}. By Pb. \ref{lb90}, $A$ is a closed subset of $\Rbb^N$.

Conversely, assume that $A$ is a bounded and closed subset of $\Rbb^N$. Then $A\subset B$ where $B$ is the product of $N$ pieces of closed intervals in $\Rbb$. Then $B$ is sequentially compact by Bolzano-Weierstrass. Since $A$ is closed in $\Rbb^N$, it is not hard to check that $A$ is closed in $B$.\footnote{If $Z$ is a metric space, if $X\subset Y\subset Z$, and if $X$ is closed in $Z$, then it is easy to check that $X$ is closed in $Y$.} Thus $A$ is sequentially compact by Pb. \ref{lb90}.
\end{proof}




\begin{eg}
Choose any $p\in \Rbb^N$ and $0\leq R<+\infty$. Then $\ovl B_{\Rbb^N}(p,R)$ is a bounded closed subset of $\Rbb^N$ (Exp. \ref{lb97}), and hence is sequentially compact by Heine-Borel.
\end{eg}

\begin{rem}
Think about the question: Equip $\Rbb^{\Zbb_+}$ with metric
\begin{align*}
d(x,y)=\sup_{n\in\Zbb_+}\frac {\min\{|x(n),y(n)|,1\}}{n}
\end{align*} 
What are the sequentially compact subsets of $\Rbb^{\Zbb_+}$? (Namely, think about how to generalize Heine-Borel theorem to $\Rbb^{\Zbb_+}$.)
\end{rem}



\begin{prob}
Do Exercise \ref{lb143}.
\end{prob}




\begin{prob}\label{lb81}
Let $V$ be a normed vector space. For every $f,g\in V^X$ define
\begin{align}
d(f,g)=\min\{1,\lVert f-g\lVert_\infty \}\label{eq55}
\end{align}
\begin{enumerate}
\item Show that $d$ defines a metric on $V^X$. 
\item Show that for every sequence $(f_n)$ in $V^X$ and every $g\in V^X$, we have $f_n\rightarrow g$ under the metric $d$ iff $f_n\rightrightarrows g$.
\end{enumerate}
\end{prob}

\begin{df}\label{lb146}
Let $X$ be a set, and let $V$ be a normed vector space. A metric on $V^X$ is called a \textbf{uniform convergence metric} \index{00@Uniform convergence metric} if it is equivalent to \eqref{eq55}. Thus, by Def. \ref{lb144}, a uniform convergence metric is one such that a sequence $(f_n)$ in $V^X$ converges to $f$ under this metric iff $f_n\rightrightarrows f$.
\end{df}




\begin{prob}\label{lb103}
Let $X,Y$ be metric spaces, and assume that $Y$ is sequentially compact. Let $V$ be a normed vector space. Choose $f\in C(X\times Y,V)$, i.e., $f:X\times Y\rightarrow V$ is continuous. For each $x\in X$, let
\begin{align*}
f_x:Y\rightarrow V\qquad y\mapsto f(x,y)
\end{align*}
Namely $f_x(y)=f(x,y)$. It is easy to check that $f_x\in C(Y,V)$. Define a new function
\begin{gather}
\begin{gathered}
\Phi(f):X\rightarrow C(Y,V)\qquad x\mapsto f_x
\end{gathered}
\end{gather}
Recall that $C(Y,V)$ is equipped with the $l^\infty$-norm. 
\begin{enumerate}
\item Prove that $\Phi(f)$ is continuous. In other words, prove that if $(x_n)$ is a sequence in $X$ converging to  $x\in X$, then $f_{x_n}\rightrightarrows f_x$ on $Y$, i.e.
\begin{align*}
\lim_{n\rightarrow\infty}\lVert f_{x_n}-f_x\lVert_{l^\infty(Y,V)}=0
\end{align*}
\item[$\star$ 2.]  Give an example of $f\in C(X\times Y,\Rbb)$ where $Y$ is not sequentially compact, $(x_n)$ converges to $x$ in $X$, and $f_{x_n}$ does not converge uniformly to $f_x$. (Note: you may consider $X=Y=\Rbb$.)
\end{enumerate}

\end{prob}

\begin{proof}[Hint]
In part 1, to prove that $\Phi(f)$ is continuous,   one can  prove the equivalent fact that for every fixed $x\in X$ the following is true:
\begin{itemize}
\item For every $\eps>0$ there exists $\delta>0$ such that for all $p\in B_X(x,\delta)$, we have $\sup_{y\in Y}\lVert f(p,y)-f(x,y)\lVert<\eps$. 
\end{itemize}
(Cf. Def. \ref{lb31}.) Prove this by contradiction and by using the sequential compactness of $Y$ appropriately.
\end{proof}


\begin{rem}\label{lb102}
Let $X=\Zbb_+\cup\{\infty\}$, equipped with the metric
\begin{align*}
d(m,n)=|m^{-1}-n^{-1}|
\end{align*} 
In other words, the metric on $X$ is $\tau^*d_\Rbb$ where $d_\Rbb$ is the Euclidean metric on $\Rbb$, and $\tau:X\rightarrow \Rbb,n\mapsto n^{-1}$. It is not hard to show that $X$ is sequentially compact: either prove it directly, or apply Heine-Borel to $\tau(X)$.

Let $Y$ be a metric space. Let $(y_n)_{n\in\Zbb_+}$ be a sequence in $Y$, and let $y_\infty\in Y$. It is not hard to see that the following two statements are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item The function $F:X\rightarrow Y,n\mapsto y_n$ is continuous.
\item The sequence $(y_n)_{n\in\Zbb_+}$ converges to $y_\infty$.
\end{enumerate}
The following problem is a generalization of this equivalence.  \hfill\qedsymbol
\end{rem}



\begin{sprob}\label{lb104}
Let $V$ be a normed vector space. Let $Y$ be a metric space. Let $X=\Zbb_+\cup\{\infty\}$ with metric defined as in Rem. \ref{lb102}. Let $(f_n)_{n\in\Zbb_+}$ be a sequence in $C(Y,V)$. Let $f_\infty\in V^Y$. Prove that the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item The following function is continuous:
\begin{align}
F:X\times Y\rightarrow V\qquad (n,y)\mapsto f_n(y)
\end{align}
In particular, by restricting $F$ to $\infty\times Y$, we see that $f_\infty\in C(Y,V)$.
\item $(f_n)_{n\in\Zbb_+}$ converges pointwise to $f_\infty$. Moreover, $(f_n)_{n\in\Zbb_+}$ is \textbf{pointwise equicontinuous}, \index{00@Pointwise equicontinuous} which means the following:
\begin{itemize}
\item For every $y\in Y$ and every $\eps>0$, there exists $\delta>0$ such that for all $p\in B_Y(y,\delta)$ we have
\begin{align*}
\sup_{n\in\Zbb_+}\lVert f_n(p)-f_n(y)\lVert<\eps
\end{align*}
\end{itemize}
\end{enumerate}
\end{sprob}

\begin{proof}[Note]\renewcommand{\qedsymbol}{}
In part (1), the only nontrivial thing to prove  is that $F$ is continuous at $(\infty,y)$ for every $y\in Y$.
\end{proof}





\begin{rem}
There is a concise way to define pointwise equicontinuity: a sequence $(f_n)_{n\in\Zbb_+}$ in $V^Y$ is pointwise equicontinuous iff the function
\begin{gather}
Y\mapsto V^{\Zbb_+}\qquad y\mapsto (f_1(y),f_2(y),\dots)
\end{gather}
is continuous, where $V^{\Zbb_+}$ is equipped with any uniform convergence metric (cf. Def. \ref{lb146}). 
\end{rem}

\begin{srem}
In Pb. \ref{lb104}, there is a quick and tricky way to conclude (1)$\Rightarrow$(2): Use Pb. \ref{lb103} and the sequential compactness of $X$. (Do not use this method in your solution. Prove (1)$\Rightarrow$(2) directly; it is a good exercise and is not difficult.)
\end{srem}



\begin{srem}\label{lb145}
Pb. \ref{lb103} and \ref{lb104}, together with Thm. \ref{lb87}, imply the following fact (can you see why?): 
\begin{itemize}
\item Let $Y$ be a sequentially compact metric space. Let $V$ be a normed vector space. Let $(f_n)_{n\in\Zbb_+}$ be a pointwise equicontinuous sequence of functions $Y\rightarrow V$ converging pointwise to some $f:Y\rightarrow V$. Then $f_n\rightrightarrows f$ on $Y$. 
\end{itemize}
You can also try to give a straightforward proof of this fact without using Pb. \ref{lb103} and \ref{lb104}.
\end{srem}








\newpage




\section{Series}

In this chapter, we assume that vector spaces are over $\Fbb\in\{\Rbb,\Cbb\}$ unless otherwise stated.





\subsection{Definitions and basic properties}


\begin{df}
Let $V$ be a Banach space (over $\Fbb$). A \textbf{series} \index{00@Series in a Banach space} in $V$ is an expression of the form
\begin{align}
\sum_{i=1}^\infty v_i  \label{eq25}
\end{align}
where $(v_i)_{i\in\Zbb_+}$ is a sequence in $V$. If $s\in V$, we say that the series \eqref{eq25} \textbf{converges to $s$} if
\begin{align*}
s=\lim_{n\rightarrow\infty} \sum_{i=1}^n v_i
\end{align*}
namely, $s_n\rightarrow s$ where $s_n$ is the \textbf{partial sum} \index{00@Partial sum} $s_n=\sum_{i=1}^n v_i$. In this case, we write
\begin{align*}
s=\sum_{i=1}^\infty v_i
\end{align*}
\end{df}


\begin{rem}\label{lb93}
Since $V$ is complete, the series \eqref{eq25} converges iff the sequence of partial sum $(s_n)$ is a Cauchy sequence: for every $\eps>0$ there exists $N\in\Zbb_+$ such that for all $n> m\geq N$ we have $\lVert s_n-s_m\lVert<\eps$, i.e.,
\begin{align}
\Big\lVert \sum_{i=m+1}^n v_i\Big\lVert<\eps  \label{eq28}
\end{align}
\end{rem}


\begin{pp}\label{lb92}
Suppose that $\sum_{i=1}^\infty v_i$ is a convergent series in a Banach space $V$. Then $\dps\lim_{n\rightarrow\infty} v_n=0$.
\end{pp}

\begin{proof}
Let $s_n=v_1+\cdots+v_n$, which converges to $s\in V$. Then $\lim_{n\rightarrow\infty} s_{n+1}=s$. So $v_n=s_{n+1}-s_n\rightarrow s-s=0$ since subtraction in continuous (Prop. \ref{lb82}).
\end{proof}

Thus, for example, $\sum_{n=1}^\infty (-1)^n$ diverges in the Banach space $\Rbb$ since $\lim_{n\rightarrow\infty} (-1)^n$ does not converge to $0$.





\begin{df}
Consider a \textbf{series} in $\ovl\Rbb_{\geq0}$: \index{00@Series in $\ovl\Rbb_{\geq0}$}
\begin{align}
\sum_{i=1}^\infty a_i \label{eq26}
\end{align}
namely, each $a_i$ is in $\ovl\Rbb_{\geq 0}$. Note that the partial sum $s_n=\sum_{i=1}^n a_i$ is increasing. We say that $\lim_{n\rightarrow\infty} s_n$ (which exists in $\ovl\Rbb_{\geq0}$ and equals $\sup\{s_n:n\in\Zbb_+\}$, cf. Rem. \ref{lb58}) is the value of the series \eqref{eq26} and write
\begin{align*}
\sum_{i=1}^\infty a_i=\lim_{n\rightarrow\infty} s_n
\end{align*}
\end{df}


\begin{df}
We say that a series $\sum_{i=1}^\infty a_i$ in $\Rbb_{\geq 0}$ \textbf{converges} if it converges in $\Rbb$ (but not just converges in $\ovl\Rbb_{\geq 0}$, which is always true). Clearly, $\sum_{i=1}^\infty a_i$ converges iff
\begin{align*}
\sum_{i=1}^\infty a_i<+\infty
\end{align*}
More generally, we say that a series $\sum_{i=1}^\infty v_i$ in a Banach space $V$ \textbf{converges absolutely}, \index{00@Absolute convergent series} if
\begin{align*}
\sum_{i=1}^\infty~ \lVert v_i\lVert <+\infty
\end{align*}
\end{df}

\begin{rem}
By the Cauchy condition of convergence, $\sum_{i=1}^\infty v_i$ converges absolutely iff for every $\eps>0$ there exists $N\in\Zbb_+$ such that for all $n> m\geq N$ we have 
\begin{align}
\sum_{i=m+1}^n~\lVert v_i\lVert<\eps \label{eq27}
\end{align}
By comparing \eqref{eq27} with \eqref{eq28} and using the subadditivity of the norm (recall Def. \ref{lb91}), we immediately see:
\end{rem}

\begin{pp}\label{lb94}
Let $\sum_{i=1}^\infty v_i$ be a series in a Banach space. The following are true.
\begin{enumerate}
\item If $\sum_{i=1}^\infty v_i$ converges absolutely, then it converges.
\item For each $i$ we choose $a_i\in\Rbb_{\geq0}$ satisfying $\lVert v_i\lVert\leq a_i$. Suppose that $\sum_{i=1}^\infty a_i<+\infty$. Then $\sum_{i=1}^\infty v_i$ converges absolutely.
\end{enumerate}
\end{pp}

\begin{proof}
Part 1 has been explained above. In part 2, we have $\sum\lVert v_i\lVert \leq \sum a_i<+\infty$. So $\sum v_i$ converges absolutely.
\end{proof}


\begin{exe}
Suppose that $\sum_{i=1}^\infty u_i$ and $\sum_{i=1}^\infty v_i$ are convergent (resp. absolutely convergent) series in a Banach space $V$. Let $\lambda\in\Fbb$. Show that the LHS of the following equations converges (resp. converges absolutely) in $V$, and that the following equations hold:
\begin{gather*}
\sum_{i=1}^\infty (u_i+v_i)=\sum_{i=1}^\infty u_i+\sum_{i=1}^\infty v_i\\
\sum_{i=1}^\infty \lambda v_i=\lambda\cdot\sum_{i=1}^\infty v_i
\end{gather*}
\end{exe}


\begin{rem}
We have seen that absolute convergence implies convergence. In fact, at least when $V=\Fbb^N$, absolute convergence is in many ways more natural than convergence. For example, we will learn that if a series $\sum_i v_i$ in $\Fbb^N$ converges absolutely, then the value of $\sum_i v_i$ is invariant under rearrangement of the series: for every bijection $\varphi:\Zbb_+\rightarrow\Zbb_+$ we have $\sum_i v_i=\sum_i v_{\varphi(i)}$. In the next semester, we shall learn Lebesgue integral theory and, more generally, measure theory. When applying measure theory to infinite sums over the countable set $\Zbb_+$, many good results (e.g. dominated convergence theorem, Fubini theorem)  hold only for absolute convergence series, but not for arbitrary convergent series in general. In fact, there is no analog of convergent (but not absolutely convergent) series in measure theory at all!

When $V$ is not necessarily finite-dimensional, the situation is subtler: there is a version of convergence which lies between the usual convergence and absolute convergence, and which coincides with absolute convergence when $V=\Fbb^N$. This version of convergence is defined using nets instead of sequences. Moreover, many good properties (as mentioned above) hold for this convergence, and these properties can be proved in a very conceptual way (rather than using brute-force computation). We will learn this convergence in the next chapter.   \hfill\qedsymbol
\end{rem}



\subsection{Basic examples}


Let us study the \textbf{geometric series} $\sum_{n=0}^\infty z^n$ where $z\in\Cbb$. We first note the famous \textbf{binomial formula}: \index{00@Binomial formula} for each $z,w\in\Cbb$ and $n\in\Nbb$,
\begin{align}
(z+w)^n=\sum_{k0}^n{n\choose k}z^kw^{n-k}  \label{eq60}
\end{align}
In particular,
\begin{align}
(1+z)^n=1+nz+\frac{n(n-1)}{2}z^2+\frac{n(n-1)(n-2)}6 z^3+\cdots+nz^{n-1}+z^n \label{eq29}
\end{align}


\begin{eg}\label{lb110}
Assume $z\in\Cbb$ and $|z|<1$. Then $\lim_{n\rightarrow\infty}z^n=0$. 
\end{eg}

\begin{proof}
If $z=0$ then it is obvious. Assume that $0<|z|<1$. Choose $\delta>0$ such that $|z|=1/(1+\delta)$. By \eqref{eq29}, $(1+\delta)^n\geq 1+n\delta$. So
\begin{align*}
0\leq |z^n|\leq (1+n\delta)^{-1}
\end{align*}
Since $\dps\lim_{n\rightarrow\infty} (1+n\delta)^{-1}=0$, we have $|z^n|\rightarrow0$ by squeeze theorem. Hence $z^n\rightarrow0$.
\end{proof}

\begin{eg}\label{lb106}
Let $z\in\Cbb$. If $|z|<1$, then $\dps\sum_{n=0}^\infty z^n$ converges absolutely, and
\begin{align}
\sum_{n=0}^\infty z^n=\frac 1{1-z}
\end{align}
where $0^0$ is understood as $1$. If $|z|\geq 1$, then $\dps\sum_{n=0}^\infty z_n$ diverges in $\Cbb$.
\end{eg}


\begin{proof}
The partial sum $s_n=1+z+z^2+\cdots +z^n$ equals $(1-z^{n+1})/(1-z)$ when $z\neq 1$. Therefore, when $|z|<1$, $s_n\rightarrow 1/(1-z)$. When $|z|\geq 1$, we have $|z^n|\geq 1$ and hence $z^n\nrightarrow 0$. So $\sum_{n=0}^\infty z^n$ diverges by Prop. \ref{lb92}.
\end{proof}


\begin{eg}\label{lb95}
The \textbf{harmonic series} $\dps\sum_{n=1}^\infty \frac 1n$ diverges (in $\Rbb$).
\end{eg}


\begin{proof}
We want to show that the Cauchy condition (cf. Rem. \ref{lb93}) does not hold. Thus, we want to prove that there exists $\eps>0$ such that for every $N\in\Zbb_+$ there exist $n>m\geq N$ such that $|(m+1)^{-1}+(m+2)^{-1}+\cdots+n^{-1}|\geq\eps$.

To see this, for each $N$ we choose  $m=2^N$ and $n=2^{N+1}$. Then $n>m>N$, and
\begin{align*}
&\Big|\sum_{i=m+1}^n i^{-1}\Big|=\Big|\frac 1{2^N+1}+\frac 1{2^N+2}+\cdots +\frac 1{2^N+2^N}  \Big|\\
\geq&\underbrace{\Big|\frac 1{2^{N+1}}+\frac 1{2^{N+1}}+\cdots +\frac 1{2^{N+1}}  \Big|}_{2^N\text{ terms}}=\eps
\end{align*}
where $\eps=\frac 12$.
\end{proof}

\begin{comment}
\begin{rem}
For every $p\in\Rbb$, assume that $n^p$ (where $n\in\Zbb_+$) is defined and that the properties we learned in high school mathematics are satisfied. If $p\leq 1$, then clearly $\sum_{n=1}^\infty n^{-p}=+\infty$ since $1/n^p\geq 1/n$. It is in fact true that for every $p>1$ we have $\sum_{n=1}^\infty n^{-p}<+\infty$. The easiest (but not the most elementary) way to see this is by using integrals: This series equals $1+\int_{x=1}^{+\infty}f(x)dx$ where $f:[1,+\infty)\rightarrow\Rbb_{\geq0}$ equals $(n+1)^{-p}$ when restricted to $[n,n+1)$. So $f(x)\leq x^{-p}$ on $[1,+\infty)$. Hence $\int_{x=1}^{+\infty}f(x)dx\leq \int_{x=1}^{+\infty}x^{-p}dx=(1-p)^{-1}x^{1-p}|_{x=1}^{+\infty}=(p-1)^{-1}<+\infty$.
\end{rem}
\end{comment}


\begin{exe}\label{lb96}
Choose any $p\in\Zbb$. Prove that $\dps\sum_{n=1}^\infty n^{-p}$ converges iff $p\geq 2$.
\end{exe}

\begin{proof}[Hint]
Use Prop. \ref{lb94} and Exp. \ref{lb95} to reduce the problem to the case $p=2$. Prove this case by proving $\sum_{n=1}^\infty 1/n(n+1)=1<+\infty$.
\end{proof}



\begin{df}
Let $V$ be a Banach space, let $X$ be a set, and let $(f_n)$ be a sequence in $l^\infty(X,V)$, and let $g\in l^\infty(X,V)$. We say that the series of functions $\dps\sum_{i=1}^\infty f_i$ \textbf{converges uniformly to $g$} \index{00@Uniform convergence of series of functions} (on $X$) if it converges to $g$ as a series in the Banach space $l^\infty(X,V)$ and under the $l^\infty$-norm. Equivalently, this means that the partial sum function $s_n=f_1+\cdots+f_n$ converges uniformly to $g$ as $n\rightarrow\infty$.
\end{df}


\begin{eg}
The series of functions $\dps\sum_{n=1}^\infty \frac{\sin|nz^3|}{n^2}$ converges uniformly on $\Cbb$ to a continuous function $g:\Cbb\rightarrow\Rbb$ which is bounded (i.e. $\dps\sup_{z\in\Cbb}|g(z)|<+\infty$).
\end{eg}

\begin{proof}
Let $f_n(z)=\sin|nz^3|/n^2$. Then each $f_n$ is in $\fk X=C(\Cbb,\Rbb)\cap l^\infty(\Cbb,\Rbb)$ where $\fk X$ is a real Banach space under the $l^\infty$-norm by Cor. \ref{lb101}. Note that $\lVert f_n\lVert_\infty\leq n^{-2}$. By Exe. \ref{lb96}, $\sum_{n=1}^\infty n^{-2}<+\infty$. Therefore, by Prop. \ref{lb94}, the series $\sum_n f_n$ converges in $\fk X$, i.e., it converges uniformly to an element $g\in \fk X$. (In particular, $\sum_n f_n(z)=g(z)$ for all $z\in\Cbb$.)
\end{proof}


\subsection{Root test and ratio test; power series; construction of $e^z$}\label{lb218}


Root test and ratio test are useful criteria for proving the convergence or divergence of series, especially  power series. In addition, the method of power series provides a unified and elegant proof for many useful formulas about limit (see Prop. \ref{lb109} and Exp. \ref{lb111}). We begin our discussion with the following easy observation:

\begin{rem}\label{lb105}
Let $(x_n)$ be a sequence in $\ovl\Rbb$, and let $A\in\ovl\Rbb$. The following are true.
\begin{enumerate}
\item If $\dps\limsup_{n\rightarrow\infty} x_n<A$, then $x_n<A$ is eventually true.
\item If $\dps\limsup_{n\rightarrow\infty} x_n>A$, then $x_n>A$ is frequently true.
\end{enumerate}
By taking negative, we obtain similar statements for $\liminf$.
\end{rem}


\begin{proof}
Recall that $\limsup x_n=\inf_{n\in\Zbb_+}\alpha_n$ where where $\alpha_n=\sup\{x_n,x_{n+1},\dots\}$. 



Assume that  $\inf_{n\in\Zbb_+}\alpha_n<A$. Then $A$ is not a lower bound of $\{\alpha_n:n\in\Zbb_+\}$. Thus, there exists $N\in\Zbb_+$ such that $\alpha_N<A$. Then $x_n<A$ for all $n\geq N$.

Assume that $\inf_{n\in\Zbb_+}\alpha_n>A$. Then for each $N\in\Zbb_+$ we have $\alpha_N>A$. So $A$ is not an upper bound of $\{x_n,x_{n+1},\dots\}$.  So there is $n\geq N$ such that $x_n>A$.
\end{proof}


We will heavily use $\sqrt[n]{x}$ (where $x\geq0$ and $n\in\Zbb_+$) in the following discussions. $\sqrt[n]{x}$ will be rigorously constructed in Exp. \ref{lb216}, whose proof does not rely on the results of this section.


\begin{pp}[\textbf{Root test}]\index{00@Root test} 
Let $\dps\sum_{n=1}^\infty v_n$ be a series in a Banach space $V$. Let $\dps\beta=\limsup_{n\rightarrow\infty}\sqrt[n]{\lVert v_n\lVert}$. Then:
\begin{enumerate}
\item If $\beta<1$, then $\sum v_n$ converges absolutely, and hence converges in $V$.
\item If $\beta>1$, then $\sum v_n$ diverges in $V$.
\end{enumerate}
\end{pp}


\begin{proof}
Suppose $\beta<1$. Then we can choose $\gamma$ such that $\beta<\gamma<1$. So $\limsup \sqrt[n]{\lVert v_n\lVert}<\gamma$. By Rem. \ref{lb104}, there exists $N\in\Zbb_+$ such that for all $n\geq N$, we have $\sqrt[n]{\lVert v_n\lVert}<\gamma$, and hence $\lVert v_n\lVert <\gamma^n$. Since $\sum_{n=0}^\infty \gamma^n=(1-\gamma)^{-1}<+\infty$ (Exp. \ref{lb106}), the series $\sum_{n=N}^\infty v_n$ converges absolutely by Prop. \ref{lb94}. So the original series converges absolutely.

Assume that $\beta>1$. Then by Rem. \ref{lb105}, for each $N$ there is $n\geq N$ such that $\sqrt[n]{\lVert v_n\lVert}>1$ and hence $\lVert v_n-0\lVert>1$. So $v_n\nrightarrow 0$. So $\sum v_n$ diverges by Prop. \ref{lb92}.
\end{proof}

\begin{eg}
Let $V=\Rbb$ and $v_n=1/n$ resp. $v_n=1/n^2$. Then $\beta=1$, and $\sum v_n$ diverges resp. converges absolutely due to Exe. \ref{lb96}. So Root test gives no information on the convergence of series when $\beta=1$. The same can be said about ratio test.
\end{eg}




\begin{pp}[\textbf{Ratio test}]\index{00@Ratio test}  
Let $\dps\sum_{n=1}^\infty v_n$ be a series in a Banach space $V$ such that $v_n\neq 0$ for all $n$. Let $\dps\alpha=\liminf_{n\rightarrow\infty}\frac{\lVert v_{n+1}\lVert}{\lVert v_n\lVert}$ and $\dps\beta=\limsup_{n\rightarrow\infty}\frac{\lVert v_{n+1}\lVert}{\lVert v_n\lVert}$. Then:
\begin{enumerate}
\item If $\beta<1$, then $\sum v_n$ converges absolutely, and hence converges in $V$.
\item If $\alpha>1$, then $\sum v_n$ diverges in $V$.
\end{enumerate}
\end{pp}

\begin{proof}
Suppose $\beta<1$. Choose $\gamma$ such that $\beta<\gamma<1$. Then by Rem. \ref{lb105}, there is $N$ such that for all $n\geq N$ we have $\lVert v_{n+1}\lVert/\lVert v_n\lVert<\gamma$. So $\lVert v_n\lVert <\gamma^{n-N}\lVert v_N\lVert$. So $\sum_{n\geq N}\lVert v_n\lVert \leq \lVert v_N\lVert \cdot\sum_{n\geq N}\gamma^{n-N}=\lVert v_N\lVert\cdot (1-\gamma)^{-1}<+\infty$. So $\sum v_n$ converges absolutely.

Suppose $\alpha>1$. Then by Rem. \ref{lb105}, there is $N$ such that for all $n\geq N$ we have $\lVert v_{n+1}\lVert/\lVert v_n\lVert>1$. So $\lVert v_n\lVert\geq\lVert v_N\lVert>0$ for all $n\geq N$. So $v_n\nrightarrow 0$ and hence $\sum v_n$ diverges, as in the proof of root test.
\end{proof}









\begin{df}
A \textbf{power series} in a complex Banach space $V$ \index{00@Power series} is an expression of the form $\dps\sum_{n=0}^\infty v_nz^n$ where the \textbf{coefficients} $v_0,v_1,v_2,\dots$ are elements of $V$, and $z$ is a \textbf{complex variable},\index{00@Complex variable} i.e., a symbol which can take arbitrary values in $\Cbb$. If the power series $\sum v_nz^n$ converges at $z_0\in\Cbb$, we often let $\sum v_n z_0^n$ denote this limit.
\end{df}


\begin{pp}\label{lb108}
Let $\sum v_n z^n$ be a power series in a complex Banach space $V$. Then there is a unique $0\leq R\leq+\infty$ satisfying the following properties:
\begin{enumerate}[label=(\alph*)]
\item If $z\in\Cbb$ and $|z|<R$, then $\sum v_n z^n$ converges absolutely in $V$.
\item If $z\in\Cbb$ and $|z|>R$, then $\sum v_n z^n$ diverges in $V$.
\end{enumerate}
Such $R$ is called the \textbf{radius of convergence} \index{00@Radius of convergence} of $\sum v_nz^n$. Moreover, we have
\begin{align}
R=\frac 1{\dps\limsup_{n\rightarrow\infty}\sqrt[n]{\lVert v_n\lVert}}=\liminf_{n\rightarrow\infty}\frac 1{\sqrt[n]{\lVert v_n\lVert}}  \label{eq30}
\end{align}
\end{pp}



\begin{proof}
Clearly, there are at most one $R$ satisfying (a) and (b). Let us define $R$ using \eqref{eq30} (note that the second and the third terms of \eqref{eq30} are clearly equal), and prove that $R$ satisfies (a) and (b). Let
\begin{gather*}
\beta(z)=\limsup_{n\rightarrow\infty} \sqrt[n]{\lVert v_n z^n\lVert}
\end{gather*}
Then $\beta(z)=|z|/R$. So (a) and (b) follow immediately from root test.
\end{proof}


\begin{rem}
Note that if one can find $0\leq r\leq R$ such that $\sum v_nz^n$ converges whenever $|z|<r$, then $r\leq R$ where $R$ is the radius of convergence: otherwise, the series diverges for any positive $z$ satisfying $R<z<r$, impossible.

It follows that if  $\sum v_nz^n$ converges for all $|z|<r$, then $\sum v_nz^n$ converges \textit{absolutely} for all $|z|<r$.  \hfill\qedsymbol
\end{rem}


Prop. \ref{lb108} provides a useful method for computing limits of a positive sequence:

\begin{pp}\label{lb109}
Let $(\lambda_n)$ be a sequence in $\Rbb_{>0}$. Then
\begin{align}
\liminf_{n\rightarrow\infty}\frac{\lambda_{n+1}}{\lambda_n}\leq \liminf_{n\rightarrow\infty}\sqrt[n]{\lambda_n}\leq \limsup_{n\rightarrow\infty}\sqrt[n]{\lambda_n}\leq \limsup_{n\rightarrow\infty} \frac{\lambda_{n+1}}{\lambda_n}  \label{eq35}
\end{align}
In particular, (by Cor. \ref{lb113}) we have
\begin{align}
\lim_{n\rightarrow\infty}\sqrt[n]{\lambda_n}=\lim_{n\rightarrow\infty} \frac{\lambda_{n+1}}{\lambda_n}\label{eq31}
\end{align}
provided that the limit on the RHS of \eqref{eq31} exists in $\ovl\Rbb$.
\end{pp}

The four numbers in \eqref{eq35} can be completely different. See \cite[Exp. 3.35]{Rud-P}.

\begin{proof}
Let $R$ be the radius of convergence of $\sum \lambda_n z^n$. Then $R=1/\limsup\sqrt[n]{\lambda_n}$ by \eqref{eq31}. Thus, by Prop. \ref{lb108}, if $|z|>R$ then $\sum \lambda_n z^n$ diverges, and hence $\limsup|\lambda_{n+1}z^{n+1}|/|\lambda_n z^n|\geq 1$ by ratio test. Therefore, 
\begin{align*}
|z|>\frac 1{\dps\limsup\sqrt[n]{\lambda_n}}\qquad\Longrightarrow \qquad |z|\cdot\limsup\frac{\lambda_{n+1}}{\lambda_n}\geq 1
\end{align*}
This proves
\begin{align*}
\limsup\sqrt[n]{\lambda_n}\leq \limsup \frac{\lambda_{n+1}}{\lambda_n}
\end{align*}
Replacing $\lambda_n$ by $\lambda_n^{-1}$, we get
\begin{align*}
\frac 1{\dps\liminf \sqrt[n]{\lambda_n}}=\limsup\sqrt[n]{\lambda_n^{-1}}\leq \limsup \frac{\lambda_n}{\lambda_{n+1}}=\frac 1{\dps \liminf\frac{\lambda_{n+1}}{\lambda_n}}
\end{align*}
This proves \eqref{eq35}.
\end{proof}

\begin{eg}\label{lb111}
Let $a\in\Rbb_{>0}$ and $p\in\Zbb$. The following formulas follow immediately from Prop. \ref{lb109} (especially, from \eqref{eq31}):
\begin{subequations}
\begin{gather}
\lim_{n\rightarrow\infty} \sqrt[n]{a}=1 \label{eq32}\\
\lim_{n\rightarrow\infty}\sqrt[n]{n!}=+\infty  \label{eq33}\\
\lim_{n\rightarrow\infty}\sqrt[n]{n^p}=1  \label{eq34}
\end{gather}
Note that \eqref{eq34} follows from 
\begin{align}
\lim_{n\rightarrow\infty}\Big(\frac n{n+1}\Big)^p=1 
\end{align}
(This is clearly true when $p=\pm1$, and hence is true for any $p$ by induction.) By \eqref{eq34}, the radius of convergence of $\sum_n n^p z^n$ is $1$. Therefore, by Prop. \ref{lb108},  $\sum n^pA^{-n}$ converges absolutely when $A>1$. Thus, by Prop. \ref{lb92},
\begin{align}
\lim_{n\rightarrow\infty} \frac{n^p}{A^n}=0 \qquad(\text{if }A>1)
\end{align}
This means that ``polynomials grow slower than exponentials".
\end{subequations}

The same conclusions hold for arbitrary $p\in\Rbb$ once we know how to define $x^p$ and prove the continuity of $x\in\Rbb_{>0}\mapsto x^p$. (See Sec. \ref{lb219}.) \hfill\qedsymbol
\end{eg}


\begin{sexe}
Prove \eqref{eq32} directly. Then use \eqref{eq32} to give a direct proof of Prop. \ref{lb109}. Do not use root test, ratio test, or any results about power series.
\end{sexe}


%% Record #4 2023/09/27   three lectures





\begin{df}\label{lb107}
By \eqref{eq33}, the power series \index{exp@$e^z=\exp(z)$}
\begin{align*}
\exp(z)\equiv e^z=\sum_{n=0}^\infty \frac{z^n}{n!}
\end{align*}
has radius of convergence $+\infty$, and hence converges absolutely on $\Cbb$. (In particular, $\lim_{n\rightarrow\infty} z^n/n!=0$ for all $z\in\Cbb$.) This gives a function $\exp:\Cbb\rightarrow\Cbb$, called the \textbf{exponential function}. \index{00@Exponential function} 
\end{df}















Part (a) of Prop. \ref{lb108} can be strengthened in the following way.

\begin{thm}\label{lb112}
Let $\sum v_n z^n$ be a power series with coefficients in a complex Banach space $V$. Let $R$ be its radius of convergence, and assume that $0<R\leq+\infty$. For each $z\in B_\Cbb(0,R)$, let $f(z)$  denote the value of this series at $z$ (which is an element of $V$). Then $f:B_\Cbb(0,R)\rightarrow V$ is continuous. Moreover,  for each $0<\rho<R$, the series of functions $\sum v_n z^n$ converges uniformly on $\ovl B_\Cbb(0,\rho)$ to $f$. 
\end{thm}

Note that by calling $\sum v_n z^n$ a series of functions, we understand each term $v_nz^n$ as a function $\Cbb\rightarrow V$.

\begin{proof}
For each $0<\rho<R$, let $X_\rho=\ovl B_\Cbb(0,\rho)$. Then $X_\rho$ is clearly a bounded closed subset of $\Cbb$, and hence is sequentially compact by Heine-Borel Thm. \ref{lb98}. Let $g_n=v_nz^n$, which is a continuous function $X_\rho\rightarrow V$. We view $g_n$ as an element of the Banach space (cf. Cor. \ref{lb101}) $C(X_\rho,V)$. Then $\lVert g_n\lVert_\infty=\rho^n\lVert v_n\lVert$. Thus
\begin{align*}
\limsup_{n\rightarrow\infty}\sqrt[n]{\lVert g_n\lVert_\infty}=\limsup_{n\rightarrow\infty}\rho\sqrt[n]{\lVert v_n\lVert}=\rho/R<1
\end{align*}
Therefore, by root test, the series $\sum g_n$ converges in the Banach space $C(X_\rho,V)$ to some $f_\rho\in C(X_\rho,V)$.

We have proved that for each $0<\rho<R$, the series of functions $\sum v_nz^n$ converges uniformly on $X_\rho$ to a continuous function $f_\rho$. Let $f:B_\Cbb(0,R)\rightarrow V$ whose value at each $z$ is the value of the original series at $z$. Thus, if $|z|\leq\rho$, then $f_\rho(z)=f(z)$. Namely, $f|_{X_\rho}=f_\rho$. This shows that $\sum v_nz^n$ converges uniformly on $X_\rho$ to $f$. It also shows that $f|_{B_\Cbb(0,\rho)}$ is continuous (because $f_\rho$ is continuous). Therefore, since $B_\Cbb(0,R)$ is covered by all open disks $B_\Cbb(0,\rho)$ (where $0<\rho<R$), we conclude from Lem. \ref{lb30} that $f$ is continuous on $B_\Cbb(0,R)$.
\end{proof}

\begin{eg}\label{lb214}
By Thm. \ref{lb112}, the exponential function $\exp:\Cbb\rightarrow\Cbb$ is continuous; moreover, $\sum_{n=0}^\infty z^n/n!$ converges uniformly to $e^z$ on $\ovl B_{\Cbb}(0,R)$ for every $0<R<+\infty$, and hence on every bounded subset of $\Cbb$.
\end{eg}













\begin{comment}

\subsection{Problems and supplementary material}

\begin{sprob}
Consider a power series $\sum a_nz^n$ where $a_n\in\Rbb_{\geq 0}$ for each $n$. Let $R$ be its radius of convergence. Prove that the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\sum a_n<+\infty$.
\item $\sum a_nz^n$ converges uniformly on $\ovl B_\Cbb(0,R)$ to a continuous function.
\item $\sum a_nz^n$ converges uniformly on $B_\Cbb(0,R)$.
\end{enumerate}
\end{sprob}
\end{comment}


\newpage


\section{Nets and discrete integrals}


\subsection{Introduction: why do we need nets?}



Nets were introduced by Moore and Smith in 1922 as a generalization of sequences. The most well-known motivation for introducing nets is that sequences are not enough for the study of non-metrizable topological spaces (i.e. topological spaces whose topologies are not induced by metrics). Here are two examples:
\begin{itemize}
\item In a general topological space, the definition of continuous maps using sequential convergence (as in Def. \ref{lb31}-(1)) is weaker than the definition using interior points and open sets (as in Def. \ref{lb31}-(2'), see also Rem. \ref{lb109}). Therefore, the dynamic intuition of sequences is not equivalent to the static intuition of open sets. 
\item Some important  topological spaces are compact (i.e. every open cover has a finite subcover) but not sequentially compact. $[0,1]^I$ (where $I$ is uncountable), equipped with the ``product topology" (i.e. ``pointwise convergence topology"), is such an example. 
\end{itemize}
As we shall see, nets provide a remedy for these issues: For a general topological space, the definition of continuity using net convergence is equivalent to that using open sets; compactness is equivalent to ``net-compactness", where the latter means that every net has a convergent subset. Thus, by generalizing sequences to nets, the dynamic intuition and the static and geometric intuition are unified again.


Nevertheless, the most common topological spaces appearing in analysis are metrizable. This raises the question: Why should we care about nets, given that our primary interest is in metrizable topological spaces? Here is my answer: Even though we are mainly interested in metrizable spaces, we can still find nets helpful in the following aspects. 

First of all, many convergence processes cannot be described by sequential convergence, but can be described by net convergence. For example, the following limits can be formulated and understood in the language of net convergence:
\begin{enumerate}[label=(\arabic*)]
\item The limit of a function $\lim_{x\rightarrow x_0}f(x)$ where $f:X\rightarrow Y$ is a map of metric spaces and $x_0\in X$.
\item The limit $\dps\lim_{m,n\rightarrow\infty}a_{m,n}$ where $(a_{m,n})_{m,n\in\Zbb_+}$ is a \textbf{double sequence} in a metric space $X$. Note that this is not the same as (but is more natural than) the \textbf{iterated limit} $\dps\lim_{n\rightarrow\infty}\lim_{m\rightarrow\infty} a_{m,n}$. Moreover, the limit $\dps\lim_{m,n\rightarrow\infty}a_{m,n}$ is the key to understanding the problem of commutativity of iterated integrals:
\begin{align*}
\lim_{n\rightarrow\infty}\lim_{m\rightarrow\infty} a_{m,n}\xlongequal{?}\lim_{m\rightarrow\infty}\lim_{n\rightarrow\infty} a_{m,n}
\end{align*}
\item The Riemann integral $\int_a^b f(x)dx$. This is the limit of the Riemann sum $\lim\sum f(\xi_i)(a_i-a_{i-1})$  as the partition of the interval $[a,b]$ is getting finer and finer.
\end{enumerate} 
Moreover, as for (3), we shall see that the net version of Cor. \ref{lb113} provides a quick and conceptual proof of the following fact: If the upper and lower Darboux integrals are equal, then the Riemann integral exists and are equal to the two Darboux integrals. Indeed, the upper and lower Darboux integrals are respectively the $\limsup$ and $\liminf$ of a net in $\Rbb$.

Second, nets provide a conceptual solution to many problems about \textbf{double series}. Let $(a_{m,n})_{m,n\in\Zbb_+}$ be a double sequence in $\Rbb$. Think about the following questions, which arise naturally when one is trying to prove $e^ze^w=e^{z+w}$.
\begin{enumerate}[label=(\alph*)]
\item When is it true that $\dps \sum_{n=1}^\infty\sum_{m=1}^\infty a_{m,n}=\sum_{m=1}^\infty\sum_{n=1}^\infty a_{m,n}$ ?
\item Since $\card(\Zbb_+\times\Zbb_+)=\card(\Zbb_+)$, why not use an ordinary series to study a double series? So let us \textbf{parametrize} $\Zbb_+\times\Zbb_+$ by $\Zbb_+$: choose a bijection $\varphi:\Zbb_+\rightarrow \Zbb_+\times\Zbb_+$. When is it true that $\dps  \sum_{k=1}^\infty a_{\varphi(k)}=\sum_{n=1}^\infty\sum_{m=1}^\infty a_{m,n}$ ?
\item Choose another parametrization (i.e. bijection) $\psi:\Zbb_+\rightarrow\Zbb_+\times\Zbb_+$. When is it true that $\dps  \sum_{k=1}^\infty a_{\varphi(k)}=\sum_{k=1}^\infty a_{\psi(k)}$ ?
\item More generally, let $X$ be a countably infinite set, and let $f:X\rightarrow\Rbb$. Intuitively, we can take an infinite sum $\dps\sum_{x\in X}f(x)$. How to define it rigorously? One may think about choosing a parametrization, i.e., a bijection $\varphi:\Zbb_+\rightarrow X$. Then one defines the infinite sum by $\sum_{k=1}^\infty f(\varphi(k))$. Is this definition independent of the choice of parametrization?
\item As a special case of (d), when is a series invariant under \textbf{rearrangement}? Namely, choose a bijection $\varphi:\Zbb_+\rightarrow\Zbb_+$, and choose a sequence $(a_n)$ in $\Rbb$, when is it true that $\dps\sum_{n=1}^\infty a_n=\sum_{n=1}^\infty a_{\varphi(n)}$ ?
\end{enumerate}

Modern differential geometry (whose ``intrinsic" spirit stems from Gauss's Theorema Egregium) teaches us that in order to answer these questions, one should first define the infinite sum $\sum_{x\in X}f(x)$ in a parametrization-independent way. (The reason we call a bijection $\varphi:\Zbb_+\rightarrow X$ a parametrization is that we want the readers to compare it with the parametrizations of curves, surfaces, and more generally manifolds.)  We will call this sum a \textbf{discrete integral}. Then, one tries to answer when this definition agrees with those that depend on parametrizations (such as the sums in (a)-(e) above). These goals can be achieved with the help of nets.



\subsection{Nets}\label{lb147}


\subsubsection{Directed sets and nets}

\begin{df}\label{lb116}
A relation $\leq$ on a set $I$ is called a \textbf{preorder} \index{00@Preorder, and preordered set} if for all $\alpha,\beta,\gamma\in I$, the following are satisfied:
\begin{itemize}
\item (Reflexivity) $\alpha\leq \alpha$.
\item (Transitivity) If $\alpha\leq \beta$ and $\beta\leq \gamma$ then $a\leq \gamma$.
\end{itemize}
The pair $(I,\leq)$ (or simply $I$) is called a \textbf{preordered set}.
\end{df}

Therefore, a partial order is a preorder satisfying antisymmetry: $(\alpha\leq \beta)\land(\beta\leq\alpha)\Rightarrow (\alpha=\beta)$.

\begin{df}
A preordered set $(I,\leq)$ is called a \textbf{directed set} \index{00@Direct set} if 
\begin{align}
\forall\alpha,\beta\in I~~~\exists\gamma\in I~~\text{ such that }\alpha\leq \gamma,\beta\leq\gamma  \label{eq37}
\end{align}
If $I$ is a directed set and $X$ is a set, then a function $x:I\rightarrow X$ is called a \textbf{net} \index{00@Net $(x_\alpha)_{\alpha\in I}$} with directed set/index set $I$. We often write $x(\alpha)$ as $x_\alpha$ if $\alpha\in I$, and write $x$ as $(x_\alpha)_{\alpha\in I}$.
\end{df}


\begin{eg}
$(\Zbb_+,\leq)$ is a directed set. A net with index set $\Zbb_+$ in a set $X$ is precisely a sequence in $X$.
\end{eg}

\begin{df}\label{lb117}
Suppose that $(I,\leq_I )$ and $(J,\leq_J)$ are preordered set (resp. directed set), then the \textbf{product} \index{00@Product preordered/directed set} $(I\times J,\leq)$ is a preordered set (resp. directed set) if for every $\alpha,\alpha'\in I,\beta,\beta'\in J$ we define
\begin{align}\label{eq36}
(\alpha,\beta)\leq (\alpha',\beta')\qquad\Longleftrightarrow\qquad \alpha\leq_I \alpha'~~\text{and}~~\beta\leq_J\beta'
\end{align}
Unless otherwise stated, the preorder on $I\times J$ is assumed to be defined by \eqref{eq36}.
\end{df}


\begin{eg}
$\Zbb_+\times\Zbb_+$ (or similarly, $\Nbb\times\Nbb$) is naturally a directed set whose preorder is defined by \eqref{eq36}. A net $(x_{m,n})_{(m,n)\in\Zbb_+\times\Zbb_+}$ with index set $\Zbb_+\times\Zbb_+$ is called a \textbf{double sequence} \index{00@Double sequence} and is written as $(x_{m,n})_{m,n\in\Zbb_+}$ or simply $(x_{m,n})$. (We will even write it as $(x_{mn})$ when no confusion arises.)

More generally, we call $(x_{\alpha,\beta})_{(\alpha,\beta)\in I\times J}=(x_{\alpha,\beta})_{\alpha\in I,\beta\in J}$ a \textbf{double net} \index{00@Double net} if its index set is $I\times J$ for some directed sets $I,J$. \hfill\qedsymbol
\end{eg}




\begin{eg}\label{lb130}
If $X$ is a set, then $(2^X,\subset)$ and $(\fin(2^X),\subset)$ \index{fin@$\fin(2^X)$} are directed sets where
\begin{align}
\fin(2^X)=\{A\subset X:A\text{ is a finite set}\}
\end{align}
We will use nets with index set $\fin(2^X)$ to study infinite sums.
\end{eg}


\begin{eg}
Let $X$ be a metric space and $x\in X$. Then $X_x=(X,\leq)$ is a directed set if for each $p_1,p_2\in X$ we define
\begin{align}
p_1\leq p_2~~\text{in }X_x\qquad\Longleftrightarrow\qquad d(p_1,x)\geq d(p_2,x)
\end{align}
(Namely, a larger element of $X_x$ is one closer to $x$.) Nets with this directed set can be used to study the limits of functions (cf. Rem. \ref{lb269}). Note that $X_x$ is our first example of directed set which is not a poset! ($d(p_1,x)=d(p_2,x)$ does not imply $p_1=p_2$.)
\end{eg}


\subsubsection{Limits of nets}

If $I$ is an preordered set and $\beta\in I$, we write \index{I@$I_{\geq\beta}$}
\begin{gather}
I_{\geq\beta}=\{\alpha\in I:\alpha\geq\beta\}
\end{gather}
\begin{df}
Let $P$ be a property about elements of a set $X$, i.e., $P$ is a function $X\rightarrow\{\text{true, false}\}$. Let $(x_\alpha)_{\alpha\in I}$ be a net in $X$. 

We say that $x_\alpha$ \textbf{eventually} \index{00@Eventually} satisfies $P$ (equivalently, we say that $x_\alpha$ satisfies $P$ for \textbf{sufficiently large} \index{00@Sufficiently large} $\alpha$) if:
\begin{itemize}
\item There exists $\beta\in I$ such that for every $\alpha\in I_{\geq\beta}$, the element $x_\alpha$ satisfies $P$.
\end{itemize}
``Sufficiently large" is also called ``\textbf{large enough}". \index{00@Large enough=sufficiently large}

We say that $x_\alpha$ \textbf{frequently} \index{00@Frequently} satisfies $P$ if:
\begin{itemize}
\item For every $\beta\in I$ there exists $\alpha\in I_{\geq\beta}$ such that $x_\alpha$ satisfies $P$.
\end{itemize}
\hfill\qedsymbol
\end{df}


\begin{rem}
Note that unlike sequences, for a general net, ``$x_\alpha$ eventually satisfies $P$" does not imply ``all but finitely many $x_\alpha$ satisfy $P$" because the complement of $I_{\geq\beta}$ is not necessarily a finite set.  
\end{rem}


\begin{rem}
Let $P$ and $Q$ be two properties about elements of $X$. Then
\begin{subequations}
\begin{gather}
\neg(\text{$x_\alpha$ eventually satisfies $P$})~~=~~(\text{$x_\alpha$ frequently satisfies $\neg P$})
\end{gather}
By the crucial condition \eqref{eq37} for directed sets, we have
\begin{gather}\label{eq38}
\begin{gathered}
(x_\alpha\text{ eventually satisfies }P)\land(x_\alpha\text{ eventually satisfies }Q)\\
\Downarrow \\
x_\alpha\text{ eventually satisfies }P\land Q
\end{gathered}
\end{gather}
By taking contraposition and replacing $P,Q$ by $\neg P,\neg Q$, we have
\begin{gather}\label{eq103}
\begin{gathered}
x_\alpha\text{ frequently satisfies }P\lor Q\\
\Downarrow\\
(x_\alpha\text{ frequently satisfies }P)\lor(x_\alpha\text{ frequently satisfies }Q)
\end{gathered}
\end{gather}
\end{subequations}
\end{rem}

\begin{df}\label{lb174}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a metric space $X$. Let $x\in X$. We say that $(x_\alpha)$ \textbf{converges to} $x$ and write \index{lim@$\lim_{\alpha\in I}x_\alpha\equiv \lim_\alpha x_\alpha$}
\begin{align*}
\lim_{\alpha\in I}x_\alpha\equiv \lim_\alpha x_\alpha=x
\end{align*}
or simply $x_\alpha\rightarrow x$ if the following statement holds:
\begin{itemize}
\item For every $\eps>0$, $x_\alpha$ is eventually in $B_X(x,\eps)$.
\end{itemize}
Clearly, $x_\alpha\rightarrow x$ iff $d(x_\alpha,x)\rightarrow 0$.
\end{df}

\begin{df}
Let $(x_{m,n})_{m,n\in\Zbb_+}$ be a double sequence in a metric space. Then we write
\begin{align}
\lim_{(m,n)\in\Zbb_+\times\Zbb_+} x_{m,n}\equiv \lim_{m,n\rightarrow\infty} x_{m,n}
\end{align}
and call it the \textbf{(double) limit} \index{00@Double limit} of $(x_{m,n})$.
\end{df}

\begin{rem}
Let us spell out the meaning of $\lim_{m,n\rightarrow\infty} x_{m,n}=x$: For each $\eps>0$ there exists $M,N\in\Zbb_+$ such that $d(x_{m,n},x)<\eps$ for all $m\geq M$ and $n\geq N$. Clearly, this is equivalent to the statement:
\begin{itemize}
\item For each $\eps>0$ there exists $N\in\Zbb_+$ such that $d(x_{m,n},x)<\eps$ for all $m,n\geq N$.
\end{itemize}
Therefore, if $(x_n)$ is a sequence in $X$, then
\begin{align}
\text{$(x_n)$ is a Cauchy sequence}\qquad\Longleftrightarrow\qquad \lim_{m,n\rightarrow\infty} d(x_m,x_n)=0
\end{align}
Thus, the Cauchyness of sequences can be studied in terms of double limits, and hence in terms of nets.
\end{rem}




\begin{pp}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a metric space $X$ converging to $x,y$. Then $x=y$.
\end{pp}

\begin{proof}
Suppose that $x\neq y$. Then there are $r,\rho>0$ such that $B(x,r)\cap B(y,\rho)=\emptyset$, say $r=\rho=d(x,y)/2$. Since $x_\alpha\rightarrow x$, the point $x_\alpha$ is eventually in $B(x,r)$. Since $x_\alpha\rightarrow y$, the point $x_\alpha$ is eventually in $B(y,\rho)$. Therefore, by the logic \eqref{eq38}, $x_\alpha$ is eventually in $B(x,r)\cap B(y,\rho)$, impossible.
\end{proof}

\begin{thm}\label{lb121}
Let $f:X\rightarrow Y$ be map of metric spaces continuous at $x\in X$. Let $(x_\alpha)_{\alpha\in I}$ be a net in $X$ converging to $x$. Then  $\dps\lim_\alpha f(x_\alpha)=f(x)$.
\end{thm}

\begin{proof}
Choose any $\eps>0$. By Def. \ref{lb31}-(2) and the continuity of $f$ at $x$, there exists $\delta>0$ such that for all $p\in B(x,\delta)$ we have $f(p)\in B(f(x),\eps)$. Since $x_\alpha\rightarrow x$, $x_\alpha$ is eventually in $B(x,\delta)$. Therefore $f(x_\alpha)$ is eventually in $B(f(x),\eps)$.
\end{proof}


This theorem implies, for example, that if $(v_\alpha)$ is a net in a complex normed vector space converging to $v$, and if $(\lambda_\alpha)$ is a net in $\Cbb$ converging to $\lambda$, then $\lambda_\alpha v_\alpha$ converges to $\lambda v$ because the scalar multiplication map is continuous (Prop. \ref{lb82}).



\begin{exe}\label{lb123}
Prove the generalization of Rem. \ref{lb58}:
\begin{enumerate}
\item If $(x_\alpha)_{\alpha\in I},(y_\alpha)_{\alpha\in I}$ are nets in $\ovl\Rbb$ converging to $A,B\in\ovl\Rbb$, and if $x_\alpha\leq y_\alpha$ for all $\alpha$, then $A\leq B$.
\item \textbf{Squeeze theorem}: \index{00@Squeeze theorem} Suppose that $(x_\alpha)_{\alpha\in I},(y_\alpha)_{\alpha\in I},(z_\alpha)_{\alpha\in I}$ are nets in $\ovl\Rbb$, $x_\alpha\leq y_\alpha\leq z_\alpha$ for all $\alpha$, and $x_\alpha$ and $z_\alpha$ both converge to $A\in\ovl\Rbb$. Then $y_\alpha\rightarrow A$.
\item If $(x_\alpha)$ is an increasing resp. decreasing net in $\ovl\Rbb$, then $\lim_\alpha x_\alpha$ exists in $\ovl\Rbb$ and equals $\sup_\alpha x_\alpha$ resp. $\inf_\alpha x_\alpha$.
\end{enumerate}
\end{exe}








\subsubsection{Subnets (in the sense of Willard)}




\begin{df}
A subset $E$ of a directed set $I$ is called \textbf{cofinal} \index{00@Cofinal subset} if:
\begin{align*}
\forall\alpha\in I~~~\exists\beta\in E~~~\text{such that }\alpha\leq\beta
\end{align*}
By the transitivity in Def. \ref{lb116} and property \eqref{eq37}, we clearly have
\begin{align*}
\forall\alpha_1,\dots,\alpha_n\in I~~~\exists\beta\in E~~~\text{such that }\alpha_1\leq\beta,\dots,\alpha_n\leq\beta
\end{align*}
\end{df}



\begin{df}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a set $X$. A \textbf{subnet} \index{00@Subnet} of $(x_\alpha)_{\alpha\in I}$ is, by definition, of the form $(x_{\alpha_s})_{s\in S}$ where $S$ is a directed set, and
\begin{align*}
(\alpha_s)_{s\in S}:S\rightarrow I\qquad s\mapsto \alpha_s
\end{align*}
is an increasing function whose range $\{\alpha_s:s\in S\}$ is cofinal in $I$.
\end{df}



\begin{rem}
There are several different definitions of subnets that are equivalent for proving the main results in point-set topology. Unfortunately, there is no common agreement on the standard definition of subnets. The definition we gave is due to Willard \cite{Wil}, and is also the one given in the famous textbook of Munkres \cite{Mun}. Some famous analysis and topology textbooks (e.g. \cite{Fol,Kel,RS}) use a weaker definition, which does not assume that the map $S\rightarrow I$ is increasing.
\end{rem}




\begin{eg}
A subsequence is a subnet.
\end{eg}

\begin{eg}\label{lb118}
Let $(x_{m,n})_{m,n\in\Zbb_+}$ be a net with index set $\Zbb_+\times\Zbb_+$. Then $(x_{k,k})_{k\in\Zbb_+}$ and $(x_{2k,k})_{k\in\Zbb_+}$ are subnets. $(x_{k,1})_{k\in\Zbb_+}$ is not a subnet, because the cofinal condition is not satisfied. More generally, it is not hard to show that for every function $\varphi,\psi:\Zbb_+\rightarrow\Zbb_+$, $(x_{\varphi(k),\psi(k)})_{k\in\Zbb_+}$ is a subnet iff $\varphi,\psi$ are increasing and $\lim_{k\rightarrow\infty}\varphi(k)=\lim_{k\rightarrow\infty}\psi(k)=+\infty$.
\end{eg}

\begin{exe}
Prove the following facts:
\begin{itemize}
\item The cofinal subset of a cofinal subset of a directed set $I$ is a cofinal subset of $I$.
\item  The subnet of a subnet of a net $(x_\alpha)$ is a subnet of $(x_\alpha)$. 
\end{itemize}
Note that in your proof you need to use the transitivity in Def. \ref{lb116}.
\end{exe}


The biggest difference between subnets and subsequences is that the index set of a subnet is not necessarily a subset of the index set of the original net. Indeed, subnets are defined in this way mainly because we want to have a net version of Pb. \ref{lb64} in any topological space. (This will be achieved in Pb. \ref{lb223}.) Let us see an elementary example of subnet whose index set is larger than that of the original net. Its importance is justified by the proofs of Exp. \ref{lb125} and Prop. \ref{lb126}.


\begin{eg}\label{lb124}
Let $J$ be a directed set. Then every net $(x_\alpha)_{\alpha\in I}$ has subnet $(x_\alpha)_{(\alpha,\beta)\in I\times J}$. The corresponding increasing map of directed sets is the projection $I\times J\rightarrow I$ onto the first component. 
\end{eg}






To appreciate the importance of cofinalness (as well as transitivity), we prove the following generalization of Prop. \ref{lb23}. This result has a wide range of surprising applications that are unavailable when one only considers sequences. (We will see them soon in this chapter. For instance, this result explains why the values of absolutely convergent series are invariant under rearrangement.) So I call this result a theorem, even though its proof is simple.


\begin{thm}\label{lb120}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a metric space (or more generally, a topological space) $X$ converging to $x\in X$. Then every subnet $(x_{\alpha_s})_{s\in S}$ converges to $x$.
\end{thm}

The following proof for metric spaces can be generalized straightforwardly to topological spaces. The readers can come back and check the details after learning topological spaces.

\begin{proof}
Choose any $\eps>0$. Since $x_\alpha\rightarrow x$, there exists $\beta\in I$ such that for all $\alpha\geq\beta$ we have $d(x_\alpha,x)<\eps$. By the cofinalness, there exists $t\in S$ such that $\alpha_t\geq\beta$. Thus, since $s\in S\mapsto \alpha_s\in I$ is increasing, for every $s\geq t$, we have $\alpha_s\geq\alpha_t\geq\beta$ and hence $\alpha_s\geq\beta$ by the transitivity in Def. \ref{lb116}. So $d(x_{\alpha_s},x)<\eps$ for all $s\geq t$. This finishes the proof.
\end{proof}

This proposition does not hold if one does not assume cofinalness in the definition of subnets:

\begin{eg}
Let $(x_n)$ be a sequence in $\Rbb$ converging to $x\in\Rbb$. Since $(x_n)$ is a Cauchy sequence, we know that $\lim_{m,n\rightarrow\infty}x_m-x_n=0$. We have seen in Exp. \ref{lb118} that $(x_{2k}-x_k)_{k\in\Zbb_+}$ is a subnet of $(x_{m,n})$. Therefore, $\lim_{k\rightarrow\infty} x_{2k}-x_k=0$. But $(x_k-x_1)_{k\in\Zbb_+}$ is not a subnet since the cofinal condition is not satisfied. And if $x\neq x_1$, then $\lim_k (x_k-x_1)=x-x_1\neq 0$, i.e.,
\begin{align*}
\lim_{k\rightarrow\infty}(x_k-x_1)\neq \lim_{m,n\rightarrow\infty} (x_m-x_n)
\end{align*}
\end{eg}


In Subsec. \ref{lb119}, we have seen two criteria for the divergence of sequence: a sequence diverges if it is unbounded, or if it has two subsequences converging to different points. By Thm. \ref{lb120}, the second criterion can be generalized to nets. However, the following example shows that the first criterion does not has its net version:


%% Record #5 2023/10/7 two lectures 



\begin{eg}
A convergent net $(x_\alpha)_{\alpha\in I}$ in a metric space $X$ is not necessarily \textbf{bounded}. Namely, it is not necessarily true that $\{x_\alpha:\alpha\in I\}$ is a bounded subset of $X$. Let $f:\Rbb_{>0}\rightarrow\Rbb$ be $f(x)=1/x$. Then $f$ is net in $\Rbb$ with directed set $(\Rbb_{> 0},\leq)$. This net is not bounded, although $\lim f(x)=0$.
\end{eg}





\begin{eg}
The double sequence $x_{m,n}=n/(m+n)$ in $\Rbb$ has subnets $x_{n,n}=n/(n+n)=1/2$ and $x_{2n,n}=1/3$. Since these two subnets converge to different values, Thm. \ref{lb120} implies that $\lim_{m,n}x_{m,n}$ does not exist. However, the \textbf{iterated limits}\index{00@Iterated limit} exist and take different values:
\begin{gather*}
\lim_{m\rightarrow\infty}\lim_{n\rightarrow\infty}\frac{n}{m+n}=1\qquad \lim_{n\rightarrow\infty}\lim_{m\rightarrow\infty}\frac{n}{m+n}=0
\end{gather*}
As we shall see, this gives another criterion for the divergence of double series: If the two iterated limits exist and are different, then the double series diverge.
\end{eg}


Finally, we do an example of convergent double sequence:

\begin{eg}\label{lb125}
Let $\dps x_{m,n}=(m^{-2}-n^{-1})\sin\frac{\pi(m+\sqrt n)}{4}$. Then $\dps\lim_{m,n\rightarrow\infty}x_{m,n}=0$.
\end{eg}

\begin{proof}
The sequence $(m^{-2})_{m\in\Zbb_+}$ converges to $0$. By Exp. \ref{lb124}, the double sequence $(m^{-2})_{m,n\in\Zbb_+}$ is its subnet, and hence converges to $0$ by Thm. \ref{lb120}. Similarly, the double sequence $(n^{-1})_{m,n\in\Zbb_+}$ converges to $0$. Therefore, $m^{-2}+n^{-1}$ converges to $0$ due to Thm. \ref{lb121} and the continuity of the addition map $(x,y)\in\Rbb^\mapsto x+y\in\Rbb$ (Prop. \ref{lb41}). Since $0\leq |x_{m,n}|\leq m^{-2}+n^{-1}$, we conclude $|x_{m,n}|\rightarrow 0$ (and hence $x_{m,n}\rightarrow0$) by squeeze theorem (Exe. \ref{lb123}).
\end{proof}







\subsubsection{Double limits and iterated limits}

\begin{thm}\label{lb122}
Let $(x_{\alpha,\beta})_{\alpha\in I,\beta\in J}$ be a double net in a metric space $X$. Assume that the following are true:
\begin{enumerate}[label=(\arabic*)]
\item The limit $\dps\lim_{(\alpha,\beta)\in I\times J}x_{\alpha,\beta}$ exists in $X$.
\item For each $\alpha\in I$, the limit $\dps\lim_{\beta\in J}x_{\alpha,\beta}$ exists in $X$.
\end{enumerate}
Then the LHS limit in the following equation exists and equals the RHS:
\begin{align}
\lim_{\alpha\in I}\lim_{\beta\in J}x_{\alpha,\beta}=\lim_{(\alpha,\beta)\in I\times J}x_{\alpha,\beta}
\end{align}
In particular, suppose that the following is also true:
\begin{itemize}
\item[(3)] For each $\beta\in J$, the limit $\dps\lim_{\alpha\in J}x_{\alpha,\beta}$ exists in $X$. 
\end{itemize}
Then the following limits exist and are equal:
\begin{align}
\lim_{\alpha\in I}\lim_{\beta\in J}x_{\alpha,\beta}=\lim_{\beta\in J}\lim_{\alpha\in I}x_{\alpha,\beta}
\end{align}
\end{thm}


\begin{proof}
Let $\dps x_\alpha=\lim_\beta x_{\alpha,\beta}$ and $\dps x=\lim_{\alpha,\beta}x_{\alpha,\beta}$. We want to show that $\dps\lim_\alpha x_\alpha=x$. Choose any $\eps>0$. Then there exist $A\in I,B\in J$ such that for every $\alpha\geq A$ and $\beta\geq B$ we have $d(x_{\alpha,\beta},x)<\eps/3$. In particular,  $d(x_{\alpha,\beta},x)\leq \eps/2$. Using Thm. \ref{lb121} and the fact that $p\in X\mapsto d(p,x)\in\Rbb$ is continuous (Exp. \ref{lb45}), we see that for every $\alpha\geq A$ we have $\dps d(x_\alpha,x)=\lim_{\beta\in J_{\geq B}} d(x_{\alpha,\beta},x)\leq \eps/2<\eps$.
\end{proof}

The readers may skip the next remark and proof and come back to them when they have learned about topological spaces.

\begin{srem}
Thm. \ref{lb122} can be generalized to the case that $X$ is a regular topological space. By saying that the topological space $X$ is \textbf{regular}, \index{00@Regular topological space} we mean that for every $x\in X$ and every open set $U$ containing $x$, there is a smaller open set $V$ containing $x$ such that the closure $\ovl V$  (cf. Def. \ref{lb183}) is contained in $U$.
\end{srem}

\begin{proof}[$\star$ Proof]
Let $\dps x_\alpha=\lim_\beta x_{\alpha,\beta}$ and $\dps x=\lim_{\alpha,\beta}x_{\alpha,\beta}$. Choose any open set $U$ containing $x$. We want to prove that $x_\alpha$ is eventually in $U$. Choose an open set $V$ containing $x$ such that $\ovl V\subset U$. Then there are $A\in I,B\in J$ such that for all $\alpha\geq A$ and $\beta\geq B$ we have $x_{\alpha,\beta}\in V$. Thus, for each $\alpha\geq A$, since $x_{\alpha,\beta}$ approaches $x_\alpha$, we have $x_\alpha\in \ovl V$ and hence $x_\alpha\in U$.
\end{proof}


\begin{co}
Let  $(x_{\alpha,\beta})_{\alpha\in I,\beta\in J}$ be a double net in $\ovl\Rbb$. Assume that $x_{\blt,\blt}$ is increasing, i.e., $x_{\alpha,\beta}\leq x_{\alpha',\beta'}$ if $\alpha\leq\alpha'$ and $\beta\leq \beta'$. Then the following equation \eqref{eq39} hold, where all the limits \eqref{eq39} exist in $\ovl\Rbb$:
\begin{align}
\lim_{\alpha\in I}\lim_{\beta\in J}x_{\alpha,\beta}=\lim_{\beta\in J}\lim_{\alpha\in I}x_{\alpha,\beta}=\lim_{(\alpha,\beta)\in I\times J}x_{\alpha,\beta}=\sup\{x_{\alpha,\beta}:\alpha\in I,\beta\in J\}  \label{eq39}
\end{align}
\end{co}

Clearly, a similar result holds for decreasing double nets in $\ovl\Rbb$.

\begin{proof}
By Exe. \ref{lb123}, the three limits $\dps\lim_\alpha x_{\alpha,\beta}$, $\dps\lim_\beta x_{\alpha,\beta}$, and $\dps\lim_{\alpha,\beta}x_{\alpha,\beta}$ exist in $\ovl\Rbb$. Therefore, by Thm. \ref{lb122}, the three limits in \eqref{eq39} exist and are equal. The last equality in \eqref{eq39} is also due to Exe. \ref{lb123}.
\end{proof}


\subsubsection{Cauchy nets}

\begin{df}
A net $(x_\alpha)_{\alpha\in I}$ in a metric space $X$ is called a Cauchy net \index{00@Cauchy net} if
\begin{align*}
\lim_{\alpha,\beta\in I}d(x_\alpha,x_\beta)=0
\end{align*}
Equivalently, this means that 
\begin{align}
\forall\eps>0~~~\exists \gamma\in I~~~\text{such that }\forall\alpha,\beta\geq\gamma~~~\text{we have }d(x_\alpha,x_\beta)<\eps
\end{align}
\end{df}

\begin{exe}
Show that the subnet of a Cauchy net is Cauchy.
\end{exe}


\begin{pp}\label{lb126}
A convergent net in a metric space is a Cauchy net.
\end{pp}

\begin{proof}
Let $(x_\alpha)_{\alpha\in I}$ converge to $x$ in a metric space $X$. Then $\lim_\alpha d(x_\alpha,x)=0$. Since $(d(x_\alpha,x))_{\alpha,\beta\in I}$ is a subnet (cf. Exp. \ref{lb124}), we have $\lim_{\alpha,\beta} d(x_\alpha,x)=0$ by Thm. \ref{lb120}. Similarly, we have $\lim_{\alpha,\beta}d(x,x_\beta)=0$. Since $0\leq d(x_\alpha,x_\beta)\leq d(x_\alpha,x)+d(x,x_\beta)$, by Squeeze theorem (Exe. \ref{lb123}) we have $\lim_{\alpha,\beta}d(x_\alpha,x_\beta)=0$.
\end{proof}

\begin{comment}
\begin{exe}
Prove Prop. \ref{lb126} directly using the definitions of convergent nets and Cauchy nets.
\end{exe}
\end{comment}


\begin{pp}\label{lb127}
Let $(x_\alpha)_{\alpha\in I}$ be a Cauchy net in a metric space $X$. Suppose that $(x_\alpha)_{\alpha\in I}$ has a convergent subnet $(x_{\alpha_s})_{s\in S}$ converging to $x\in X$. Then $(x_\alpha)_{\alpha\in I}$ converges to $x$.
\end{pp}

\begin{proof}
Choose any $\eps>0$. Since $(x_\alpha)$ is a Cauchy net, there exists $\gamma\in I$ such that $d(x_\alpha,x_\beta)\leq \eps$ for all $\alpha,\beta\geq \gamma$. Since $(\alpha_s)_{s\in S}$ has cofinal range, $\alpha_{s_0}\geq \gamma$ for some $s_0\in S$. Thus $\alpha_s\geq \gamma$ for all $s\geq s_0$ because $(\alpha_s)_{s\in S}$ is increasing and because of the transitivity in Def. \ref{lb116}. Thus, for every $\beta\geq\gamma$, $d(x_{\alpha_s},x_{\beta})\leq\eps$ for sufficiently large $s$. By taking limit over $s$ and using the continuity of $y\in X\mapsto d(y,x_\beta)$ as well as Thm. \ref{lb121}, we get $d(x,x_\beta)\leq \eps$ for all $\beta\geq\gamma$.
\end{proof}

\begin{df}\label{lb155}
Two nets $(x_\alpha)_{\alpha\in I}$ and $(y_\alpha)_{\alpha\in I}$ in a metric space $X$ are called \textbf{Cauchy-equivalent} \index{00@Cauchy-equivalent} if
\begin{align*}
\lim_{\alpha\in I}d(x_\alpha,y_\alpha)=0
\end{align*}
Two Cauchy nets are simply called \textbf{equivalent} if they are Cauchy-equivalent. It is not hard to see that Cauchy-equivalence is an equivalence relation (recall Def. \ref{lb156}) on $X^I$.
\end{df}


\begin{exe}\label{lb128}
Let $(x_\alpha)_{\alpha\in I}$ and $(y_\alpha)_{\alpha\in I}$ be nets in a metric space $X$. 
\begin{enumerate}
\item Assume that $(x_\alpha)_{\alpha\in I}$ and $(y_\alpha)_{\alpha\in I}$ are Cauchy-equivalent. Prove that $(x_\alpha)$ is a Cauchy net iff $(y_\alpha)$ is a Cauchy net.
\item Assume that $(x_\alpha)_{\alpha\in I}$ converges to $x$. Prove that $(y_\alpha)_{\alpha\in I}$ converges to $x$ iff $(x_\alpha)_{\alpha\in I}$ and $(y_\alpha)_{\alpha\in I}$ are Cauchy-equivalent.
\end{enumerate}
\end{exe}



\begin{thm}\label{lb129}
Every Cauchy net $(x_\alpha)_{\alpha\in I}$ in a complete metric space $X$ is convergent.
\end{thm}

We give a hint of the proof and leave the details to the readers as an exercise.

\begin{proof}[Hint]
Construct an increasing sequence $(\alpha_n)_{n\in\Zbb_+}$ in $I$ such that for every $\beta,\gamma\geq\alpha_n$ we have $d(x_\beta,x_\gamma)<1/n$. Prove that $(x_{\alpha_n})_{n\in\Zbb_+}$ is a Cauchy sequence, and hence converges to some $x\in X$. Prove that $(x_\alpha)_{\alpha\in I}$ converges to $x$. (Warning: $(x_{\alpha_n})_{n\in\Zbb_+}$ is not necessarily a subnet of $(x_\alpha)_{\alpha\in I}$.)
\end{proof}

\begin{comment}
\begin{proof}
Let $(x_\alpha)_{\alpha\in I}$ be a Cauchy net in a complete metric space $X$. Then $I\times \Nbb$ is a directed set. Define
\begin{align*}
S=\{(\alpha,n)\in I\times\Nbb:\forall \beta,\gamma\geq\alpha\text{ we have }d(x_\beta,x_\gamma)<1/n\}
\end{align*}
Then $(x_\alpha)_{(\alpha,n)\in S}$ is a subnet of $X$. Recall that every subnet of a Cauchy net is Cauchy. By Prop. \ref{lb127}, it suffices to show that the Cauchy net $(x_\alpha)_{(\alpha,n)\in S}$ converges.

For each $n\in\Zbb_+$, choose $\beta_n$ such that $(\beta_n,n)\in S$: the existence of $\beta_n$ is due to the Cauchyness of $(x_\alpha)_{\alpha\in I}$. Thus, if $(\alpha,n)\in S$, we have $d(x_\alpha,x_{\beta_n})<1/n$, and hence
\begin{align*}
\lim_{(\alpha,n)\in S}d(x_\alpha,x_{\beta_n})=0
\end{align*}
Therefore, by Exe. \ref{lb128}, it suffices to prove that the equivalent Cauchy net $(x_{\beta_n})_{(\alpha,n)\in S}$ is convergent. But this is a subnet of the sequence $(x_{\beta_n})_{n\in\Nbb}$. And clearly, the Cauchyness of $(x_{\beta_n})_{(\alpha,n)\in S}$ implies that of $(x_{\beta_n})_{n\in\Nbb}$. So the Cauchy sequence $(x_{\beta_n})_{n\in\Nbb}$ converges because $X$ is complete. So its subnet $(x_{\beta_n})_{(\alpha,n)\in S}$ converges.
\end{proof}
\end{comment}



\subsection{Discrete integrals $\sum_{x\in X}f(x)$}

In this section, we fix $V$ to be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$. We fix a (non-necessarily countable) set $X$. Note that if $f:X\rightarrow V$ is a function and $X$ is finite, then $\sum_{x\in X}f(x)$ can be understood in its most obvious way.



\begin{df}\label{lb131}
Let $f:X\rightarrow V$ be a map. The expression
\begin{align*}
\sum_{x\in X}f(x)
\end{align*}
(or simply $\sum_X f$) is called a \textbf{discrete integral}.\index{00@Discrete integral} If $v\in V$, we say that $\sum_{x\in X}f(x)$ equals (or \text{converges} to) $v$, if
\begin{align}
\lim_{A\in\fin(2^X)}\sum_{x\in A}f(x)=v
\end{align}
In this case, we write
\begin{align}
\sum_{x\in X}f(x)=v \label{eq40}
\end{align}
\end{df}

\begin{rem}
Recall from Exp. \ref{lb130} that $\fin(2^X)$ is the directed set of finite subsets of $2^X$. Its preorder is ``$\subset$". So \eqref{eq40} means more precisely that:
\begin{itemize}
\item For every $\eps>0$, there exists a finite set $B\subset X$ such that for every finite set $A$ satisfying $B\subset A\subset X$, we have $\lVert v-\sum_{x\in A}f(x)\lVert<\eps$.
\end{itemize}
\end{rem}


\begin{rem}
Discrete integrals are one of the most important and representative examples in Moore and Smith's original paper on nets (cf. \cite{MS22}), explaining why nets are called nets: Imagine an infinitely large \textit{fishing net} whose vertices form the set $X=\Zbb^2$. You grab the net with your hands and pull it up. As you pull it up, the lifted part $A\in\fin(2^X)$ becomes larger and larger.
\end{rem}


\begin{rem}\label{lb141}
One of the advantages of discrete integrals over series is that  discrete integrals are clearly invariant under rearrangement: For every bijection $\varphi:X\rightarrow X$, if one side of the following equation converges in $V$, then the other side converges, and the equation holds true:
\begin{align}
\sum_{x\in X}f(x)=\sum_{x\in X}f(\varphi(x))
\end{align} 
or simply $\sum_Xf=\sum_Xf\circ\varphi$.
\end{rem}


\begin{rem}\label{lb133}
Let us spell out what Cauchyness means for the net $(\sum_A f)_{A\in\fin(2^X)}$:
\begin{itemize}
\item[(1)] For every $\eps>0$, there exists a finite set $B\subset X$ such that for any finite sets $A_1,A_2$ satisfying $B\subset A_1\subset X,B\subset A_2\subset X$, we have
\begin{align*}
\Big\lVert \sum_{A_1\setminus A_2}f-\sum_{A_2\setminus A_1}f \Big\lVert<\eps
\end{align*} 
\end{itemize}
Note that the term inside the norm is $\sum_{A_1}f-\sum_{A_2}f$. This is also equivalent to:
\begin{itemize}
\item[(2)] For every $\eps>0$, there exists a finite set $B\subset X$ such that for any finite set $E\subset X\setminus B$, we have 
\begin{align*}
\Big\lVert \sum_Ef\Big\lVert<\eps
\end{align*}
\end{itemize}
We shall mainly use (2) as the Cauchy criterion for the convergence of $\sum_Xf$.
\end{rem}

\begin{proof}[Proof of the equivalence]
(2) follows from (1) by taking $A_1=B$ and $A_2=B\cup E$. (1) follows from (2) by taking $E_1=A_1\setminus A_2$ and $E_2=A_2\backslash A_1$ and then concluding $\lVert \sum_{E_1}f-\sum_{E_2}f\lVert<2\eps$.
\end{proof}





\begin{df}\label{lb132}
Let $g:X\rightarrow\ovl\Rbb_{\geq0}$ be a map. Note that the net $(\sum_A g)_{A\in\fin(2^X)}$ is increasing. Hence, its limit exists in $\ovl\Rbb$ and equals $\sup_{A\in\fin(2^X)}\sum_Ag$ (by Exe. \ref{lb123}). We write this as $\sum_Xg$, or more precisely:
\begin{align}
\sum_Xg\equiv\sum_{x\in X}g(x)\xlongequal{\mathrm{def}} \lim_{A\in \fin(2^X)}\sum_A g=\sup_{A\in \fin(2^X)}\sum_A g
\end{align}
We say that $\sum_Xg$ \textbf{converges} or \textbf{converges absolutely}, if $\sum_Xg<+\infty$. 
\end{df}

It is clear that $\sum_Xg<+\infty$ iff there exists $C\in\Rbb_{\geq0}$ such that $\sum_Ag<C$ for all $A\in\fin(2^X)$.


\begin{rem}
Note that when $g:X\rightarrow\Rbb_{\geq0}$, the convergence in Def. \ref{lb132} agrees with that in Def. \ref{lb131}. Therefore, Rem. \ref{lb133} still gives a Cauchy criterion for convergence.
\end{rem}






\begin{df}
Let $f:X\rightarrow V$. We say that $\sum_Xf$ \textbf{converges absolutely} \index{00@Absolutely convergent discrete integral} if 
\begin{align*}
\sum_{x\in X}\lVert f(x)\lVert<+\infty
\end{align*}
\end{df}



\begin{pp}\label{lb142}
Let $f:X\rightarrow V$. If $\sum_Xf$ converges absolutely, then it converges, and
\begin{align}
\Big\lVert \sum_{x\in X} f(x) \Big\lVert\leq\sum_{x\in X} \lVert f(x)\lVert \label{eq41}
\end{align}
We write this simply as $\lVert\sum_X f\lVert\leq\sum_X |f|$. (Recall Def. \ref{lb150}.)
\end{pp}

\begin{proof}
\eqref{eq41} clearly holds when $X$ is finite. In the general case, assume that $\sum_Xf $ converges absolutely. Then by Cauchy criterion (Rem. \ref{lb133}-(2)), for every $\eps>0$ there is $A\in \fin(2^X)$ such that for each finite $E\subset X\setminus A$ we have $\sum_E|f|<\eps$, and hence $\lVert \sum_E f\lVert<\eps$. Therefore $\sum_Xf$ converges by Cauchy criterion again.

By the continuity of the norm function $v\in V\mapsto \lVert v\lVert\in\Rbb_{\geq0}$, and by Thm. \ref{lb121}, we have
\begin{align*}
\Big\lVert \sum_X f \Big\lVert=\Big\lVert \lim_A \sum_Af \Big\lVert=\lim_A \Big\lVert \sum_Af \Big\lVert
\end{align*}
Since $\lVert \sum_A f\lVert\leq\sum_A|f|$, by Exe. \ref{lb123}, the above expression is no less than
\begin{align*}
\lim_A\sum_A|f|=\sum_X|f|
\end{align*}
\end{proof}

The following proposition gives another demonstration that discrete integrals are more natural than series. We leave the proof to the readers.

\begin{pp}\label{lb134}
Let $f:X\rightarrow\Rbb^N$ where $N\in\Zbb_+$. Then
\begin{align*}
\sum_{x\in X}f(x)~~\textrm{converges}\qquad\Longleftrightarrow\qquad \sum_{x\in X}f(x)~~\text{converges absolutely}
\end{align*}

\end{pp}

\begin{proof}[Hint]
Reduce to the case $N=1$. Consider $A=\{x\in X:f(x)\geq 0\}$ and $B=X\setminus A$. 
\end{proof}

When $\Rbb^N$ is replaced by an infinite-dimensional Banach space, the convergence of a discrete integral may not imply absolute convergence. See Pb. \ref{lb149}.







\subsection{Fubini theorem for discrete integrals}\index{00@Fubini theorem (for discrete integrals)}\label{lb138}

Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Let $X,Y$ be sets.

\begin{thm}[\textbf{Fubini theorem-A}]\label{lb135} 
Let $f:X\times Y\rightarrow V$. Assume that $\sum_{X\times Y}f$ converges. Then $\sum_Y f(x,\cdot)$ converges for each $x\in X$, and $\sum_X f(\cdot,y)$ converges for each $y\in Y$, and 
\begin{align}\label{eq45}
\sum_{x\in X}\sum_{y\in Y}f(x,y)=\sum_{y\in Y}\sum_{x\in X}f(x,y)=\sum_{(x,y)\in X\times Y}f(x,y)
\end{align}
where all discrete integrals converge in $V$.
\end{thm}

We abbreviate \eqref{eq45} to $\sum_X\sum_Yf=\sum_Y\sum_Xf=\sum_{X\times Y}f$.

\begin{proof}
For each $x\in X$, let $f_x(y)=f(x,y)$. Let us prove that $\sum_Y f_x$ converges.
Choose any $\eps>0$. Since $\sum_{X\times Y}f$ converges, by Cauchy criterion (Rem. \ref{lb133}-(2)), there exists a finite $S\subset X\times Y$ such that the sum of $f$ over any finite subset outside $S$ has norm $<\eps$. The projection $X\times Y\rightarrow Y$ maps $S$ to a finite set $B\subset Y$. Thus, for each finite $E\subset Y\setminus B$, we have $\lVert\sum_E f_x\lVert<\eps$ since $x\times E$ is outside $S$. Therefore $\sum_Y f_x$ converges. By the same reasoning, $\sum_Xf(\cdot,y)$ converges for all $y$.

Recall that $\sum_{X\times Y}f$ is the limit of the net $(\sum_S f)_{S\in\fin(2^{X\times Y})}$. This net has subnet
\begin{align*}
\Big(\sum_{(x,y)\in A\times B} f(x,y)\Big)_{A\in \mc I,B\in \mc J}\qquad \text{ where }\mc I=\fin(2^X)~~\mc J=\fin(2^Y)
\end{align*}
(Its index set is $\mc I\times\mc J$.) Thus, by Thm. \ref{lb120},
\begin{align}
\sum_{(x,y)\in X\times Y}f(x,y)=\lim_{A\in\mc I,B\in\mc J} \sum_{(x,y)\in A\times B} f(x,y) \label{eq43}
\end{align}
We are now going to use Thm. \ref{lb122} to show that
\begin{align}
\lim_{A\in\mc I,B\in\mc J} \sum_{(x,y)\in A\times B} f(x,y)=\lim_{A\in\mc I}\lim_{B\in\mc J} \sum_{(x,y)\in A\times B} f(x,y) \label{eq42}
\end{align}
where the RHS limit exists. For that purpose, we need to check for each $A\in\mc I$ the convergence of $\lim_B\sum_{(x,y)\in A\times B}f(x,y)$. Since $\sum_Yf_x$ converges, we have
\begin{align}
&\lim_{B\in\mc J}\sum_{(x,y)\in A\times B}f(x,y)=\lim_{B\in\mc J}\sum_{x\in A}\sum_{y\in B}f(x,y)\nonumber\\
=&\sum_{x\in A}\lim_{B\in\mc J}\sum_{y\in B}f(x,y)=\sum_{x\in A}\sum_{y\in Y}f(x,y)\label{eq44}
\end{align}
(Note that $\sum_A$ is a finite sum and hence commutes with $\lim_B$.) Thus, the assumption in Thm. \ref{lb122} ensuring \eqref{eq42} has now been proved true. So \eqref{eq42} is true. Moreover, combining \eqref{eq43}, \eqref{eq42}, \eqref{eq44}  together, we get
\begin{align*}
\sum_{(x,y)\in X\times Y}f(x,y)=\lim_{A\in\mc I}\sum_{x\in A}\sum_{y\in Y}f(x,y)=\sum_{x\in X}\sum_{y\in Y}f(x,y)
\end{align*}
where the second and the third limits exist. This proves a half of \eqref{eq45}. The other half can be proved in the same way.
\end{proof}








\begin{thm}[\textbf{Fubini theorem-B}]\label{lb137}
Let $g:X\times Y\rightarrow\ovl\Rbb_{\geq0}$. Then the five discrete integrals in \eqref{eq46} exist in $\ovl\Rbb_{\geq0}$, and equations \eqref{eq46} hold in $\ovl\Rbb_{\geq0}$:
\begin{align}\label{eq46}
\sum_{x\in X}\sum_{y\in Y}f(x,y)=\sum_{y\in Y}\sum_{x\in X}f(x,y)=\sum_{(x,y)\in X\times Y}f(x,y)
\end{align}
\end{thm}

\begin{proof}
The existence in $\ovl\Rbb_{\geq0}$ of the five discrete integrals is clear. (Recall Def. \ref{lb132}.) Formula \eqref{eq46} can be proved in the same way as \eqref{eq45}. Note that when applying Thm. \ref{lb122} to prove \eqref{eq46}, the assumption in Thm. \ref{lb122} on the existence of limits is satisfied because all nets involved are increasing in $\ovl\Rbb$. (Recall Exe. \ref{lb123}.)
\end{proof}

\begin{co}[\textbf{Fubini theorem-C}]
Let $f:X\times Y\rightarrow V$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\sum_{X\times Y}f$ converges absolutely.
\item $\sum_{x\in X}\sum_{y\in Y}\lVert f(x,y)\lVert<+\infty$.
\item $\sum_{y\in Y}\sum_{x\in X} \lVert f(x,y)\lVert<+\infty$.
\end{enumerate}
\end{co}

\begin{proof}
Immediate from Thm. \ref{lb137}. It is also not hard to prove it directly.
\end{proof}

%% Record #6 2023/10/9 two lectures 



\subsection{Parametrization theorem for discrete integrals}

We fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. In the following sections, we shall apply the results about discrete integrals to the study of series and double series. For the convenience of applications (e.g. the proof of  $e^ze^w=e^{z+w}$), we enlarge the concept of series a little:

\subsubsection{Series over $\Zbb$}\label{lb151}


\begin{df}
A \textbf{series over $\Zbb$} \index{00@Series over $\Zbb$} is an expression
\begin{align*}
\sum_{n=-\infty}^{+\infty} f(n)
\end{align*}
where $f$ is a function from $\Zbb$ to either $V$ or $\ovl\Rbb_{\geq0}$. We say that this series \textbf{converges to}  (or equals) $\mu\in V$ resp.  equals $\mu\in\ovl\Rbb_{\geq0}$, if
\begin{align}
\lim_{m,n\rightarrow+\infty} \sum_{i=-m}^n f(i)=\mu \label{eq47}
\end{align}
In this case, we write
\begin{align*}
\sum_{n=-\infty}^{+\infty} f(n)=\mu
\end{align*}
We say that $\sum_{n=-\infty}^{+\infty}f(n)$ \textbf{converges absolutely} \index{00@Absolute convergent series over $\Zbb$} if $\sum_{n=-\infty}^{+\infty}\lVert f(n)\lVert<+\infty$.
\end{df}

\begin{rem}
Note that in the case of $\ovl\Rbb_{\geq0}$, the limit on the LHS of \eqref{eq47} must exist in $\ovl\Rbb_{\geq0}$. Again, this is due to the fact that the involved net is increasing, and so one can use Exe. \ref{lb123}.
\end{rem}



\begin{exe}\label{lb152}
Let $\sum_{n=-\infty}^{+\infty} f(n)$ be a series in either $V$ or $\ovl\Rbb_{\geq0}$.
\begin{enumerate}
\item Fix $k\in\Zbb$. Prove that $\sum_{n=-\infty}^{+\infty} f(n)$ converges iff the following limits converge:
\begin{subequations}
\begin{gather}
\sum_{n=k}^{+\infty} f(n)=\lim_{n\rightarrow+\infty} \sum_{i=k}^nf(i)\\
\sum_{n=-\infty}^{k-1} f(n)=\lim_{m\rightarrow+\infty} \sum_{i=-m}^{k-1}f(i)
\end{gather}
\end{subequations}
Moreover, if these limits converge, then
\begin{align}
\sum_{n=-\infty}^{+\infty}f(n)=\sum_{n=k}^{+\infty} f(n)+\sum_{n=-\infty}^{k-1} f(n)
\end{align}
\item In the case that $f$ has codomain $V$, prove that $\sum_{n=-\infty}^{+\infty} f(n)$ converges if it converges absolutely.
\item Prove that if $f$ is zero outside $\Zbb_+$, then
\begin{align}
\sum_{n=-\infty}^{+\infty}f(n)=\sum_{n=1}^{+\infty}f(n)  \label{eq48}
\end{align}
\end{enumerate}
\end{exe}

Thus, by \eqref{eq48}, our following results about series over $\Zbb$ can be directly applied to series over $\Zbb_+$ (or over $\Zbb_{\geq k}$ where $k\in\Zbb$).



\subsubsection{Parametrization theorem}

The following theorem relates series and discrete integrals. The structure of this theorem is similar to that of Fubini theorem-A,B,C in Sec. \ref{lb138}.

\begin{thm}[\textbf{Parametrization theorem}]\label{lb136}
Let $X$ be an infinite countable set. Let $\varphi:\Zbb\rightarrow X$ be a bijection (called a \textbf{parametrization} of $X$). \index{00@Parametrization (in discrete integrals)} The following are true.
\begin{enumerate}
\item Let $f:X\rightarrow V$. If the RHS of \eqref{eq49} converges in $V$, then the LHS converges, and \eqref{eq49} holds:
\begin{align}
\sum_{n=-\infty}^{+\infty}f\circ\varphi(n)=\sum_{x\in X}f(x)\label{eq49}
\end{align}
\item Let $f:X\rightarrow\ovl\Rbb_{\geq0}$. Then \eqref{eq49} holds in $\ovl\Rbb_{\geq0}$.
\item Let $f:X\rightarrow V$. Then the discrete integral $\dps\sum_{x\in X} f(x)$ converges absolutely iff the series $\dps\sum_{n=-\infty}^{+\infty} f\circ\varphi(n)$ converges absolutely.
\end{enumerate}
The same conclusions hold if we assume that $\varphi:\Zbb_+\rightarrow X$ is a bijection.
\end{thm}



\begin{proof}
We prove the case $\varphi:\Zbb\rightarrow X$; the other case is similar. 

Assume that $\sum_Xf$ converges, which means that the limit of the net $(\sum_Af)_{A\in\fin(2^X)}$ converges to some $v\in V$. Therefore, by Thm. \ref{lb120}, the subnet
\begin{align*}
\Big(\sum_{x\in A_{m,n}}f(x)\Big)_{m,n\in\Zbb_+}=\Big(\sum_{i=-m}^n f\circ\varphi(i)\Big)_{m,n\in\Zbb_+}
\end{align*}
converges to $v$, where $A_{m,n}=\{\varphi(i):i\in\Zbb,-m\leq i\leq n\}$. This proves part 1. The same method proves part 2. Part 3 follows directly from part 2.
\end{proof}

\subsection{Application to (double) series and power series; $e^ze^w=e^{z+w}$}

Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$.

\subsubsection{General results about series and double series}

\begin{co}\label{lb140}
Let $f:\Zbb\rightarrow V$, and let $\psi:\Zbb\rightarrow\Zbb$ be a bijection. Suppose that
\begin{align}
\sum_{n=-\infty}^{+\infty}\lVert f(n)\lVert<+\infty \label{eq54}
\end{align}
Then \eqref{eq50} holds true, where the RHS of \eqref{eq50} converges absolutely:
\begin{align}
\sum_{n=-\infty}^{+\infty} f(n)=\sum_{n=-\infty}^{+\infty}f\circ\psi(n) \label{eq50}
\end{align}
\end{co}
The same conclusion clearly holds if $\Zbb$ is replaced by $\Zbb_+$.

\begin{proof}
By \eqref{eq54} and Thm. \ref{lb136}-3, the discrete integral $\sum_\Zbb f$ converges absolutely, and hence converges. By Thm. \ref{lb136}-1, the LHS resp. RHS of \eqref{eq50} converges to the value of $\sum_\Zbb f$ if we choose the parametrization to be $\id_\Zbb$ resp. $\psi$. This proves \eqref{eq50} and the convergence of the RHS of \eqref{eq50}. Applying the same conclusion to $\lVert f(\cdot)\lVert$ proves the absolute convergence of the RHS of \eqref{eq50}.
\end{proof}


\begin{co}\label{lb139}
Let $f:\Zbb^2\rightarrow V$. Let $\Phi:\Zbb^2\rightarrow\Zbb^2$ be a bijection. Suppose that
\begin{align}\label{eq51}
\sum_{m=-\infty}^{+\infty}\sum_{n=-\infty}^{+\infty}\lVert f(m,n)\lVert<+\infty
\end{align}
Then \eqref{eq52} holds true, where the six series involved in \eqref{eq52} converge absolutely.
\begin{align}\label{eq52}
\sum_{m=-\infty}^{+\infty}\sum_{n=-\infty}^{+\infty} f(m,n)=\sum_{n=-\infty}^{+\infty}\sum_{m=-\infty}^{+\infty}f(m,n)=\sum_{k=-\infty}^{+\infty}\sum_{l=-\infty}^{+\infty} f\circ\Phi(k,l)
\end{align}
\end{co}

Similar results clearly hold if $\Zbb^2$ is replaced by $\Zbb_+^2$: One extends the domain of $f$ and the domain and codomain of $\Phi$ from $\Zbb_+^2$ to $\Zbb^2$. Then one apply Cor. \ref{lb139}. 

Also, note that the second term of \eqref{eq52} is redundant: it follows from the equality of the first and the third terms of \eqref{eq52} if we choose $\Phi(k,l)=(l,k)$.


\begin{proof}
By Thm. \ref{lb136}-2 and Thm. \ref{lb137}, we have
\begin{align}
\sum_{m=-\infty}^{+\infty}\sum_{n=-\infty}^{+\infty}\lVert f(m,n)\lVert=\sum_{x\in\Zbb}\sum_{y\in \Zbb}\lVert f(x,y)\lVert=\sum_{(x,y)\in\Zbb^2}\lVert f(x,y)\lVert\label{eq53}
\end{align}
where all the limits exist in $\ovl\Rbb_{\geq0}$. Therefore, by \eqref{eq51}, the discrete integral $\sum_{\Zbb^2}f$ is absolutely convergent and hence convergent. 

Similar to the argument for \eqref{eq53}, Thm. \ref{lb136}-1 and Thm. \ref{lb135} imply that the first two terms of \eqref{eq52}  exist and are both equal to the discrete integral $\sum_{\Zbb^2}f$. Since $\sum_{\Zbb^2}f=\sum_{\Zbb^2}f\circ\Phi$ (recall Rem. \ref{lb141}), by Thm. \ref{lb136}-1 and Thm. \ref{lb135} again, the last term of \eqref{eq52} converges to $\sum_{\Zbb^2}f\circ\Phi$.

We have proved that the six series in \eqref{eq52} converge, and \eqref{eq52} holds. Replacing $f(\cdot,\cdot)$ with $\lVert f(\cdot,\cdot)\lVert$ and applying a similar argument, we see that the six series in \eqref{eq52} converge absolutely. %(Note that one needs Prop. \ref{lb142} to show that $\sum\lVert \sum(\cdots)\lVert\leq\sum\sum\lVert(\cdots)\lVert<+\infty$.)
\end{proof}


\begin{rem}
Using the same method as in the above proof, one can easily prove a more general version of Cor. \ref{lb139}: Let $N\in\Zbb_+$. Let $f:\Zbb^2\rightarrow V$ such that \eqref{eq51} holds true. Let $\Psi:\Zbb^N\rightarrow\Zbb^2$ be a bijection. Then the $N$ series involved in the expression of \eqref{eq58} (from innermost to outermost) converge absolutely:
\begin{align}\label{eq58}
\sum_{n_1=-\infty}^{+\infty}\cdots \sum_{n_N=-\infty}^{+\infty}f\circ\Psi(n_1,\dots,n_N)
\end{align}
Moverover, the outermost series of \eqref{eq58} converges to \eqref{eq52}. And of course, a similar result holds if $\Zbb^2$ is replaced by $\Zbb^M$ for every $M\in\Zbb_+$. We leave it to the readers to fill in the details.
\end{rem}



%We remark that \eqref{eq52} is the most important reason we introduced series over $\Zbb$: If we restrict to series over $\Zbb_+$, the proof of \eqref{eq52} will involve more technical discussions.

\begin{co}\label{lb153}
Assume that
\begin{align*}
A=\sum_{n=-\infty}^{+\infty} a_n\qquad B=\sum_{n=-\infty}^{+\infty}b_n
\end{align*}
are absolutely convergent series in $\Cbb$. Then for each $k\in\Zbb$, the series
\begin{align*}
c_k=\sum_{l=-\infty}^{+\infty}a_{k-l}b_l
\end{align*}
converges absolutely. Moreover, the LHS of the \eqref{eq59} converges absolutely to the RHS:
\begin{align}\label{eq59}
\sum_{k=-\infty}^{+\infty}c_k=AB
\end{align}
\end{co}


\begin{proof}
Apply Cor. \ref{lb139} to the case that $f(m,n)=a_mb_n$ and $\Phi(k,l)=(k-l,l)$.
\end{proof}


\subsubsection{Application to power series}


\begin{co}\label{lb362}
Let $\dps f(z)=\sum_{n=0}^{+\infty}a_nz^n$ and $\dps g(z)=\sum_{n=0}^{+\infty}b_nz^n$ be power series in $\Cbb$ with radii of convergence $R_1,R_2$ respectively. Let $R=\min\{R_1,R_2\}$. For each $k\in\Zbb_+$, let
\begin{align*}
c_k=\sum_{l=0}^k a_{k-l}b_l
\end{align*}
Then the power series $\dps h(z)=\sum_{k=0}^{+\infty}c_kz^k$ has radius of convergence $\geq R$. Moreover, for each $z\in\Cbb$ satisfying $0\leq |z|<R$, we have
\begin{align*}
h(z)=f(z)\cdot g(z)
\end{align*}
\end{co}

\begin{proof}
For each $0\leq |z|<R$, apply Cor. \ref{lb153} by replacing the $a_n,b_n,c_k$ of Cor. \ref{lb153} with $a_nz^n,b_nz^n,c_kz^k$. This shows that $h(z)$ converges absolutely to $f(z)\cdot g(z)$. Since this is true for all $|z|<R$, $h(z)$ must have radius of convergence at least $R$ by Rem. \ref{lb108}.
\end{proof}

The above result also holds more generally for Laurent series. See Exe. \ref{lb154}.

\begin{co}\label{lb215}
For each $z,w\in\Cbb$ we have 
\begin{align*}
e^ze^w=e^{z+w}
\end{align*}
\end{co}

\begin{proof}
Apply Cor. \ref{lb153} to the case $a_n=z^n/n!$ and $b_n=w^n/n!$. (We set $a_n=b_n=0$ if $n<0$.) Then
\begin{align*}
c_k=\sum_{l=0}^{k} \frac{z^{k-l}}{(k-l)!}\cdot\frac{w^l}{l!}=\sum_{l=0}^k {k\choose l}\frac{z^{k-l}w^l}{k!}=\frac{(z+w)^k}{k!}
\end{align*}
by \eqref{eq60}.
\end{proof}




\subsection{Summary}


The following are some fundamental questions about series and double series:
\begin{itemize}
\item[(a)] Are they invariant under rearrangement? (Cf. \eqref{eq50}.) 
\item[(b)] Does the value of an interated double series remain unchanged if the order of the two infinite sums is changed? (Cf. the first equality in \eqref{eq52}.)
\item[(c)] A mixture of the above two questions. (Cf. the last term of \eqref{eq52}.)
\end{itemize}
We address these questions by relating them to discrete integral, a version of infinite sums which is parametrization-independent. The following are some key features of this theory.
\begin{enumerate}
\item (General principle)  A discrete integral is to a series (defined by parametrization) as a net to a subnet.
\item (Net $\Rightarrow$ Subnet) All subnets of a convergent net converge to the same value: the limit of the original net.
\item (Discrete integral $\Rightarrow$ Series) Therefore, different series converge to the same value if they are different parametrizations of the same convergent discrete integral.
\item (Discrete integrals $\Rightarrow$ Series) Fubini-type theorems (any theorems about exchanging the orders of iterated sums/integrals) hold for convergent double discrete integrals. Therefore, they hold when passing to subnets, in particular, when passing to double series.
\item (Subnet $\Rightarrow$ Net) Every increasing net in $\ovl\Rbb_{\geq0}$  has a limit in $\ovl\Rbb_{\geq0}$. Therefore, if an increasing net in $\Rbb_{\geq0}$ has a subnet converging to a number $<+\infty$, then the original net converges in $\Rbb_{\geq 0}$ (to a finite number).
\item (General principle) The discrete integral $\sum_{x\in X}\lVert f(x)\lVert$ is defined by the limit of an increasing net in $\ovl\Rbb_{\geq0}$.
\item (Series $\Rightarrow$ Discrete integral) Therefore, if any series or double series corresponds in a reasonable way to a discrete integral, then the absolute convergence of this (double) series (more specifically: \eqref{eq54} or \eqref{eq51}) implies the absolute convergence (and hence convergence) of the original discrete integral. This implies the absolute convergence of any other (double) series arising from that discrete integral.
\item (Conclusion) Thus, when a (double) series converges absolutely (in the form of \eqref{eq54} or \eqref{eq51}), the three problems (a), (b), (c) have satisfying answers. The reason absolutely convergent (double) series are so good is because increasing nets in $\ovl\Rbb_{\geq0}$ are very good!
\item (Counterexamples) Non-absolutely convergent series in $\Rbb$ have rearrangements converging to different values. This is because non-convergent nets may have two subnets converging to different values, cf. Pb. \ref{lb204}. (Recall from Prop. \ref{lb134} that for discrete integrals in $\Rbb$, absolute convergence is equivalent to convergence.)
\end{enumerate}











\subsection{Problems and supplementary material}


Let $X$ be a set, and let $V$ be a Banach space over $\Rbb\in\{\Rbb,\Cbb\}$.





\begin{comment}
\begin{prob}
Prove Thm. \ref{lb129}, namely, prove that every Cauchy net $(x_\alpha)_{\alpha\in I}$ in a complete metric space $X$ is convergent.
\end{prob}


\begin{sprob}\label{lb266}
Let $f:X\rightarrow\Rbb$ be a function. Recall that $\fin(2^X)$ is the set of finite subsets of $X$. For each $A\in\fin(2^X)$, define
\begin{align*}
s_A=\sum_{x\in A}f(x)
\end{align*}
We say that $s\in\Rbb$ is a cluster point of the net $(s_A)_{A\in\fin(2^X)}$ if the following condition holds: 
\begin{itemize}
\item[($\varstar$)] For every $\eps>0$, $s_A$ is frequently in $B_\Rbb(s,\eps)$. (Namely, for every $\eps>0$ and $A\in\fin(2^X)$ there exists $B\in\fin(2^X)$ such that $A\subset B$ and $|s-s_B|<\eps$.) 
\end{itemize}
Answer the following questions.
\begin{enumerate}
\item Prove that if there is a bijection $\varphi:\Zbb_+\rightarrow X$ such that the series $\sum_{n=1}^\infty f\circ\varphi(n)$ converges to $s$, then $s$ is a cluster point of the net $(s_A)_{A\in\fin(2^X)}$.
\item Suppose that $s$ is a cluster point of $(s_A)_{A\in\fin(2^X)}$. Define $(I,\leq)$ to be
\begin{gather*}
I=\big\{(A,\eps)\in\fin(2^X)\times\Rbb_{>0}:|s-s_A|<\eps \big\}\\[0.5ex]
(A,\eps)\leq (A',\eps')\qquad\Longleftrightarrow\qquad A\subset A'\text{ and }\eps\geq\eps'
\end{gather*}
For each $\alpha=(A,\eps)\in I$, let $s_\alpha=s_A$. Prove that $I$ is a directed set. Prove that $(s_\alpha)_{\alpha\in I}$ is a subnet of $(s_A)_{A\in\fin(2^X)}$ if the increasing map $I\rightarrow \fin(2^X)$ is defined to be $(A,\eps)\mapsto A$. Prove that $(s_\alpha)_{\alpha\in I}$ converges to $s$.
\end{enumerate}
\end{sprob}
\end{comment}


\begin{prob}
Compute $\lim_{p,q\rightarrow+\infty}a_{p,q}$ where $(a_{p,q})_{p,q\in\Zbb_+}$ are given below. Or explain why the limit does not exist.
\begin{gather*}
a_{p,q}=\frac{(-1)^p\cdot p}{p+q} \qquad 
a_{p,q}=\frac{(-1)^p}{p}\qquad
a_{p,q}=\frac{\cos(p\pi/4)}{p+q}
\end{gather*}
\end{prob}


\begin{prob}
Prove Thm. \ref{lb129}. (Every Cauchy net in a complete metric space converges.)
\end{prob}



\begin{prob}
Let $f:X\rightarrow V$. Define the \textbf{support} \index{00@Support $\Supp(f)$} \index{Supp@$\Supp(f)$} of $f$ to be
\begin{align}
\Supp(f)=\{x\in X:f(x)\neq 0\}
\end{align}
Prove that if $\sum_Xf$ converges absolutely, then $\Supp(f)$ is a countable set.
\end{prob}

\begin{proof}[Hint]
Consider $\{x\in X:|f(x)|\geq\eps\}$ where $\eps>0$.
\end{proof}




\begin{prob}
Prove Prop. \ref{lb134}. 
\end{prob}


\begin{sprob}\label{lb204}
Prove \textbf{Riemann rearrangement theorem}, which says the following: Let $\sum_{n=1}^{+\infty} x_n$ be a series in $\Rbb$ which converges and which does not converge absolutely. Choose any  $A\in\ovl\Rbb$. Then $\sum_{n=1}^{+\infty} x_n$ has a rearrangement converging to $A$ (i.e., there is bijection $\varphi:\Zbb_+\rightarrow\Zbb_+$ such that $\sum_{n=1}^{+\infty} f\circ\varphi(n)=A$).
\end{sprob}


\begin{rem}
By Riemann rearrangement theorem, it is clear that every convergent series in $\Rbb^N$ which is not absolutely convergent must have two  rearrangements converging to two different points. However, when $\Rbb^N$ is replaced by an infinite dimensional Banach space, one may find a series $\sum_{n=1}^{+\infty} v_n$ which does not converge absolutely but converge to some $v$, and every rearrangement of $\sum_{n=1}^{+\infty} v_n$ converges to $v$. See Pb. \ref{lb149}.
\end{rem}


\begin{prob}\label{lb149}
Consider the case that $V$ is the real Banach space $V=l^\infty(\Zbb_+,\Rbb)$. For each $n\in\Zbb_+$, let $e_n\in V$ be the characteristic function $\chi_{\{n\}}$. Namey, $e_n$ takes value $1$ at $n$, and takes $0$ at the other points. Prove that the discrete integral
\begin{align}
\sum_{n\in\Zbb_+}\frac 1{n}e_n \label{eq57}
\end{align}
converges in $V$, and find the limit. Prove that \eqref{eq57} does not converge absolutely.
\end{prob}


\begin{rem}
A more important example that will be considered later is $V=l^2(\Zbb_+,\Cbb)$, the set of all functions $f:\Zbb_+\rightarrow\Cbb$ satisfying that the \pmb{$l^2$}\textbf{-norm} $\lVert f\lVert_{l^2}=\sqrt{\sum_{n\in\Zbb_+} |f(n)|^2}$ is finite. Then $V$ is in fact a Banach space. (Actually, it is a so-called \textbf{Hilbert space}.) Again, let $e_n=\chi_{\{n\}}$. (These $e_n$ will be called an \textbf{orthonormal basis} of $V$.) Then for each $f\in V$, the discrete integral $\sum_{n\in\Zbb_+}f(n)\cdot e_n$ converges to $f$. But it does not converge absolutely if $\sum_{n\in\Zbb_+}|f(n)|=+\infty$. Take for example $f(n)=n^{-1}$. We will study these objects in the second semester.
\end{rem}


\begin{prob}
Define $(x_{j,k})_{(j,k)\in\Zbb_+\times\Zbb_+}$ to be
\begin{align*}
x_{j,k}=\left\{
\begin{array}{ll}
\dps\frac 1{j^2-k^2}
 & \text{if }j\neq k\\[2ex]
0&\text{if }j=k
\end{array}\right.
\end{align*}
Prove that the discrete integral $\dps\sum_{(j,k)\in\Zbb_+^2}x_{j,k}$ does not converge in $\Rbb$.
\end{prob}

\begin{proof}[Hint]
Consider $(x_{j,k})$ as a net over $\Zbb^2$. Find a good bijection $\Phi:\Zbb^2\rightarrow\Zbb^2$.
\end{proof}




\begin{df}
For each $f\in V^X$, define the \pmb{$l^1$}\textbf{-norm} \index{l1@$\lVert\cdot\lVert_{l^1}=\lVert\cdot\lVert_{1}$}
\begin{align*}
\lVert f\lVert_{l^1(X,V)}\equiv\lVert f\lVert_{l^1}\equiv\lVert f\lVert_1=\sum_{x\in X}\lVert f(x)\lVert
\end{align*}
Define the $l^1$-space \index{l1XV@$l^1(X,V)$}
\begin{gather*}
l^1(X,V)=\{f\in V^X:\lVert f\lVert_{l^1}<+\infty \}
\end{gather*}
Namely, $l^1(X,V)$ is the set of all $f\in V^X$ where $\sum_Xf$ converges absolutely. In particular, $\sum_Xf$ converges for such $f$. 
\end{df}


\begin{exe}
Prove that for each $f,g\in V^X$ and $\lambda\in\Fbb$, we have
\begin{gather}
\lVert f+g\lVert_1\leq \lVert f\lVert_1+\lVert g\lVert_1\qquad \lVert\lambda f\lVert_1=|\lambda|\cdot\lVert f\lVert_1
\end{gather}
Show that $l^1(X,V)$ is a linear subspace of $l^\infty(X,V)$, and that $\lVert\cdot\lVert_{l^1}$ is a norm on $l^1(X,V)$
\end{exe}

\begin{prob}
Prove that $l^1(X,V)$ is a Banach space. Namely, prove that the metric on $l^1(X,V)$ defined by the $l^1$-norm is complete.
\end{prob}


\begin{prob}
Prove the \textbf{dominated convergence theorem} for discrete integrals: Let $(f_\alpha)_{\alpha\in I}$ be a net in $V^X$ satisfying the following conditions:
\begin{enumerate}[label=(\arabic*)]
\item There exists $g\in l^1(X,\Rbb)$ satisfying $g\geq0$ (i.e. $g(x)\geq0$ for all $x\in X$) such that for every $\alpha\in I,x\in X$ we have
\begin{align*}
\lVert f_\alpha(x)\lVert\leq g(x)
\end{align*}
We simply write the above condition as $|f_\alpha|\leq g$.
\item $(f_\alpha)_{\alpha\in I}$ converges pointwise some $f\in V^X$. Namely, $\lim_\alpha f_\alpha(x)=f(x)$ for every $x\in X$.
\end{enumerate}
Prove that $f\in l^1(X,V)$. Prove that the LHS of \eqref{eq56} exists and equals the RHS: 
\begin{align}\label{eq56}
\lim_{\alpha\in I}\sum_{x\in X}f_\alpha(x)=\sum_{x\in X}f(x)
\end{align}
\end{prob}


\begin{sprob}
Assume $v_n\in V$ for each $n$. Let $z$ be a complex variable. Then the expression
\begin{align*}
f(z)=\sum_{n=-\infty}^{+\infty}v_nz^n
\end{align*}
is called a \textbf{Laurent series} \index{00@Laurent series} in $V$.

Prove that there exist unique $r,R\in\ovl\Rbb_{\geq0}$ such that $f(z)$ converges absolutely when $|r|<z<|R|$, and that $f(z)$ diverges when $|z|<r$ or $|z|>R$. Prove that
\begin{align}
r=\limsup_{n\rightarrow+\infty} \sqrt[n]{\lVert v_{-n}\lVert}\qquad R=\frac{1}{\dps\limsup_{n\rightarrow+\infty} \sqrt[n]{\lVert v_n\lVert}}
\end{align}
(Recall that by Exe. \ref{lb152}, $f(z)$ diverges iff either $\sum_{n=0}^\infty v_nz^n$ or $\sum_{n=-\infty}^{-1} v_nz^n$ diverges.) We call $r$ and $R$ the \textbf{radii of convergence} of $f(z)$. \hfill\qedsymbol
\end{sprob}

\begin{sexe}\label{lb154}
Consider Laurent series $f(z)=\sum_{n=-\infty}^{+\infty}a_nz^n$ (with radii of convergence $r_1<R_1$) and $g(z)=\sum_{n=-\infty}^{+\infty}b_nz^n$ (with radii of convergence $r_2<R_2$) in $\Cbb$. Let
\begin{align*}
r=\max\{r_1,r_2\}\qquad R=\min\{R_1,R_2\}
\end{align*}
Assume that $r<R$. Prove that for each $k\in\Zbb$, the series
\begin{align*}
c_k=\sum_{l=-\infty}^{+\infty}a_{k-l}b_l
\end{align*}
converges absolutely. Prove that for each $z\in\Cbb$ satisfying $r<|z|<R$, the LHS of the following equation converges absolutely to the RHS:
\begin{align}
\sum_{k=-\infty}^{+\infty}c_kz^k= f(z)g(z)
\end{align}
\end{sexe}





\newpage


\section{$\star$ Construction of $\Rbb$ from $\Qbb$}  \label{lb167}


The goal of this chapter is to construct real numbers from rationals. More precisely, our goal is to prove Thm. \ref{lb3}. We use the method of Cantor to construct real numbers using equivalence classes of Cauchy sequences in $\Qbb$. The idea is quite simple: If we admit the existence of $\Rbb$ satisfying Thm. \ref{lb3}, then by Prop. \ref{lb2}, each $x\in\Rbb$ is the limit of a sequence $(x_n)$ in $\Qbb$, which must be a Cauchy sequence. Moreover, if $(y_n)$ is a sequence in $\Qbb$ converging to $y\in\Rbb$, then by Exe. \ref{lb128} we have $x=y$ iff $(x_n)$ and $(y_n)$ are Cauchy equivalent. Motivated by this, we now do not assume the existence of $\Rbb$, and make the following definition:

\begin{df}
We let $\scr R$ be the set of Cauchy sequences in $\Qbb$, \footnote{Only in this chapter do we use $\scr R$ for this meaning.} namely, the set of $(x_n)_{n\in\Zbb_+}\in\Qbb^{\Zbb_+}$ satisfying 
\begin{align*}
\lim_{m,n\rightarrow+\infty} (x_m-x_n)=0
\end{align*}
We say that two elements $(x_n),(y_n)$ of $\scr R$ are Cauchy-equivalent and write $(x_n)\sim(y_n)$ if $\lim_{n\rightarrow\infty}(x_n-y_n)=0$.
\end{df}


Note that the above definition does not rely on the existence of $\Rbb$, because the limit of nets in $\Qbb$ can be defined using only rational numbers: a net $(\xi_\alpha)_{\alpha\in I}$ converges to $\xi$ iff for every $\eps\in\Qbb_{>0}$, $\xi_\alpha$ is eventually satisfies $|\xi_\alpha-\xi|<\eps$. The readers can check that all the properties about limit used in this chapter does not rely on the existence of $\Rbb$.

Cauchy-equivalence is clearly an equivalence condition on $\scr R$: For example, if $\lim (x_n-y_n)=\lim(y_n-z_n)=0$ then $|x_n-z_n|\leq |x_n-y_n|+|y_n-z_n|\rightarrow 0$. So $|x_n-z_n|\rightarrow0$. This proves the transitivity. The other two conditions are obvious. Therefore, we can define:

\begin{df}
We let $\Rbb=\scr R/\sim$ where $\sim$ is the Cauchy-equivalence relation. (Recall Def. \ref{lb157}). The equivalence class of $(x_n)_{n\in\Zbb_+}$ is denoted by $[x_n]_{n\in\Zbb_+}=[x_1,x_2,\dots]$, simply written as $[x_n]$. The zero element $0$ of $\Rbb$ is defined to be $[0,0,\dots]$. 
\end{df}




\begin{exe}\label{lb160}
Choose $[x_n]\in\Rbb$ (i.e. $(x_n)\in\scr R$) . The following are equivalent:
\begin{itemize}
\item[(1)] $[x_n]\neq 0$. Namely, $\lim_n x_n$ does not converge to $0$.
\item[(2)] There exists $\eps\in\Qbb_{>0}$ such that either $x_n>\eps$ eventually, or $x_n<-\eps$ eventually. In particular, $x_n\neq 0$ eventually.
\end{itemize}
Consequently, the map $a\in\Qbb\mapsto [a,a,\dots]\in\Rbb$ is injective. With the help of this injective map, $\Qbb$ can be viewed as a subset of $\Rbb$.
\end{exe}


\begin{exe}\label{lb159}
Let $[x_n]\in\Rbb$ and $a\in\Qbb$. Suppose that $x_n\geq a$ (resp. $x_n\leq a$) frequently.  Then for every $\eps\in\Qbb_{>0}$, we have that $x_n\geq a-\eps$ (resp. $x_n\leq a+\eps$) eventually.
\end{exe}





\begin{df}\label{lb158}
If $\xi,\eta\in\Rbb$, write $\xi=[x_n]$ and $\eta=[y_n]$. In the case that $\eta\neq0$, we assume $y_n\neq 0$ for all $n$, which is possible by Exe. \ref{lb160}. Define
\begin{gather*}
[x_n]+ [y_n]=[x_n+ y_n]\\
-[x_n]=[-x_n]\\
[x_n]\cdot [y_n]=[x_ny_n]\\
1/[y_n]=[1/y_n]\qquad(\text{if }[y_n]\neq0)
\end{gather*}
\end{df}


\begin{exe}
Prove that the above formulas are well-defined: For example, if $(x_n)\sim(x_n')$ in $\scr R$, then $(x_ny_n)\sim (x_n'y_n)$. (You may need the easy fact that every Cauchy sequence is bounded.)
\end{exe}

\begin{rem}\label{lb166}
It is clear that Def. \ref{lb158} makes $\Rbb$ a \textbf{field}, \index{00@Field} which means that for every $\alpha,\beta,\gamma\in\Rbb$, the following are satisfied:
\begin{gather*}
\alpha+\beta=\beta+\alpha\qquad  (\alpha+\beta)+\gamma=\alpha+(\beta+\gamma)\qquad 0+\alpha=\alpha\qquad \alpha+(-\alpha)=0\\
\alpha\beta=\beta\alpha\qquad (\alpha\beta)\gamma=\alpha(\beta\gamma)\qquad 1\cdot\alpha=\alpha\qquad(\alpha+\beta)\gamma=\alpha\gamma+\beta\gamma\\
\alpha\cdot \frac 1\alpha=1\qquad(\text{if }\alpha\neq0)
\end{gather*}
Moreover, $\Qbb$ is a subfield of $\Rbb$ where the addition, taking negative, multiplication, and inverse of $\Rbb$ restrict to those of $\Qbb$.
\end{rem}

\begin{df}\label{lb164}
Let $[x_n],[y_n]\in\Rbb$. We write $[x_n]<[y_n]$ if one of the following equivalent (due to Exe. \ref{lb159}) statements hold:
\begin{itemize}
\item There exists $\eps\in\Qbb_{>0}$ such that $y_n-x_n>\eps$ eventually. 
\item There exists $\eps\in\Qbb_{>0}$ such that $y_n-x_n>\eps$ frequently. 
\end{itemize}
It is not hard show that $``<"$ is well-defined, and that (by Exe. \ref{lb160}) if $[x_n]<[y_n]$ then $[x_n]\neq [y_n]$. We write $[x_n]\leq [y_n]$ if $[x_n]<[y_n]$ or $x_n=y_n$.
\end{df}

\begin{lm}
$(\Rbb,\leq)$ is a totally ordered set.
\end{lm}

\begin{proof}
Choose $[x_n],[y_n],[z_n]\in\Rbb$. If $[x_n]<[y_n]$ and $[y_n]<[z_n]$, then clearly $[x_n]<[z_n]$. This proves that $\leq$ is a preorder. 

Suppose $[x_n]\leq[y_n]$ and $[y_n]\leq[x_n]$. Let us prove $[x_n]=[y_n]$. Suppose not. Then $[x_n]<[y_n]$ and $[y_n]<[x_n]$ by the definition of ``$\leq$". So there is $\eps>0$ such that $y_n-x_n>\eps$ eventually, and $x_n-y_n>\eps$ eventually. Impossible. So $\leq$ is a partial order.

Suppose $[x_n]\neq [y_n]$. Then $(x_n-y_n)\nsim 0$. So Exe. \ref{lb160} implies that either $[x_n]<[y_n]$ or $[y_n]<[x_n]$. So $\leq$ is a total order.
\end{proof}

\begin{lm}\label{lb165}
Let $[x_n],[y_n]\in\Rbb$. Then the following are equivalent.
\begin{itemize}
\item $[x_n]\geq[y_n]$.
\item For every $\eps\in\Qbb_{>0}$, $x_n-y_n\geq-\eps$ frequently.
\item For every $\eps\in\Qbb_{>0}$, $x_n-y_n\geq-\eps$ eventually.
\end{itemize}
\end{lm}

\begin{proof}
Since $\leq$ is a total order, the negation of $>$ is $\leq$. So the statements follow immediately by negating Def. \ref{lb164}.
\end{proof}



\begin{lm}
$\Rbb$ is an ordered field extension of $\Qbb$, and $\Rbb$ is Archimedean.
\end{lm}

\begin{proof}
The order of $\Rbb$ clearly restricts to that of $\Qbb$. We want to prove that $\Rbb$ is an ordered field. (Recall Def. \ref{lb161}). Clearly, if $[x_n]<[y_n]$ and $[z_n]\in\Rbb$, then $[x_n]+[z_n]=[x_n+z_n]<[y_n+z_n]=[y_n]+[z_n]$. If $[x_n]>0$ and $[y_n]>0$, then there are $\eps>0$ such that $x_n>\eps$ eventually and $y_n>\eps$ eventually. So $x_ny_n>\eps^2$ eventually. So $[x_n][y_n]>0$. This proves that $\Rbb$ is an ordered field.

Now let $[x_n]>0$ and $[y_n]\in\Rbb$. So there exist $\eps\in\Qbb_{>0}$ such that $x_n>\eps$ eventually. Since $(y_n)$ is Cauchy, one checks easily that $(y_n)$ is bounded. So there is $M\in\Qbb_{>0}$ such that $|y_n|\leq M$ for all $n$. Since $\Qbb$ is Archimedean, there exists $k\in\Zbb_+$ such that $k\eps>M+1$. So $kx_n>M+1$ eventually. So $k[x_n]>M$. This proves that $\Rbb$ is Archimedean.
\end{proof}




To finish the proof of Thm. \ref{lb3}, it remains to prove that $\Rbb$ satisfies the least-upper-bound property.


\begin{lm}\label{lb162}
Thm. \ref{lb3} holds if every bounded increasing sequence in $\Rbb$ converges.
\end{lm}

\begin{proof}
Suppose that every bounded increasing sequence in $\Rbb$ converges. Choose any nonempty $E\subset \Rbb$ bounded from above. We shall show that $E$ has a least upper bound.

Let $F$ be the set of upper bounds of $E$. Namely, $F=\{\eta\in\Rbb:\eta\geq\xi,\forall\xi\in E\}$. So $F\neq\emptyset$. We construct an increasing sequence $(\xi_k)$ in $E$ and an decreasing sequence $(\eta_k)$ in $F$ as follows. Since $E,F$ are nonempty, we choose arbitrary $\xi_1\in E$ and $\eta_1\in F$. Then $\xi_1\leq \eta_1$. Suppose $\xi_1\leq\cdots\leq\xi_k\in E$ and $\eta_1\geq\cdots\geq\eta_k\in F$ have been constructed. Let $\psi_k=(\xi_k+\eta_k)/2$. Let
\begin{gather*}
\left\{
\begin{array}{ll}
\xi_{k+1}=\psi_k,\eta_{k+1}=\eta_k &\text{ if }\psi_k\in E\\
\xi_{k+1}=\xi_k,\eta_{k+1}=\psi_k &\text{ if }\psi_k\in F
\end{array}
\right.
\end{gather*}
Then the sequences we have constructed satisfy $\lim_{k\rightarrow\infty}(\eta_k-\psi_k)=0$.

By assumption, $\alpha=\lim_{k\rightarrow\infty}\xi_k$ exists, and it equals $\lim_k \eta_k$.  So $\alpha$ is an upper bound of $E$. (If $\lambda\in E$, then $\lambda\leq \eta_k$ for all $k$ since $\eta_k\in F$. So $\lambda\leq\lim_k\eta_k=\alpha$.) We now show that $\alpha$ is the least upper bound. Let $\eps>0$. Since $\xi_k\rightarrow\alpha$, there is $k$ such that $\alpha-\xi_k<\eps$. So $\xi_k>\alpha-\eps$, and hence $\alpha-\eps$ is not an upper bound of $E$.
\end{proof}


\begin{lm}\label{lb163}
Thm. \ref{lb3} holds if every bounded increasing sequence in $\Qbb$ converges to an element of $\Rbb$.
\end{lm}

\begin{proof}
Suppose that every increasing sequence in $\Qbb$ converges in $\Rbb$. By Lem. \ref{lb162}, it suffices to prove that every increasing sequence $(\xi_k)$ in $\Rbb$ converges. If $\{\xi_{k}:k\in\Zbb_+\}$ is a finite subset of $\Rbb$, then $(\xi_k)$ clearly converges. If $\{\xi_{k}:k\in\Zbb_+\}$ is infinite, then $(\xi_k)$ clearly has a strictly increasing subsequence $(\xi_{k_l})$. If we can prove that $(\xi_{k_l})$ converges to some $\psi\in\Rbb$, then $(\xi_k)$ converges to $\psi$. (Choose any $\eps>0$. Choose $L\in\Zbb_+$ such that $|\psi-\xi_{k_L}|<\eps$ and hence $0\leq \psi-\xi_{k_L}<\eps$. Then for all $k\geq k_L$ we have $0\leq\psi-\xi_k<\eps$.)

Thus, it remains to prove that every strictly increasing sequence $(\eta_k)$ in $\Rbb$ converges. Since we have proved that $\Rbb$ is an Archimedean ordered field extension of $\Qbb$, by Prop. \ref{lb2}, for each $k$, there exists $a_k\in\Qbb$ such that $\xi_k< a_k< \xi_{k+1}$. By assumption, $(a_k)$ converges to some $\alpha\in\Rbb$. Since $a_{k-1}<\xi_k< a_k$, by squeeze theorem, $(\xi_k)$ converges to $\alpha$.
\end{proof}




\begin{proof}[\textbf{Proof of Thm. \ref{lb3}}]
By Lem. \ref{lb163}, it suffices to show that every bounded increasing sequence $(a_k)$ in $\Qbb$ converges in $\Rbb$. Let $M\in\Qbb$ such that $a_k\leq M$ for all $k$.

We first prove that $(a_k)$ is a Cauchy sequence. If not, then there exists $\eps\in\Qbb_{>0}$ such that for every $K\in\Zbb_+$ there is $k>K$ such that $|a_k-a_K|>\eps$, and hence $a_k-a_K>\eps$. Thus, we can find a subsequence $(a_{k_l})$ such that $a_{k_{l+1}}-a_{k_l}>\eps$. By the Archimedean property for $\Qbb$, there is $l\in\Zbb_+$ such that $a_{k_1}+l\cdot\eps>M$. So $a_{k_{l+1}}>M$, impossible.



Note that each $a_k$ is identified with $\xi_k=[a_k,a_k,\dots]$. Let $\psi=[a_1,a_2,a_3,\dots]$, which is an element of $\Rbb$ since we just proved that $(a_n)\in\scr R$. Then for each $k$, $\psi-\xi_k=[a_1-a_k,a_2-a_k,\dots]$, where the terms are eventually $\geq0$. So $\xi_k\leq\psi$ by Lem. \ref{lb165}. We have proved that $\psi$ is an upper bound for the sequence $(\xi_k)$.

Let us prove that $\lim_k\xi_k=\psi$. Choose any $\eps\in\Qbb_{>0}$. Let us prove that there exists $k$ such that $\psi-\eps<\xi_k$. Then for every $k'\geq k$ we have $\psi-\eps<\xi_{k'}\leq\psi$, finishing the proof of $\lim_k\xi_k=\psi$.

We have proved that $a_1,a_2,\dots$ is a Cauchy sequence in $\Qbb$. So there exists $k$ such that $a_l-a_k<\eps/2$ for all $l\geq k$. Thus, for all $l\geq k$ we have $a_k-(a_l-\eps)>\eps/2$. Thus, the $l$-th term of $\xi_k=[a_k,a_k,\dots]$ minus that of $\psi-\eps=[a_1-\eps,a_2-\eps,\dots]$ is $>\eps/2$ for sufficiently large $l$. By Def. \ref{lb164}, we have that $\psi-\eps<\xi_k$.
\end{proof}

\newpage

\section{Topological spaces}\label{lb350}


\subsection{The topologies of metric spaces}\label{lb169}

In this chapter, we begin our study of topological spaces, which were introduced by Hausdorff in 1914 \cite{Hau14} as a generalization of metric spaces. As we have seen, focusing on metrics in order to study convergence and continuity is often distracting. For example, in $\ovl\Rbb$, we only care about how the convergence of sequences look like, but not about the particular metrics. The same is true about the countable product of metric spaces $S=\prod_{i\in\Zbb_+}X_i$: the metrics \eqref{eq16} and \eqref{eq61} give the same topology, although they look very different. Moreover, the shapes of the open balls defined by these two metrics are not very simple. This makes it more difficult to study the continuity of functions on $S$ by using (2) or (2') of Def. \ref{lb31}.




Topological spaces generalize metric spaces by giving a set of axioms satisfied by the open sets of the spaces.

\begin{df}\label{lb168}
Let $X$ be a metric space, and let $E\subset X$. A point $x\in E$ is called an \textbf{interior point} of $E$ if $B_X(x,r)\subset E$ for some $r>0$. We say that $E$ is an \textbf{open (sub)set} of $X$, \index{00@Open set of a metric spaces} if every point of $E$ is an interior point.
\end{df}

\begin{df}
Let $\mc T$ be the set of open sets of $X$. We call $\mc T$ the \textbf{topology of the metric space} $X$. \index{00@Topology of a metric space}
\end{df}

\begin{eg}
By triangle inequality, every open ball of a metric space $X$ is open. $\emptyset$ and $X$ are open subsets of $X$. If $p,q\in\Rbb^N$ and $d(p,x)=r$ (where $0\leq r<+\infty$), then $p$ is not an interior point of $\ovl B_{\Rbb^N}(x,r)$. So the closed balls of $\Rbb^N$ are not open sets. In particular, $[a,b]$ are not open subsets of $\Rbb$ since $a,b$ are not interior points.
\end{eg}

\begin{eg}
It is not hard to see that a finite intersection of open sets is open.
\end{eg}



In topological spaces, open sets play the role of open balls in metric spaces due to the following facts:

\begin{exe}
Let $(x_n)$ be a sequence in a metric space $X$. Let $x\in X$. Show that the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $(x_n)$ converges to $x$.
\item For every \textbf{neighborhood $U$ of $x$} (i.e. every open set containing $x$) there is $N\in\Zbb_+$ such that for every $n\geq N$ we have $x_n\in U$.
\end{enumerate}
\end{exe}
\begin{exe}
Let $f:X\rightarrow Y$ be a map of metric spaces. Let $x\in X$ and $y=f(x)$. Prove that the following are equivalent. 
\begin{enumerate}[label=(\arabic*)]
\item $f$ is continuous at $x$.
\item For every neighborhood $V$ of $y$ there is a neighborhood $U$ of $x$ such that $f(U)\subset V$ (equivalently, $U\subset f^{-1}(V)$.)
\end{enumerate}
\end{exe}


But there is an important difference between the intuitions of open sets and open balls: We want the open balls at a point $x$ to be small so that they can be used to describe the approximation to $x$. However, an arbitrary open set can be very large. For example, when studying convergence and continuity in $\Rbb$, we really want a neighborhood of $1$ to be $(1-\eps,1+\eps)$ but not the more complicated and bigger one $(-\infty,-2)\cup (0,100-\eps)$. Indeed, open sets can be very big:


\begin{lm}
Let $X$ be a metric space. If $(U_\alpha)_{\alpha\in I}$ is a family of open subsets of $X$, then $W=\bigcup_{\alpha\in I}U_\alpha$ is open in $X$.
\end{lm}

\begin{proof}
Choose $x\in W$. Then $x\in U_\alpha$ for some $\alpha$. So $B_X(x,r)\subset U_\alpha$ for some $r>0$. So $x$ is an interior point of $W$.
\end{proof}



Thus, people very often choose a class $\mc B$ of smaller open sets (such as the set of open balls) to study the analytic properties of a topological space. 
\begin{df}\label{lb170}
Let $\mc B$ be a set of open sets of a metric space (or more generally, a topological space) $X$. We say that $\mc B$ is a \textbf{basis for the topology} \index{00@Basis for topology} $\mc T$ of $X$ if one of the following (clearly) equivalent statements holds:
\begin{itemize}
\item For every point $x\in X$ and every neighborhood $W$ of $x$ there exists $U\in\mc B$ such that $x\in U$ and $U\subset W$.
\item Every open subset of $X$ is a union of some members of $\mc B$.
\end{itemize}
\end{df}

Thus, according to Def. \ref{lb168}, the set of open balls of a metric space $X$ form a basis for the topology of $X$. Nevertheless, even in the case of metric spaces, we sometimes consider more convenient bases than the set of open balls. We will see this when we study the topologies of infinite product spaces.


\subsection{Topological spaces}


\subsubsection{Definitions and basic examples}

\begin{df}\label{lb178}
We say that a pair $(X,\mc T)$ (or simply $X$) is a \textbf{topological space} \index{00@Topological space} if $X$ is a set, and if $\mc T$ (called the \textbf{topology} of $X$) is a set of subsets of $X$ satisfying the following conditions
\begin{itemize}
\item $\emptyset\in\mc T$ and $X\in \mc T$.
\item (Union property) If $(U_\alpha)_{\alpha\in I}$ is a family of elements of $\mc T$, then $\bigcup_{\alpha\in I}U_\alpha$ is an element of $\mc T$.
\item (Finite intersection property) If $n\in\Zbb_+$ and $U_1,\dots,U_n\in\mc T$, then $U_1\cap\cdots\cap U_n$ is an element of $\mc T$.
\end{itemize}
Elements of $\mc T$ are called \textbf{open (sub)sets} \index{00@Open set} of $X$.
\end{df}


\begin{df}
Let $X$ be a topological space, and $x\in X$. A subset $U\subset X$ is called  a \index{00@Neighborhood=open set containing the point} \textbf{neighborhood} of $x$, if $U$ is an open subset of $X$ containing $x$.\footnote{We are following the convention in \cite{Mun,Rud-R}. But many people refer to the word "neighborhood" with slightly different meaning: a subset $A$ is called a neighborhood of $x$ if there is an open set $U$ such that $x\in U\subset A$. And our neighborhoods are called ``open neighborhoods" by them.} We define $(\Nbh_X(x),\leq)$, \index{Nbh@$\Nbh_X(x)=\Nbh(x)$}  the \textbf{directed set of neighborhoods of $x$}, \index{00@Directed set of neighborhoods of a point} to be 
\begin{gather}
\begin{gathered}
\Nbh_X(x)=\{\text{neighborhoods of }x\text{ in }X\}\\
U\leq U'\qquad\Longleftrightarrow \qquad U\supset U'
\end{gathered}
\end{gather} 
(Note that one needs the finite intersection property to show that $\Nbh_X(x)$ is a directed set.) We abbreviate this set to $\Nbh_X(x)$ or simply $\Nbh(x)$.
\end{df}


\begin{eg}
In Subsec. \ref{lb169}, we have proved that the topology of a metric space satisfies the above axioms of a topological space. 

In particular,   if $X$ is a normed vector space, the topology induced by the metric $d(x,x')=\lVert x-x'\lVert$ is called the \textbf{norm topology}. \index{00@Norm topology} If $X$ is a subset of $\Rbb^N$ or $\Cbb^N$, the topology on $X$ induced by the Euclidean metric is called the \textbf{Euclidean topology}. \index{00@Euclidean topology}  \hfill\qedsymbol
\end{eg}

\begin{df}
A topological space $(X,\mc T)$ is called \textbf{metrizable}, \index{00@Metrizable topological space} if there is a metric on $X$ inducing the topology $\mc T$. 
\end{df}


We have seen that the open balls of a metric space generate a topology. In general, one may ask what possible subsets of $2^X$ generate a topology on a set $X$. Here is a description, whose proof is left to the readers as an exercise.

\begin{pp}\label{lb171}
Let $X$ be a set, and let $\mc B\subset 2^X$. Define
\begin{align}
\mc T=\{\text{Unions of elements of }\mc B\}
\end{align}
The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $(X,\mc T)$ is a topological space.
\item The following are satisfied:
\begin{itemize}
\item[(2-a)] $X=\bigcup_{U\in\mc B}U$.
\item[(2-b)] If $U_1,U_2\in\mc B$, then $U_1\cap U_2\in\mc T$ (i.e., for each $x\in U_1\cap U_2$ there exists $V\in\mc B$ such that $x\in V$ and $V\subset U_1\cap U_2$).
\end{itemize} 
\end{enumerate}
\end{pp}

When (1) or (2) holds, we call $\mc T$ the \textbf{topology generated by $\mc B$}. \index{00@Topology generated by the basis} Clearly, $\mc B$ is a basis for $\mc T$ (cf. Def. \ref{lb170}).





\begin{exe}\label{lb172}
Let $X$ be a set. Let $\mc B,\mc B'$ be subsets of $2^X$ generating topologies $\mc T,\mc T'$ respectively. Prove that the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\mc T=\mc T'$.
\item Each $U\in\mc B$ is a union of elements of $\mc B'$. Each $U'\in\mc B'$ is a union of elements of $\mc B$.
\item For each $U\in\mc B$ and $x\in U$, there exists $U'\in\mc B'$ such that $x\in U'\subset U$. For each $U'\in\mc B$ and $x\in U'$, there exists $U\in\mc B$ such that $x\in U\subset U'$.
\end{enumerate} 
\end{exe}

\begin{eg}
If $(X,\mc T)$ is a topological space, then $\mc T$ is a basis for $\mc T$.
\end{eg}

\begin{eg}
Let $(X,d)$ be a metric space with topology $\mc T$. Let $\mc B=\{B_X(x,r):x\in X,0<r<+\infty\}$. Then $\mc B$ is a basis for $\mc T$. For each $\eps>0$, the set $\mc B'=\{B_X(x,r):x\in X,0<r<\eps\}$ is also a basis for $\mc T$.
\end{eg}

\begin{eg}
Let $\mc B\subset 2^{\ovl\Rbb}$ be defined by
\begin{gather}
\mc B=\{(a,b),(c,+\infty],[-\infty,d):a,b,c,d\in\Rbb \}  \label{eq62}
\end{gather}
Using Prop. \ref{lb171}, one easily checks that $\mc B$ is a basis for a topology $\mc T$. We call this the \textbf{standard topology} of $\ovl\Rbb$. \index{00@Topology of $\ovl\Rbb$}

Let $\varphi:\ovl\Rbb\rightarrow[u,v]$ be a strictly increasing bijection where $-\infty<u<v<+\infty$. Let $d_{[u,v]}$ be the Euclidean metric, and let $\mc T'$ be the topology on $\ovl\Rbb$ defined by $d_{\ovl\Rbb}=\varphi^*d_{[u,v]}$. Then the set of open balls under $\mc T'$ is
\begin{align*}
\mc B'=\{&(\varphi^{-1}(y-\eps),\varphi^{-1}(y+\eps)), (\varphi^{-1}(v-\eps'),+\infty],[-\infty,\varphi^{-1}(u+\eps'')):\\
&y\in(u,v)\text{ and } \eps,\eps',\eps''\in\Rbb_{>0} \}
\end{align*}
(Note that the three types of intervals in the definition of $\mc B'$ are open balls centered at $\varphi^{-1}(y),+\infty,-\infty$ respectively.) Using Exe. \ref{lb172}, one easily checks $\mc T=\mc T'$. \hfill\qedsymbol 
\end{eg}

\begin{cv}\label{lb173}
Unless otherwise stated, the topology on $\ovl\Rbb$ is defined to be the standard one, i.e., the one generated by \eqref{eq62}. We shall forget about the metric on $\ovl\Rbb$, and view $\ovl\Rbb$ only as a (metrizable) topological space.
\end{cv}



\begin{df}\label{lb180}
Let $A$ be a subset of a topological space $(X,\mc T_X)$. Then
\begin{align*}
\mc T_A=\{U\cap A:U\in \mc T_X\}
\end{align*} 
is clearly a topology on $A$, called the \textbf{subspace topology}. \index{00@Subspace topology} Unless otherwise stated, when viewing a subset as a topological subspace, we always choose the subspace topology for the subset.
\end{df}




\begin{exe}
Let $(X,d_X)$ be a metric space, inducing a topology $\mc T_X$. Let $A$ be a metric subspace of $X$. (So $A\subset X$, and $d_X$ restricts to $d_A$.) Prove that the topology on $A$ induced by $d_A$ is the subspace topology.
\end{exe}


According to the above exercise, if $X$ is a metric space, then viewing a subset $A$ as a topological subspace is compatible with viewing $A$ as a metric subspace.

\begin{exe}\label{lb209}
Let $A$ be a subset of a topological space $X$. Let $\mc B$ be a basis for the topology of $X$. Show that $\{U\cap A:U\in\mc B\}$ is a basis for the subspace topology of $A$.
\end{exe}




\subsubsection{Convergence of nets}



\begin{df}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a topological space $X$. Let $x\in X$. We say that $(x_\alpha)$ \textbf{converges to} $x$ and write \index{lim@$\lim_{\alpha\in I}x_\alpha\equiv \lim_\alpha x_\alpha$}
\begin{align*}
\lim_{\alpha\in I}x_\alpha\equiv \lim_\alpha x_\alpha=x
\end{align*}
or simply write $x_\alpha\rightarrow x$, if the following statement holds:
\begin{itemize}
\item For every $U\in\Nbh_X(x)$, we have that $x_\alpha$ is eventually in $U$.
\end{itemize}
Clearly, if $\mc B$ is a basis for the topology, then $x_\alpha\rightarrow x$ iff:
\begin{itemize}
\item For every $U\in\mc B$ containing $x$, we have that $x_\alpha$ is eventually in $U$.
\end{itemize}
In the case that $X$ is a metric space (and the topology of $X$ is induced by the metric), the definition here agrees with Def. \ref{lb174}.
\end{df}

\begin{exe}
Let $(x_\alpha)$ be a net in $X$ converging to $x\in X$. Prove that every subnet of $(x_\alpha)$ converges to $x$.
\end{exe}

\begin{exe}
Let $A$ be a subset of a topological space $X$, equipped with the subspace topology. Let $(x_\alpha)$ be a net in $A$, and let $x\in A$. Show that $x_\alpha\rightarrow x$ in $A$ iff $x_\alpha\rightarrow x$ in $X$.
\end{exe}




\begin{eg}
Let $X$ be a set. Let $\mc T=\{\emptyset,X\}$. Then every net in $X$ converges to every point of $X$. Thus, if $X$ has at least two elements, then the limit of a net in $X$ is not unique. Therefore, a general topological space might be very pathological. To avoid this uniqueness issue, we introduce the following notion:
\end{eg}



\begin{df}\label{lb268}
Let $X$ be a topological space with a basis for the topology $\mc B$. We say that $X$ is a \textbf{Hausdorff space} if the following equivalent conditions are satisfied:
\begin{itemize}
\item[(1)] (Hausdorff condition) If $x,y\in X$ and $x\neq y$, then there exist neighborhoods $U$ of $x$ and $V$ of $y$ such that $U\cap V=\emptyset$.
\item[(1')] If $x,y\in X$ and $x\neq y$, then there exist $U\in\mc B$ containing $x$ and $V\in\mc B$ containing $y$ such that $U\cap V=\emptyset$.
\item[(2)] If $(x_\alpha)_{\alpha\in I}$ is a net in $X$ converging to both $x$ and $y$, then $x=y$.
\end{itemize}
\end{df}

\begin{proof}[Proof of the equivalence]
(1)$\Leftrightarrow$(1'): Obvious.

(1)$\Rightarrow$(2): Suppose that $(x_\alpha)$ converges to $x$ and $y$. Suppose $x\neq y$. By (1),  we have disjoint neighborhoods $U\ni x$ and $V\ni y$. Since $x_\alpha\rightarrow x$, $x_\alpha$ is eventually in $U$. Similarly, $x_\alpha$ is eventually in $V$. Therefore, by the logic \eqref{eq38}, $x_\alpha$ is eventually in $U\cap V=\emptyset$, impossible.

$\neg$(1)$\Rightarrow$ $\neg$(2): Suppose that (1) is not true. Then there exist $x\neq y$ such that every neighborhood of $x$ intersects every neighborhood of $y$. Let $I=\Nbh_X(x)\times\Nbh_X(y)$. For each $\alpha=(U,V)\in I$, by assumption, there exists $x_\alpha\in U\cap V$. Then $(x_\alpha)_{\alpha\in I}$ is a net in $X$. We leave it to the readers to check that $x_\alpha\rightarrow x$ and $x_\alpha\rightarrow y$.
\end{proof}


\begin{rem}
In Hausdorff's 1914 paper introducing topological spaces, the Hausdorff condition is one of the axioms of topological spaces. Non-Hausdorff topological spaces were studied much later. The reason that Hausdorff spaces appeared first may be as follows: The original motivation for topological spaces lies in the study of analysis (especially functional analysis). But in analysis, most spaces are Hausdorff, because we want the limits of sequences or nets to be unique. 

In differential geometry and in topology\footnote{Here, I mean genuine topology, such as algebraic topology, differential topology, geometric topology, etc., but not point-set topology, which is analysis under the guise of topology.}, people are also mainly concerned with topological spaces that are Hausdorff. This is related to the fact that in these areas people often use tools from analysis. But in algebraic geometry, the main examples of topological spaces (e.g. varieties and schemes, whose topologies are called \textbf{Zariski topology}) are not Hausdorff.  As a related fact, sequences and nets are not effective tools in the study of algebraic geometry.  \hfill\qedsymbol
\end{rem}

\subsection{Closures, interiors, and closed sets}


In this section, we fix a topological space $X$.


\subsubsection{Closure points; dense subsets}





\begin{df}\label{lb183}
Let $A$ be a subset of $X$. We say that $x\in X$ is a \textbf{closure point} \index{00@Closure, closure point} of $A$, if the following equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item There is a net $(x_\alpha)_{\alpha\in I}$ in $A$ converging to $x$.
\item Each $U\in \Nbh_X(x)$ intersects $A$. 
\end{enumerate}
The \textbf{closure} of $A$ is defined to be \index{Acl@$\ovl A=\Cl_X(A)$, closure}
\begin{align*}
\ovl A\equiv\Cl(A)\equiv\Cl_X(A)=\{\text{closure points of }A\}
\end{align*}
Clearly $A\subset \ovl A$. Clearly, if $A\subset B\subset X$, then $\ovl A\subset\ovl B$.
\end{df}

Unless otherwise stated, if several subsets are involved, we always understand $\ovl A$ as $\Cl_X(A)$ where $X$ is the ambient topological space.

\begin{proof}[Proof of equivalence]
(1)$\Rightarrow$(2): Assume (1). Choose any $U\in\Nbh_X(x)$. Since $x_\alpha\rightarrow x$, we have that $x_\alpha$ is eventually in $U$. So $U$ must contain some $x_\alpha$. But $x_\alpha\in A$. So $U\cap A\neq\emptyset$.

(2)$\Rightarrow$(1):  By (2), for each $U\in\Nbh_X(x)$ we can choose $x_U\in U\cap A$. Then $(x_U)_{U\in\Nbh_X(x)}$ is a net in $A$ converging to $x$. 
\end{proof}







\begin{exe}
Let $\mc B$ be a basis for the topology of $X$. Show that $x\in X$ is a closure point of $A$ iff every $U\in\mc B$ containing $x$ must intersect $A$.
\end{exe}

\begin{exe}
Let $A$ be a subset of a metric space. Show that $x\in X$ is a closure point of $A$ iff there is a sequence $(x_n)_{n\in\Zbb_+}$ in $A$ converging to $x$.
\end{exe}

\begin{exe}
Recall that if $X$ is a metric space, then $\ovl B_X(x,r)=\{y\in X:d(x,y)\leq r\}$. Show that 
\begin{align}
\ovl{B_X(x,r)}\subset \ovl B_X(x,r)
\end{align}
and that these two sets are not necessarily equal.
\end{exe}



\begin{rem}\label{lb270}
Our proof of (2)$\Rightarrow$(1) in Def. \ref{lb183} is an indirect proof, because it uses axiom of choice. (Given $U\in\Nbh_X(x)$, the choice of $x_U\in U\cap A$ is highly arbitrary.) Here is a direct proof: Assume (2). Define  a direct set $(I,\leq)$ where
\begin{gather*}
I=\{(p,U):U\in\Nbh_X(x),p\in U\cap A\}\\[0.5ex]
(p,U)\leq(p',U')\qquad\Longleftrightarrow\qquad U\supset U'
\end{gather*} 
The fact that $I$ is a directed set is due to (2). Then $(p)_{(p,U)\in I}$ is a net in $A$ converging to $x$.

We will often prove results about nets in topological spaces using axiom of choice, not only because it is simpler than direct proofs (as above), but also because it is parallel to our use of sequences in metric spaces. (For example, see the proof of (1)$\Rightarrow$(2) in Def. \ref{lb31}.) However, it is important to know how to give a direct proof. This is because the studies of topological spaces using nets and using open sets are often equivalent, and direct proofs using nets can be more easily translated into proofs using open sets and vice versa.   \hfill \qedsymbol
\end{rem}


\begin{exe}
Prove $\neg$(1)$\Rightarrow$ $\neg$(2) of Def. \ref{lb268} without using axiom of choice.
\end{exe}



\begin{rem}\label{lb176}
There is a notion closely related to closure points, called accumulation points. Let $A$ be a subset of $X$. A point $x\in X$ is called a \textbf{accumulation point} \index{00@Accumulation point of a subset}  (or \textbf{limit point} or \textbf{cluster point}) of $A$, if $x$ is a closure point of $A\setminus\{x\}$.

We will not use the notion of accumulation points, although this concept is widely used in many textbooks on analysis or point-set topology. We use closure points instead. (But note that if $x\notin A$, then $x$ is a closure point iff $x$ is an accumulation point.) On the other hand, the following opposite notion of accumulation points is important and has a clear geometric picture:   \hfill\qedsymbol
\end{rem}

\begin{df}
We say that $x\in X$ is an \textbf{isolated point} of $X$, if the following (clearly) equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item $x\notin\ovl {X\setminus\{x\}}$.
\item There is no net in $X\setminus\{x\}$ converging to $x$.
\item There is a neighborhood of $x$ disjoint from $X\setminus\{x\}$.
\end{enumerate}
If $X$ is a metric space, then $x$ is an isolated point iff there is no sequence in $X\setminus\{x\}$ converging to $x$.
\end{df}


We return to the study of closures.

\begin{pp}\label{lb177}
Let $A$ be a subset of $X$. Then $\ovl{\ovl A}=\ovl A$.
\end{pp}

\begin{proof}
Choose any $x\in \ovl{\ovl A}$. To prove $x\in \ovl A$, we choose any  $U\in\Nbh_X(x)$, and try to prove $U\cap A\neq\emptyset$. Since $x$ is a closure point of $\ovl A$, $U$ intersects $\ovl A$. Pick $y\in U\cap\ovl A$. Then $y$ is a closure point of $A$, and $U\in\Nbh_X(y)$. So $U$ intersects $A$. 
\end{proof}


One should think of $\ovl{\ovl A}=\ovl A$ not only as a ``geometric" fact about closures. Instead, one should also understand its analytic content: A closure point of $A$ is a point which can be approximated by elements of $A$. Thus, $\ovl{\ovl A}=\ovl A$ says that ``approximation is transitive": If $x$ can be approximated by some elements which can be approximated by elements of $A$, then $x$ can be approximated by elements of $A$. Alternatively, one can use the language of density:

\begin{df}
A subset $A$ of $X$ is called \textbf{dense} (in $X$) \index{00@Dense subset} if $\ovl A= X$.
\end{df}

\begin{exe}
Show that $A$ is dense in $X$ iff every nonempty open subset of $X$ intersects $A$.
\end{exe}



\begin{rem}\label{lb182}
Let $A\subset B\subset X$.  From Def. \ref{lb183}-(1), it is clear that
\begin{align}
\Cl_B(A)=\Cl_X(A)\cap B \label{eq64}
\end{align}
Thus, $A$ is dense in $B$ iff $B\subset \Cl_X(A)$.
\end{rem}

Thus, the following property has the same meaning as $\ovl{\ovl A}=A$.
\begin{co}
Let $A\subset B\subset X$. Assume that $A$ is dense in $B$, and $B$ is dense in $X$, then $A$ is dense in $X$.
\end{co}
\begin{proof}
Choose any $x\in X$. Then $x\in\Cl_X(B)$ since $B$ is dense in $X$. Since $A$ is dense in $B$, we have $B\subset \Cl_X(A)$. Therefore $x\in\Cl_X(\Cl_X(A))$, and hence $x\in\Cl_X(A)$ by Prop. \ref{lb177}.
\end{proof}

\begin{eg}
Let $X=C([0,1],\Rbb)$, equipped with the $l^\infty$-norm. Let $B$ be the set of polynomials with real coefficients, regarded as continuous functions on $[0,1]$. By Weierstrass approximation theorem (which will be studied in the future), $B$ is a dense subset of $X$. Then the set $A$ of polynomials with rational coefficients is clearly a dense subset of $B$ under the $l^\infty$-norm. (Proof: Let $f(x)=a_0+a_1x+\cdots+a_{k}x^k$. For each $0\leq i\leq k$, choose a sequence $(a_{i,n})_{n\in\Zbb_+}$ in $\Qbb$ converging to $a_i$. Let $f_n(x)=a_{0,n}+a_{1,n}x+\cdots+a_{k,n}x^k$. Then $f_n\rightrightarrows f$ on $[0,1]$.) Therefore, $A$ is dense in $X$. To summarize:
\begin{itemize}
\item Since each continuous function on $[0,1]$ can be uniformly approximated by polynomials with $\Rbb$-coefficients, and since each polynomial can be uniformly approximated polynomials with $\Qbb$-coefficients, therefore each continuous function on $[0,1]$ can be uniformly approximated by polynomials with $\Qbb$-coefficients.
\end{itemize}
\end{eg}


%% Record #7 2023/10/11 three lectures 



\subsubsection{Interior points}

Interior points are dual to closure points:

\begin{df}\label{lb187}
Let $A$ be a subset of $X$. A point $x\in X$ is called an \textbf{interior point} \index{00@Interior point} of $A$ if the following equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item There exists $U\in \Nbh_X(x)$ such that $U\subset A$.
\item $x$ is not a closure point of $X\setminus A$. 
\end{enumerate}
The set of interior points of $A$ is called the \textbf{interior} \index{00@Interior} of $A$ and is denoted by $\Int_X(A)$ or simply $\Int(A)$. \index{Int@$\Int_X(A)=\Int(A)$} So
\begin{align}
X\setminus\Int(A)=\ovl{X\setminus A}\qquad(\text{or simply }\Int(A)^c=\ovl{A^c})
\end{align}
according to (2). In particular, $\Int(A)\subset A$.
\end{df}

\begin{proof}[Proof of equivalence]
$A$ contains no neighborhoods of $x$ with respect to $X$ iff $A^c$ intersects every neighborhood of $x$ iff $x$ is a closure point of $A^c$.
\end{proof}

It is clear that if $\mc B$ is a basis for the topology, then $x\in\Int(A)$ iff there exists $U\in\mc B$ such that $x\in U\subset A$.

In analysis, interior points are not as commonly used as closure points. The following property is an important situation where interior points are used:

\begin{pp}\label{lb179}
Let $U$ be a subset of $X$. Then $U$ is open iff every point of $U$ is an interior point.
\end{pp}

In other words, $U$ is open iff $U=\Int_X(U)$.

\begin{proof}
If $U$ is open and $x\in U$, then $U\in \Nbh_X(x)$. So $x$ is an interior point of $U$.

Conversely, suppose that each $x\in U$ is interior. Choose $V_x\in\Nbh_X(x)$. Then $U=\bigcup_{x\in U}V_x$. So $U$ is open by the union property in Def. \ref{lb178}.
\end{proof}

Note that this is the first time we seriously use the fact that a union of open sets is open. 




\subsubsection{Closed sets and open sets}\label{lb228}


\begin{df}
We say that $A\subset X$ is a \textbf{closed (sub)set} \index{00@Closed subset} of $X$ if $\ovl A=A$.
\end{df}

\begin{exe}
Show that the above definition of closed subsets agrees with Def. \ref{lb99} when $X$ is a metric space.
\end{exe}

\begin{exe}
Show that a finite subset of a Hausdorff space is closed. Give an example of non-closed finite subset of a non-Hausdorff topological space.
\end{exe}


\begin{rem}\label{lb242}
The closure $\ovl A$ is the smallest closed set containing $A$. (Proof: By Prop. \ref{lb177},  $\ovl A$ is closed. If $B$ is closed and contains $A$, then $\ovl A\subset\ovl B=B$.)
\end{rem}



\begin{thm}\label{lb181}
Let $A$ be a subset of $X$. Then $A$ is closed iff $X\setminus A$ is open.
\end{thm}

\begin{proof}
Let $B=X\setminus A$. Then $A$ is closed iff every closure point of $A$ is in $A$, iff every non-interior point of $B$ is not in $B$, iff every point in $B$ is an interior point of $B$. By Prop. \ref{lb179}, this is equivalent to that $B$ is open.
\end{proof}

\begin{co}\label{lb186}
$\emptyset$ and $X$ are closed subsets of $X$. An intersection of closed subsets is closed. A finite union of closed subsets is closed. 
\end{co}

\begin{proof}
Take the complement of Def. \ref{lb178}, and apply Thm. \ref{lb181}. (Of course, they can also be proved directly using the condition $A=\ovl A$ for closedness.)
\end{proof}


\begin{co}\label{lb243}
$X$ is Hausdorff iff for every distinct $x,y\in X$ there exists $U\in\Nbh_X(x)$ such that $y\notin \ovl U$.
\end{co}

\begin{proof}
``$\Leftarrow$": Let $x\neq y$. Choose $U\in\Nbh(x)$ such that $y\notin\ovl U$. Then $X\setminus \ovl U\in\Nbh(y)$ by Thm. \ref{lb181}. So $x$ and $y$ are separated by neighborhoods $U,X\setminus\ovl U$.

``$\Rightarrow$": Let $x\neq y$. Choose disjoint $U\in\Nbh(x)$ and $V\in\Nbh(y)$. Then $X\setminus V$ is closed by Thm. \ref{lb181}. So $\ovl U\subset X\setminus V$ by Rem. \ref{lb242}. So $y\notin\ovl U$.
\end{proof}

\begin{co}\label{lb190}
Let $Y$ be a subset of $X$, and let $A\subset Y$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $A$ is a closed subset of $Y$.
\item $A=B\cap Y$ for some closed subset $B$ of $X$.
\end{enumerate}
\end{co}

Note that the ``open subset" version of this corollary is true due to the definition of the subspace topology of $Y$ (cf. Def. \ref{lb180}).

\begin{proof}[First proof]
$A$ is closed in $Y$ iff $Y\setminus A=Y\cap A^c$ is open in $Y$, iff $Y\cap A^c$ equals $Y\cap U$ for some open subset $U\subset X$, iff $Y\cap A$ (which is $A$) equals $Y\cap U^c$ for some open subset $U\subset X$. This finishes the proof, thanks to Thm. \ref{lb181}.
\end{proof}

\begin{proof}[Second proof]
Recall by \eqref{eq64} that $\ovl A\cap Y$ is the closure of $A$ in $Y$. Then $A$ is closed in $Y$ iff $A=\ovl A\cap Y$. This proves (1)$\Rightarrow$(2) since $\ovl A$ is closed by Prop. \ref{lb177}. Assume (2). Then $A=B\cap Y$ where $B=\ovl B$. So $\ovl A\cap Y=\ovl{B\cap Y}\cap Y\subset \ovl B\cap Y=B\cap Y=A$. This proves (1).
\end{proof}


As an immediate consequence of Def. \ref{lb178} and Cor. \ref{lb190}, we have:

\begin{exe}\label{lb341}
Let $A\subset B\subset X$. 
\begin{enumerate}
\item Prove that if $B$ is open in $X$, then $A$ is open in $B$ iff $A$ is open in $X$.
\item Prove that if $B$ is closed in $X$, then $A$ is closed in $B$ iff $A$ is closed in $X$.
\end{enumerate}
\end{exe}







\begin{rem}
Many people define a closed set to be the complement of an open set, and then proves that a set $A$ is closed iff $A=\ovl A$. I went the other way because I believe that $A=\ovl A$ is more essential for understanding of closedness from the viewpoint of analysis. In Thm. \ref{lb87}, we have already seen a classical example of closed set in analysis: $C([0,1],\Rbb)$ is a closed subset of $l^\infty([0,1],\Rbb)$, which has the clear analytic meaning that the uniform limit of a sequence/net of continuous functions $[0,1]\rightarrow\Rbb$ is continuous. And we will see many more examples of this type in the future.
\end{rem}

\begin{rem}\label{lb229}
I defined closedness using $A=\ovl A$, and hence using the limits of nets. This is because the intuition of closed sets is very closely related to the intuition of limits of nets/sequences. On the other hand, the intuition of open sets is very different. Let me say a few words about this.

Without a doubt, the keyword I give for the intuition of limits of nets is ``\uline{approximation}": Limit is not only a dynamic process, but also gives an impression of "getting smaller and smaller". When dealing with closed sets, we often do the same thing! We take an intersection of possibly infinitely many closed subsets, and the result we get is still a closed set (cf. Cor. \ref{lb185}). 

The keyword I give for open sets is ``local", or more precisely, ``\uline{local-to-global}" (as opposed to ``getting smaller and smaller"!). This is not only because a union of open sets is open, but also because open sets are very often used to prove a global result by reducing to local problems. One easy example is Exe. \ref{lb184}, which says that in order to prove that a function is continuous on the whole space $X$, it suffices to prove this locally. (We have already used this strategy in Sec. \ref{lb185}.) Here is a more advanced example: to define the integral for a function on a large set, one can first define it locally (i.e. on small enough open subsets), and then patch these local values together. We will see many examples in the future, for example, in the following chapter about compactness.
\end{rem}

\begin{rem}
Very often, a theorem is an important result establishing two seemingly different (systems of) intuitions, and hence two different ways of mathematical thinking. This is why I call ``closed sets are the complements of open sets" a theorem. The term ``complement" implies that this theorem often manifests itself in the following way: If solving a problem using open sets is a direct proof, then solving the problem using limits of sequences/nets is a proof by contradiction/contrapositive. And vise versa.
\end{rem}


\subsection{Continuous maps and homeomorphisms}



Unless otherwise stated,  $X$ and $Y$ are topological spaces.


\subsubsection{Continuous maps}



\begin{df}\label{lb188}
Let $f:X\rightarrow Y$ be a map. Let $x\in X$. We say that $f$ is \textbf{continuous at} \index{00@Continuity} $x$ if the following equivalent conditions hold:
\begin{enumerate}
\item[(1)] For every net $(x_\alpha)_{\alpha\in I}$ in $X$ converging to $x$, we have $\lim_{\alpha\in I}f(x_\alpha)=f(x)$.
\item[(2)] For every $V\in \Nbh_Y(f(x))$, there exists $U\in\Nbh_X(x)$ such that for every $p\in U$ we have $f(p)\in V$.
\item[(2')] For every $V\in \Nbh_Y(f(x))$, the point $x$ is an interior point of $f^{-1}(V)$.
\end{enumerate}
We say that $f$ is a \textbf{continuous} function/map, if $f$ is continuous at every point of $X$. 
\end{df}

It is clear that ``for every $V\in\Nbh_{Y}(f(x))$" in (2) and (2') can be replaced by ``for every $V\in\mc B$ containing $f(x)$" if $\mc B$ is a basis for the topology of $Y$. 

Note that in the case that $(x_\alpha)$ or $(f(x_\alpha))$ has more than one limits (which could happen when $X$ or $Y$ is not Hausdorff), condition (1) means that $f(x)$ is one of the limits of $(f(x_\alpha))_{\alpha\in I}$ if $x$ is one of the limits of $(x_\alpha)$.


\begin{proof}[Proof of equivalence]
Clearly (2) is equivalent to (2'). The proof of (2)$\Rightarrow$(1) is similar to the case of sequences in metric spaces. (See Def. \ref{lb31}.) We leave the details to the reader.

$\neg$(2)$\Rightarrow$ $\neg$(1): Assume that (2) is not true. Then there is a neighborhood $V$ of $f(x)$ such that for every neighborhood $U$ of $x$ there exists $x_U\in U$ such that $f(x_U)\notin V$. Then $(x_U)_{U\in\Nbh_X(x)}$ is a net in $X$, and $\lim_Ux_U=x$ since $x_U\in U$. However, for each $U$ we have $f(x_U)\in Y\setminus V$. So $\lim_U f(x_U)$ cannot converge to $f(x)$.
\end{proof}

\begin{exe}
Show that when $X,Y$ are metric spaces, Def. \ref{lb188} agrees with Def. \ref{lb31}.
\end{exe}

\begin{exe}\label{lb321}
Let $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ be maps of topological spaces. Assume that $f$ is continuous at $x\in X$, and $g$ is continuous at $f(x)$. Prove that $g\circ f:X\rightarrow Z$ is continuous at $x$.
\end{exe}








The proof of (1)$\Rightarrow$(2) in Def. \ref{lb188} is indirect, since it uses the axiom of choice. (The merit of this proof is that it is parallel to the proof for metric spaces in Sec. \ref{lb185}.) One can also give a direct proof. Indeed, there is a particular net $(x_\alpha)$ converging to $x$ such that  $\lim f(x_\alpha)=f(x)$ iff (2) is true:

\begin{exe}\label{lb198}
Define $(\Pnbh_X(x),\leq)$, \index{PNbh@$\Pnbh_X(x)$} the \textbf{directed set of pointed neighborhoods} of $x$,  to be
\begin{gather}
\begin{gathered}
\Pnbh_X(x)=\big\{(p,U):U\in\Nbh_X(x),p\in U  \big\}\\
(p,U)\leq(p',U')\qquad\Longleftrightarrow\qquad U\supset U'
\end{gathered}
\end{gather}
For each $\alpha=(p,U)\in\Pnbh_X(x)$, let $x_\alpha=p$. Then $(x_\alpha)_{\alpha\in\Pnbh_X(x)}$ is a net in $X$ converging to $x$. Prove that $f$ is continuous at $x$ iff $\lim_\alpha f(x_\alpha)=f(x)$.
\end{exe}



\begin{pp}\label{lb191}
Let $f:X\rightarrow Y$ be a map. The following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $f$ is continuous.
\item If $V\subset Y$ is open in $Y$, then $f^{-1}(V)$ is open in $X$.
\item If $F\subset Y$ is closed in $Y$, then $f^{-1}(F)$ is closed in $Y$.
\end{enumerate}
\end{pp}

\begin{proof}
(1)$\Leftrightarrow$(2): By Def. \ref{lb188}-(2') and Prop. \ref{lb179}. (2)$\Leftrightarrow$(3): By Thm. \ref{lb181} and the fact that $f^{-1}(B^c)=f^{-1}(B)^c$ for every $B\subset Y$.
\end{proof}



\begin{rem}
We first defined the continuity of $f$ at a point, and then used this to define a continuous function $f$ to be one continuous at every point. However, it seems that the notion of continuity at a point is used only in analysis. In geometry and in topology, only continuous maps (but not a map continuous at a point) are used, and they are defined by Prop. \ref{lb191}-(2).

One might think that continuous functions are special cases of functions  which are continuous at given points. But in fact, the latter notion can also be derived from the former: \hfill\qedsymbol
\end{rem}

\begin{exe}\label{lb308}
Let $f:(X,\mc T)\rightarrow (Y,\mc T')$ be a map of topological spaces. Let $x\in X$. Define a new topological space $(X_x,\mc T_x)$ as follows. $X_x$ equals $X$ as a set. The topology $\mc T_x$ of $X_x$ is generated by the basis
\begin{align}
\mc B_x=\Nbh_X(x)\cup\big\{\{p\}:p\neq x\big\}
\end{align}
Prove that if $X$ is Hausdorff, then $X_x$ is Hausdorff. Prove that the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $f:X\rightarrow Y$ is continuous at $x$.
\item $f:X_x\rightarrow Y$ is continuous.
\end{enumerate}
\end{exe}

Continuous functions are determined by their values on a dense subset: 

\begin{pp}\label{lb196}
Let $A$ be a subset of $X$, and let $f,g:\ovl A\rightarrow Y$ be continuous. Then
\begin{subequations}
\begin{gather}
f(\ovl A)\subset\ovl{f(A)}  \label{eq80}
\end{gather}
If $Y$ is moreover Hausdorff, then
\begin{gather}
f=g\qquad\Longleftrightarrow\qquad f|_A=g|_A
\end{gather}
\end{subequations}
\end{pp}
\begin{proof}
If $y\in f(\ovl A)$, then $y=f(x)$ for some $x\in\ovl A$. Choose a net $(x_\alpha)$ in $A$ converging to $x$. Then $\lim_\alpha f(x_\alpha)=f(x)$, and hence $f(x)\in\ovl {f(A)}$.

Assume that $Y$ is Hausdorff. If $f=g$ then clearly $f|_A=g|_A$. Assume that $f|_A=g|_A$. For each $x\in\ovl A$, choose a net $(x_\alpha)$ in $A$ converging to $x$. Then $f(x)=\lim f(x_\alpha)=\lim g(x_\alpha)=g(x)$. So $f=g$.
\end{proof}


You are encouraged to prove Prop. \ref{lb196} using open sets instead of using nets.



\subsubsection{Homeomorphisms}

\begin{df}
A map $f:X\rightarrow Y$ is called \textbf{open} (resp. \textbf{closed}) \index{00@Open map} \index{00@Closed map} if for every open (resp. closed) subset $A\subset X$, the image $f(A)$ is open (resp. closed) in $Y$.
\end{df}


\begin{df}
A bijection $f:X\rightarrow Y$ is called a \textbf{homeomorphism} \index{00@Homeomorphism} if the following clearly equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item $f$ and $f^{-1}$ are continuous.
\item For every net $(x_\alpha)$ in $X$ and each $x\in X$, we have that  $\lim_\alpha x_\alpha=x$ iff $\lim_\alpha f(x_\alpha)=f(x)$.
\item $f$ is continuous and open.
\item $f$ is continuous and closed.
\end{enumerate}
If a homeomorphism $f:X\rightarrow Y$ exists, we say that $X,Y$ are \textbf{homeomorphic}.
\end{df}

Recall from Def. \ref{lb189} that when $X,Y$ are metric spaces, the sequential version of (2) holds. 

\begin{rem}
Let $\mc T_1,\mc T_2$ be two topologies on a set $X$. Clearly, we have $\mc T_1=\mc T_2$ iff
\begin{gather}
\varphi:(X,\mc T_1)\rightarrow(X,\mc T_2)\qquad x\mapsto x
\end{gather}
is a homeomorphism. Thus, the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\mc T_1=\mc T_2$.
\item For every net $(x_\alpha)$ in $X$ and $x\in X$, we have that $\lim_\alpha x_\alpha=x$ under $\mc T_1$ iff $\lim_\alpha x_\alpha=x$ under $\mc T_2$.
\end{enumerate}
This equivalence implies that
\begin{align*}
\boxed{\text{ topologies are determined by net convergence }}
\end{align*}
Therefore, instead of using open sets or bases of topologies to describe a topology, one can also describe a topology $\mc T$ on a set $X$ in the following way:
\begin{gather}\label{eq65}
\begin{gathered}
\text{$\mc T$ is the unique topology on $X$ such that}\\
\text{a net $(x_\alpha)$ in $X$ converges to $x\in X$ iff ...}
\end{gathered}
\end{gather}
Similarly, by Def. \ref{lb189}, metrizable topologies are determined by sequential convergence. Therefore, metrizable topologies can be described in the following way:
\begin{gather}
\begin{gathered}
\text{$\mc T$ is the unique metrizable topology on $X$ such that}\\
\text{a sequence $(x_n)$ in $X$ converges to $x\in X$ iff ...}
\end{gathered}
\end{gather}
\end{rem}


%% Record #8 2023/10/16 two lectures 



\subsection{Examples of topological spaces described by net convergence}

\begin{eg}
Let $A$ be a subset of a topological space $(X,\mc T_X)$. Then the subspace topology $\mc T_A$ of $A$ is the unique topology such that a net $(x_\alpha)$ in $A$ converges to $x\in A$ under $\mc T_A$ iff it converges to $x$ under $\mc T_X$.
\end{eg}

\begin{seg}\label{lb193}
Let $X=\bigsqcup_{\alpha\in \scr A}X_\alpha$ be a disjoint union where each $(X_\alpha,\mc T_\alpha)$ is a topology space. Then
\begin{align*}
\mc B=\bigcup_{\alpha\in\scr A}\mc T_\alpha
\end{align*}
is clearly a basis generating a topology $\mc T$ on $X$, called \textbf{disjoint union topology}. \index{00@Disjoint union of topological spaces}   $\mc T$ is the unique topology on $X$ such that for every net $(x_\mu)_{\mu\in I}$ in $X$ and any $x\in X$, the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $(x_\mu)_{\mu\in I}$ converges to $x$ under $\mc T$.
\item There exists $\nu\in I$ such that for every $\mu\geq\nu$, the element $x_\mu$ belongs to the unique $X_\alpha$ containing $x$. Moreover, $\lim_{\mu\in I_{\geq\nu}}x_\mu=x$ in $X_\alpha$.
\end{enumerate}
We call $(X,\mc T)$ the \textbf{disjoint union topological space} \index{00@Disjoint union topological space} of $(X_\alpha)_{\alpha\in\scr A}$.
\end{seg}

The following exercise says that ``disjoint union of topological spaces" is synonymous with ``disjoint union of open subsets".

\begin{sexe}
Assume that $X=\bigsqcup_{\alpha\in\scr A}X_\alpha$. Assume that $X$ has a topology $\mc T$, and equip each $X_\alpha$ with the subspace topology. Show that $(X,\mc T)$ is the disjoint union topological space of $(X_\alpha)_{\alpha\in\scr A}$ iff each $X_\alpha$ is an open subset of $(X,\mc T)$.
\end{sexe}

Thus, for example, $\bigcup_{n\in\Nbb} [2n,2n+1)$ (under the Euclidean topology) is the disjoint union topological space of the family $\big([2n,2n+1)\big)_{n\in\Nbb}$.


\begin{sexe}
In Exp. \ref{lb193}, assume that each $(X_\alpha,\mc T_\alpha)$ is metrizable. Prove that $(X,\mc T)$ is metrizable. More precisely: Choose a metric $d_\alpha$ inducing $\mc T_\alpha$, and assume that $d_\alpha\leq 1$ (cf. Prop. \ref{lb195}). Define a metric $d$ on $X$ as in Pb. \ref{lb194}. Solve \textit{the net version} of part 2 of Pb. \ref{lb194}. Conclude from this that $d$ induces the topology $\mc T$. (Warning: we cannot conclude this from the original sequential version of Pb. \ref{lb194}-2.)
\end{sexe}


\begin{df}
Let $(X_\alpha)_{\alpha\in\mc A}$ be a family of topological spaces. Elements of the product space
\begin{align*}
S=\prod_{\alpha\in\scr A}X_\alpha
\end{align*}
are denoted by $x=(x(\alpha))_{\alpha\in\scr A}$. One checks easily that
\begin{align}
\begin{aligned}
\mc B=\Big\{&\prod_{\alpha\in\scr A} U_\alpha: \text{each $U_\alpha$ is open in $X_\alpha$},\\
& \text{$U_\alpha=X_\alpha$ for all but finitely many $\alpha$}\Big\}
\end{aligned}
\end{align}
is a basis for a topology $\mc T$, called the \textbf{product topology} \index{00@Product topology} or \textbf{pointwise convergence topology} \index{00@Pointwise convergence topology} of $S$. We call $(S,\mc T)$ the \textbf{product topological space}. \index{00@Product topological space} 

Equivalently, let
\begin{gather}
\pi_\alpha:S\rightarrow X_\alpha \qquad x\mapsto x(\alpha)
\end{gather}
be the \textbf{projection map onto the $X_\alpha$ component}. Then
\begin{align}
\mc B=\Big\{\bigcap_{\alpha\in E} \pi_\alpha^{-1}(U_\alpha):E\in\fin(2^{\scr A}), \text{ $U_\alpha$ is open in $X_\alpha$ for each $\alpha\in E$}    \Big\}
\end{align}
Unless otherwise stated, a product of topological spaces is equipped with the product topology.  \hfill\qedsymbol
\end{df}



\begin{eg}\label{lb226}
Let $S=X_1\times\cdots\times X_N$ be a finite product of topological spaces. Then the product topology has a basis
\begin{align}
\mc B=\{ U_1\times\cdots \times U_N:\text{ each $U_i$ is open in $X_i$}\}
\end{align}
\end{eg}


\begin{thm}\label{lb192}
Let $S=\prod_{\alpha\in\scr A}X_\alpha$ be a product of topological spaces, equipped with the product topology. Then each projection map $\pi_\alpha$ is continuous. Moreover, for every net $(x_\mu)_{\mu\in I}$ in $S$ and every $x\in S$, the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\dps\lim_{\mu\in I}x_\mu=x$ in $S$.
\item For every $\alpha\in\scr A$, we have $\dps\lim_{\mu\in I}x_\mu(\alpha)=x(\alpha)$ in $X_\alpha$.
\end{enumerate}
\end{thm}

If $(x_\mu)$ satisfies (1) or (2), we say that $x_\mu$ \textbf{converges pointwise} \index{00@Pointwise convergence} to $x$ if we view $(x_\mu)$ as a net of functions with domain $\scr A$. 

\begin{proof}
We leave the proof to the readers. Note that the continuity of $\pi_\alpha$ follows easily from the basis-for-topology version of Prop. \ref{lb191}-(2). And from the continuity of $\pi_\alpha$ one easily deduce (1)$\Rightarrow$(2).
\end{proof}

\begin{rem}
In the spirit of \eqref{eq65}, one says that:
\begin{itemize}
\item The product topology $\mc T$ on $S=\prod_{\alpha\in\scr A}X_\alpha$ is the unique topology such that a net $(x_\mu)$ converges to $x$ under $\mc T$ iff $x_\mu$ converges pointwise to $x$ as a net of functions on $\scr A$.
\end{itemize}
\end{rem}


\begin{co}
If each $X_\alpha$ is a Hausdorff space, then $S=\prod_{\alpha\in\scr A}X_\alpha$ is Hausdorff.
\end{co}

\begin{proof}
Either prove this directly using the basis for the topology, or prove that any net cannot converge to two different values using Thm. \ref{lb192}.
\end{proof}

\begin{co}\label{lb260}
Let $X_1,X_2,\dots$ be a possibly finite sequence of metric spaces. Then $S=\prod_i X_i$ is metrizable. More precisely, for each $i$, choose a metric $d_i$ on $X_i$ topologically equivalent to the original one such that $d_i\leq 1$ (cf. Prop. \ref{lb194}). Then the metric $d$ on $S$ defined by
\begin{align}
d(f,g)=\sup_{i} \frac {d_i(f(i),g(i))}{i} 
\end{align}
induce the product topology.
\end{co}

We note that the product topology is also induced by
\begin{align}
\delta(f,g)=\sum_{i}2^{-i} d_i(f(i),g(i))
\end{align}

\begin{proof}
The same method for solving Pb. \ref{lb78} also applies to its net version: One shows that a net $(f_\alpha)$ in $S$ converges to $f$ under $d$ (or under $\delta$) iff $(f_\alpha)$ converges pointwise to $f$. Thus, by Thm. \ref{lb192}, $d$ and $\delta$ induce the product topology.
\end{proof}

The next example discusses the topologies induced by uniform convergence metrics. (Recall Def. \ref{lb146}.) In this example, $Y$ is usually a normed vector space.

\begin{eg}\label{lb272}
Let $X$ be a set, and let $(Y,d_Y)$ be a metric space. Then there is a unique topology $\mc T$ on $Y^X$ such that for every net $(f_\alpha)_{\alpha\in I}$ in $Y^X$ and every $f\in Y^X$, the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item The net $(f_\alpha)$ converges to $f$ under $\mc T$.
\item We have $\dps\lim_{\alpha\in I}~\sup_{x\in X} d_Y(f_\alpha(x),f(x))=0$.
\end{enumerate}
(If $(f_\alpha)$ satisfies (2), we say that $f_\alpha$  \textbf{converges uniformly} \index{00@Uniform convergence} to $f$.) For example, one checks easily that $\mc T$ is induced by any \textbf{uniform convergence metric}, \index{00@Uniform convergence metric} i.e., any metric on $Y^X$ equivalent to $d$ where
\begin{align}\label{eq78}
d(f,g)=\min\Big\{1,\sup_{x\in X} d_Y(f(x),g(x))\Big\}
\end{align}
So $\mc T$ is metrizable. We call $\mc T$ the \textbf{uniform convergence topology} \index{00@Uniform convergence topology} on $Y^X$. 
\end{eg}

\begin{comment}
According to \eqref{eq78}, $\mc T$ has a basis
\begin{gather}\label{lb275}
\begin{gathered}
\mc B=\{U_{f,\eps}:f\in Y^X,\eps\in\Rbb_{>0}\}\quad\text{where}\\
U_{f,\eps}=\Big\{g\in Y^X:\sup_{x\in X}d_Y(f(x),g(x))<\eps   \Big\}
\end{gathered}
\end{gather}
\end{comment}

\begin{thm}\label{lb339}
Let $Y$ be a complete metric space. Let $X$ be a set. Then $Y^X$, equipped with the metric \eqref{eq78}, is complete.
\end{thm}

\begin{proof}
It can be proved in a similar way as Thm. \ref{lb85}. We leave the details to the readers.
\end{proof}



\begin{thm}\label{lb279}
Let $V$ be a normed vector space over $\Rbb$ or $\Cbb$. Let $X$ be a topological space. Equip $V^X$ with the uniform convergence topology. Then $C(X,V)$ is a closed subset of $V^X$.
\end{thm}

\begin{proof}
This is similar to the proof of Thm. \ref{lb87}. Let $(f_\alpha)$ be a net $C(X,V)$ converging uniformly to $f:X\rightarrow V$. Choose any $x\in X$ and $\eps>0$. Then there is $\alpha\in I$ such that $\sup_{p\in X}\lVert f(x)-f_\alpha(x)\lVert<\eps$. Since $f_\alpha$ is continuous, there is $U\in\Nbh_X(x)$ such that for each $p\in U$ we have $\lVert f_\alpha(x)-f_\alpha(p)\lVert<\eps$. Thus, for each $p\in U$ we have 
\begin{align*}
\lVert f(x)-f(p)\lVert\leq \lVert f(x)-f_\alpha(x)\lVert +\lVert f_\alpha(x)-f_\alpha(p)\lVert+\lVert f_\alpha(p)-f(p)\lVert<3\eps
\end{align*}
So $f$ is continuous.
\end{proof}


\begin{rem}
Note that the uniform convergence topology depends on the equivalence class (not just the topological equivalence class) of $d_Y$. Thus, one needs metrics when talking about uniform convergence. On the other hand, the study of pointwise convergence does not require metrics.
\end{rem}

\subsection{Limits of functions}\label{lb290}




By Prop. \ref{lb196}, if $A\subset X$,  and if $f:\ovl A\rightarrow Y$ is continuous, then the value of $f$ is uniquely determined by $f|_A$ provided that $Y$ is Hausdorff (cf. Prop. \ref{lb196}). We now consider the opposite question of \uwave{extension of continuous functions}: Suppose that $f:A\rightarrow Y$ is continuous. Can we extend $f$ to a continuous function $f:\ovl A\rightarrow Y$? (We know that such extension must be unique if it exists.) The classical concept of the limits of functions can be understood in this light.



\begin{df}\label{lb197}
Let $A$ be a subset of $X$. Let $f:A\rightarrow Y$ be a map. Let $x\in\ovl A\setminus A$. Let $y\in Y$. We say that the \textbf{limit of the function} \index{00@Limit of a function} $f$ at $x$ is $y$ and write \index{lim@$\lim_{p\rightarrow x}f(p)$}
\begin{align*}
\lim_{
\begin{subarray}{c}
p\in A\\
p\rightarrow x
\end{subarray}
} f(p)\equiv\lim_{p\rightarrow x}f(p)=y
\end{align*}
if the following equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item If we extend $f$ to a function $A\cup\{x\}\rightarrow Y$ satisfying $f(x)=y$, then $f:A\cup\{x\}\rightarrow Y$ is continuous at $x$.
\item For every $V\in\Nbh_Y(y)$, there exists $U\in\Nbh_X(x)$ such that for every $p\in U\cap A$, we have $f(p)\in V$.
\item For every net $(x_\alpha)_{\alpha\in I}$ in $A$ converging to $x$, we have $\lim_{\alpha\in I} f(x_\alpha)=y$.
\end{enumerate}
When $X,Y$ are metric spaces, the above three conditions and the following two are equivalent:
\begin{itemize}
\item[(2m)] For every $\eps>0$, there exists $\delta>0$ such that for every $p\in A$, if $d(p,x)<\delta$ then $d(f(p),y)<\eps$.
\item[(3m)] For every sequence $(x_n)_{n\in\Zbb_+}$ in $A$ converging to $x$, we have $\lim_{n\rightarrow\infty}f(x_n)=y$.
\end{itemize}
\end{df}


Recall that by the definition of subspace topology, we have
\begin{align}
\Nbh_A(x)=\{U\cap A:A\in\Nbh_X(x)\}  \label{eq104}
\end{align}

\begin{proof}[\textbf{Proof of equivalence}]
Extend $f$ to $\wtd f:A\cup\{x\}\rightarrow Y$ by setting $\wtd f(x)=y$. Then by Def. \ref{lb188}-(2), condition (1) of Def. \ref{lb197} means that for every $V\in\Nbh_Y(y)$ there is a neighborhood of $x$ in $A\cup\{x\}$ (which, by \eqref{eq104}, must be of the form $U\cap (A\cup\{x\})$ where $U\in\Nbh_X(x)$) such that for every $p\in U\cap (A\cup\{x\})$ we have $\wtd f(p)\in V$. This is clearly equivalent to (2), since $\wtd f(x)=y\in V$. The equivalence (2)$\Leftrightarrow$(3) can be proved in a similar way as the equivalence of (1) and (2) in Def. \ref{lb188}. We leave the details to the readers. When $X,Y$ are metric spaces, (2) is clearly equivalent to (2m). The equivalence (2m)$\Leftrightarrow$(3m) can be proved in a similar way as the equivalence of (1) and (2) in Def. \ref{lb31}.
\end{proof}






The following remarks show that the limit of a function at a point is the limit of a single net, rather than the limit of many nets (as in Def. \ref{lb197}-(3)).

\begin{rem}\label{lb275}
Assume the setting of Def. \ref{lb197}. In the same spirit of Exe. \ref{lb198}, define a directed set $(\Pnbh_A(x),\leq)$ where
\begin{gather}
\begin{gathered}
\Pnbh_A(x)=\big\{(p,U):U\in\Nbh_X(x),p\in U\cap A  \big\}\\[0.5ex]
(p,U)\leq(p',U')\qquad\Longleftrightarrow\qquad U\supset U'
\end{gathered}
\end{gather}
(That it is a directed set is due to $x\in \ovl A$.) We have seen this directed set in Rem. \ref{lb270}. Then $(p)_{(p,U)\in\Pnbh_A(x)}$ is a net converging to $x$, and
\begin{align}
\lim_{p\rightarrow x} f(p)=\lim_{(p,U)\in\Pnbh_A(x)}f(p)
\end{align}
where the convergence of the LHS is equivalent to that of the RHS.
\end{rem}

\begin{rem}\label{lb269}
In the setting of Def. \ref{lb197}, assume moreover that $X$ is a metric space, then $\lim_{p\rightarrow x}f(p)$ can be described by the limit of a simpler net. Define a directed set $(A_x,\leq)$ where
\begin{gather}
\begin{gathered}
A_x=A\text{ as sets}\\[0.5ex]
p\leq p'\qquad\Longleftrightarrow \qquad d(p',x)\geq d(p,x)
\end{gathered}
\end{gather}
Then $(p)_{p\in A_x}$ is a net in $A$ converging to $x$, and
\begin{align}
\lim_{p\rightarrow x} f(p)=\lim_{p\in A_x}f(p)
\end{align}
where the convergence of the LHS is equivalent to that of the RHS.
\end{rem}


\begin{rem}\label{lb318}
Thanks to the above two remarks, limits of functions enjoy all the properties that limits of nets enjoy. For example, they satisfy Squeeze theorem; if $f:A\rightarrow \Fbb$ and $g:A\rightarrow V$ (where $V$ is a normed vector space over $\Fbb\in\{\Rbb,\Cbb\}$), if $x\in\ovl A\setminus A$,  and if $\lambda=\lim_{p\rightarrow x}f(p)$ and $v=\lim_{p\rightarrow x}g(p)$ exist, then $\lim_{p\rightarrow x}f(p)g(p)$ converges to $\lambda v$. 

Of course, you can also conclude them by using Def. \ref{lb197}-(3) instead of Rem. \ref{lb275}. In practice, it makes no difference whether you view $\lim_{p\rightarrow x}f(p)$ as the limit of $f(x_\alpha)$ for an arbitrary net $x_\alpha$ in $A$ converging to $x$, or whether you view  $\lim_{p\rightarrow x}f(p)$ as the limit of the particular net in Rem. \ref{lb275} or Rem. \ref{lb269}. The explicit constructions of nets in these two remarks are not important for proving results about limits of functions.   \hfill\qedsymbol
\end{rem}




\begin{rem}\label{lb202}
Let $f:A\rightarrow Y$, and let $x\in\ovl A\setminus A$. Suppose that $Y$ is Hausdorff. Assume that there exist two nets $(x_\alpha)_{\alpha\in I}$ and $(y_\beta)_{\beta\in J}$ in $A$ converging to $x$ such that $(f(x_\alpha))$ and $(f(y_\beta))$ converge to two different values. Then by Def. \ref{lb197}-(2), the limit $\lim_{p\rightarrow x}f(p)$ does not exist.
\end{rem}


The above remark gives a useful criterion for the non-convergence of limits of functions. The following proposition, on the other hand, gives a method of computing limits of functions by decomposing the domain into (non-necessarily mutually disjoint) subsets.


\begin{pp}\label{lb199}
Assume the setting of Def. \ref{lb197}. Assume that $A=A_1\cup\cdots\cup A_N$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item We have $\dps \lim_{p\rightarrow x} f(p)=y$.
\item For every $1\leq i\leq N$ such that $x\in\ovl {A_i}$, we have $\dps\lim_{p\rightarrow x} f|_{A_i}(p)=y$
\end{enumerate}
\end{pp}

\begin{proof}
(1)$\Rightarrow$(2): Assume (1). Extend $f:A\rightarrow Y$ to a function $\wtd f:A\cup\{x\}\rightarrow Y$ by setting $\wtd f(x)=y$. Then $\wtd f$ is continuous by (1). Thus, if $x\in\ovl{A_i}$, then $\wtd f|_{A_i\cup\{x\}}$ is continuous. This proves (2).

(2)$\Rightarrow$(1): Assume (2). Choose any $V\in\Nbh_Y(y)$. By (2), for each $i$, either $x\in\ovl{A_i}$ so that there is $U_i\in\Nbh_X(x)$ satisfying $U_i\cap A_i\subset f^{-1}(V)$ (recall \eqref{eq104}), or that $x\notin\ovl{A_i}$ so that there is $U_i\in\Nbh_X(x)$ disjoint from $A_i$. In either case, we have $U_i\cap A_i\subset f^{-1}(V)$. Let $U=U_1\cap\cdots\cap U_N$. Then
\begin{align*}
U\cap A=U\cap(A_1\cup\cdots\cup A_N)=\bigcup_i U\cap A_i\subset\bigcup_i U_i\cap A_i
\end{align*}
which is therefore a subset of $f^{-1}(V)$.

Another proof of $\neg$(1)$\Rightarrow$ $\neg$(2): Assume (1) is not true. Then there is a net $(x_\alpha)_{\alpha\in I}$ in $A$ converging to $x$ such that $f(x_\alpha)$ does not converge to $y$. So there exists $V\in\Nbh_Y(y)$ such that $f(x_\alpha)$ is not eventually in $V$, i.e., $f(x_\alpha)$ is frequently in $V^c$. Then $(f(x_\alpha))$ has a subnet $(f(x_{\beta}))_{\beta\in J}$ which is always in $V^c$. For example, take
\begin{align*}
J=\{\beta\in I:f(x_\beta)\in V^c\}
\end{align*}
Since $(x_\beta)_{\beta\in J}$ is always in $A$, by the logic \eqref{eq103}, there is $1\leq i\leq N$ such that $(x_\beta)$ is frequently in $A_i$. Thus, by the same argument as above, $(x_\beta)$ has a subnet $(x_\gamma)_{\gamma\in K}$ which is always in $A_i$. Since $x_\alpha\rightarrow x$, we have $x_\gamma\rightarrow x$, and hence $x\in\ovl{A_i}$. But $f(x_\gamma)\in V^c$. So we have found a net $(x_\gamma)$ in $A_i$ converging to $x$ such that $(f(x_\gamma))$ does not converge to $y$. This disproves (2).
\end{proof}








\begin{rem}
In many textbooks, $\lim_{p\rightarrow x}f(x)$ is also defined more generally when $x$ is an accumulation point of $A$, i.e., when $x\in\ovl{A\setminus\{x\}}$. In this case, the limit of $f$ at $x$ simply means \index{lim@$\lim_{p\rightarrow x}f(p)$}
\begin{align}\label{eq66}
\lim_{p\rightarrow x}f(p)
\xlongequal{\mathrm{def}} \lim_{p\rightarrow x}f|_{A\setminus\{x\}}(p)
\end{align}
This more general case is important in classical analysis, but is less useful in abstract analysis. (As a matter of fact, accumulation points are less convenient than closure points.) In order not to deviate too far from the traditional analysis textbooks, let's take a look at some examples.
\end{rem}


\begin{df}\label{lb200}
Let $A\subset\Rbb$ and $x\in\Rbb$. Let $f:A\rightarrow Y$ be a function. If $x$ is a closure point of $A\cap\Rbb_{<x}$ resp. $A\cap \Rbb_{>x}$, we define the \textbf{left limit} resp. \textbf{right limit} \index{00@Left and right limit} \index{lim@$\lim_{t\rightarrow x^-}$ and $\lim_{t\rightarrow x^+}$} to be
\begin{subequations}
\begin{gather}
\lim_{t\rightarrow x^-}f(t)=\lim_{t\rightarrow x}f|_{A\cap \Rbb_{<x}}(t)\\
\lim_{t\rightarrow x^+}f(t)=\lim_{t\rightarrow x}f|_{A\cap \Rbb_{>x}}(t)
\end{gather}
\end{subequations}
If $x\in\ovl\Rbb$ and $x$ is a closure point of $A\setminus \{x\}$, then $\lim_{t\rightarrow x}f(t)$ is understood by \eqref{eq66}.
\end{df}

\begin{rem}\label{lb201}
In Def. \ref{lb200}, if $x\in\Rbb$ is a closure point of $A\setminus\{x\}$, then by Prop. \ref{lb199},
\begin{align}
\lim_{p\rightarrow x}f(p)=y\qquad\Longleftrightarrow\qquad \lim_{p\rightarrow x^-}f(p)=\lim_{p\rightarrow x^+}f(p)=y
\end{align}
In particular, the existence of the limit on the LHS is equivalent to the existence and the equality of the two limits on the RHS.
\end{rem}



\begin{eg}
Let $g,h:\Rbb\rightarrow \Rbb$ be continuous functions. Let $c\in\Rbb$. Define $f:\Rbb\rightarrow\Rbb$ to be
\begin{align*}
f(x)=\left\{
\begin{array}{ll}
g(x)&\text{ if }x<0\\
c&\text{ if }x=0\\
h(x)&\text{ if }x>0\\
\end{array}
\right.
\end{align*}
Since $g|_{(-\infty,0]}$ is continuous, by Def. \ref{lb197}-(1) we have that $\lim_{x\rightarrow 0^-}f(x)=\lim_{x\rightarrow 0,t<0}g(t)=g(0)$. Similarly, $\lim_{x\rightarrow 0^+}f(x)=h(0)$. Therefore, by Rem. \ref{lb201}, $\lim_{x\rightarrow0}f(x)$ exists iff $g(0)=h(0)$, and it converges to $g(0)$ if $g(0)=h(0)$. The value $c$ is irrelevant to the limits.
\end{eg}


\begin{eg}
Let $f:X=\Rbb^2\setminus\{(0,0)\}\rightarrow \Rbb$ be $f(x,y)=\frac{x}{x+y}$. Then $(1/n,0)$ and $(0,1/n)$ are sequences in $X$ converging to $0$. But $f(1/n,0)=1$ and $f(0,1/n)=0$. So $\lim_{(x,y)\rightarrow(0,0)}f(x,y)$ does not exist by Rem. \ref{lb202}.
\end{eg}
















\subsection{Connected spaces}

Let $X$ be a topological space. In this section, we shall define a notion of connected space. Based on our usual geometric intuition, one might attempt to define a connected space as one satisfying that any two points can be linked by a path. Such spaces are actually called \textbf{path-connected spaces} and is stronger than connected spaces. In fact, connected spaces arise from the study of intermediate value problem.

\subsubsection{Connected $\Leftrightarrow$ IVP}

\begin{df}\label{lb212}
We say that the topological space $X$ is \textbf{connected} \index{00@Connected topological space} if $X$ can not be written as the disjoint of two nonempty open sets. Namely, if $X=U\sqcup V$ where $U,V$ are open subsets of $X$, then either $U=\emptyset$ (and hence $V=X$) or $V=\emptyset$ (and hence $U=X$).
\end{df}

Equivalently (by Thm. \ref{lb181}), $X$ is connected iff every $U\subset X$ which is both closed and open must be $\emptyset$ or $X$.


\begin{df}
We say that $X$ satisfies the \textbf{intermediate value property} (abbreviated to \textbf{IVP}) \index{00@IVP=Intermediate value property} if for every continuous function $f:X\rightarrow\Rbb$ and every $x,y\in X$ we have
\begin{align}
f(x)<f(y)\qquad\Longrightarrow\qquad [f(x),f(y)]\subset f(X)  \label{eq67}
\end{align}
\end{df}


\begin{thm}\label{lb206}
$X$ is connected iff $X$ satisfies IVP. Moreover, if $X$ is not connected, then there is a continuous $f:X\rightarrow\Rbb$ such that $f(X)=\{0,1\}$.
\end{thm}

\begin{proof}
First, assume that $X$ does not satisfy IVP. Choose a continuous $f:X\rightarrow\Rbb$ with real numbers $a<b<c$ such that $a,c\in f(X)$ but $b\notin f(X)$. So $U=f^{-1}(-\infty,b)$ and $V=f^{-1}(b,+\infty)$ are disjoint non-empty open subsets of $X$, and $X=U\sqcup V$. They are open, because $f$ is continuous (cf. Prop. \ref{lb191}). So $X$ is not connected.

Next, assume that $X$ is not connected. Then $X=U\sqcup V$ where $U,V$ are open subsets of $X$. Define $f:X\rightarrow\Rbb$ to be constantly $0$ on $U$ and constantly $1$ on $V$. It is easy to check that $f$ is continuous. (See also Rem. \ref{lb205}.) That $f(X)=\{0,1\}$ means that $X$ does not satisfy IVP.
\end{proof}



We now give a couple of elementary examples.


\subsubsection{Connected subsets of $\ovl\Rbb$ are precisely intervals}


\begin{pp}\label{lb207}
Let $A$ be a dense subset of $X$. Assume that $A$ is connected. Then $X$ is connected.
\end{pp}

\begin{proof}
If $X=\ovl A$ is not connected, then by Thm. \ref{lb206}, there exists a continuous surjection $f:\ovl A\rightarrow\{0,1\}$. By Prop. \ref{lb196}, $\ovl{f(A)}$ contains $f(\ovl A)$. So $f(A)$ has closure $\{0,1\}$. So $f(A)=\{0,1\}$. $A$ does not satisfy IVP, and hence is not connected.
\end{proof}









\begin{thm}\label{lb211}
Let $A$ be a nonempty subset of $\ovl\Rbb$. Then $A$ is connected iff $A$ is an interval.
\end{thm}

\begin{proof}
Step 1. Suppose that $A$ is connected. Let $a=\inf A$ and $b=\sup B$. To show that $A$ is one of $(a,b)$, $(a,b]$, $[a,b)$, $[a,b]$, it suffices to show that every $c\in (a,b)$ belongs to $A$. Suppose that some $c\in (a,b)$ does not belong to $A$. Then $A$ is the disjoint union of two nonempty open subsets $A\cap[-\infty,c)$ and $A\cap (c,+\infty]$, impossible.\\[-1ex]


Step 2. Every single point is clearly connected. Since every interval containing at least two points is homeomorphic to one of $[0,1]$, $(0,1]$, $[0,1)$, $(0,1)$, it suffices to prove that these four intervals are connected. Since $(0,1)$ is dense in the other three intervals, by Prop. \ref{lb207}, it suffices to prove that $(0,1)$ is connected.

Suppose that $(0,1)$ is not connected. Then $(0,1)=U\sqcup V$ where $U,V$ are disjoint open nonempty subsets. Choose $x_1\in U$ and $y_1\in V$, and assume WLOG that $x_1<y_1$. In the following, we construct an increasing sequence $(x_n)$ in $U$ and a decreasing one $(y_n)$ in $V$ satisfying $x_n<y_n$ for all $n$ by induction.  Suppose $x_n,y_n$ has been constructed. Let $z_n=(x_n+y_n)/2$. 
\begin{itemize}
\item If $z_n\in U$, then let $x_{n+1}=z_n$ and $y_{n+1}=y_n$.
\item If $z_n\in V$, then let $x_{n+1}=x_n$ and $y_{n+1}=z_n$.
\end{itemize}
Then $y_n-x_n$ converges to $0$. So $x_n$ and $y_n$ converge to the same point $\xi\in\Rbb$. We have $\xi\in(0,1)$ since $x_1<\xi<y_1$. Since $V$ is open, $U=(0,1)\setminus V$ is closed in $(0,1)$ by Thm. \ref{lb181}. So $\xi\in\Cl_{(0,1)}(U)=U$. Similarly, $\xi\in\Cl_{(0,1)}(V)=V$. This is impossible.
\end{proof}







\subsubsection{More examples of connected spaces}


\begin{df}
Let $x,y\in X$. A \textbf{path} \index{00@Path in a topological space} in $X$ from $x$ to $y$ is defined to be a continuous map $\gamma:[a,b]\rightarrow X$ where $-\infty<a<b<+\infty$, such that $\gamma(a)=x$ and $\gamma(b)=y$. Unless otherwise stated, we take $[a,b]$ to be $[0,1]$. We call $x$ and $y$ respectively the \textbf{initial point} and the \textbf{terminal point} of $\gamma$. 
\end{df}

\begin{df}
We say that $X$ is \textbf{path-connected} \index{00@Path-connected space} if for every $x,y\in X$ there is a path in $X$ from $x$ to $y$.
\end{df}


\begin{eg}
$\Rbb^N$ is path-connected. $B_{\Rbb^N}(0,R)$ and $\ovl B_{\Rbb^N}(0,R)$ (where $R<+\infty$) are path connected. $\{x\in \Rbb^N:r<x<R\}$ (where $0\leq r<R<+\infty$) are path connected. The region enclosed by a triangle is a connected subset of $\Rbb^2$. $[0,1]^N$ is connected.
\end{eg}

\begin{thm}\label{lb220}
Assume that $X$ is path-connected. Then $X$ is connected.
\end{thm}

\begin{proof}
If $X$ is not connected, then $X=U\sqcup V$ where $U,V$ are nonempty open subsets of $X$. Since $X$ is path-connected, there is a path $\gamma$ from a point of $U$ to a point of $V$. So $[0,1]=\gamma^{-1}(U)\sqcup\gamma^{-1}(V)$ where $\gamma^{-1}(U),\gamma^{-1}(V)$ are open (by Prop. \ref{lb191}) and nonempty. This contradicts the fact that $[0,1]$ is connected (cf. Thm. \ref{lb211}).
\end{proof}



\begin{pp}\label{lb210}
Let $f:X\rightarrow Y$ be a continuous map of topological spaces. Suppose that $X$ is connected. Then $f(X)$ is connected.
\end{pp}

\begin{proof}
By replacing $f:X\rightarrow Y$ by the restricted continuous map $f:X\rightarrow f(X)$, it suffices to assume $Y=f(X)$. If $Y$ is not connected, then $Y=U\sqcup V$ where $U,V$ are open and nonempty. Then $X=f^{-1}(U)\sqcup f^{-1}(V)$ are open (by Prop. \ref{lb191}) and nonempty subsets of $X$. So $X$ is not connected, impossible. (One can also use Thm. \ref{lb206} to prove that $f(X)$ is connected.)
\end{proof}

\begin{rem}
When $Y=\Rbb$, Prop. \ref{lb210} and Thm. \ref{lb211} imply that $f(X)$ is an interval. So $f$ satisfies \eqref{eq67}. Therefore, Prop. \ref{lb210} can be viewed as a generalization of IVP for connected spaces.
\end{rem}



\begin{co}\label{lb213}
Let $I$ be an interval, and let $f:I\rightarrow\ovl\Rbb$ be a strictly increasing continuous map. Then $J=f(I)$ is an interval, and the restriction $f:I\rightarrow J$ is a homeomorphism.
\end{co}


\begin{proof}
By Thm. \ref{lb211} and Prop. \ref{lb210}, $J$ is connected and hence is an interval. Therefore, by Thm. \ref{lb65}, $f$ is a homeomorphism.
\end{proof}


\begin{seg}
Not all connected spaces are path-connected. Let $f:(0,1]\rightarrow\Rbb^2$ be defined by $f(x)=(x,\sin(x^{-1}))$. Then the range $f((0,1])$ is connected by Prop. \ref{lb210}. Since $f((0,1])$ is a dense subset of $X=f((0,1])\cup\{(0,0)\}$, by Prop. \ref{lb207}, $X$ is connected. However, it can be checked that $X$ is not path-connected. (Prove it yourself, or see \cite[Sec. 24]{Mun}.) $X$ is called the \textbf{topologist's sine curve}.
\end{seg}




The following proposition can be used to decompose (for example) an open subset of $\Rbb^N$ into open connected subsets. (See Pb. \ref{lb221}.)


\begin{pp}\label{lb208}
Assume that $X=\bigcup_{\alpha\in\scr A}X_\alpha$ where each $X_\alpha$ is connected. Assume that $\bigcap_{\alpha\in\scr A}X_\alpha\neq\emptyset$. Then $X$ is connected.
\end{pp}

\begin{proof}
Suppose that $X$ is not connected. By Thm. \ref{lb206}, there is a continuous surjection $f:X\rightarrow\{0,1\}$. Let $p\in\bigcap_\alpha X_\alpha$. Then $f(p)$ is $0$ or $1$. Assume WLOG that $f(p)=0$. Choose $x\in X$ such that $f(x)=1$. Choose $\alpha$ such that $x\in X_\alpha$. Then $f|_{X_\alpha}:X_\alpha\rightarrow\{0,1\}$ is a continuous surjection. So $X_\alpha$ does not satisfy IVP, and hence is not connected.
\end{proof}

\begin{exe}
Prove a path-connected version of Prop. \ref{lb208}.
\end{exe}

\begin{exe}
Prove Prop. \ref{lb207} and \ref{lb208} directly using Def. \ref{lb212} (but not using IVP).
\end{exe}



%% Record #9 2023/10/18 three lectures 





\subsection{Rigorous constructions of $\sqrt[n]{x}$, $\log x$, and $a^x$}\label{lb219}


With the help of Cor. \ref{lb213}, one can construct a lot of well-known functions rigorously. 

%Our first construction is $\sqrt[n]{x}$. This expression was used in Sec. \ref{lb218} to prove root test and hence to construct $e^x$. Therefore, $\sqrt[n]{x}$ will also be needed in the construction of its generalization $a^x=e^{x\log a}$. Thus, we shall construct $\sqrt[n]{x}$ before we construct $a^x$.


\begin{eg}\label{lb216}
Let $f:\Rbb_{\geq0}\rightarrow\Rbb_{\geq0}$ be $f(x)=x^n$ where $n\in\Zbb_+$. Then by Cor. \ref{lb213}, $J=f(\Rbb_{\geq 0})$ is an interval. Clearly $J\subset[0,+\infty)$. Since $0\in J$ and $\sup J=+\infty$, we have $J=[0,+\infty)$. Therefore $f$ is a homeomorphism. Its inverse function is a homeomorphism: the \textbf{$n$-th root} function
\begin{gather*}
\sqrt[n]{~}:\Rbb_{\geq0}\rightarrow\Rbb_{\geq0} \qquad x\mapsto\sqrt[n]x
\end{gather*}
This gives the rigorous construction of $\sqrt[n]{x}$.
\end{eg}

A similar method gives the rigorous construction of $\log$. 
\begin{eg}\label{lb217}
By Exp. \ref{lb214}, the exponential function $\exp:\Rbb\rightarrow\Rbb$ is continuous. We claim that $\exp$ is a strictly increasing homeomorphism from $\Rbb$ to $\Rbb_{>0}$. Its inverse function is called the \textbf{logarithmic function} \index{log@$\log$} 
\begin{align*}
\log:\Rbb_{>0}\rightarrow\Rbb \qquad x\mapsto \log x
\end{align*}
\end{eg}

\begin{proof}
From $e^x=\sum_{n=0}^\infty x^n/n!$ we clearly have $e^0=1$ and $e^x>1$ if $x>0$. From $e^{x+y}=e^xe^y$ proved in Cor. \ref{lb215}, we have $e^xe^{-x}=e^0=1$, which shows that $e^x\in\Rbb_{>0}$ for all $x\in\Rbb$. If $x<y$, then $e^y>e^x>0$ since $e^y=e^{y-x}e^x$ and $e^{y-x}>1$. So $\exp$ is strictly increasing. Thus, by Cor. \ref{lb213}, $\exp$ is a homeomorphism from $\Rbb$ to $J=\exp(\Rbb)$, and $J$ is an interval. 

When $x\geq 0$, we have $e^x\geq x$ from the definition of $e^x$. So $\sup_{x\geq 0}e^x=+\infty$. When $x\leq 0$, since $e^xe^{-x}=1$, we have $\inf_{x\leq 0}e^x=1/\sup_{x\geq 0}e^x=0$. So $\sup J=+\infty$ and $\inf J=0$. Since $0\notin\exp(\Rbb)$ (if $e^x=0$, then $1=e^xe^{-x}=0$, impossible), we have $J=\Rbb_{>0}$.
\end{proof}

\begin{eg}
Let $a\in\Rbb_{>0}$. For each $z\in\Cbb$, define
\begin{align}
a^z=e^{z\log a}
\end{align}
By Exp. \ref{lb217}, if $a>1$ (resp. $0<a<1$), the map
\begin{align}
\Rbb\rightarrow\Rbb_{>0}\qquad x\mapsto a^x
\end{align}
is an increasing (resp. decreasing) homeomorphism, since it is the composition of the increasing (resp. decreasing) homeomorphism $x\in\Rbb\mapsto x\log a\in\Rbb$ and the increasing one $\exp:\Rbb\rightarrow\Rbb_{>0}$. By the proof of Exp. \ref{lb217}, we have 
\begin{align}
a^0=1\qquad a^xa^{-x}=1\qquad a^xa^y=a^{x+y}
\end{align}
And clearly
\begin{align}
a^1=a.
\end{align}
It follows that for every $n\in\Zbb_+$, $a^n=a^{1+\cdots+1}=a\cdots a$. Namely, $a^n=e^{n\log a}$ agrees with the usual understanding of $a^n$. Thus, since $(a^{1/n})^n$ equals $a^{1/n}\cdots a^{1/n}=a^{1/n+\cdots+1/n}=a^1=a$, we conclude
\begin{align*}
a^{\frac 1n}=\sqrt[n]{a}
\end{align*}
\end{eg}



\begin{eg}
By Exp. \ref{lb217}, if $p>0$ (resp. $p<0$), then
\begin{align}
\Rbb_{>0}\rightarrow\Rbb_{> 0} \qquad x\mapsto x^p=e^{p\log x}\label{eq102}
\end{align}
is an increasing (resp. decreasing) homeomorphism, since it is the composition of the increasing (resp. decreasing)  homeomorphism $x\in\Rbb_{>0}\rightarrow p\log x\in\Rbb$ and the increasing homeomorphism $\exp:\Rbb\rightarrow\Rbb_{>0}$. 
\end{eg}







\subsection{Problems and supplementary material}


Let $X$ and $Y$ be topological spaces.

\subsubsection{Open sets, closed sets, closures}



\begin{prob}
Let $A,B\in X$. Let $(A_\alpha)_{\alpha\in \scr A}$ be a family of subsets of $X$. Prove that
\begin{subequations}
\begin{gather}
\ovl{A\cup B}=\ovl A\cup \ovl B\label{eq73}\\
\ovl{\bigcap_{\alpha\in \scr A}A_\alpha}\subset\bigcap_{\alpha\in\scr A}\ovl{A_\alpha}\label{eq74}
\end{gather}
\end{subequations}
\end{prob}

\begin{prob}\label{lb223}
Let $(x_\alpha)_{\alpha\in I}$ be a net in $X$. Let $x\in X$. Prove that the following statements are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $(x_\alpha)_{\alpha\in I}$ has a subnet converging to $x$.
\item For every neighborhood $U$ of $x$, we have that $x_\alpha$ is frequently in $U$.
\item $x$ belongs to $\dps\bigcap_{\alpha\in I}\ovl{\{x_\beta:\beta\geq\alpha\}}$.
\end{enumerate}
Any $x\in X$ satisfying one of these three conditions is called a \textbf{cluster point} \index{00@Cluster point of a net} of $(x_\alpha)_{\alpha\in I}$. (Compare Pb. \ref{lb64}.)
\end{prob}

\begin{proof}[Hint]
(2)$\Leftrightarrow$(3) is a direct translation. Assume (2). To prove (1), show that $(J,\leq)$ is a directed set, where
\begin{subequations}
\begin{gather}
\begin{gathered}
J=\{(\alpha,U)\in I\times\Nbh_X(x):x_\alpha\in U \}\\[0.5ex]
(\alpha,U)\leq (\alpha',U')\qquad\Longleftrightarrow\qquad \alpha\leq \alpha'\text{ and }U\supset U'
\end{gathered}
\end{gather}
Prove that $(x_\mu)_{\mu\in J}$ is a subnet of $(x_\alpha)_{\alpha\in I}$ if for each $(\alpha,U)\in J$ we set
\begin{align}
x_{(\alpha,U)}=x_\alpha
\end{align}
\end{subequations}
(Namely, the increasing map $J\rightarrow I$ is defined to be $(\alpha,U)\mapsto \alpha$.) Prove that $(x_\mu)_{\mu\in J}$ converges to $x$. You should point out where (2) is used in your proofs.
\end{proof}





\begin{rem}\label{lb238}
By Pb. \ref{lb223}-(3), the set of cluster points of $(x_\alpha)$ is a closed subset, since intersections of closed subsets are closed (cf. Cor. \ref{lb186}, or by \eqref{eq74}).
\end{rem}

For the reader's convenience, we present below the sequential version of Pb. \ref{lb223}. ``(1)$\Leftrightarrow$(2)" is due to Pb. \ref{lb64}. ``(2)$\Leftrightarrow$(3)" is due to Pb. \ref{lb223}.

\begin{pp}\label{lb256}
Assume that $X$ is a metric space. Let $(x_n)_{n\in\Zbb_+}$ be a sequence in $X$. Let $x\in X$. The following statements are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $(x_n)_{n\in\Zbb_+}$ has a subsequence converging to $x$.
\item For every neighborhood $U$ of $x$, we have that $x_n$ is frequently in $U$.
\item $x$ belongs to $\dps\bigcap_{n\in\Zbb_+}\ovl{\{x_k:k\geq n\}}$.
\end{enumerate}
Any $x\in X$ satisfying one of these three conditions is called a \textbf{cluster point} of  $(x_n)_{n\in\Zbb_+}$.
\end{pp} 

Prop. \ref{lb256}-(3) should remind you of the definitions of $\limsup$ and $\liminf$.



\begin{srem}\label{lb263}
It is not hard to show that \textit{the (1,2,3) of Prop. \ref{lb256} are equivalent in the more general case that $X$ is a first countable topological space} (see below for the definition). The proof is similar to that for metric spaces, and is left to the readers as an exercise.
\end{srem}

\begin{df}\label{lb262}
Let $X$ be a topological space. A subset $\mc B_x$ of $\Nbh_X(x)$ is called a \textbf{neighborhood basis} \index{00@Neighborhood basis} of $x$, if for every $U\in\Nbh_X(x)$ there exists $V\in\mc B_x$ such that $V\subset U$. We say that $X$ \textbf{first countable} \index{00@First countable} if every point $x$ has a neighborhood basis $\mc B_x$ which is a countable set.  
\end{df}


\begin{eg}
If $X$ is a metric space, then $X$ is first countable, since for every $x\in X$, $\{B_X(x,1/n):n\in\Zbb_+\}$ is a neighborhood basis of $x$.
\end{eg}

\begin{rem}\label{lb267}
By Pb. \ref{lb223}-(2) and Prop. \ref{lb256}-(2), if $(x_n)$ is a sequence in a metric space $X$, then $(x_n)$ has a subsequence converging to $x\in X$ iff $(x_n)$ has a subnet converging to $x$. This is not necessarily true when $X$ is a general topological space. Note that in the general case, cluster points of a sequence $(x_n)$ mean cluster points of $(x_n)$ as a net. Thus, they are not the limits of convergent \textit{subsequences} of $(x_n)$. 
\end{rem}


\begin{comment}
\begin{rem}
Although the word ``cluster point" is also used for a subset $A$ of $X$ (cf. Rem. \ref{lb176}), it is not parallel to the notion of cluster points of net. The true analogous concept of ``cluster points of a net" is ``closure points of a subset".
\end{rem}
\end{comment}



\begin{prob}\label{lb251}
Assume that $X$ is a metric space. Let $E\subset X$. Recall that $d(x,E)=d(E,x)=\inf_{e\in E}d(e,x)$. Prove that
\begin{align}
\{x\in X:d(x,E)=0\}=\ovl E
\end{align}
\end{prob}





\begin{rem}\label{lb257}
If $E,F$ are disjoint subsets of a metric space $X$, a continuous function $f:X\rightarrow[0,1]$ is called an \textbf{Urysohn function} \index{00@Urysohn function of a metric space} with respect to $E,F$, if
\begin{align*}
f^{-1}(1)=E\qquad f^{-1}(0)=F
\end{align*} 
For example, it is easy to check that
\begin{gather}
f:X\rightarrow[0,1]\qquad f(x)=\frac{d(x,F)}{d(x,E)+d(x,F)}
\end{gather}
is an Urysohn function.
\end{rem}


\begin{sprob}
A topological space $X$ is called \textbf{normal} \index{00@Normal topological space} if for every closed disjoint $E,F\subset X$, there exist disjoint open subsets $U,V\subset X$ such that $E\subset U$ and $F\subset V$. 
\begin{enumerate}
\item Prove that $X$ is normal iff for each $E\subset W\subset X$ where $E$ is closed and $W$ is open, there exists an open subset $U\subset X$ such that $E\subset U\subset \ovl U\subset W$.
\item Prove that if $X$ is metrizable, then $X$ is normal.
\end{enumerate}
\end{sprob}








\subsubsection{Continuous maps}


\begin{exe}\label{lb184}
Let $f:X\rightarrow Y$ be a map.
\begin{enumerate}
\item Suppose that $F$ is a subset of $Y$ containing $f(X)$. Show that $f:X\rightarrow Y$ is continuous iff $f:X\rightarrow F$ is continuous.
\item (Local to global principle) Suppose that $X=\bigcup_{\alpha\in I}U_\alpha$ where each $U_\alpha$ is an open subset of $X$, Prove that $f$ is continuous iff $f|_{U_\alpha}:U_\alpha\rightarrow Y$ is continuous for every $\alpha$. 
\end{enumerate}
\end{exe}

\begin{rem}\label{lb205}
The above local-to-global principle for continuous functions can be rephrased in the following way. Suppose that $X=\bigcup_{\alpha\in I}U_\alpha$ where each $U_\alpha$ is an open subset of $X$. Suppose that for each $\alpha$ we have a continuous map $f_\alpha:U_\alpha\rightarrow Y$. Assume that for each $\alpha,\beta\in I$ we have
\begin{align*}
f_\alpha|_{U_\alpha\cap U_\beta}=f_\beta|_{U_\alpha\cap U_\beta}
\end{align*}
Then there is a (necessarily unique) continuous function $f:X\rightarrow Y$ such that $f|_{U_\alpha}=f_\alpha$ for every $\alpha$.
\end{rem}



\begin{prob}
Let $f:X\rightarrow Y$ be a map. Suppose that $X=A_1\cup\cdots \cup A_N$ where $N\in\Zbb_+$ and $A_1,\dots,A_N$ are closed subsets of $X$. Suppose that $f|_{A_i}:A_i\rightarrow Y$ is continuous for each $1\leq i\leq N$. Prove that $f$ is continuous. 

Does the conclusion remain true if $A_1,\dots,A_N$ are not assumed closed? If no, find a counterexample.  \hfill\qedsymbol
\end{prob}

\begin{proof}[Note]
Do not use Prop. \ref{lb199} in your proof. But you can think about how this problem is related to Prop. \ref{lb199}.
\end{proof}




\begin{df}\label{lb175}
Let $(I,\leq)$ be a directed set. Let $\infty_I$ (often abbreviated to $\infty$) be a new symbol not in $I$. Then
\begin{align*}
I^*=I\cup\{\infty_I\}
\end{align*}
is also a directed set if we extend the preorder $\leq$ of $I$ to $I^*$ by setting
\begin{align*}
\alpha\leq\infty_I\qquad(\forall\alpha\in I^*)
\end{align*}
For each $\alpha\in I$, let
\begin{align*}
I^*_{\geq\alpha}=\{\beta\in I^*:\beta\geq\alpha\}
\end{align*}
The \textbf{standard topology} \index{00@Topology of $\mc I^*=I\cup\{\infty_I\}$ where $I$ is an index set} on $I^*$ is defined to be the one induced by the basis
\begin{align}\label{eq63}
\mc B=\big\{\{\alpha\}:\alpha\in I \big\}\cup \big\{I^*_{\geq\alpha}:\alpha\in I \big\}
\end{align}
\end{df}



\begin{prob}\label{lb278}
Let $(I,\leq)$ be a directed set. Let $I^*$ be as in Def. \ref{lb175}.
\begin{enumerate}
\item Check that $\mc B$ (defined by \eqref{eq63}) is a basis for a topology. (Therefore, $\mc B$ generates a topology $\mc T$ on $I^*$.) 
\item Let $(x_\alpha)_{\alpha\in I}$ be a net in a topological space $X$. Let $x_\infty\in X$. (So we have a function $x:I^*\rightarrow X$.) Prove that 
\begin{align}
x\text{ is a continuous function}\qquad\Longleftrightarrow\qquad \lim_{\alpha\in I} x_\alpha=x_\infty
\end{align}
\item Is $I^*$ Hausdorff? Prove it, or find a counterexample.
\end{enumerate} 
\end{prob}




\subsubsection{Product spaces}













\begin{prob}
Prove Thm. \ref{lb192}.
\end{prob}


\begin{prob}\label{lb258}
Let $(X_\alpha)_{\alpha\in\scr A}$ and $(Y_\alpha)_{\alpha\in\scr A}$ be families of nonempty topological spaces. Let $Z$ be a nonempty topological space. For each $\alpha\in\scr A$, choose maps $f_\alpha:X_\alpha\rightarrow Y_\alpha$ and $g_\alpha:Z\rightarrow X_\alpha$. 
\begin{enumerate}
\item Use Thm. \ref{lb192} to prove that 
\begin{align}
\prod_{\alpha\in\scr A}f_\alpha:\prod_\alpha X_\alpha\rightarrow\prod_\alpha Y_\alpha\qquad (x(\alpha))_{\alpha\in\scr A}\mapsto (f_\alpha(x(\alpha)))_{\alpha\in\scr A}
\end{align}
is continuous iff each $f_\alpha$ is continuous.
\item Use Thm. \ref{lb192} to prove that
\begin{align}
\bigvee_{\alpha\in\scr A}g_\alpha:Z\rightarrow \prod_\alpha X_\alpha \qquad z\mapsto (g_\alpha(z))_{\alpha\in\scr A}
\end{align}
is continuous iff each $g_\alpha$ is continuous.
\end{enumerate}
\end{prob}







\begin{prob}\label{lb203}
Let $(X_\alpha)_{\alpha\in\scr A}$ be an uncountable family of metric spaces, where each $X_\alpha$ has at least two elements. Let $S=\prod_{\alpha\in\scr A}X_\alpha$ be the product space, equipped with the product topology. Prove that $S$ is not first countable (recall Def. \ref{lb262}), and hence not metrizable.
\end{prob}

\subsubsection{Limits of functions}

\begin{prob}
Prove the equivalence of (2) and (3) in Def. \ref{lb197}.
\end{prob}


\begin{prob}
Find $\dps\lim_{(x,y)\rightarrow(0,0)}f(x,y)$, or explain why it does not exist:
\begin{gather*}
f(x,y)=\frac{x^2-y^2}{x^2+y^2}\\
f(x,y)=\frac{(xy)^2}{(xy)^2+(x-y)^2}\\
f(x,y)=\frac{x^6y^2}{(x^4+y^2)^2}
\end{gather*}
\end{prob}








\subsubsection{Connectedness}

\begin{prob}
Assume that $X,Y$ are not empty. Prove that $X$ and $Y$ are connected iff $X\times Y$ is connected.
\end{prob}

\begin{proof}[Hint]
Write $X\times Y$ as a union of sets of the form $(X\times\{y\})\cup (\{x\}\times Y)$.
\end{proof}


\begin{df}
A topological space $X$ is called \textbf{locally connected} \index{00@Locally connected} if every $x\in X$ has a neighborhood basis $\mc B_x$ (recall Def. \ref{lb262}) whose members are all connected.
\end{df}

\begin{eg}
Every open subset of a locally connected space is clearly locally connected.
\end{eg}

\begin{eg}
$\Rbb^N$ is locally connected, since open balls are path-connected and hence connected (by Thm. \ref{lb220}). Therefore, every open subset of $\Rbb^N$ is locally connected.
\end{eg}


\begin{prob}\label{lb221}
Suppose that $X$ is locally connected. Prove that $X$ has a unique (disjoint) decomposition $X=\bigsqcup_{\alpha\in\scr A}X_\alpha$ where each $X_\alpha$ is a nonempty connected open subset of $X$. (Each $X_\alpha$ is called a \textbf{connected component} of $X$.) \index{00@Connected component}
\end{prob}

\begin{proof}[Hint]
For each $x\in X$, consider the union of all connected neighborhoods containing $x$.
\end{proof}




\newpage

\section{Compactness}\label{lb351}


\subsection{Overture: two flavors of compactness}

Let $X$ be a topological space.



\begin{df}
An \textbf{open cover} \index{00@Open cover} of $X$ means a family $\fk U=(U_\alpha)_{\alpha\in\scr A}$ of open subsets of $X$ such that $X=\bigcup_{\alpha\in\scr A} U_\alpha$. $\fk U$ is called \textbf{finite} resp. \textbf{countable} if $\scr A$ is finite resp. countable. A \textbf{subcover} \index{00@Subcover} of $\fk U$ means an open cover $\fk V=(V_\beta)_{\beta\in\scr B}$ of $X$ such that each $V_\beta$ equals $U_\alpha$ for some $\alpha\in\scr A$.
\end{df}

\begin{df}
We say that $X$ is a \textbf{compact space} \index{00@Compact space} if every open cover of $X$ has a finite subcover.  We say that $X$ is a \textbf{Lindel\"of space} \index{00@Lindel\"of space} if every open cover of $X$ has a countable subcover.
\end{df}

\begin{rem}
For the purpose of this chapter, it suffices to consider open covers $\fk U=(U_\alpha)_{\alpha\in\scr A}$ such that $\alpha\in\scr A\mapsto U_\alpha\in 2^X$ is injective. (Namely, one can throw away repeated open sets.) In this case, we write
\begin{align*}
\fk U\subset 2^X
\end{align*}
and view $\fk U$ as a subset of $2^X$. A \textbf{subcover} of such $\fk U$ is an open cover $\fk V$ such that $\fk V\subset\fk U$. The readers can check that this assumption does not affect the definition of compact spaces and Lindel\"of spaces.
\end{rem}

\begin{rem}\label{lb244}
Compactness can also be formulated in a relevant version: If $A$ is a compact subset of $X$, and if $\fk U$ is a set of open subsets of $X$ such that $A\subset\bigcup_{U\in\fk U}U$, then since $\{U\cap A:U\in\fk U\}$ gives an open cover of $A$, we know that
\begin{align*}
A\subset U_1\cup\cdots\cup U_n
\end{align*}
some $U_1,\dots,U_n\in\fk U$. Conversely, any $A$ satisfying such property is a compact subspace of $X$.
\end{rem}


\begin{exe}
Show that a finite union of compact spaces is compact. Show that a finite set is compact.
\end{exe}

\begin{exe}
Show that $X$ is compact iff $X$ satisfies the \textbf{finite intersection property}: \index{00@Finite intersection property} The intersection of a family of non-empty closed subsets is nonempty.
\end{exe}



A main goal of this chapter is to prove the following theorem. Its proof will be finished at the end of Sec. \ref{lb253}. In fact, the formal proof will only be in Sec. \ref{lb254} and Sec. \ref{lb253}. 

\begin{thm}\label{lb222}
Let $X$ be a metric space. Then $X$ is sequentially compact iff $X$ is compact.
\end{thm}
This is the first fundamental theorem we prove in this course. Its significance lies in the fact that it connects two seemingly very different notions of compactness, and hence two strikingly different intuitions. We hope that the reader can not only follow the logical chains of the proofs, but also understand the pictures behind the proof. More precisely, we hope that the readers can have an intuitive understanding of the following questions:
\begin{itemize}
\item Why are these two compactness both powerful in solving certain problems? What roles do they play in the proof? How are the roles played by these two compactness related? (This is more important than just knowing why these notions are \textit{logically} equivalent.)
\item Why is one version of compactness more powerful than the other one in solving certain problems?
\end{itemize}




Thus, I feel that it is better to look at some applications of compactness before we prove Thm. \ref{lb222} rigorously. For pedagogical purposes, we also introduce two related notions of compactness:

\begin{df}
We say that $X$ is \textbf{net-compact}, \index{00@Net-compact} if every net $(x_\alpha)_{\alpha\in I}$ in $X$ has at least one cluster point (recall Pb. \ref{lb223}), equivalently, at least one convergent subnet. We say that $X$ is \textbf{countably compact}, \index{00@Countably compact} if every countable open cover of $X$ has a finite subcover.
\end{df}



Sequential compactness and net-compactness clearly share the same intuition. Countable compactness is intuitively similar to compactness. Also, compactness clearly implies countable compactness. But net-compactness does not imply sequential compactness in general: In a net-compact space, every sequence has a convergent subnet, but not necessarily a convergent subsequence. 

The relationship between these four versions of compactness is as follows. (We will prove this in the course of proving Thm. \ref{lb222}.)
\begin{subequations}\label{eq68}
\begin{align}
&\text{Metric spaces:} & \text{all the four versions}&\text{ of compactness are equivalent}\\
&\text{Topological spaces:} & \text{net-compact}&\Longleftrightarrow\text{compact}
\end{align}
\end{subequations}
After proving \eqref{eq68}, we will not use the notions of countable compactness and net-compactness. This is because net-compactness is equivalent to compactness, and countable compactness is more difficult to use than compactness. (Nevertheless, proving/using compactness by proving/using the existence of cluster points is often helpful.)




\subsection{Act 1: case studies}\label{lb293}



\subsubsection{Extreme value theorem (EVT)}

\begin{lm}[\textbf{Extreme value theorem}]\label{lb224} \index{00@EVT=Extreme value theorem}
Let $X$ be a compact topological space. Let $f:X\rightarrow\Rbb$ be a continuous function. Then $f$ attains its maximum and minimum at points of $X$. In particular, $f(X)$ is a bounded subset of $\Rbb$.
\end{lm}



We have seen the sequential compactness version of \textbf{EVT} (extreme value theorem) in Lem. \ref{lb56}. There, we find the point $x\in X$ at which $f$ attains its maximum by first finding a sequence $(x_n)$ such that $f(x_n)$ converges to $\sup f(X)$. Then we choose any convergent subsequence $(x_{n_k})$, which converges to the desired point $x$. The same method can be used to prove EVT for net-compact spaces if we replace sequences by nets.


For compact spaces, EVT is proved in a completely different way. In fact, without the tools of sequences and nets, one can not easily find $x$ at which $f$ attains it maximum. The argument is rather indirect:

\begin{proof}[Proof of Lem. \ref{lb224}]
It suffices to prove that $f(X)$ is bounded for all continuous maps $f:X\rightarrow\Rbb$. Then $a=\sup f(X)$ is in $\Rbb$. If $a\notin f(X)$, we choose a homeomorphism $\varphi:(-\infty,a)\rightarrow\Rbb$. So $\varphi\circ f:X\rightarrow\Rbb$ is  continuous but has no upper bound. This is impossible.
\end{proof}

Thus, to prove EVT, it remains to prove:
\begin{eg}\label{lb227}
Assume that $X$ is compact and $f:X\rightarrow\Rbb$ is continuous. Then $f(X)$ is a bounded subset of $\Rbb$.
\end{eg}

\begin{proof}
For each $x\in X$, since $f$ is continuous at $x$, there exists $U_x\in\Nbh(x)$ such that $|f(p)-f(x)|<1$ for all $p\in U_x$. In particular, $f(U_x)$ is bounded. Since $X=\bigcup_{x\in X}U_x$ is an open cover of $X$, by compactness, $X=U_{x_1}\cup\cdots\cup U_{x_n}$ for some $x_1,\dots,x_n\in X$. Thus $X= f(U_{x_1})\cup\cdots\cup f(U_{x_n})$ is bounded.
\end{proof}



The above proof is typical. It suggests that compactness is powerful for \uline{proving finiteness properties} rather than finding solutions of functions satisfying certain requirements. Thus, if you want to prove a finiteness property using sequential or net-compactness, you have to prove it indirectly. For example, you need to prove by contradiction:

\begin{eg}
Assume that $X$ is net-compact or sequentially compact. Assume that $f:X\rightarrow\Rbb$ is continuous. Then $f(X)$ is a bounded subset of $\Rbb$.
\end{eg}


\begin{proof}
Assume that $X$ is net-compact. If $f(X)$ is not bounded above, then there is a sequence $(x_\alpha)$ in $X$ (viewed as a net)  such that $\lim_\alpha f(x_\alpha)=+\infty$. By net-compactness, $(x_\alpha)$ has a subnet $(x'_\beta)$ converging to $x\in X$. So $f(x)=\lim_\beta f(x'_\beta)=+\infty$, impossible. So $f$ is bounded above, and hence bounded below by a similar argument. The case where $X$ is sequentially compact can be proved by a similar method.
\end{proof}




Let us look at a more complicated example.













\subsubsection{Uniform convergence in multivariable functions}\label{lb271}


\begin{eg}\label{lb225}
Let $X,Y$ be topological spaces. Assume that $Y$ is compact. Let $V$ be a normed vector space. Choose $f\in C(X\times Y,V)$. For each $x\in X$, let
\begin{align*}
f_x:Y\rightarrow V\qquad y\mapsto f(x,y)
\end{align*}
Equip $C(Y,V)$ with the $l^\infty$-norm. Then the following map is continuous:
\begin{align}
\Phi(f):X\rightarrow C(Y,V)\qquad x\mapsto f_x
\end{align}
\end{eg}



\begin{rem}
Note that since $Y$ is compact, each $g\in C(Y,V)$ is bounded by EVT (applied to $|g|$). So $C(Y,V)\subset l^\infty(Y,V)$.
\end{rem}


The continuity of $\Phi(f)$ means that for each $x\in X$, the following statement holds:
\begin{itemize}
\item[\textleaf] For every $\eps>0$ there exists $U\in\Nbh_X(x)$ such that for all $p\in U$ and all $y\in Y$ we have $\lVert f(p,y)-f(x,y)\lVert<\eps$.
\end{itemize}
This is clearly a finiteness property. Thus, its sequentially compact version or net-compact version should be proved indirectly. Indeed, when $X,Y$ are metric spaces and $Y$ is sequentially compact, we have proved this in Pb. \ref{lb103} by contradiction. The same method (with sequences replaced by nets) also works for the net-compact case:

\begin{eg}
Example \ref{lb225} is true, assuming that $Y$ is net-compact rather than compact.
\end{eg}


\begin{proof}
Suppose ``\textleaf" is not true. Then there is $\eps>0$ such that for every $U\in\Nbh_X(x)$ there is $x_U\in U$ and $y_U\in Y$ such that $\lVert f(x_U,y_U)-f(x,y_U)\lVert\geq\eps$. Then $(x_\alpha)_{\alpha\in\Nbh_X(x)}$ is a net converging to $x$, where $x_\alpha=x_U$ if $\alpha=U$. Since $Y$ is net-compact, $(y_\alpha)$ has a subnet $(y_\beta)$ converging to  some $y\in Y$. Since the subnet $(x_\beta)$ also converges to $x$, we have $\lim_\beta f(x_\beta,y_\beta)=f(x,y)$ and $\lim_\beta f(x,y_\beta)=f(x,y)$ by the continuity of $f$. This contradicts the fact that $\lVert f(x_\beta,y_\beta)-f(x,y_\beta)\lVert\geq\eps$ for all $\beta$.
\end{proof}



On the other hand, the solution of Exp. \ref{lb225} using open covers is a direct proof:

\begin{proof}[\textbf{Proof of Exp. \ref{lb225}}]
``\textleaf" is a finiteness property global over $Y$. We prove ``\textleaf" by first proving it locally, and then passing to the global space $Y$ using the compactness of $Y$.

Fix $x\in X$. Choose any $\eps>0$. For each $y\in Y$, since $f$ is continuous at $(x,y)$, there is a neighborhood $W$ of $(x,y)$ such that for every $(p,q)\in W$ we have $\lVert f(p,q)-f(x,y)\lVert<\eps/2$. By Exp. \ref{lb226}, we can shrink $W$ to a smaller neighborhood of the form $U_y\times V_y$ where $U_y\in\Nbh_X(x)$ and $V_y\in\Nbh_Y(y)$. Then for each $p\in U_y$ and $q\in U_y$ we have $\lVert f(p,q)-f(x,y)\lVert<\eps/2$, and hence $\lVert f(p,q)-f(x,q)\lVert<\eps$. This proves the special case of ``\textleaf" where $Y$ is replaced by $U_y$.

Now we pass from local to global in the same way as in the proof of Exp. \ref{lb227}. Since $Y=\bigcup_{y\in Y}V_y$ is an open cover of $Y$, by the compactness of $Y$, there is a finite subset $F\subset Y$ such that $Y=\bigcup_{y\in F}V_y$. Then ``\textleaf" is true if we let $U=\bigcap_{y\in F}U_y$.
\end{proof}


\subsubsection{Conclusions}

\begin{enumerate}%[label=(\arabic*)]
\item Sequential compactness and net-compactness are useful for finding solutions of a function satisfying some given conditions. 
\item Compactness is useful for proving finiteness properties. The proof is usually a  local-to-global argument. It is usually a direct argument (rather than proof by contradiction).
\item If one uses sequential/net-compactness to prove a finiteness property, one usually proves it by contradiction: Assume that this finiteness is not true. Find a sequence/net $(x_\alpha)$ that violates this finiteness property, and pass to a convergent subsequence/subnet to find a contradiction.
\item Therefore, for sequential/net-compact spaces, the argument is in the direction of ``getting smaller and smaller", opposite to the argument for compact spaces.
\end{enumerate}

Let me emphasize that the proof for sequentially/net-compact spaces is opposite to the one for compact spaces in two aspects: (1) If one argument is direct, the other is a proof by contradiction for the same problem. (2) The former has the intuition of ``getting smaller", while the latter local-to-global argument has the intuition of ``getting larger". 

I have already touched on this phenomenon in Rem. \ref{lb229}: \textit{The reason that sequences and nets run in the opposite direction to that of open sets is because closed sets are opposite to open sets}, as proved in Thm. \ref{lb181}.  Thus, you can expect that the transition between closed and open sets plays a crucial role in the following proof of Thm. \ref{lb222}. 

\begin{comment}

Now, we have a more vivid sense of the following sentence: 
\begin{itemize}
\item[$\Sun/\Moon$] The seemingly simple fact that ``closed sets are the complements of open sets" is the golden key that allows you to walk between the worlds of dark and light.
\end{itemize}



Let me close this section with a final remark.

\begin{rem}
Logically equivalent concepts are usually not intuitively equivalent. Proving directly is sometimes more transparent and intuitive than proving by contradiction, and sometime vice versa. For example: You will never prove a property by contradiction unless you already know what should be proved, and what statement is expected true. However, a direct argument can be more helpful in finding an unknown property and setting the goal.
\end{rem}
\end{comment}


\subsection{Act 2: ``sequentially compact $\Leftrightarrow$ countably compact'' for metric spaces, just as ``net-compact $\Leftrightarrow$ compact''}\label{lb254}

\epigraph{The road up and the road down is one and the same.}{Heraclitus}


As mentioned before, our goal of this chapter is to prove ``sequentially compact $\Leftrightarrow$ compact" for metric spaces. Our strategy is as follows: We reformulate the sequential compactness condition in terms of decreasing chains of closed sets, and reformulate the compactness condition in terms of increasing chains of open sets. Then we relate these two pictures easily using Thm. \ref{lb181}.

The following difficulty arises when carrying out this strategy. Sequences are countable by nature, whereas open covers can have arbitrarily large cardinality. Thus, sequences are related to countable decreasing chains, and hence countable open covers. Therefore, the above idea only implies the equivalence
\begin{subequations}\label{eq69}
\begin{align}\label{eq70}
\text{sequentially compact}\quad\Longleftrightarrow\quad \text{countably compact}\qquad(\text{for metric spaces})
\end{align}
Accordingly, it will only imply
\begin{align}\label{eq71}
\text{net-compact}\qquad\Longleftrightarrow\qquad \text{compact}
\end{align}
\end{subequations}
since there are no constraints on the cardinalities of indexed sets of nets. We will prove \eqref{eq69} in this section, and leave the proof of ``countably compact $\Leftrightarrow$ compact" for metric spaces to Sec. \ref{lb253}.


Since the proofs of \eqref{eq70} and \eqref{eq71} are similar, we first discuss \eqref{eq71}.


\begin{pp}\label{lb230}
Let $X$ be a topological space. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is compact.
\item (\textbf{Increasing chain property}) If $(U_\mu)_{\mu\in I}$ is an increasing net of open subsets of $X$ satisfying $\bigcup_{\mu\in I}U_\mu=X$, then $U_\mu=X$ for some $\mu$.
\item (\textbf{Decreasing chain property}) If $(E_\mu)_{\mu\in I}$ is a decreasing net of nonempty closed subsets of $X$, then $\bigcap_{\mu\in I}E_\mu\neq\emptyset$.
\end{enumerate}
\end{pp}

Here, ``increasing net" means $U_\mu\subset U_\nu$ if $\mu\leq \nu$, and ``decreasing net" means the opposite.

\begin{proof}
(1)$\Rightarrow$(2): Assume (1). Then $X=\bigcup_\mu U_\mu$ is an open cover of $X$. So, by the compactness of $X$, we have $X=U_{\mu_1}\cup\cdots\cup U_{\mu_n}$ for some $\mu_1,\dots,\mu_n\in I$. Choose $\mu\in I$ which is $\geq \mu_1,\dots,\mu_n$. Then $X=U_\mu$.

(2)$\Rightarrow$(1): Assume (2). Let $X=\bigcup_{\alpha\in\scr A} W_\alpha$ be an open cover of $X$. Let $I=\fin(2^{\scr A})$. For each $\mu=\{\alpha_1,\dots,\alpha_n\}\in I$, let $U_\mu=W_{\alpha_1}\cup\cdots\cup W_{\alpha_n}$. Then $(U_\mu)_{\mu\in I}$ is an increasing net of open sets covering $X$. Thus, by (2), we have $U_\mu=X$ for some $\mu$. This proves (1).

(2)$\Leftrightarrow$(3): If we let $E_\mu=X\setminus U_\mu$, then (2) says that if $(E_\mu)$ is a decreasing net of closed sets whose intersection is $\emptyset$, then $E_\mu=\emptyset$ for some $\mu$. This is the contraposition of (3).
\end{proof}


We now relate decreasing chain property and cluster points of nets using \eqref{eq76}.




%% Record #10 2023/10/23 two lectures 


\begin{thm}\label{lb232}
Let $X$ be a topological space. Then $X$ is net-compact iff $X$ is compact. 
\end{thm}


\begin{proof}
Assume that $X$ is net-compact. By Prop. \ref{lb230}, it suffices to prove that $X$ satisfies the decreasing chain property. Let $(E_\mu)_{\mu\in I}$ be a decreasing net of nonempty closed subsets of $X$. For each $\mu$ we choose $x_\mu\in E_\mu$, which gives a net $(x_\mu)_{\mu\in I}$ in $X$. The fact that $(E_\mu)$ is decreasing implies that $F_\mu\subset E_\mu$ if we set
\begin{subequations}\label{eq76}
\begin{align}
F_\mu=\{x_\nu:\nu\in I,\nu\geq\mu \}\label{eq72}
\end{align}
Thus, the closure $\ovl F_\mu$ is a subset of $E_\mu$ since $E_\mu$ is closed. It suffices to prove that $\bigcap_{\mu\in I}\ovl F_\mu\neq\emptyset$. By Pb. \ref{lb223}, 
\begin{align}
\bigcap_\mu \ovl F_\mu=\big\{\text{the cluster points of the net $(x_\mu)_{\mu\in I}$}\big\}
\end{align}
\end{subequations}
which is nonempty because $X$ is net-compact. This finishes the proof that $X$ is compact.

Now we assume that $X$ is compact. Let $(x_\mu)_{\mu\in I}$ be a net in $X$. Define $F_\mu$ by \eqref{eq72}. Then $(\ovl F_\mu)_{\mu\in I}$ is a decreasing net of nonempty closed subsets. So $\bigcap_\mu\ovl F_\mu$, the set of cluster points of $(x_\mu)_{\mu\in I}$, is nonempty by the decreasing chain property (cf. Prop. \ref{lb230}). So $X$ is net-compact.
\end{proof}



\begin{pp}\label{lb231}
Let $X$ be a topological space. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is countably compact.
\item (\textbf{Increasing chain property}) If $(U_n)_{n\in\Zbb_+}$ is an increasing sequence of open subsets of $X$ satisfying $\bigcup_{n\in\Zbb_+}U_n=X$, then $U_n=X$ for some $n$.
\item (\textbf{Decreasing chain property}) If $(E_n)_{n\in\Zbb_+}$ is a decreasing sequence of nonempty closed subsets of $X$, then $\bigcap_{n\in\Zbb_+}E_n\neq\emptyset$.
\end{enumerate}
\end{pp}

\begin{proof}
Similar to the proof of Prop. \ref{lb230}.
\end{proof}

\begin{exe}
Fill in the details of the proof of Prop. \ref{lb231}.
\end{exe}



\begin{lm}\label{lb247}
Let $X$ be a metric space. Then $X$ is sequentially compact iff $X$ is countably compact. 
\end{lm}

\begin{proof}
This lemma can be proved in a similar way as Thm. \ref{lb231}. The only difference is that one should use the sequential version of Pb. \ref{lb223}, namely, Prop. \ref{lb256}. Note that the ``(1)$\Leftrightarrow$(2)" of Prop. \ref{lb256} does not hold for general topological spaces. Thus, sequential compactness is not equivalent to countable compactness for general topological spaces. 
\end{proof}

\begin{srem}\label{lb264}
As pointed out in Rem. \ref{lb263}, Prop. \ref{lb256} holds more generally for first countable topological spaces (Def. \ref{lb262}). Therefore, Lem. \ref{lb247} also holds for such spaces:
\begin{gather}
\begin{gathered}
\text{sequentially compact}\quad\Longleftrightarrow\quad \text{countably compact}\\
(\text{for first countable spaces})
\end{gathered}
\end{gather}
\end{srem}



\subsection{Intermezzo: elementary properties about compactness}






\begin{pp}\label{lb233}
Suppose that $X\subset Y$ where $Y$ is a topological space. The following are true.
\begin{enumerate}
\item Assume that $Y$ is compact and $X$ is closed in $Y$. Then $X$ is compact.
\item Assume that $Y$ is Hausdorff and $X$ is compact, then $X$ is a closed subset of $Y$. 
\end{enumerate} 
\end{pp}

\begin{proof}
Part 1. Let $(x_\alpha)$ be a net in $X$. Since $Y$ is compact, $(x_\alpha)$ has a subnet $(x'_\beta)$ converging to some $p\in Y$. Since $(x'_\beta)$ is in $X$, we have $p\in \ovl X=X$. So $X$ is compact.

Part 2. To prove $\ovl X=X$, we choose any net $(x_\alpha)$ in $X$ converging to $p\in Y$ and show that $p\in X$. Indeed, since $X$ is compact, $(x_\alpha)$ has a subnet $(x_\beta')$ converging to some $x\in X$. Since $(x_\beta')$ also converges to $p$, we have $p=x$ because $Y$ is Hausdorff. So $p\in X$.
\end{proof}

I have mentioned that non-Hausdorff spaces are not often used in analysis. Thus, we mainly use the following special case of Prop. \ref{lb233}:


\begin{co}\label{lb234}
Let $Y$ be a Hausdorff space and $X\subset Y$. If $X$ is compact, then $X$ is closed in $Y$. If $X$ is closed in $Y$ and if $Y$ is compact, then $X$ is compact.
\end{co}

Recall that a similar property holds for complete metric spaces, cf. Prop. \ref{lb86}.







\begin{thm}\label{lb236}
Suppose that $f:X\rightarrow Y$ is a continuous map of topological spaces where $X$ is compact. Then $f(X)$ is compact. Moreover, if $f$ is injective and $X,Y$ are Hausdorff, then $f$ restricts to a homeomorphism $f:X\rightarrow f(X)$.
\end{thm}

\begin{proof}
Choose any net $(f(x_\alpha))$ in $f(X)$ where $x_\alpha\in X$. Since $X$ is compact, $(x_\alpha)$ has a subnet $(x'_\beta)$ converging to some $x\in X$. Then $f(x'_\beta)$ converges to $f(x)$. So $f(X)$ is compact.

Now assume that $f$ is injective and $Y$ is Hausdorff. Then the subspace $f(X)$ is also Hausdorff. By replacing $Y$ by $f(X)$, we assume that $f$ is bijective. To show that $f^{-1}$ is continuous, by Prop. \ref{lb191}, it suffices to prove that $f$ is a closed map, i.e., $f$ sends every closed $E\subset X$ to a closed subset $f(E)$. Indeed, since $X$ is compact, $E$ is also compact by Cor. \ref{lb234}. So $f(E)$ is compact by the first paragraph. So $f(E)$ is closed in $X$ by Cor. \ref{lb234}.
\end{proof}

The second part of Thm. \ref{lb236} can also be proved in a similar way as Pb. \ref{lb235} by replacing sequences with nets. But that argument relies on the fact that every net in a compact Hausdorff space with only one cluster point is convergent. We leave the proof of this fact to the readers (cf. Pb. \ref{lb237}).


\begin{exe}
The first part of Thm. \ref{lb236} can be viewed as a generalization of extreme value theorem. Why?
\end{exe}



\begin{pp}\label{lb239}
Suppose that $X,Y$ are compact topological spaces. Then $X\times Y$ is compact.
\end{pp}

\begin{proof}
Take a net $(x_\alpha,y_\alpha)$ in $X\times Y$. Since $X$ is compact, $(x_\alpha)$ has a convergent subnet $(x_{\alpha_\beta})$. Since $Y$ is compact,  $(y_{\alpha_\beta})$ has a convergent subnet $(y_{\alpha_{\beta_\gamma}})$. So $(x_{\alpha_{\beta_\gamma}},y_{\alpha_{\beta_\gamma}})$ is a convergent subnet of $(x_\alpha,y_\alpha)$.
\end{proof}

\begin{rem}
Note that if $X\times Y$ is compact, then $X$, as the image of $X\times Y$ under the projection map, is compact by Thm. \ref{lb236}. Therefore, we conclude that $X\times Y$ is compact iff $X$ and $Y$ are compact.
\end{rem}










\subsection{Act 3: ``countably compact $\Leftrightarrow$ compact" for Lindel\"of spaces}\label{lb253}


Assume in this section that $X$ is a topological space. Recall that $X$ is Lindel\"of iff every open cover has a countable subcover. Thus, it is obvious that
\begin{align}
\text{countably compact}\quad\Longleftrightarrow\quad \text{compact}\qquad(\text{for Lindel\"of spaces})
\end{align}
Thus, by Lem. \ref{lb247}, to prove that sequentially/countably compact metric spaces are compact, it suffices to prove that they are Lindel\"of.

We introduce two related concepts that are more useful than Lindel\"of spaces:
\begin{df}
We say that a topological space $X$ is \textbf{separable} \index{00@Separable} if $X$ has a countable dense subset. We say that $X$ is \textbf{second countable} \index{00@Second countable} if the topology of $X$ has a countable basis (i.e., a basis $\mc B$ with countably many elements).
\end{df}

\begin{eg}\label{lb248}
$\Rbb^N$ is separable, since $\Qbb^N$ is a dense subset. 
\end{eg}


As we shall immediately see, these two notions are equivalent for metric spaces. It is often easier to visualize and prove separability for concrete examples (such as Exp. \ref{lb248}). Indeed, Exp. \ref{lb248} is the typical example that helps us imagine more general separable spaces. However, for general topological spaces, second countability behaves better than separability.  The following property gives one reason.
\begin{pp}
If $Y$ is a subset of a second countable space $X$, then $Y$ is second countable.
\end{pp}

\begin{proof}
Let $\mc B$ be a countable basis for the topology of $X$. Then $\{Y\cap U:U\in\mc B\}$ is a countable basis for the topology of $Y$.
\end{proof}
Another reason that second countability is better is because it implies Lindel\"of property. This is mainly due to the following fact:

\begin{pp}\label{lb249}
Let $\mc B$ be a basis for the topology of $X$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is Lindel\"of (resp. compact).
\item If $X$ has open cover $\fk U$ where each member of $\fk U$ is an element of $\mc B$, then $\fk U$ has a countable (resp. finite) subcover.
\end{enumerate}
\end{pp}

\begin{proof}
``(1)$\Rightarrow$(2)" is obvious.  Assume (2). Let $\fk W$ be an open cover of $X$. Let
\begin{align*}
\fk U=\{U\in \mc B:U\subset W\text{ for some }W\in\fk W\}
\end{align*} 
For each $x\in X$, since there is $W\in\fk W$ containing $x$, and since $\mc B$ is a basis, there is $U\in\mc B$ such that $x\in U\subset W$. This proves that $\fk U$ is an open cover of $X$. So $\fk U$ has a countable (resp. finite) subcover $\fk U_0$. For each $U\in\fk U_0$, choose $W_U\in\fk W$ containing $U$. Then $\{W_U:U\in\fk U_0\}$ is a countable (resp. finite) subcover of $\fk W$.
\end{proof}

\begin{co}\label{lb265}
Every second countable topological space $X$ is Lindel\"of.
\end{co}

\begin{proof}
Let $\mc B$ be a countable basis for the topology of $X$. Let $\fk U$ be an open cover of $X$ such that each member of $\fk U$ is in $\mc B$. Then by discarding duplicated terms, $\fk U$ becomes countable. This verifies (2) of Prop. \ref{lb249}.
\end{proof}


\begin{thm}\label{lb250}
Consider the following statements:
\begin{enumerate}[label=(\arabic*)]
\item $X$ is second countable.
\item $X$ is separable.
\end{enumerate}
Then (1)$\Rightarrow$(2). If $X$ is metrizable, then (2)$\Rightarrow$(1).
\end{thm}



\begin{proof}
Assume that $X$ is second countable with countable basis $\mc B$. For each $U\in\mc B$, choose $x_U\in U$. Then one checks easily that $\{x_U:U\in\mc B\}$ is dense by checking that it intersects every nonempty open subset of $X$.

Assume that $X$ is a metric space with countable dense subset $E$. Let us prove that the countable set
\begin{align*}
\mc B=\{B_X(e,1/n):e\in E,n\in\Zbb_+\}
\end{align*}
is a basis for the topology of $X$. Choose any open $W\subset X$ with $x\in W$. We want to show that some member of $\mc B$ contains $x$ and is in $W$. By shrinking $W$, we may assume that $W=B_X(x,1/n)$ for some $n\in\Zbb_+$. Since $E$ is dense, $B(x,1/2n)$ contains some $e\in E$. So $d(x,e)<1/2n$. Therefore,  $B(e,1/2n)$ contains $x$  and is inside $W$ by triangle inequality.
\end{proof}


It can be proved that Lindel\"of metric spaces are separable. (Therefore, the three notions agree for metric spaces.) We will not use this fact. So we leave its proof to the readers as an exercise (cf. Pb. \ref{lb255}). The following chart is a summary of the relationships between the various topological properties about countability.





\begin{align}
\begin{aligned}
&\text{Topological spaces:} &  \text{second countable }&\Longrightarrow \left\{
{\begin{array}{l}
\text{subset is second countable}\\
\text{separable}\\
\text{Lindel\"of}
\end{array}}
\right.\\[0.5ex]
&\text{Metric spaces:}  & \text{second countabl}&\text{e }\Longleftrightarrow\text{ separable}\Longleftrightarrow \text{ Lindel\"of} 
\end{aligned}
\end{align}

\begin{eg}
Since $\Rbb^N$ is separable and hence second countable, every subset of $\Rbb^N$ is second countable, and hence is separable.
\end{eg}



\begin{thm}\label{lb252}
Let $X$ be a sequentially compact metric space. Then $X$ is separable, and hence second countable and Lindel\"of.
\end{thm}

\begin{proof}
We claim that for every $\delta>0$, there is a finite set $E\subset X$ such that for every $x\in E$, the distance $d(x,E)<\delta$. Suppose that there is no such a finite set for some given number $\delta>0$. Pick any $x_1\in X$. Suppose $x_1,\dots,x_k\in X$ have been constructed. Then there is a point $x_{k+1}\in X$ whose distance to $\{x_1,\dots,x_k\}$ is $\geq \delta$. This defines inductively a sequence $(x_k)_{k\in\Zbb_+}$ in $X$ such that any two elements have distance $\geq \delta$. So $(x_k)$ has no convergent subsequence, contradicting the sequential compactness of $X$.

Thus, for each $n\in\Zbb_+$, we can choose a finite  $E_n\subset X$ satisfying  $d(x,E_n)<1/n$ for all $x\in X$. Let $E=\bigcup_{n\in\Zbb_+}E_n$, which is countable. Then for each $x\in X$, $d(x,E)\leq d(x,E_n)<1/n$ for every $n$, which implies $d(x,E)=0$ and hence $x\in\ovl E$ by Pb. \ref{lb251}. So $E$ is dense in $X$.
\end{proof}


\begin{rem}
The above proof is indirect because it proves the existence of $E_n$ by contradiction but not by explicit construction. However, if $X$ is a bounded closed subset of $\Rbb^N$, one can find an explicit countable basis for the topology of $X$:
\begin{align*}
\mc B=\{X\cap B(x,1/n):n\in\Zbb_+,x\in\Qbb^N\}
\end{align*}
and hence has an explicit countable dense subset $\{x_U:U\in\mc B,U\neq\emptyset\}$ where for each $U$ we choose some $x_U\in U$. More generally, if $X$ is a closed subset of the sequentially compact space $[0,1]^{\Zbb_+}$, one can find a countable basis for the topology of $X$ and hence a countable dense subset of $X$ in a similar way. (You will be asked to construct them in Pb. \ref{lb305}.) 


We shall see in Thm. \ref{lb261} that every sequentially compact metric space is homeomorphic to a closed subset of $[0,1]^{\Zbb_+}$. Therefore, for any sequentially compact metric space $X$ you will see in the real (mathematical) life, you don't need the indirect construction in the proof of Thm. \ref{lb252} to prove the separability of $X$. So what is the point of giving an indirect proof of Thm. \ref{lb252}? Well, you need Thm. \ref{lb252} to prove Thm. \ref{lb261}.  \hfill\qedsymbol
\end{rem}


%%  ad



\begin{proof}[\textbf{Proof of Thm. \ref{lb222}}]
Let $X$ be a metric space. Assume that $X$ is compact. Then $X$ is clearly countably compact, and hence sequentially compact by Lem. \ref{lb247}. Conversely, assume that $X$ is sequentially compact. Then by Lem. \ref{lb247} and Thm. \ref{lb252}, $X$ is countably compact and Lindel\"of, and hence compact.
\end{proof}




\begin{srem}
Since second countable spaces are first countable, by Rem. \ref{lb264} and Cor. \ref{lb265}, we have
\begin{gather}\label{eq75}
\begin{gathered}
\text{sequentially compact}\quad\Longleftrightarrow\quad \text{countably compact}\quad\Longleftrightarrow\quad \text{compact}\\[0.5ex]
(\text{for second countable topological spaces})
\end{gathered}
\end{gather}
Relation \eqref{eq75} not only generalizes Thm. \ref{lb222}, but also tells us what are the crucial properties that ensure the equivalence of compactness and sequential compactness for metric spaces. 
\end{srem}






















\subsection{Problems and supplementary material}


Let $X,Y$ be topological spaces.


\subsubsection{Compactness}

\begin{prob}\label{lb237}
Let $(x_\alpha)_{\alpha\in I}$ be a net in a compact Hausdorff space $X$. Prove that $(x_\alpha)$ is convergent iff $(x_\alpha)$ has exactly one cluster point.
\end{prob}


\begin{prob}\label{lb346}
Let $(x_\alpha)_{\alpha\in I}$ be a net in the compact space $\ovl\Rbb$. Let $S$ be the (automatically nonempty) set of cluster points of $(x_\alpha)$ in $\ovl\Rbb$. Recall that $S$ is a closed subset by Rem. \ref{lb238}. For each $\alpha\in I$, define  \index{00@Limit inferior and superior}
\begin{gather}
A_\alpha=\inf\{x_\beta:\beta\geq \alpha \}\qquad  B_\alpha=\sup\{x_\beta:\beta\geq  \alpha \}
\end{gather}
Then $(A_\alpha)$ is increasing and $(B_\alpha)$ is decreasing. So they converge in $\ovl\Rbb$. Define \index{liminfsup@$\liminf,\limsup$}
\begin{subequations}
\begin{gather}
\liminf_{\alpha\in I}x_\alpha=\sup\{A_\alpha:\alpha\in I\}=\lim_{\alpha\in I} A_\alpha \\
\limsup_{\alpha\in I}x_\alpha=\inf\{B_\alpha:\alpha\in I\}=\lim_{\alpha\in I} B_\alpha
\end{gather}
\end{subequations}
Prove that
\begin{align}
\liminf_{\alpha\in I}x_\alpha=\inf S\qquad \limsup_{\alpha\in I}x_\alpha=\sup S
\end{align}
\end{prob}

\begin{proof}[Note]
You will get a quick proof by choosing the right one of the three equivalent definitions of cluster points in Pb. \ref{lb223}. A wrong choice will take you much more effort.
\end{proof}


\begin{co}
Let $(x_\alpha)$ be a net in $\ovl\Rbb$. Then $(x_\alpha)$ converges in $\ovl\Rbb$ iff $\limsup_\alpha x_\alpha=\liminf_\alpha x_\alpha$. 
\end{co}


\begin{proof}
$\ovl\Rbb$ is compact. Therefore, $\limsup_\alpha x_\alpha=\liminf_\alpha x_\alpha$ iff $(x_\alpha)$ has only one cluster point (by Pb. \ref{lb346}), iff $(x_\alpha)$ converges (by Pb. \ref{lb237}).
\end{proof}




\begin{prob}
Given a general topological space $X$, which one implies the other between the conditions of ``sequential compactness" and ``countable compactness"? Prove your conclusion with details.
\end{prob}

\begin{proof}[Hint]
Check the proof of Thm. \ref{lb232}, and think about the question: If $(x_n)$ is a sequence in $X$, what is the inclusion relation between $\bigcap_n\ovl{\{x_k:k\geq n\}}$ and the set of limits of the convergent subsequences (rather than subnets) of $(x_n)$?
\end{proof}







\begin{prob}\label{lb246}
Prove Prop. \ref{lb233} using the original definition of compact spaces (i.e. every open cover has a finite subcover) instead of using nets.
\end{prob}

\begin{sprob}
Prove Prop. \ref{lb239} using the original definition of compact spaces instead of using nets.
\end{sprob}

\begin{prob}\label{lb240}
Assume that $Y$ is compact. Let $(x_\alpha,y_\alpha)_{\alpha\in I}$ be a net in $X\times Y$. Assume that $x\in X$ is a cluster point of $(x_\alpha)$. Prove that there exists $y\in Y$ such that $(x,y)$ is a cluster point of $(x_\alpha,y_\alpha)$.
\end{prob}

\begin{prob}\index{00@Tychonoff theorem, countable version} \label{lb241}
(\textbf{Tychonoff theorem, countable version}) Let $(X_n)_{n\in\in\Zbb_+}$ be a sequence of compact topological spaces. Prove that the product space $S=\prod_{n\in\Zbb_+}X_n$ (equipped with the product topology) is compact using the following hint.
\end{prob}

\begin{proof}[Hint]
Let $(f_\alpha)_{\alpha\in I}$ be a net in $S$ where $f_\alpha=(f_\alpha(1),f_\alpha(2),\dots)$. Use Pb. \ref{lb240} to construct inductively an element $x=(x(1),x(2),\dots)\in S$ such that for every $n\in\Zbb_+$, the element $(x(1),\dots,x(n))$ is a cluster point of $(f_\alpha(1),\dots,f_\alpha(n))_{\alpha\in I}$ in $X_1\times\cdots\times X_n$. Prove that $x$ is a cluster point of $(f_\alpha)_{\alpha\in I}$ in $S$.
\end{proof}


\begin{rem}
The same idea as above can be used to prove the general Tychonoff theorem (the version where the index set $\Zbb_+$ in Pb. \ref{lb241} is replaced by an arbitrary set) by replacing mathematical induction by Zorn's lemma. 
\end{rem}



\begin{sprob}
Let $X$ be the set of two elements: $X=\{0,1\}$, viewed as a metric subspace of $\Rbb$. Let $S=X^{[0,1]}$, the product space of uncountably many $X$, where the index set is the  interval $[0,1]$. $S$ is equipped with the product topology. According to Tychonoff theorem (to be proved in the future), $S$ is compact. Prove that $S$ is not sequentially compact.
\end{sprob}


\begin{proof}[Hint]
Use binary representations in $[0,1]$.
\end{proof}




\subsubsection{LCH spaces}


\begin{df}
A subset $A$ of a Hausdorff space $X$ is called \textbf{precompact} \index{00@Precompact subset} if its closure $\ovl A$ is compact. By Cor. \ref{lb234}, this is equivalent to saying that $A$ is contained in a compact subset of $X$.   
\end{df}

It is clear that a subset of a precompact set is precompact.

\begin{prob}\label{lb287}
Suppose that $X$ is metric space. Let $A\subset X$. Prove that the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $A$ is precompact, i.e., $\ovl A$ is compact.
\item Every sequence in $A$ has a subsequence converging to some point of $X$.
\end{enumerate}
\end{prob}



\begin{df}
A Hausdorff space $X$ is called a \textbf{locally compact Hausdorff (LCH) space} \index{00@LCH=Locally compact Hausdorff} if every point has a precompact neighborhood.
\end{df}






\begin{pp}\label{lb245}
Let $X$ be an LCH space. Then the closed subsets and the open subsets of $X$ are LCH. 
\end{pp}

\begin{proof}
Let $E\subset X$ be closed. Let $x\in E$. Then there is $U\in\Nbh_X(x)$ with compact closure $\Cl_X(U)$. So $\Cl_X(U)\cap E$ is compact by Cor. \ref{lb234}. Note that $U\cap E\in \Nbh_E(x)$, and $U\cap E$ is a subset of $\Cl_X(U)\cap E$. So $U\cap E$ is precompact in $E$. This proves that $E$ is LCH.

Next, we let $W$ be an open subset of $X$. Let $x\in W$. We want to show that there exists $U\in\Nbh_X(x)$ such that $\ovl U=\Cl_X(U)$ is compact and $\ovl U\subset W$. Then $U$ is clearly precompact in $W$.

Since $X$ is LCH, there is $\Omega\in\Nbh_X(x)$ with compact closure $\ovl\Omega=\Cl_X(\Omega)$. Then every open subset $U$ of $\Omega$ containing $x$ has compact closure in $X$. Thus, it suffices to prove that there exists $U\in\mc I=\Nbh_\Omega(x)$  such that $\ovl U\subset W$. Suppose that this is not true. Then for each $U\in\mc I$ there is $x_U\in \ovl U\setminus W$. So $(x_U)_{U\in\mc I}$ is a net in $\ovl\Omega\setminus W$, where $\ovl\Omega\setminus W$ is compact by Cor. \ref{lb234}. So $(x_U)_{U\in\mc I}$ has a cluster point $y\in\ovl\Omega\setminus W$. In particular, $y\neq x$.

Since $X$ is Hausdorff, by Cor. \ref{lb243}, there is $V_0\in\Nbh_X(x)$ such that $y\notin\ovl V_0$. Let $V=V_0\cap\Omega$. Then $V\in\mc I$ and $y\notin \ovl V$. For every $U\in\mc I$ satisfying $U\subset V$ we have $x_U\in\ovl V$. So $(x_U)_{U\in\mc I}$ is eventually not in the neighborhood $X\setminus \ovl V$ of $y$. This is impossible.
\end{proof}


\begin{comment}


Case 1: Assume that $X$ is compact Hausdorff.  Then $B=X\setminus W$ is compact by Cor. \ref{lb234}. Since $X$ is Hausdorff, for each $y\in B$ there exists $V_y\in\Nbh_X(y)$ such that $x\notin \ovl V_y$. (Recall Cor. \ref{lb243}.) Since $B$ is compact, there are $y_1,\dots,y_n\in B$ such that $B\subset V$ where $V=V_{y_1}\cup\cdots\cup V_{y_n}$. By \eqref{eq73}, $\ovl V=\ovl V_{y_1}\cup\cdots\cup \ovl V_{y_n}$. So $x\notin\ovl V$. Now $X\setminus V$ is a closed subset of $X$, and hence 

Let $U=X\setminus \ovl V$. Then $x\in U$. Since $U\subset X\setminus V$ and $X\setminus V$ is closed, $\ovl U\subset X\setminus V$, and hence $\ovl U\subset W$. Since $X$ is compact, $U$ is precompact.


Case 2: $X$ is LCH. Choose a precompact $\Omega\in\Nbh_X(x)$. By case 1 (applied to $\ovl\Omega$), and by the definition of subspace topology, there exists $V\in\Nbh_X(x)$ such that $x\in V\cap \ovl\Omega\subset\Cl_{\ovl\Omega}(V\cap \ovl\Omega)\subset W\cap\ovl\Omega$. By Rem. \ref{lb182}, we have $\Cl_{\ovl\Omega}(V\cap \ovl\Omega)= \ovl{V\cap\ovl\Omega}\cap\ovl\Omega$, which contains $\ovl{V\cap\Omega}$ since both $\ovl{V\cap\ovl\Omega}$ and $\ovl\Omega$ do. So $x\in U\subset\ovl U\subset W$ where $U=V\cap\Omega$. Since $\Omega$ is precompact, $U$ is precompact.
\end{comment}



\begin{df}\label{lb334}
Let $X$ be LCH. Let $Y$ be a metric space. Let $(f_\alpha)_{\alpha\in I}$ be a net in $Y^X$. Let $f\in Y^X$.  We say that $(f_\alpha)$  \textbf{converges locally uniformly} \index{00@Locally uniform convergence} to $f$ if the following equivalent conditions are satisfied:
\begin{enumerate}[label=(\arabic*)]
\item For each $x\in X$, there exists $U\in\Nbh_X(x)$ such that $(f_\alpha|_U)$ converges uniformly to $f|_U$.
\item For each precompact open subset $W\subset X$, the net $(f_\alpha|_W)$ converges uniformly to $f|_W$.
\end{enumerate}
\end{df}

\begin{prob}
Prove that in Def. \ref{lb334}, conditions (1) and (2) are equivalent. 
\end{prob}


\begin{exe}
Let $X$ be LCH. Let $\mc V$ be a normed vector space. Let $(f_\alpha)_{\alpha\in I}$ be a net in $C(X,\mc V)$ converging pointwise to $f:X\rightarrow 
\mc V$. Assume that the net $(f_\alpha)$ converges locally uniformly on $X$ (clearly to $f$). Prove that $f$ is continuous. 
\end{exe}






An example of locally uniform convergence was given in Thm. \ref{lb112}.



\subsubsection{Countability in topological spaces}



\begin{prob}
Let $(X,\mc T)$ be a second countable LCH space. Prove that the topology $\mc T$ has a countable basis $\mc B$ whose members are all precompact.
\end{prob}

\begin{proof}[Hint]
Use Lindel\"of property for open subsets of $X$.
\end{proof}

\begin{sprob}\label{lb255}
Let $X$ be a Lindel\"of metric space. Prove that $X$ is separable. 
\end{sprob}


\begin{prob}\label{lb305}
Let $(X_n)_{n\in\Zbb_+}$ be a sequence of topological spaces. Equip $S=\prod_{n\in\Zbb_+}X_n$ with the product topology. 
\begin{enumerate}
\item Prove that $S$ is second countable if each $X_n$ is second countable.
\item Prove that $S$ is separable if each $X_n$ is separable
\end{enumerate}
\end{prob}


Recall from Pb. \ref{lb221} the basic facts about connected components.

\begin{prob}
Let $X$ be a locally connected topological space. Prove that if $X$ is second countable, then $X$ has countably many connected components. Use this result to show that every open subset of $\Rbb$ is a countable disjoint union of open intervals.
\end{prob}



\subsubsection{The problem of embedding}


\begin{df}\label{lb327}
Let $\scr F$ be a set of functions $X\rightarrow Y$. We say that $\scr F$ \textbf{separates points} \index{00@Separates points} of $X$, if for every distinct $x_1,x_2\in X$ there exists $f\in\scr F$ such that $f(x_1)\neq f(x_2)$. 
\end{df}

\begin{prob}\label{lb259}
Let $X$ be a nonempty compact metric space.
\begin{enumerate}
\item Prove that there is a sequence of continuous functions $(f_n)_{n\in\Zbb_+}$ from $X$ to $[0,1]$ separating points of $X$. 
\item Prove that 
\begin{gather*}
\Phi:X\rightarrow [0,1]^{\Zbb_+} \qquad x\mapsto (f_1(x),f_2(x),\dots)
\end{gather*}
gives a homeomorphism $\Phi:X\rightarrow\Phi(X)$ where $\Phi(X)$ is a closed subspace of $[0,1]^{\Zbb_+}$ (equipped with the subspace topology).
\end{enumerate}
\end{prob}

The topological space $[0,1]^{\Zbb_+}$, equipped with the product topology, is called the \textbf{Hilbert cube}. \index{00@Hilbert cube}




\begin{proof}[Hint]
Part 1: Choose an infinite countable basis $\mc B=(U_1,U_2,\dots)$ for the topology of $X$ where each $U_n$ is nonempty. (Why can you do so?) Use Urysohn functions (Rem. \ref{lb257}) to construct $f_n:X\rightarrow [0,1]$ such that $f^{-1}(0)=X\setminus U_n$.

Part 2: Notice Pb. \ref{lb258}.
\end{proof}


\begin{thm}\label{lb261}
Let $X$ be a topological space. The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is a compact metrizable space.
\item $X$ is homeomorphic to a closed subset of the Hilbert cube $[0,1]^{\Zbb_+}$.
\end{enumerate}
\end{thm}

\begin{proof}
(1)$\Rightarrow$(2): By Pb. \ref{lb259}. (2)$\Rightarrow$(1): By Cor. \ref{lb260}, $[0,1]^{\Zbb_+}$ is metrizable. By countable Tychonoff theorem (Thm. \ref{lb89} or Pb. \ref{lb241}), $[0,1]^{\Zbb_+}$ is compact. So its closed subsets are compact by Cor. \ref{lb234}.
\end{proof}



\begin{sexe}
Let $N\in\Zbb_+$. Prove that the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $X$ is a compact Hausdorff space. Moreover, there exist $f_1,\dots,f_N\in C(X,\Rbb)$ separating points of $X$.
\item $X$ is homeomorphic to a bounded closed subset of $\Rbb^N$.
\end{enumerate}
\end{sexe}


\newpage

\section{The injection \pmb{$\Psi:C(X,C(Y,\mc V))\rightarrow C(X\times Y,\mc V)$}}\label{lb352}





In this chapter, unless otherwise stated,  $X,Y$ are topological spaces, all normed vectors spaces are over $\Fbb\in\{\Rbb,\Cbb\}$, and $\mc V$ is a normed vector space.

\begin{rem}
Let $\Phi:\mc V\rightarrow\mc W$ be a linear map of normed vector spaces. Recall that $\Phi$ is an isometry (in the category of metric spaces) iff $\lVert \Phi(u)-\Phi(v)\lVert=\lVert u-v\lVert$ for all $u,v\in\mc V$. By linearity, we have
\begin{align}
\Phi\text{ is an isometry}\qquad \Longleftrightarrow\qquad \lVert\Phi(v)\lVert=\lVert v\lVert~~\text{for all } v\in\mc V
\end{align}
\end{rem}


\begin{df}\label{lb302}
A linear map $\Phi:\mc V\rightarrow\mc W$ of normed vector spaces over $\Fbb$ is a called an \textbf{isomorphism of normed vector spaces} \index{00@Isomorphism of normed vector spaces/Banach spaces} if $\Phi$ is an isometric isomorphism. If $\Phi$ is an isomorphism, and if one of $\mc V,\mc W$ is complete, then the other one is also complete. In this case, we call $\Phi$ an \textbf{isomorphism of Banach spaces}.
\end{df}


Recall that if $X$ is compact, then the norm on $C(X,\mc V)$ is assumed to be the $l^\infty$-norm (cf. Conv. \ref{lb88}).


\subsection{$\Psi$ is bijective when $Y$ is compact}



\begin{thm}\label{lb274}
Equip $C(Y,\mc V)$ with the uniform convergence topology (cf. Exp. \ref{lb272}). Then there is a well-defined injective linear map
\begin{gather}\label{eq77}
\begin{gathered}
\Psi:C(X,C(Y,\mc V))\rightarrow C(X\times Y,\mc V)\\[0.5ex]
\Psi(F)(x,y)=F(x)(y)
\end{gathered}
\end{gather}
(for each $F\in C(X,C(Y,\mc V))$ ). Moreover, the following are true:
\begin{enumerate}[label=(\alph*)]
\item If $Y$ is compact, then $\Psi$ is a linear isomorphism of vector spaces.
\item If $X,Y$ are compact, then $\Psi$ is an isomorphism of normed vector spaces.
\end{enumerate}
\end{thm}



\begin{proof}
Step 1. Write $\mc W=C(Y,\mc V)$ for simplicity. To prove that \eqref{eq77} is a well-defined map, we need to prove that for each $F\in C(X,\mc W)$, the map $f=\Psi(F):X\times Y\rightarrow \mc V$ sending $(x,y)$ to $f(x,y)=F(x)(y)$ is continuous.

The continuity of $F:X\rightarrow\mc W$ means that if $(x_\alpha)_{\alpha\in I}$ is a net in $X$ converging to $x$, then $F(x_\alpha)$ converges to $F(x)$, i.e., 
\begin{subequations}
\begin{align}
\lim_{\alpha\in I}A_\alpha=0
\end{align}
where each $A_\alpha\in\ovl\Rbb_{\geq0}$ is 
\begin{align}
A_\alpha= \sup_{y\in Y}\lVert f(x_\alpha,y)-f(x,y)\Vert
\end{align}
\end{subequations}
Now, suppose that $(x_\alpha,y_\alpha)_{\alpha\in I}$ is a net in $X\times Y$ converging to $(x,y)$. Then
\begin{align*}
&\lVert f(x_\alpha,y_\alpha)-f(x,y)\lVert\leq \lVert f(x_\alpha,y_\alpha)-f(x,y_\alpha)\lVert +\lVert f(x,y_\alpha)-f(x,y)\lVert\\
\leq& A_\alpha+\lVert f(x,y_\alpha)-f(x,y)\lVert
\end{align*}
Since $F(x):y\in Y\mapsto f(x,y)\in\mc V$ is continuous, we have $\lim_\alpha \lVert f(x,y_\alpha)-f(x,y)\lVert=0$. Therefore, by Squeeze theorem, we have
\begin{align}
\lim_{\alpha\in I}\lVert f(x_\alpha,y_\alpha)-f(x,y)\lVert=0
\end{align}
Thus, $f$ is continuous at every $(x,y)$.\\[-1ex]

Step 2. Clearly $\Psi$ is linear and injective. Assume that $Y$ is compact. Then the surjectivity of $\Psi$ follows from Exp. \ref{lb225}. This proves (a). Assume that $X$ is also compact. Choose any $F\in C(X,\mc W)$ and write $f=\Psi(F)$.  Then
\begin{align*}
\sup_{x\in X}\sup_{y\in Y}~\lVert f(x,y)\lVert=\sup_{x\in X,y\in Y}\lVert f(x,y)\lVert
\end{align*}
by the easy Lem. \ref{lb273}. This proves that $\Phi$ is an isometry, and hence proves (b).
\end{proof}

\begin{lm}\label{lb273}
Let $g:A\times B\rightarrow\ovl\Rbb$ be a function where $A,B$ are sets. Then
\begin{align*}
\sup_{a\in A}\sup_{b\in B}g(a,b)=\sup_{(a,b)\in A\times B}g(a,b)
\end{align*}
\end{lm}

\begin{proof}
Write $\lambda_a=\sup_{b\in B}g(a,b)$ and $\rho=\sup_{(a,b)\in A\times B}g(a,b)$. Then, clearly $\lambda_a\leq\rho$ for each $a$. So $\sup_a\lambda_a\leq\rho$. For each $a,b$ we have $g(a,b)\leq \lambda_a$, and hence $g(a,b)\leq \sup_a\lambda_a$. Taking $\sup$ over $a,b$ yields $\rho\leq\sup_a\lambda_a$.
\end{proof}





\begin{rem}
Thanks to Thm. \ref{lb274}, we can reduce many problems about multi-variable functions to problems about single-variable functions. Here is an example we will study in the future: If $I=[a,b],J=[c,d]$ are compact intervals and $F\in C(I\times J,\Rbb)$, then with the help of Thm. \ref{lb274}, the Fubini theorem
\begin{align*}
\int_a^b \int_c^d F(x,y)dxdy=\int_c^d\int_a^b F(x,y)dxdy
\end{align*}
for Riemann integrals follows directly from the easy general fact
\begin{align*}
\int_a^b\Lambda\circ f(x)dx=\Lambda\Big(\int_a^b f(x)dx \Big)
\end{align*}
where $f:[a,b]\rightarrow\mc W$ is a continuous map to a real Banach space $\mc W$, and $\Lambda:\mc W\rightarrow\Rbb$ is a continuous linear map.

Thm. \ref{lb274} can be used the other way round: We will prove in the future that every Banach space $\mc V$ is isomorphic to a closed linear subspace of $C(Y,\Fbb)$ for some compact Hausdorff space $Y$. Thus, a problem about continuous maps $X\rightarrow \mc V$ (where $\mc V$ is an abstract Banach space) can be reduced to a problem about continuous scalar-valued functions $X\times Y\rightarrow\Fbb$. \hfill\qedsymbol
\end{rem}


In the following sections, we give a surprising application of Thm. \ref{lb274}: We show that uniform-convergence and equicontinuity, two closely related but different notions, can be understood in the same context.

\subsection{Equicontinuity}

Let $I$ be a set not necessarily preordered or directed.

\begin{df}
Assume that $Y$ is a metric space. A family of functions $(f_\alpha)_{\alpha\in I}$ from $X$ to $Y$ is called \textbf{equicontinuous at} $x\in X$ \index{00@Equicontinuous at a point} if the following equivalent conditions hold:
\begin{enumerate}[label=(\arabic*)]
\item The function
\begin{gather}
X\rightarrow Y^{I}\qquad x\mapsto (f_\alpha(x))_{\alpha\in I}
\end{gather}
is continuous at $x$, where $Y^{I}$ is equipped with the uniform convergence topology (Exp. \ref{lb272}).
\item For every $\eps>0$, there exists $U\in\Nbh_X(x)$ such that for every $p\in U$ we have
\begin{align*}
\sup_{\alpha\in I}d_Y(f_\alpha(p),f_\alpha(x))<\eps
\end{align*}
\end{enumerate}
Clearly, if $(f_\alpha)_{\alpha\in I}$ is equicontinuous at $x$, then $f_\alpha:X\rightarrow Y$ is continuous at $x$ for every $\alpha\in I$. We say that $(f_\alpha)_{\alpha\in I}$ is \textbf{(pointwise) equicontinuous} \index{00@Equicontinuous, pointwise} if it is equicontinuous at every point of $X$.
\end{df}



\begin{proof}[Proof of equivalence]
This is immediate if we choose the uniform convergence metric on $Y^I$ to be
\begin{align*}
d((y_\alpha),(y_\alpha'))=\min\Big\{\sup_{\alpha\in I}d_Y(y_\alpha,y'_\alpha),1\Big\}
\end{align*}
and use (the base version of) Def. \ref{lb188}-(2).
\end{proof}

\begin{rem}
Warning: The above definition of equicontinuity is weaker than the one in Rudin's book \cite[Ch. 7]{Rud-P}, which will be called \textbf{uniform equicontinuity} in this course (cf. Def. \ref{lb316}).
\end{rem}



\begin{eg}\label{lb309}
Assume that $X$ and $Y$ are metric spaces. Fix $C\geq 0$. Then
\begin{align*}
\{f\in Y^X:f\text{ has Lipschitz constant }C\}
\end{align*}
is an equicontinuous family of functions $X\rightarrow Y$.

In the future, we will see that if $f:[a,b]\rightarrow\Rbb$ (where $[a,b]\subset\Rbb$) is differentiable and satisfies $|f'(x)|\leq C$ for all $x\in[a,b]$, then $f$ has Lipschitz constant $C$. (For example, if we assume moreover that $f'$ is continuous, then for each $a\leq x<y\leq b$ we have $|f(y)-f(x)|=|\int_x^y f'|\leq \int_x^y|f'|\leq C(y-x)$.) Therefore, all such functions form an equicontinuous family of functions.  \hfill\qedsymbol
\end{eg}


Equicontinuity is important for several reasons. First, equicontinuity is closely related to compactness, both under the uniform convergence topology and under the pointwise convergence topology. This is hinted at in Rem. \ref{lb304}, and will be explored in more detail in a future chapter. Second, equicontinuity and uniform convergence are symmetric notions:

\begin{rem}
Note that if $x\in \ovl{X\setminus x}$, a map $\varphi:X\rightarrow Y$ is continuous at $x$ iff $\lim_{p\rightarrow x}f(p)=f(x)$, where $\lim_{p\rightarrow x}f(p)$ is the limit of a net (cf. Rem. \ref{lb275}). Now, assume that $Y$ is a metric space. Then we see that a family $(f_\alpha)_{\alpha\in I}$ in $Y^X$ satisfies that
\begin{align}
(f_\alpha)_{\alpha\in I}\text{ is equicontinuous at }x\qquad\Longleftrightarrow\qquad\lim_{p\rightarrow x}\sup_{\alpha\in I}d(f_\alpha(p),f_\alpha(x))=0
\end{align} 
If we compare this with
\begin{align}
f_\alpha\rightrightarrows f \qquad\Longleftrightarrow\qquad \lim_{\alpha\in I}\sup_{x\in X}d(f_\alpha(x),f(x))=0
\end{align}
(if $I$ is a directed set and $f\in Y^X$), we see that equicontinuity and uniform convergence are ``symmetric about the diagonal line of the Cartesian product $I\times X$": Equicontinuity is a uniform convergence over the index set $I$, and the uniform convergence $f_\alpha\rightrightarrows f$ is uniform over $X$.
\end{rem}



The symmetry of equicontinuity and uniform convergence will be further studied in Sec. \ref{lb306}. (Indeed, we will see that it is better to view ``uniform convergence + continuity" and ``pointwise convergence + equicontinuity" as symmetric conditions.) As an application, we will see that equicontinuity is equivalent to uniform convergence for any sequence $(f_n)$ of pointwise convergent continuous functions on a compact topological space. (See Cor. \ref{lb284}.) Thus, in this case, one can prove the uniform convergence of $(f_n)$ by proving for instance that it converges pointwise and has a uniform Lipschitz constant.







\begin{comment}

By \eqref{eq78}, the uniform topology of $Y^I$ has a base generated by $\{V_{(y_\alpha)}^\eps:(y_\alpha)\in Y^I,\eps>0\}$ where
\begin{align*}
V_{(y_\alpha)}^\eps=\Big\{(y'_\alpha)\in Y^I:\sup_{\alpha\in I}d(y'_\alpha,y_\alpha)<\eps\Big\}
\end{align*}
So the equivalence of (1) and (2) follows immediately from the base version of Def. \ref{lb188}-(2).
\end{comment}


\subsection{Uniform convergence and equicontinuity: two faces of $\Psi$}\label{lb306}



\subsubsection{Main results}\label{lb282}

In this subsection, we fix a directed set $I$, and let
\begin{align*}
I^*=I\cup\{\infty_I\}
\end{align*}
where $\infty_I$ is a new symbol not in $I$. Write $\infty_I$ as $\infty$ for simplicity. Equip $I^*$ with the standard topology as in Def. \ref{lb175}. Recall that this topology has basis
\begin{align*}
\mc B=\big\{\{\alpha\}:\alpha\in I \big\}\cup \big\{I^*_{\geq\alpha}:\alpha\in I \big\}
\end{align*}
Clearly $I\times X$ is dense in $I^*\times X$. Equip $C(X,\mc V)$ and $C(I^*,\mc V)$ with the uniform convergence topologies.

Throughout this subsection, we fix a net $(f_\alpha)_{\alpha\in I}$ in $\mc V^X$ and an element $f_\infty\in\mc V^X$. Define
\begin{align}
F:I^*\times X\rightarrow\mc V\qquad F(\mu,x)=f_\mu(x)  \label{eq88}
\end{align} 
The meaning of the title of this section is illustrated by the following theorem.





\begin{thm}\label{lb280}
We have (a)$\Leftrightarrow$(b) and (1)$\Leftrightarrow$(2)  where:
\begin{itemize}
\item[(a)] $(f_\alpha)_{\alpha\in I}$ converges uniformly to $f_\infty$, and $f_\alpha:X\rightarrow\mc V$ is continuous for each $\alpha\in I$.
\item[(b)] $F$ gives rise to a continuous map $I^*\rightarrow C(X,\mc V)$
\item[(1)] $(f_\alpha)_{\alpha\in I}$ is equicontinuous and converges pointwise to $f_\infty$.
\item[(2)] $F$ gives rise to a continuous map $X\rightarrow C(I^*,\mc V)$.
\end{itemize}
\end{thm}



%% Record #11 2023/10/25 three lectures 



\begin{proof}
Assume (a). Then we have $f_\infty\in C(X,\mc V)$ due to Thm. \ref{lb279}. So $F$ gives rise to a map $I^*\rightarrow C(X,\mc V)$. Since $f_\alpha\rightrightarrows f_\infty$, we see that $F$ is continuous by Pb. \ref{lb278}-2. This proves (b).

Assume (b), then the fact that the map $I^*\rightarrow C(X,\mc V)$ has range in $C(X,\mc V)$ means precisely that each $f_\alpha$ and $f_\infty$ are continuous. The continuity of the map $I^*\rightarrow C(X,\mc V)$ at $\infty$ means $f_\alpha\rightrightarrows f_\infty$. This proves (a).


(1)$\Rightarrow$(2): Assume (1). The equicontinuity of $(f_\alpha)$ is equivalent to that
\begin{align}
x\in X\mapsto (f_\alpha(x))_{\alpha\in I}\in \mc V^I  \label{eq84}
\end{align}
is continuous where $\mc V^I$ is equipped with the uniform convergence topology. Equivalently, for each $x\in X$ and $\eps>0$ there is $U\in\Nbh_X(x)$ such that for all $p\in U$ and all $\alpha\in I$ we have
\begin{align}
\lVert f_\alpha(p)-f_\alpha(x)\lVert\leq\eps \label{eq83}
\end{align} 
Since $(f_\alpha)_{\alpha\in I}$ converges pointwise to $f_\infty$, by applying $\lim_{\alpha\in I}$ to \eqref{eq83}, we see that $\lVert f_\mu(p)-f_\mu(x)\lVert\leq\eps$ for all $p\in U$ and $\mu\in I^*$. So
\begin{align}\label{eq86}
x\in X\mapsto (f_\mu(x))_{\mu\in I^*}\in \mc V^{I^*}
\end{align}
is continuous, where $\mc V^{I^*}$ is given the uniform convergence metric. By Pb. \ref{lb278}-2, the pointwise convergence of $(f_\alpha)_{\alpha\in I}$ to $f_\infty$ is equivalent to the continuity of
\begin{align}\label{eq85}
\mu\in I^*\rightarrow f_\mu(x)\in\mc V
\end{align}
for each $x\in X$. So the map \eqref{eq86} has range inside $C(I^*,\mc V)$.

(2)$\Rightarrow$(1): Assume (2). The continuity of $X\rightarrow C(I^*,\mc V)$ implies that of \eqref{eq84}. So $(f_\alpha)_{\alpha\in I}$ is equicontinuous. Its pointwise convergence to $f$ is due to the continuity of \eqref{eq85} for each $x$, which is clearly true by (2).
\end{proof}


\begin{rem}\label{lb340}
Conditions (a) and (1) in Thm. \ref{lb280} are symmetric. Condition (a) says roughly that $F$ converges uniformly under the limit over $I$, and converges pointwise under the limit over $X$. Condition (1) says roughly that $F$ converges uniformly under the limit over $X$, and pointwise under the limit over $I$. The next theorem clarifies the relationship between these two symmetric conditions. We will see a similar condition in Thm. \ref{lb289}.
\end{rem}



\begin{thm}\label{lb277}
Consider the following statements:
\begin{enumerate}
\item[(1)] The function $F:I^*\times X\rightarrow\mc V$ is continuous.
\item[(2)] $(f_\alpha)_{\alpha\in I}$ converges uniformly to $f_\infty$, and $f_\alpha:X\rightarrow\mc V$ is continuous for each $\alpha\in I$.
\item[(3)] $(f_\alpha)_{\alpha\in I}$ is equicontinuous and converges pointwise to $f_\infty$.
\end{enumerate}
Then we have
\begin{gather*}
(2)\Longrightarrow(1)\Longleftarrow (3)\\
(2)\Longleftrightarrow(1)\qquad \text{if $X$ is compact}\\
(1)\Longleftrightarrow (3) \qquad \text{if }I=\Nbb
\end{gather*}
where $\Nbb$ is equipped with the usual order.
\end{thm}






\begin{proof}
This follows immediately from Thm. \ref{lb280}, Thm. \ref{lb274}, and the fact that $I^*$ is compact if $I=\Nbb$. 
\end{proof}

\begin{rem}
From the above proof, we see that the equivalence (1)$\Leftrightarrow$(3) holds in the more general case that $I^*$ is compact. See Pb. \ref{lb283} for an equivalent description of the compactness of $I^*$.
\end{rem}


In the following, let me give a more explicit description of condition (1) of Thm. \ref{lb277} in terms of $(f_\alpha)$ and $f_\infty$. 


\begin{pp}\label{lb281}
Assume that $f_\alpha:X\rightarrow\mc V$ is continuous for each $\alpha\in I$. Then part (1) of Thm. \ref{lb277} is equivalent to both (1') and (1''), where
\begin{itemize}
\item[(1')] For each $x\in X$, we have 
\begin{align}\label{eq87}
\lim_{
\begin{subarray}{c}
(\alpha,p)\in I\times X\\
(\alpha,p)\rightarrow (\infty,x)
\end{subarray}
}
f_\alpha(p)=f_\infty(x)
\end{align}
\item[(1'')] For each $x\in X$ and $\eps>0$, there exist $\beta\in I$ and $U\in\Nbh_X(x)$ such that 
\begin{align}\label{eq79}
\lVert f_\alpha(p)-f_\infty(x)\lVert<\eps\qquad (\text{for all } \alpha\in I_{\geq\beta}\text{ and } p\in U)
\end{align}
\end{itemize}
\end{pp}

Note that one can make sense of the LHS of \eqref{eq87} because $I\times X$ is clearly dense in $I^*\times X$. %Also, if $(f_\alpha)$ and $f_\infty$ satisfy (1) or (2), we say that $(f_\alpha)$ \textbf{converges quasi-uniformly} to $f_\infty$. \index{00@Quasi-uniform convergence} If $(f_\alpha)$ and $f_\infty$ satisfy (1) or (2) for a given $x$ (instead of for all $x$), we say that $(f_\alpha)$ \textbf{converges quasi-uniformly} to $f_\infty$ at $x$.

\begin{proof}
The equivalence of (1') and (1'') is clear by Def. \ref{lb197}. The continuity of $f_\alpha$ for all $\alpha\in I$ means precisely that $F|_{I\times X}$ is continuous. Therefore, (1') is equivalent to part (1) of Thm. \ref{lb277}, thanks to the following Thm. \ref{lb276}.
\end{proof}


We refer the readers to Pb. \ref{lb285} and Thm. \ref{lb286} for an interesting example of $(f_\alpha)$ satisfying (1') of Prop. \ref{lb281}.


\subsubsection{Proving continuity using limits}



\begin{thm}\label{lb276}
Let $\varphi:X\rightarrow Y$ be a map of topological spaces where $Y$ is metrizable. Let $A$ be a dense subset of $X$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\varphi:X\rightarrow Y$ is continuous.
\item The restriction $\varphi|_A:A\rightarrow Y$ is continuous. Moreover, for each $x\in X\setminus A$, the restriction $\varphi|_{A\cup\{x\}}$ is continuous at $x$, namely (cf. Def. \ref{lb197}) ,
\begin{align}
\lim_{
\begin{subarray}{c}
p\in A\\
p\rightarrow x
\end{subarray}
}\varphi(p)=\varphi(x)
\end{align}
\end{enumerate}
\end{thm}




\begin{proof}
Clearly (2) is equivalent to
\begin{align}\label{eq81}
\varphi|_{A\cup\{x\}}\text{ is continuous at $x$ for all $x\in X$}
\end{align}
So (1)$\Rightarrow$(2). Assume (2) and that $Y$ is a metric space. Choose any $x\in X$. We want to show that $\varphi:X\rightarrow Y$ is continuous at $x$. Choose any $\eps>0$. Recall that an open subset of $A$ is precisely the intersection of $A$ and an open subset of $X$. Thus, by \eqref{eq81}, there is $U\in\Nbh_X(x)$ such that for all $p\in A\cap U$ we have
\begin{align}
d(\varphi(p),\varphi(x))\leq \eps/2\label{eq82}
\end{align}


It remains to prove \eqref{eq82} for all $p\in U$. Choose any $p\in U$. By Lem. \ref{lb342}, $A\cap U$ is dense in $U$.  Thus, there is a net $(p_\alpha)$ in $A\cap U$ converging to $p$. In particular, for each $\alpha$ we have
\begin{align*}
d(\varphi(p_\alpha),\varphi(x))\leq \eps/2
\end{align*}
Since $\varphi|_{A\cup\{p\}}$ is continuous (by \eqref{eq81}), we have $\lim_\alpha\varphi(p_\alpha)=\varphi(p)$. Thus, applying $\lim_\alpha$ to the above inequality proves \eqref{eq82} for $p\in U$.
\end{proof}


\begin{lm}\label{lb342}
Suppose that $A$ is a dense subset of $X$. Let $U$ be an open subset of $A$. Then $A\cap U$ is dense in $U$.
\end{lm}

This lemma is clearly false if $U$ is not assumed to be open: simply take $U=X\setminus A$.

\begin{proof}[First proof]
Choose any $x\in U$. We want to find a net in $A\cap U$ converging to $x$. Since $A$ is dense in $X$, there is a net $(x_\alpha)_{\alpha\in J}$ in $X$ converging to $x$. Since $U$ is a neighborhood of $x$, $(x_\alpha)$ is eventually in $U$, say $x_\alpha\in U$ whenever $\alpha\geq\beta$. Then $(x_\alpha)_{\alpha\in J_{\geq \beta}}$ is a net in $A\cap U$ converging to $x$.
\end{proof}

\begin{proof}[Second proof]
We want to show that every nonempty open subset of $U$ intersects $A\cap U$. But an open subset of $U$ is precisely an open subset of $X$ contained inside $U$ (Exe. \ref{lb341}). So this set (when nonempty) must intersect $A$ because $A$ is dense in $X$.
\end{proof}



\begin{sexe}
In Thm. \ref{lb276}, weaken the metrizability of $Y$ to the condition that $Y$ is regular. (See the definition below.) Prove the conclusion of Thm. \ref{lb276}.
\end{sexe}

\begin{sdf}
A topological space $Y$ is called \textbf{regular} \index{00@Regular topological space} if for every $y\in Y$ and every $U\in\Nbh_Y(y)$ there exists $V\in\Nbh_Y(y)$ such that $\Cl_Y(V)\subset U$.
\end{sdf}



\subsubsection{Immediate consequences of Thm. \ref{lb277}}

The following result is parallel to Thm. \ref{lb279} for uniformly convergent nets of continuous functions.

\begin{co}\label{lb303}
Let $(f_\alpha)_{\alpha\in I}$ be an equicontinuous net of functions $X\rightarrow\mc V$. Assume that $(f_\alpha)$ converges pointwise to $f:X\rightarrow\mc V$. Then $f$ is continuous.
\end{co}

\begin{proof}
Write $f_\infty=f$ and define $F$ by \eqref{eq88}. Then $F$ is continuous by (3)$\Rightarrow$(1) of Thm. \ref{lb277}. So $f=f_\infty$ is continuous, since $f$ is the composition of $F$ with the inclusion map $x\in X\mapsto (\infty,x)\in I^*\times X$.
\end{proof}






\begin{rem}\label{lb304}
In the future, we will study the general Tychonoff theorem, which says for example that if $(f_\alpha)_{\alpha\in I}$ is a net of functions $X\rightarrow\Rbb^N$ which is pointwise bounded, i.e. $\sup_{\alpha\in I}\lVert f(x)\lVert<+\infty$ for all $x\in X$, then $(f_\alpha)$ has a subnet converging poinwise. However, if we assume moreover that each $f_\alpha$ is continuous, we cannot conclude in general that $(f_\alpha)$ has a subnet converging pointwise to a \textit{continuous} function. But we can make such a conclusion when $(f_\alpha)$ is equicontinuous, thanks to Cor. \ref{lb303}. Therefore, Cor. \ref{lb303} tells us that equicontinuity is useful for studying the problems of compactness of families of continuous functions (under the pointwise convergence topology).
\end{rem}


In fact, we have a slightly stronger version of Cor. \ref{lb303}:

\begin{co}\label{lb310}
Let $(f_\alpha)_{\alpha\in I}$ be a net of functions $X\rightarrow\mc V$ equicontinuous at $x$. Assume that $(f_\alpha)$ converges pointwise to $f:X\rightarrow\mc V$. Then $f$ is continuous at $x$.
\end{co}

\begin{proof}
Define $X_x$ to be the same as $X$ as a set, but has a different topology: the one generated by the basis
\begin{align*}
\mc B_x=\Nbh_X(x)\cup\big\{\{p\}:p\neq x\big\}
\end{align*}
(cf. Exe. \ref{lb308}). Define $g_\alpha:X_x\rightarrow\mc V$ and $g:X_x\rightarrow\mc V$ to be the same as $f_\alpha$ and $f$. Then $(g_\alpha)$ is a net of equicontinuous functions converging pointwise to $g$. Therefore, by Cor. \ref{lb303}, $g$ is continuous. So $f$ is continuous at $x$. 
\end{proof}



\begin{rem}
By a similar argument, we can generalize Thm. \ref{lb279} to the following form: Let $(f_\alpha)$ be a net of functions $X\rightarrow \mc V$ converging uniformly to $f:X\rightarrow\mc V$. Suppose that each $f_\alpha$ is continuous at $x$. Then $f$ is continuous at $x$.
\end{rem}






\begin{eg}\label{lb311}
In this example, we pretend to know derivatives. Let $(f_n)$ be a sequence of functions $\Rbb_{\geq0}\rightarrow\Rbb$ defined by $f_n(x)=x^{1/n}$. (We understand $0^{\frac 1n}=0$.) Find all $x\in\Rbb_{\geq 0}$ at which $(f_n)$ is equicontinuous.
\end{eg}

\begin{proof}
We prove that $\Rbb_{>0}$ is the set of all points at which $(f_n)$ is equicontinuous. First, assume $x>0$. Choose $0<a<1<b$ such that $a<x<b$. Then, on $[a,b]$, $f_n'(x)=\frac 1n x^{\frac 1n-1}$ is bounded by $C=\max\{a^{-1},b\}$. So $(f_n|_{[a,b]})$ has Lipschitz constant $C$ by Exp. \ref{lb309}. So $(f_n)$ is equicontinuous at $x$. 

Note that $(f_n)$ converges pointwise to $f$ where $f(x)=1$ if $x>0$ and $f(0)=0$. But $f$ is not continuous at $0$. So $(f_n)$ is not equicontinuous at $0$ due to Cor. \ref{lb310}.
\end{proof}

One can also prove that $(f_n)$ is equicontinuous on $(0,1)\cup(1,+\infty)$ without using derivatives: See Thm. \ref{lb286}-1.





\begin{co}\label{lb284}
Let $(f_\alpha)_{\alpha\in I}$ be a net in $C(X,\mc V)$ converging pointwise to $f\in\mc V^X$. Consider the following statements:
\begin{enumerate}[label=(\arabic*)]
\item $(f_\alpha)_{\alpha\in I}$ converges uniformly to $f$.
\item $(f_\alpha)_{\alpha\in I}$ is equicontinuous.
\end{enumerate}
Then the following are true.
\begin{enumerate}
\item If $(f_\alpha)_{\alpha\in I}$ is a sequence $(f_n)_{n\in\Zbb_+}$, then (1)$\Rightarrow$(2).
\item If $X$ is compact, then (2)$\Rightarrow$(1).
\end{enumerate}
\end{co}


\begin{proof}
Immediate from Thm. \ref{lb277}.
\end{proof}









\subsection{Uniform convergence and double limits}



In this section, we use Thm. \ref{lb277} to study the problem of when the order of double limits can be exchanged. We first give a useful criterion on uniform convergence.


\begin{pp}\label{lb288}
Assume that $\mc V$ is a Banach space. Let $(f_\alpha)_{\alpha\in I}$ be a net in $C(X,\mc V)$. Assume that $(f_\alpha)$ converges uniformly on a dense subset $E$ of $X$. Then $(f_\alpha)$ converges uniformly on $X$.
\end{pp}

The completeness of $\mc V$ is important here.

\begin{proof}
Since $E$ is dense, by \eqref{eq80} (applied to the function $|f_\alpha-f_\beta|$), we have
\begin{align*}
\sup_{x\in X}\lVert f_\alpha(x)-f_\beta(x)\lVert=\sup_{x\in E}\lVert f_\alpha(x)-f_\beta(x)\lVert
\end{align*}
where the RHS converges to $0$ under $\lim_{\alpha,\beta}$. Thus, $(f_\alpha)$ is a Cauchy net in $\mc V^X$ where $\mc V^X$ is equipped with the uniform convergence metric as in Exp. \ref{lb272}. So $(f_\alpha)_{\alpha\in I}$ converges uniformly on $X$ by Thm. \ref{lb339}.
\end{proof}




\begin{thm}[\textbf{Moore-Osgood theorem}]\label{lb289}  \index{00@Moore-Osgood theorem}
Let $(f_{\alpha,\beta})_{(\alpha,\beta)\in I\times J}$ be a net in a Banach space $\mc V$ with index set $I\times J$ where $I,J$ are directed sets. Assume the following conditions:
\begin{enumerate}[label=(\arabic*)]
\item For each $\alpha\in I$, there exists $f_{\alpha,\infty}\in\mc V$ such that 
\begin{align}
\lim_{\beta\in J} f_{\alpha,\beta}=f_{\alpha,\infty}
\end{align}
\item For each $\beta\in J$, there exists $f_{\infty,\beta}\in\mc V$ such that
\begin{align}
\lim_{\alpha\in I}\sup_{\beta\in J}\lVert f_{\alpha,\beta}-f_{\infty,\beta}\lVert=0
\end{align}
\end{enumerate}
Then the following are true:
\begin{enumerate}
\item The following limits exist and are equal:
\begin{align}
\lim_{(\alpha,\beta)\in I\times J}f_{\alpha,\beta}=\lim_{\alpha\in I}f_{\alpha,\infty}=\lim_{\beta\in J}f_{\infty,\beta}
\end{align}
\item If $I=\Nbb$ where $\Nbb$ is equipped with the usual order, then
\begin{align}\label{eq89}
\lim_{\beta\in J}\sup_{\alpha\in I}\lVert f_{\alpha,\beta}-f_{\alpha,\infty}\lVert=0
\end{align}
\end{enumerate}
\end{thm}

Conditions (1) and (2) in Thm. \ref{lb289}, which say that the limit of $f_{\alpha,\beta}$ is pointwise over one index and uniform in another index, should remind you of conditions (2) and (3) in Thm. \ref{lb277} (cf. Rem. \ref{lb340}). In fact, we shall use Thm. \ref{lb277} to understand and prove the Moore-Osgood theorem.


\begin{proof}
Part 1: By Thm. \ref{lb122}, it suffices to prove that $\lim_{\alpha,\beta}f_{\alpha,\beta}$ converges. Define topological spaces $I^*=I\cup\{\infty_I\}$ and $J^*=J\cup\{\infty_J\}$ as in Subsec. \ref{lb282}. Define 
\begin{gather*}
g_\alpha:J^*\rightarrow\mc V\qquad g_\alpha(\nu)=f_{\alpha,\nu}
\end{gather*}
where $g_\alpha(\infty_J)=f_{\alpha,\infty}$. By (1), for each $\alpha\in I$, the function $g_\alpha$ is continuous at $\infty_J$, and hence $g_\alpha\in C(J^*,\mc V)$. By (2), $(g_\alpha)_{\alpha\in I}$ converges uniformly on $J$. Since $J$ is a dense subset of $J^*$, by Prop. \ref{lb288}, $(g_\alpha)_{\alpha\in I}$ converges uniformly on $J^*$ to some $g_{\infty_I}:J^*\rightarrow\mc V$. Thus, by Thm. \ref{lb277}, the function
\begin{align*}
F:I^*\times J^*\rightarrow\mc V\qquad F(\mu,\nu)=g_\mu(\nu)
\end{align*}
is continuous. Its continuity at $(\infty_I,\infty_J)$ implies that $\dps\lim_{\alpha\in I,\beta\in J}f_{\alpha,\beta}=\dps\lim_{\alpha\in I,\beta\in J} F(\alpha,\beta)$ converges to $F(\infty_I,\infty_J)$.

Part 2: By Thm. \ref{lb277}, $(g_\alpha)_{\alpha\in I}$ is an equicontinuous family of functions $J^*\rightarrow\mc V$. Its equicontinuity at $\infty_J$ means precisely \eqref{eq89}.
\end{proof}


\begin{rem}
Whenever you see a theorem stated in very plain language but proved using a huge machinery, you should always ask yourself if a direct proof is possible. A huge machinery or fancy language is not always necessary for the proof, but often helps to understand the nature of the problem.

Now, since Thm. \ref{lb289} is stated without using the language of topological spaces and continuous maps, it is desirable to have a direct and elementary proof. It could be done by directly translating the above proof (and the proof of the results cited in that proof) into the pure language of nets. However, we prefer to give a simpler proof which is related to, but is not a direct translation of, the above topological proof.   \hfill\qedsymbol
\end{rem}





\begin{proof}[\textbf{A direct proof of Thm. \ref{lb289}}]
Part 1: By Thm. \ref{lb122}, it suffices to prove that $\lim_{\alpha,\beta}f_{\alpha,\beta}$ converges.  Since $V$ is complete, it suffices to prove the Cauchy condition:
\begin{itemize}
\item[(i)] For each $\eps>0$, there exists $(\alpha,\beta)\in I\times J$ such that for all $(\mu,\nu)\in I_{\geq\alpha}\times J_{\geq\beta}$, we have $\lVert f_{\alpha,\beta}-f_{\mu,\nu}\lVert<\eps$.
\end{itemize}
Choose $\eps>0$. By condition (2) of Thm. \ref{lb289}, $(f_{\alpha,\beta})$, as a net of functions $J\rightarrow \mc V$ with index set $I$, converges uniformly. Thus, by the Cauchy condition for the uniform convergence metric as in Exp. \ref{lb272} (which is available due to Thm. \ref{lb339}), we have:
\begin{itemize}
\item[(ii)] There exists $\alpha\in I$ such that for all $\mu\geq\alpha$, we have $\sup_{\nu\in J}\lVert f_{\alpha,\nu}-f_{\mu,\nu}\lVert<\eps/2$.
\end{itemize}
Fix $\alpha$ as above. By condition (1), the limit $\lim_{\beta\in J} f_{\alpha,\beta}$ exists. Thus:
\begin{itemize}
\item[(iii)] There exists $\beta\in J$ such that for all $\nu\geq\beta$ we have $\lVert f_{\alpha,\beta}-f_{\alpha,\nu}\lVert<\eps/2$.
\end{itemize}
Combining (ii) and (iii) and using triangle inequality, we see that for each $\mu\geq\alpha$ and $\nu\geq\beta$,
\begin{align*}
\lVert f_{\alpha,\beta}-f_{\mu,\nu}\lVert\leq \lVert f_{\alpha,\beta}-f_{\alpha,\nu}\lVert+\lVert f_{\alpha,\nu}-f_{\mu,\nu}\lVert<\eps
\end{align*}
This proves (i).\\[-1ex]

Part 2: Assume $I=\Nbb$. Since $(f_{\alpha,\beta})$ is a Cauchy net, for each $\eps>0$ there exist $\alpha_0\in I,\beta_0\in J$ such that for all $\alpha\geq\alpha_0$ and  $\beta,\nu\geq\beta_0$ we have $\lVert f_{\alpha,\beta}-f_{\alpha,\nu}\lVert<\eps$. Applying $\lim_\nu$ gives
\begin{align*}
\lVert f_{\alpha,\beta}-f_{\alpha,\infty}\lVert\leq\eps\qquad(\forall\alpha\geq\alpha_0,\beta\geq\beta_0)
\end{align*}
Since $I=\Nbb$, there are finitely many $\alpha$ not $\geq\alpha_0$.  For any such $\alpha$, by condition (1) of Thm. \ref{lb289}, there exists $\beta_\alpha\in J$ such that for all $\beta\geq\beta_{\alpha}$, we have $\lVert f_{\alpha,\beta}-f_{\alpha,\infty}\lVert\leq\eps$. Choose $\wtd\beta$  greater than or equal to $\beta_0$ and all these (finitely many) $\beta_\alpha$. Thus, we have $\lVert f_{\alpha,\beta}-f_{\alpha,\infty}\lVert\leq\eps$ for all $\alpha\in I$ and all $\beta\geq\wtd\beta$. This proves \eqref{eq89}.
\end{proof}



%% Record #12 2023/10/30 two lectures 





















\subsection{Problems and supplementary materials}


Recall from the beginning of this chapter that $X$ is a topological space and $\mc V$ is a normed vector space over $\Fbb\in\{\Rbb,\Cbb\}$.

\begin{prob}\label{lb283}
Let $I$ be a directed set. Let $I^*=I\cup\{\infty\}$, equipped with the standard topology as in Subsec. \ref{lb282}. Prove that the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $I^*$ is compact.
\item For each $\alpha\in I$, the complement of $I^*_{\geq\alpha}=\{\beta\in I^*:\beta\geq\alpha\}$ is a finite set.
\end{enumerate}
\end{prob}


\begin{proof}[Note]
Prop. \ref{lb249} can make your proof shorter.
\end{proof}


\begin{prob}
Let $(f_n)$ be a sequence of functions $\Rbb_{\geq0}\rightarrow\Rbb$ defined by $f(x)=x^{\frac 1n}$ (as in Exp. \ref{lb311}). Give a direct proof that $(f_n)$ is not equicontinuous at $0$ using the definition of equicontinuity. Do not use Cor. \ref{lb310}.
\end{prob}


\begin{prob}
Give a direct proof of Cor. \ref{lb303} without using Thm. \ref{lb274} (and its consequences) or using Thm. \ref{lb289}.
\end{prob}

\begin{prob}
Give a direct proof of Cor. \ref{lb284} without using Thm. \ref{lb274} (and its consequences) or using Thm. \ref{lb289}.
\end{prob}

\begin{sprob}\label{lb285}
Let $(f_\alpha)_{\alpha\in I}$ be a net in $C(X,\Rbb)$. Assume that $(f_\alpha)_{\alpha\in I}$ is increasing, i.e., $f_\alpha\leq f_\beta$ whenever $\alpha\leq\beta$. Assume that $(f_\alpha)_{\alpha\in I}$ converges pointwise to $f\in C(X,\Rbb)$. Prove that for every $x\in X$,
\begin{align}
\lim_{
\begin{subarray}{c}
\alpha\in I\\
p\rightarrow x
\end{subarray}
}
f_\alpha(p)=f(x)
\end{align}
Namely, prove that for every $x\in X$ and $\eps>0$ there exist $\beta\in I$ and $U\in\Nbh_X(x)$ such that $|f_\alpha(p)-f(x)|<\eps$ for all $\alpha\geq\beta$ and all $p\in U$.
\end{sprob}

\begin{proof}[Note]
Let $g_\alpha=f-f_\alpha$. Then $(g_\alpha)_{\alpha\in I}$ is a decreasing net of continuous functions converging pointwise to $0$. It suffices to prove the easier statement that $\dps\lim_{\alpha\in I,p\rightarrow x}g_\alpha(p)=0$ for every $x\in X$. (Why is this sufficient?)
\end{proof}

\begin{sthm}\label{lb286}
Let $(f_\alpha)_{\alpha\in I}$ be a net in $C(X,\Rbb)$. Assume that $(f_\alpha)_{\alpha\in I}$ is increasing and converges pointwise to $f\in C(X,\Rbb)$. The following statements are true.
\begin{enumerate}
\item If $(f_\alpha)_{\alpha\in I}$ is a sequence $(f_n)_{n\in\Zbb_+}$, then $(f_n)_{n\in\Zbb_+}$ is equicontinuous.
\item (\textbf{Dini's theorem}) \index{00@Dini's theorem} If $X$ is compact, then $(f_\alpha)_{\alpha\in I}$ converges uniformly to $f$.
\end{enumerate}
\end{sthm}


\begin{proof}
By Pb. \ref{lb285}, $(f_\alpha)_{\alpha\in I}$ and $f_\infty=f$ satisfy (1') of Prop. \ref{lb281}. Therefore, the two statements follow directly from Thm. \ref{lb277}.
\end{proof}


















\newpage


\section{Extending continuous functions to the closures}



\subsection{Introduction}

Let $\wtd X,Y$ be topological spaces, let $X\subset \wtd X$, and let $f:X\rightarrow Y$ be a continuous map. The \textbf{extension problem} asks whether $f$ can be extended to a continuous map $\wtd f:\wtd X\rightarrow Y$. ``Extended" means that $\wtd f|_X=f$.  Since we can try to first extend $f$ from $X$ to its closure $\Cl_{\wtd X}(X)$, and then from $\Cl_{\wtd X}(X)$ to $\wtd X$, the extension problem can naturally be divided into two cases: (1) $X$ is dense in $\wtd X$. (2) $X$ is closed in $\wtd X$.

In this chapter, we study the first case. Assume that $X$ is dense in $\wtd X$. Then by Prop. \ref{lb196}, we know that $f$ can have at most one extension if $Y$ is Hausdorff. So there is essentially no uniqueness issue. 


The study of extension problem in this case has a long history. As we have seen in Subsec. \ref{lb290}, the limits of functions can be understood in this light: If $x\in\wtd X\setminus X$, then $f$ can be extended to a continuous function on $X\cup\{x\}$ iff $\lim_{p\rightarrow x}f(p)$ exists. Of course, this is simply a rephrasing of the definition of $\lim_{p\rightarrow x}f(p)$. But the idea of ``extending $f$ to $\wtd X$ by first extending $f$ to a slightly larger set with one extra point $\{x\}$" is helpful and can sometimes simplify proofs.

Indeed, recall that in the proof of Prop. \ref{lb281} we used Thm. \ref{lb276}, which tells us that if $\lim_{p\rightarrow x}f(p)$ converges for all $x\in\wtd X\setminus X$, then $f$ can be extended (necessarily uniquely) to a continuous $\wtd f:\wtd X\rightarrow Y$ where $Y$ is assumed metrizable. Thus, \uline{the extensibility of $f$ to $\wtd X$ can be checked pointwise}. Thm. \ref{lb276} is our first important general result on the extension problem. Let me state Thm. \ref{lb276} in the following equivalent way, which is more convenient for the  study of extension problems.
\begin{co}\label{lb307}
Let $\varphi:X\rightarrow Y$ be a continuous map of topological spaces where $Y$ is metrizable and $X$ is a dense subspace of a topological space $\wtd X$. The following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item There exists a continuous map $\wtd\varphi:\wtd X\rightarrow Y$ such that $\wtd\varphi|_X=\varphi$.
\item For each $x\in\wtd X\setminus X$, the limit $\lim_{p\rightarrow x}f(p)$ exists.
\end{enumerate}
\end{co}

\begin{proof}
If (1) is true, then $\wtd\varphi|_{X\cup\{x\}}$ is continuous whenever $x\in\wtd X\setminus X$. This proves (2). Conversely, assume (2). Extend $\varphi$ to a map $\wtd\varphi:\wtd X\rightarrow Y$ by setting $\wtd\varphi(x)=\lim_{p\rightarrow x}f(p)$ if $x\in\wtd X\setminus X$. Then $\wtd\varphi$ is continuous by Thm. \ref{lb276}.
\end{proof}







This chapter will focus on another useful method for extending continuous functions in the setting of metric spaces. A main result (cf. Cor. \ref{lb298}) is that if $\wtd X,Y$ are metric spaces and $Y$ is complete, then a sufficient condition for the extensibility of $f$ onto $\wtd X$ is that $f$ is uniformly continuous. Moreover, uniform continuity is also a necessary condition if $\wtd X$ is compact. (Recall that compact metric spaces are complete, cf. Thm. \ref{lb79}.) It is worth mentioning that the extensibility of $f$ is a purely topological question, whereas the uniform continuity of $f$ depends on the metric on $\wtd X$.



\subsection{Uniform continuity}


Fix metric spaces $\wtd X,Y$ and a dense subset $X\subset\wtd X$. 

\subsubsection{Basics}


We first give some examples of $f\in C(X,Y)$ that cannot be extended to a continuous function on $\wtd X$. Since uniform continuity is a sufficient condition for the extensibility (when $Y$ is complete), these examples are not uniformly continuous. Recall our convention that subsets of $\Rbb$ or $\Rbb^N$ are equipped with the Euclidean metrics.

\begin{eg}
Assume $X=(0,+\infty)$, $\wtd X=[0,+\infty)$, $Y=\Rbb$, and $f:X\rightarrow Y$ is defined by $f(x)=1/x$. Then $\lim_{x\rightarrow 0}f(x)$ is $+\infty$ in $\ovl\Rbb$, and hence does not converge in $Y$. So $f$ cannot be extended onto $\wtd X$.
\end{eg}

\begin{eg}
$X=(0,1]$, $\wtd X=[0,1]$, $Y=\Rbb$, $f:X\rightarrow Y$ is defined by $f(x)=\sin(1/x)$. Then $\lim_{n\rightarrow\infty}f(x_n)$ equals $0$ if the sequence $(x_n)$ in $X$ converging to $0$ is defined by $x_n=1/2n\pi$, and equals $1$ if $x_n=1/(2n+\frac 12)\pi$. Thus, by Rem. \ref{lb202}, $\lim_{x\rightarrow0}f(x)$ does not exist in $Y$. So $f$ cannot be extended onto $\wtd X$.
\end{eg}


\begin{df}\label{lb291}
A map $f:X\rightarrow Y$ is called \textbf{uniformly continuous} \index{00@Uniformly continuous} if the following equivalent conditions are satisfied:
\begin{enumerate}[label=(\arabic*)]
\item For every $\eps>0$ there exists $\delta>0$ such that for all $x,x'\in X$ we have
\begin{subequations}
\begin{align}\label{eq91}
d(x,x')<\delta\qquad\Longrightarrow\qquad d(f(x),f(x'))<\eps
\end{align}
\item For every nets $(x_\alpha)_{\alpha\in I},(x'_\alpha)_{\alpha\in I}$ in $X$ (with the same index set $I$) we have
\begin{align}\label{eq92}
\lim_{\alpha\in I} d(x_\alpha,x'_\alpha)=0\qquad\Longrightarrow\qquad\lim_{\alpha\in I} d(f(x_\alpha),f(x'_\alpha))=0
\end{align}
\item For every sequences $(x_n)_{n\in\Zbb_+},(x_n')_{n\in\Zbb_+}$ in $X$ we have
\begin{align}\label{eq93}
\lim_{n\rightarrow\infty} d(x_n,x'_n)=0\qquad\Longrightarrow\qquad\lim_{n\rightarrow\infty} d(f(x_n),f(x'_n))=0
\end{align}
\end{subequations}
\end{enumerate}
\end{df}

Uniformly continuous maps are clearly continuous. Def. \ref{lb291}-(2) says that uniformly continuous functions are those sending Cauchy-equivalent nets (cf. Def. \ref{lb155}) to Cauchy-equivalent nets. 


\begin{proof}[Proof of equivalence]
(1)$\Rightarrow$(2): Assume (1). Choose nets $(x_\alpha)_{\alpha\in I},(x'_\alpha)_{\alpha\in I}$ in $X$. Choose any $\eps>0$. Choose $\delta>0$ such that \eqref{eq91} holds. If $d(x_\alpha,x'_\alpha)\rightarrow0$, then $d(x_\alpha,x'_\alpha)<\delta$ for sufficiently large $\alpha$. So $d(f(x_\alpha),f(x'_\alpha))<\eps$ for sufficiently large $\alpha$. This proves (2).

(2)$\Rightarrow$(3): Obvious.

$\neg$(1) $\Rightarrow$ $\neg$(3): Assume that (1) is false. Then there exists $\eps>0$ such that for all $\delta>0$  there exist $x,x'\in X$ with $d(x,x')<\delta$ such that $d(f(x),f(x'))\geq\eps$. Thus, for each $n\in\Zbb_+$, there exist $x_n,x_n'\in X$ such that $d(x_n,x_n')<1/n$ and $d(f(x_n),f(x_n'))\geq\eps$. The sequences $(x_n),(x_n')$ imply that \eqref{eq93} is false.
\end{proof}




\begin{co}\label{lb292}
Assume that $f:X\rightarrow Y$ is uniformly continuous. Let $(x_\alpha)_{\alpha\in I}$ be a Cauchy net in $X$. Then $(f(x_\alpha))_{\alpha\in I}$ is a Cauchy net in $Y$.
\end{co}

\begin{proof}
Apply Def. \ref{lb291}-(2) to the nets $(x_\alpha)_{(\alpha,\beta)\in I^2}$ and $(x_\beta)_{(\alpha,\beta)\in I^2}$ of $X$.
\end{proof}








\subsubsection{Extensibility of uniformly continuous functions}



The following theorem can be viewed as the uniform continuity version of Prop. \ref{lb288}. In particular, both results assume the completeness of the codomain.

\begin{thm}\label{lb297}
Let $f:X\rightarrow Y$ be uniformly continuous, and assume that $Y$ is complete. Then there exists a (necessarily unique) uniformly continuous $\wtd f:\wtd X\rightarrow Y$ satisfying $\wtd f|_X=f$.
\end{thm}


\begin{proof}
Choose any $x\in\wtd X$. Since $X$ is dense in $\wtd X$, we can choose a sequence $(x_n)$ in $X$ converging to $x$ in $\wtd X$. In particular, $(x_n)$ is a Cauchy sequence. Therefore, by Cor. \ref{lb292}, $(f(x_n))$ is a Cauchy sequence in $Y$, which converges to some point $\wtd f(x)\in Y$ by the completeness of $Y$. If $x\in X$, we assume that $(x_n)$ is the constant sequence $x$. This shows that $\wtd f(x)=f(x)$ if $x\in X$.

We have constructed a function $\wtd f:\wtd X\rightarrow Y$ satisfying $\wtd f|_X=f$. Let us prove that $\wtd f$ is uniformly continuous. Choose any $\eps>0$. Since $f$ is uniformly continuous, there is $\delta>0$ such that for all $p,q\in X$ we have
\begin{align*}
d(p,q)<2\delta\qquad\Longrightarrow\qquad d(f(p),f(q))<\eps/2
\end{align*}
Choose any $x,x'\in \wtd X$ satisfying $d(x,x')<\delta$. By our construction of $\wtd f$, there are sequences $(x_n)$ in $X$ converging to $x$ and $(x_n')$ in $X$ converging to $x'$ such that $f(x_n)\rightarrow \wtd f(x)$ and $f(x_n')\rightarrow\wtd f(x')$. Thus, there exist $N\in\Zbb_+$ such that for all $n\geq N$ we have
\begin{align*}
d(x,x_n)<\frac\delta 2\qquad d(x',x_n')<\frac\delta 2\qquad d(\wtd f(x),f(x_n))<\frac\eps 4\qquad d(\wtd f(x'),f(x'_n))<\frac\eps 4
\end{align*}
Choose $n=N$. Then by triangle inequality, we have $d(x_n,x_n')<2\delta$, and hence $d(f(x_n),f(x_n'))<\eps/2$. So $d(\wtd f(x),\wtd f(x'))<\eps$ by triangle inequality again.
\end{proof}






We now study the other direction. The following theorem implies that if a continuous $f:X\rightarrow Y$ can be extended to a continuous $\wtd f:\wtd X\rightarrow Y$, and if $\wtd X$ is compact, then $f$ is uniformly continuous.


\begin{thm}\label{lb294}
Suppose that $f:X\rightarrow Y$ is continuous and $X$ is compact. Then $f$ is uniformly continuous.
\end{thm}

In the same spirit as in Sec. \ref{lb293}, we give two proofs for this theorem, one using sequences and the other using open covers.

\begin{proof}[First proof]
Assume that $f$ is not uniformly continuous. By Def. \ref{lb291}-(3), there exist sequences $(x_n)$ and $(x_n')$ in $X$ such that $\lim_{n\rightarrow\infty}d(x_n,x_n')=0$, and that $d(f(x_n),f(x_n'))\nrightarrow0$. The latter means that there is $\eps>0$ such that $d(f(x_n),f(x_n'))$ is frequently $\geq0$. Thus, by passing to a subsequence, we may assume that $d(f(x_n),f(x_n'))\geq\eps$ for all $n$. Since $X\times X$ is sequentially compact, by replacing $(x_n,x_n')$ with a convergent subsequence, we assume that $x_n\rightarrow x$ and $x_n'\rightarrow x'$ where $x,x'\in X$. Since $d(x_n,x_n')\rightarrow0$, we must have $x=x'$. By the continuity of $f$, $f(x_n)$ and $f(x_n')$ converge to $f(x)=f(x')$, contradicting the fact that $d(f(x_n),f(x_n'))\geq\eps$ for all $n$. 
\end{proof}


To prove Thm. \ref{lb294} using open covers, we prove a more general result instead. The following theorem is useful for proving properties of the form ``there exists $\delta>0$ such that for all $x,x'\in X$ satisfying $d(x,x')<\delta$, we have ...".


\begin{thm}[\textbf{Lebesgue number lemma}]\index{00@Lebesgue number lemma}  \label{lb295}
Assume that the metric space $X$ is compact. Let $\fk U\subset 2^X$ be an open cover of $X$. Then there exists $\delta>0$ satisfying the following conditions:
\begin{itemize}
\item For every $x\in X$ there exists $U\in\fk U$ such that $B_X(x,\delta)\subset U$.
\end{itemize}
\end{thm}

The number $\delta$ in Thm. \ref{lb295} is called a \textbf{Lebesgue number of $\fk U$}. \index{00@Lebesgue number} In the following proof, we follow the local-to-global strategy as in Sec. \ref{lb293}. 

\begin{proof}
Choose any $p\in X$. Then there is $U\in\fk U$ containing $p$. So there is $\delta_p>0$ such that $B(p,2\delta_p)\subset U$. Therefore, there exists $V_p\in\Nbh_X(p)$ such that for every $x$ in $V_p$ we have $B(x,\delta_p)\subset U$. (Simply take $V_p=B(p,\delta_p)$.) This solves the problem locally: for each $x\in V_p$, the ball $B(x,\delta_p)$ is a subset of some member of $\fk U$.

Since $X=\bigcup_{p\in X}V_p$ and since $X$ is compact, there is a finite subset $E\subset X$ such that $X=\bigcup_{p\in E}V_p$. Take $\delta=\min\{\delta_p:p\in E\}$. For each $x\in X$, choose $p\in E$ such that $x\in V_p$. Then $B(x,\delta_p)$ is a subset of some member of $\fk U$ by the last paragraph. So the same is true for $B(x,\delta)$.
\end{proof}


Of course, similar to the examples studied in Sec. \ref{lb293}, Thm. \ref{lb295} can also be proved by contradiction and by using sequential compactness. See Pb. \ref{lb296}.



\begin{proof}[\textbf{Second proof of Thm. \ref{lb294}}]
Let us verify Def. \ref{lb291}-(1). Choose any $\eps>0$. For each $x\in X$, the set $U_x=f^{-1}(B_Y(f(x),\eps/2))$ is a neighborhood of $x$ by Prop. \ref{lb191}. So $\{U_x:x\in X\}$ is an open cover of $X$. Let $\delta$ be a Lebesgue number of $\fk U$. Choose any $x,y\in X$ satisfying $d(x,y)<\delta$.  Then $B_X(x,\delta)\subset U_{z}$ for some $z\in X$. So $x,y\in B_X(x,\delta)$ and hence $x,y\in U_z$. Therefore,
\begin{align*}
d(f(x),f(y))\leq d(f(x),f(z))+d(f(z),f(y))<\eps/2+\eps/2=\eps
\end{align*}
\end{proof}



\begin{co}\label{lb298}
Choose $f\in C(X,Y)$. Consider the following statements:
\begin{enumerate}[label=(\arabic*)]
\item $f$ is uniformly continuous.
\item There exists $\wtd f\in C(\wtd X,Y)$ such that $\wtd f|_X=f$.
\end{enumerate}
Then (1)$\Rightarrow$(2) if $Y$ is complete, and (2)$\Rightarrow$(1) if $\wtd X$ is compact.
\end{co}

\begin{proof}
Immediate from Thm. \ref{lb297} and Thm. \ref{lb294}.
\end{proof}


\begin{eg}
Let $D=B_\Cbb(0,1)=\{z\in\Cbb:|z|<1\}$ and $S^1=\{z\in\Cbb:|z|=1\}$. Let $f:D\rightarrow Y$ be a continuous function where $Y$ is a complete metric space. Then $f$ is uniformly continuous iff $\lim_{w\rightarrow z}f(w)$  exists for every $z\in S^1$.
\end{eg}

\begin{proof}
By Cor. \ref{lb307}, the limit $\lim_{w\rightarrow z}f(w)$  exists for every $z\in S^1$  iff $f$ can be extended to a continuous function $\wtd f:\ovl D=D\cup S^1\rightarrow Y$.  By Cor. \ref{lb298}, this is equivalent to that $f$ is uniformly continuous (because $\ovl D$ is compact and $Y$ is compete).
\end{proof}



\subsubsection{Uniform equicontinuity}

Although the notion of uniform equicontinuity will rarely be used in our notes, it is used in many textbooks.  So let me give a brief account of uniform equicontinuity. 



\begin{df}\label{lb316}
Let $(f_\alpha)_{\alpha\in I}$ be a family of functions $X\rightarrow Y$. (Here, the index set $I$ is not necessarily directed.) Define a metric $d$ on $Y^I$ in a similar way as \eqref{eq78}, namely, if $\mbf y,\mbf y'\in Y^I$ then
\begin{align*}
d(\mbf y,\mbf y')=\min\Big\{1,\sup_{\alpha\in I} d_Y(\mbf y(\alpha),\mbf  y'(\alpha))\Big\}
\end{align*}
We say that $(f_\alpha)_{\alpha\in I}$ is \textbf{uniformly equicontinuous} \index{00@Uniformly equicontinuous} if the map
\begin{gather}\label{eq106}
X\rightarrow Y^I\qquad x\mapsto (f_\alpha(x))_{\alpha\in I}
\end{gather}
is uniformly continuous with respect to the metric $d$. Clearly, this is equivalent to saying that:
\begin{itemize}
\item For every $\eps>0$ there exists $\delta>0$ such that for every $x,x'\in X$ satisfying $d_(x,x')<\delta$, we have
\begin{align*}
\sup_{\alpha\in I}d_Y(f_\alpha(x),f_\alpha(x'))<\eps
\end{align*}
\end{itemize}
\end{df}

Uniformly equicontinuous families of functions are equicontinuous, because uniformly continuous functions are continuous. Conversely, we have:

\begin{pp}
Assume that $X$ is compact and $(f_\alpha)_{\alpha\in I}$ is a family of functions $X\rightarrow Y$. Then $(f_\alpha)_{\alpha\in I}$ is equicontinuous iff it is uniformly equicontinuous.
\end{pp}

\begin{proof}
``$\Leftarrow$" is obvious, as mentioned above. ``$\Rightarrow$" follows immediately by applying Thm. \ref{lb294} to the continuous map \eqref{eq106}.
\end{proof}






\subsection{Completion of metric spaces}



Fix a metric space $X$ in this section. We are going to apply uniform continuity to the study of completions of metric spaces. Roughly speaking, a completion of $X$ is a complete metric space $\wht X$ containing $X$ as a dense subspace. However, completions are not unique, but are unique ``up to equivalence". So we want to show that two completions $\wht X,\wtd X$ of the same metric space $X$ are equivalent. However,  it is confusing to view $X$ as a subset of $\wtd X$ and $\wtd X$ simultaneously. A better approach is to consider (automatically injective) isometries $\varphi:X\rightarrow\wht X,\psi:X\rightarrow\wtd X$, and to show that $\varphi$ and $\psi$ are equivalent using the language of commutative diagrams (cf. Sec. \ref{lb299}).


\begin{df}
A \textbf{completion} \index{00@Completion of metric space} of the metric space $X$ is an isometry $\varphi:X\rightarrow\wht X$ where $\wht X$ is a complete metric space, and $\varphi(X)$ is dense in $\wht X$. We sometimes just say that $\wht X$ is a completion of $X$.
\end{df}


Thus, if $A$ is a dense subset of a complete metric space $B$, then the inclusion map $A\hookrightarrow B$ is a completion. Therefore, $\Rbb$ is a completion of $\Qbb$, and $[0,1]$ is a completion of $(0,1)$, $[0,1)$, $[0,1]\cap\Qbb$.



\begin{eg}
Let $A$ be a dense subset of a metric space $X$. Suppose that $\varphi:X\rightarrow\wht X$ is a completion of $X$. Then $\varphi|_A:A\rightarrow \wht X$ is clearly a completion of $A$. 
\end{eg}

\begin{eg}
Let $X$ be a subset of a complete metric space $Y$. Then $X\hookrightarrow\Cl_Y(X)$ is a completion of $X$ because every closed subset of $Y$ is complete (cf. Prop. \ref{lb86}), and hence $\Cl_Y(X)$ is complete. For example, $\{(x,y)\in\Rbb^2:x\geq0\}$ is a completion of both $A=\{(x,y)\in\Rbb^2:x>0\}$ and $B=A\cap\Qbb^2$.
\end{eg}



We want to prove that every metric space $X$ has a completion $\wht X$. First, we need a lemma, which can be viewed as analogous to Pb. \ref{lb287}.

\begin{lm}\label{lb300}
Suppose that $X$ is a dense subspace of a metric space $\wht X$. Suppose that every Cauchy sequence in $X$ converges to an element of $\wht X$. Then $\wht X$ is complete.
\end{lm}

\begin{proof}
Let $(x_n)$ be a Cauchy sequence in $\wht X$. Since $X$ is dense, there exists $x_n'\in X$ such that $d(x_n,x_n')<1/n$. So $(x_n')$ is Cauchy-equivalent to $(x_n)$. Thus $(x_n')$ is a Cauchy sequence by Exe. \ref{lb128}. By assumption, $(x_n')$ converges to some $x\in X$. So $(x_n')$ also converges to $x$ by Exe. \ref{lb128}.
\end{proof}



\begin{thm}\label{lb301}
Every metric space $X$ has a completion $\varphi:X\rightarrow\wht X$. Moreover, any completion $\psi:X\rightarrow\wtd X$ is \textbf{equivalent} (also called \textbf{isomorphic}) to $\varphi$ in the sense that there is an isometric isomorphism of metric spaces $\Phi:\wht X\rightarrow\wtd X$ such that the following diagram commutes:
\begin{equation}\label{eq94}
\begin{tikzcd}[column sep=small]
                     & X \arrow[ld,"\varphi"'] \arrow[rd,"\psi"] &   \\
\wht X \arrow[rr, "\Phi","\simeq"'] &                         & \wtd X
\end{tikzcd}
\end{equation}
\end{thm}


Recall that the commutativity of \eqref{eq94} means that $\psi=\Phi\circ\varphi$.

\begin{proof}[\textbf{Proof of existence}]
The construction of $\varphi:X\rightarrow\wht X$ is similar to the construction of $\Rbb$ from $\Qbb$ in Ch. \ref{lb167}. Let $\scr C$ be the set of Cauchy sequences in $X$. Let $\wht X=\scr C/\sim$ be the quotient set (cf. Def. \ref{lb157}) where $\sim$ is the Cauchy-equivalence relation: $(x_n)\sim(y_n)$ iff $\lim_{n\rightarrow\infty}d(x_n,y_n)=0$. We let $[x_n]_{n\in\Zbb_+}$ or simply let $[x_n]$ denote the equivalence class of $(x_n)$ in $\wht X$. The map $\varphi$ is defined by
\begin{align*}
\varphi:X\rightarrow\wht X\qquad x\mapsto [x]_{n\in\Zbb_+}
\end{align*}
where $[x]_{n\in\Zbb_+}$ is the equivalence class of the constant sequence $(x,x,\dots)$.\\[-0.5ex]


Step 1: Let us define a metric $d_{\scr C}$ on $\scr C$. Note that if $(x_n),(y_n)\in \scr C$, then by triangle inequality,
\begin{align*}
|d(x_m,y_m)-d(x_n,y_n)|\leq d(x_m,x_n)+d(y_m,y_n)
\end{align*}
where the RHS converges to $0$ as $m,n\rightarrow+\infty$. Therefore, the LHS also converges to $0$. This shows that $(d(x_n,y_n))_{n\in\Zbb_+}$ is a Cauchy sequence in $\Rbb_{\geq0}$, and hence converges. Therefore, we define
\begin{gather*}
d_{\scr C}:\wht X\times \wht X\rightarrow\Rbb_{\geq0}\\
d_{\scr C}([x_n],[y_n])=\lim_{n\rightarrow\infty}d(x_n,y_n)
\end{gather*}
This is well-defined: If $[x_n]=[x_n']$, and $[y_n]=[y_n']$, then 
\begin{align*}
|d(x_n,y_n)-d(x_n',y_n')|\leq d(x_n,x_n')+d(y_n,y_n')
\end{align*}
which converge to $0$ as $n\rightarrow\infty$. So $(d(x_n',y_n'))_{n\in\Zbb_+}$ and $(d(x_n,y_n))_{n\in\Zbb_+}$ are Cauchy-equivalent, and hence converge to the same number.

Clearly $d_{\scr C}([x_n],[y_n])=0$ iff $(x_n)\sim (y_n)$ iff $[x_n]=[y_n]$. And clearly $d_{\scr C}([x_n],[y_n])=d_{\scr C}([y_n],[x_n])$. If $[x_n],[y_n],[z_n]$ are in $\scr C$, applying $\lim_{n\rightarrow\infty}$ to
\begin{align*}
d(x_n,z_n)\leq d(x_n,y_n)+d(y_n,z_n)
\end{align*}
yields $d([x_n],[z_n])\leq d([x_n],[y_n])+d([y_n],[z_n])$. So $d_{\scr C}$ is a metric.\\[-0.5ex]

Step 2. The map $\varphi:X\rightarrow\wht X$ is clearly an isometry. Let us show that it has dense range. Choose any $[x_n]_{n\in\Zbb_+}\in\wht X$. We shall show that $\varphi(x_k)=[x_k,x_k,\dots]$ approaches $[x_n]_{n\in\Zbb_+}$ as $k\rightarrow\infty$. 

For each $k$, we have
\begin{align}
d_{\scr C}(\varphi(x_k),[x_n]_{n\in\Zbb_+})=\lim_{n\rightarrow\infty}d(x_k,x_n)\label{eq96}
\end{align}
where the RHS converges because $d_{\scr C}$ is defined. Since $(x_n)_{n\in\Zbb_+}$ is a Cauchy sequence in $X$, we have
\begin{align}
\lim_{k,n\rightarrow\infty} d(x_k,x_n)=0   \label{eq95}
\end{align}
Therefore, by \eqref{eq95} and the convergence of the RHS of \eqref{eq96}, we can use  Thm. \ref{lb122} to conclude that
\begin{align*}
\lim_{k\rightarrow\infty}d_{\scr C}(\varphi(x_k),[x_n]_{n\in\Zbb_+})=\lim_{k\rightarrow\infty}\lim_{n\rightarrow\infty}d(x_k,x_n)=\lim_{k,n\rightarrow\infty}d(x_k,x_n)=0
\end{align*}


Step 3. It remains to prove that $\wht X$ is complete. By Lem. \ref{lb300} (applied to $\varphi(X)\subset\wht X$) and the fact that $\varphi$ is an isometry, it suffices to prove that for every Cauchy sequence $(x_k)_{k\in\Zbb_+}$ in $X$, the sequence $(\varphi(x_k))_{k\in\Zbb_+}$ converges in $\wht X$. But this is true: we have shown in Step 2 that $(\varphi(x_k))_{k\in\Zbb_+}$ converges to $[x_n]_{n\in\Zbb_+}$.
\end{proof}

\begin{proof}[\textbf{Proof of equivalence}]
Suppose that $\psi:X\rightarrow\wtd X$ is another completion. The map
\begin{gather*}
\Phi:\varphi(X)\rightarrow \psi(X)\qquad \varphi(x)\mapsto\psi(x)
\end{gather*}
is well-defined since $\varphi$ is injective. Moreover, $\Phi$ is an isometry since $\varphi$ and $\psi$ are isometries. In particular, $\Phi$ is uniformly continuous. Therefore, by Cor. \ref{lb298}, $\Phi$ can be extended to a uniformly continuous map $\Phi:\wht X\rightarrow\wtd X$. Clearly $\psi=\Phi\circ\varphi$. The continuous map
\begin{gather*}
\wht X\times\wht X\rightarrow \Rbb\\
(p,q)\mapsto d_{\wtd X}(\Phi(p),\Phi(q))-d_{\wht X}(p,q)
\end{gather*}
is zero on the dense subset $\varphi(X)\times \varphi(X)$ of its domain. Therefore it is constantly zero by Prop. \ref{lb196}. This proves that $\Phi$ is an isometry.

It remains to prove that $\Phi$ is surjective. Since $\wht X$ is complete and $\Phi$ restricts to an isometric isomorphism of metric spaces $\wht X\rightarrow\Phi(\wht X)$, $\Phi(\wht X)$ is a complete metric subspace of $\wht X$. Thus, by Prop. \ref{lb86}, $\Phi(\wht X)$ is a closed subset of $\wtd X$. But $\Phi(\wht X)$ is dense in $\wtd X$ since it contains $\psi(X)$. Therefore $\Phi(\wht X)=\wtd X$.
\end{proof}

The proof of Thm. \ref{lb301} is complete.





\subsection{Why did Hausdorff believe in completion?}


Thm. \ref{lb301}, the existence and uniqueness of completion of an arbitrary metric space, was proved by Hausdorff in his 1914 work introducing Hausdorff topological spaces \cite[Sec. 8.8, p.315]{Hau14}. The construction of completion using equivalence classes of Cauchy sequences is quite abstract: Although it mimics  Cantor's construction of real numbers (cf. Ch. \ref{lb167}), its main application is not in the realm of finite-dimensional geometric objects, but in the world of function spaces. But what is the practical significance of the equivalence classes of Cauchy sequences of functions? In concrete analysis problems about functions, these objects are much more difficult to deal with than functions themselves.


Strangely enough, the only nontrivial examples we have now is $\Qbb\hookrightarrow\Rbb$. Besides this, we do not yet have  any exciting new metric spaces arising from completion. For example: we know that $[0,1]$ is the completion of $(0,1)$ under the Euclidean metric, and that $C([0,1],\Rbb)$ is the completion of the set of real polynomials $\Rbb[x]$ under the norm $\sup_{x\in[0,1]}|f(x)|$. (The density of the set of polynomials in $C([0,1],\Rbb)$ is due to Weierstrass.) But $[0,1]$ and $C([0,1],\Rbb)$ are examples we are already familiar with. 


Mathematicians do not generalize just for the sake of generalization. They want to solve problems by generalizing old concepts to a broader context. Moreover, mathematicians do not randomly choose a way of generalization and then build a huge theory. Instead, they develop a theory only in the direction that has already proved useful in solving explicit problems. Thus, Hausdorff proved Thm. \ref{lb301} because he was already convinced of the importance of abstract completion by certain powerful examples. For the moment, we are not ready to study these examples rigorously. (We will do this in the next semester.) But I want to give an informal introduction to one of these examples which historically has paved the way for many important ideas in analysis.


\subsubsection{The Fourier series method in integral equations}




In the years of 1900-1907, many important progress has been made in integral equations, which originated from the study of Dirichlet problems (finding solutions of harmonic equation $\Delta \varphi(x,y)=0$ with given boundary condition, where $\Delta=(\partial_x)^2+(\partial_y)^2$). For example, one asks if there is $\lambda\in\Rbb$ and a  single-variable complex valued function $f:[0,2\pi]\rightarrow\Cbb$ satisfying the eigenvalue problem $Tf=\lambda f$ where
\begin{align*}
(Tf)(x)=\int_{0}^{2\pi}K(x,y)f(y)dx
\end{align*}
and $K:[0,2\pi]^2\rightarrow\Rbb$ is given. (Cf. also Sec. \ref{lb55}.)

Hilbert studied this problem using Fourier series. In the modern language, the theory of Fourier series claims that $C([0,2\pi],\Cbb)$, under the $L^2$-norm
\begin{align}\label{eq100}
\lVert f\lVert_{L^2}=\sqrt{\int_0^{2\pi}|f(x)|^2\cdot \frac{dx}{2\pi} }
\end{align}
has a completion
\begin{gather}\label{eq98}
\tcboxmath{\begin{gathered}
\Phi:C([0,2\pi],\Cbb)\rightarrow l^2(\Zbb,\Cbb)\\
\Phi(f)=\wht f
\end{gathered}}
\end{gather}
where $l^2(\Zbb,\Cbb)$ is the space of all $g:\Zbb\rightarrow\Cbb$ satisfying $\sqrt{\sum_{n\in\Zbb} |g(n)|^2}<+\infty$, and 
\begin{gather*}
\wht f:\Zbb\rightarrow\Cbb\qquad 
\wht f(n)=\int_0^{2\pi} f(x)e^{-\im nx}\cdot \frac{dx}{2\pi}
\end{gather*}
gives the \textbf{Fourier coefficients} of $f$.

Hilbert studied the eigenvalue problem $Tf=\lambda f$ by transforming it into a linear algebra problem on $l^2(\Zbb,\Cbb)$ (so that $T$ becomes an $\infty\times\infty$ discrete matrix $\wht T$), finding the possible eigenvectors $\wht f$ and eigenvalues $\lambda$ satisfying
\begin{align*}
\wht T\wht f=\lambda\wht f
\end{align*}
and returning to the original problems by finding the function $f$ whose Fourier coefficents are $\wht f$. It is easy to return: if $f\in C([0,2\pi],\Cbb)$, then one gets $f$ from $\wht f$ by the formula
\begin{align}
f(x)=\sum_{n\in\Zbb} \wht f(n)e^{\im nx} \label{eq97}
\end{align} 
where the RHS is called the \textbf{Fourier series} of $f$.

Here comes the crucial point: the range of $\Phi$, namely $\{\wht f:f\in C([0,1],\Cbb)\}$, is not the whole space $l^2(\Zbb,\Cbb)$ but only its dense subspace. This remains true if we enlarge $C([0,1],\Cbb)$ to the space of Riemann integrable functions $\scr R([0,1],\Cbb)$. However, the eigenvectors of $\wht T$ found by Hilbert were only known to be elements of $l^2(\Zbb,\Cbb)$. If we use \eqref{eq97} to find the eigenvectors of the original $T$, namely, if for an arbitrary $g\in l^2(\Zbb,\Cbb)$ we write 
\begin{align}
\Psi(g)=\sum_{n\in\Zbb} g(n)e^{\im nx}  \label{eq99}
\end{align}
then $\Psi(g)$ is not necessarily continuous or even Riemann integrable. It seems that, in the light of the proof of Thm. \ref{lb301}, the only way to make sense of \eqref{eq99} is as follows:
\begin{itemize}
\item One views $\Psi(g)$ as the Cauchy-equivalence class of the Cauchy sequence $(s_n)_{n\in\Zbb_+}$ in $C([0,2\pi],\Cbb)$ under the $L^2$-norm, where $s_n(x)=\sum_{k=-n}^n g(k)e^{\im kx}$.
\end{itemize}
But how can we understand $\Psi(g)$ as an actual function on $[0,2\pi]$ solving the eigenvalue problem $T\Psi(g)=\lambda\Psi(g)$? 


Therefore, \eqref{eq98} and \eqref{eq99} are the very first example of abstract completion, and also one of the most important examples motivating Hausdorff's study of completion in general. %The completion \eqref{eq98} is abstract, in the sense that an element $g$ in the completion $l^2(\Zbb,\Cbb)$ of $C([0,2\pi],\Cbb)$ gives an ``abstract function" $\Psi(g)$ as described by \eqref{eq99}. $\Psi(g)$ is abstract, because it is not necessarily continuous or even Riemann integrable. 


\subsubsection{Riesz-Fischer theorem}


Hilbert established these results by  1906, the same year Fr\'echet defined metric spaces. The story was finished by Riesz and Fischer, who proved in 1907 that $\Psi(g)$ can actually be represented by a \textbf{Lebesgue measurable} $f:[0,2\pi]\rightarrow\Cbb$ satisfying $\lVert f\lVert_{L^2}<+\infty$, where the norm $\lVert f\lVert_{L^2}$ is defined by \eqref{eq100} using Lebesgue integral instead of Riemann integral. Let $L^2([0,2\pi],\Cbb)$ denote the space of all such functions, and view $C([0,2\pi],\Cbb)$ as its subspace. Then using the language of Thm. \ref{lb301}, we have a commutative diagram
\begin{equation}\label{eq101}
\begin{tikzcd}[column sep=tiny]
                     & C([0,2\pi],\Cbb) \arrow[ld,"\Phi"'] \arrow[rd,hook,"\iota"] &   \\
l^2(\Zbb,\Cbb) \arrow[rr,"\simeq"',"\Psi"] &                         & L^2([0,2\pi],\Cbb)
\end{tikzcd}
\end{equation}
where $\iota$ is the inclusion map. Thus:
\begin{itemize}
\item The abstract completion $\Phi:C([0,2\pi],\Cbb)\rightarrow l^2(\Zbb,\Cbb)$ is equivalent to the concrete completion $C([0,2\pi],\Cbb)\subset L^2([0,2\pi],\Cbb)$, where ``concrete" means that it is function-theoretic.
\end{itemize}
This equivalence of abstract and concrete completions, connecting Hilbert's algebraic  approach to integral equations and Lebesgue's function-theoretic approach to measure theory, is the original form of \textbf{Riesz-Fischer theorem}  \index{00@Riesz-Fischer theorem} (proved in 1907),\footnote{More precisely, the original form of Riesz-Fischer theorem says that the $\Psi$ in \eqref{eq101} (whose explicit description is as in \eqref{eq99}) is an isometric isomorphism. As a consequence, the space $L^2$ is complete, because it is fairly easy to show that $l^2$ is complete. However, most modern analysis textbooks present Riesz-Fischer theorem in the following simple  form: ``$L^2$ is a complete metric space".} one of the most significant theorems in early 20th century.


Thanks to Riesz-Fischer theorem, people were convinced that abstract completions of function spaces could deepen one's understanding of analysis in much the same way that Lebesgue's measure theory broadened one's understanding of functions by harmonizing with Hilbert's $l^2$ theory. So what is the hesitation in trying to prove an abstract theorem like Thm. \ref{lb301}?

Hausdorff proved the existence and uniqueness of completion in general, but it was Riesz-Fischer theorem that proved the value of abstract completion.



\subsection{Completion of normed vector spaces}

Fix a normed vector space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$.

\begin{df}
A \textbf{completion of the normed vector space $V$} \index{00@Completion of normed vector spaces} is a \textit{linear} isomertry $\varphi:V\rightarrow\wht{V}$ such that $\wht{V}$ is a Banach space, and $\varphi(V)$ is dense in $\wht V$.
\end{df}

Thus, if $\varphi:V\rightarrow\wht V$ is a completion, then $V$ is isomorphic to $\varphi(V)$ as normed vector spaces, and hence can be viewed as equivalently a dense normed subspace of $\wht V$.



\begin{thm}\label{lb312}
Every normed vector space $V$ has a completion $\varphi:V\rightarrow\wht V$. Morevoer, every completion $\psi:V\rightarrow\wtd V$ is \textbf{isomorphic} to $\varphi$ in the sense that there is an isomorphism of Banach spaces (cf. Def. \ref{lb302}) $\Phi:\wht V\rightarrow\wtd V$ such that the following diagram commutes:
\begin{equation}
\begin{tikzcd}[column sep=small]
                     & V \arrow[ld,"\varphi"'] \arrow[rd,"\psi"] &   \\
\wht V \arrow[rr, "\Phi","\simeq"'] &                         & \wtd V
\end{tikzcd}
\end{equation}
\end{thm}


\begin{proof}[\textbf{Proof of existence}]
By Thm. \ref{lb301}, we have a completion $\varphi:V\rightarrow\wht V$ in the context of metric spaces. So $\wht V$ is a completion of metric space with metric $d_{\wht V}$, and $\varphi$ is an isometry of metric spaces with dense range. We need to show that $\wht V$ is a complete normed vector space, and that $\varphi$ is linear with dense range. Indeed, we shall identify $V$ with $\varphi(\wht V)$ via $\varphi$ so that $V$ is the metric subspace of $\wht V$. We shall extend the structure of normed vector space from $V$ to $\wht V$. In the following, the convergence in $\wht V$ and the continuity of the maps about $\wht V$ are understood using the metric $d_{\wht V}$. 

The map
\begin{gather*}
+:V\times V\rightarrow V\qquad (u,v)\mapsto u+v
\end{gather*}
is Lipschitz continuous, and hence uniformly continuous. Therefore, by Cor. \ref{lb298}, it can be extended (uniquely) to a continuous map $+:\wht V\times\wht V\rightarrow\wht V$. Similarly, if $\lambda\in\Fbb$, the Lipschitz continuous map
\begin{gather*}
V\rightarrow V\qquad v\mapsto \lambda\cdot v
\end{gather*}
can be extended to a continuous map $\wht V\rightarrow\wht V$. Thus, we have defined the addition $+$ and the scalar multiplication $\cdot$ for $\wht V$. Similarly, the map
\begin{align*}
\lVert \cdot\lVert:V\rightarrow\Rbb_{\geq0}  \qquad v\mapsto\lVert v\lVert
\end{align*}
is uniformly continuous and hence can be extended to a map $\wht V\rightarrow\Rbb_{\geq0}$.


We want to show that the above addition, scalar multiplication, and norm function make $\wht V$ a normed vector space.  For example, suppose that $\lambda\in\Fbb$. We want to prove that $\lambda(u+v)=\lambda u+\lambda v$ for all $u,v\in\wht V$, and we know that this is true when $u,v\in V$. Indeed, since $V\times V$ is dense in $\wht V\times\wht V$, and since the following two continuous maps
\begin{gather*}
(u,v)\in\wht V\times\wht V\mapsto \lambda(u+v)\in\wht V\\
(u,v)\in\wht V\times\wht V\mapsto \lambda u+\lambda v\in\wht V
\end{gather*}
are equal on $V\times V$, these two maps are the same by Prop. \ref{lb196}. The same argument proves that $\wht V$ is a vector space.


Since the following continuous map
\begin{gather*}
\varphi:(u,v)\in\wht V\times\wht V\mapsto \lVert u\lVert+\lVert v\lVert-\lVert u+v\lVert\in\Rbb
\end{gather*}
satisfies $\varphi(V\times V)\subset\Rbb_{\geq0}$, by $\varphi(\wht V\times \wht V)\subset\ovl{\varphi(V\times V)}$ (due to Prop. \ref{lb196} again), we conclude that $\varphi(\wht V\times\wht V)\subset \Rbb_{\geq0}$. So $\lVert u+v\lVert\leq \lVert u\lVert+\lVert v\lVert$ for all $u,v\in\wht V$. A similar argument shows $\lVert\lambda v\lVert=|\lambda|\cdot\lVert v\lVert$. Finally, suppose that $v\in\wht V$ satisfies $\lVert v\lVert =0$. Pick a sequence $(u_n)$ in $V$ satisfying $d_{\wht V}(u_n,v)\rightarrow 0$. Since $\lVert\cdot\lVert:\wht V\rightarrow\Rbb$ is continuous, 
\begin{align*}
\lim_{n\rightarrow\infty}d_V(u_n,0)=\lim_{n\rightarrow\infty}\lVert u_{n}\lVert=\big\lVert \lim_{n\rightarrow\infty}u_n\big\lVert=\lVert v\lVert=0
\end{align*}
Therefore $(u_n)$ converges to $0$ in $V$. This proves $v=0$. So $\wht V$ is normed.

Since the metric on $V$ is induced by the norm of $V$, the map
\begin{gather*}
\wht V\times\wht V\rightarrow\Rbb\qquad (u,v)\mapsto d_{\wht V}(u,v)-\lVert u-v\lVert
\end{gather*}
is zero on the dense subset $V\times V$ of $\wht V\times\wht V$. Since this map is continuous, it is constantly zero. So the complete metric $d_{\wht V}$ on $\wht V$ (arising from the metric-space-completion of $V$) equals the metric induced by the norm of $\wht V$. So latter is complete. Therefore, $\wht V$ is a Banach space. Since $V$ is dense in $\wht V$ under $d_{\wht V}$, $V$ is dense in $\wht V$ under the norm of $\wht V$.
\end{proof}


\begin{proof}[\textbf{Proof of uniqueness}]
By Thm. \ref{lb301}, there is a unique isometric isomorphism of metric spaces $\Phi:\wht V\rightarrow\wtd V$ such that $\psi=\Phi\circ\varphi$. Since $\varphi$ and $\psi$ are linear injections, $\Phi$ is a linear isomorphism when restricted to $\varphi(V)\rightarrow\psi(V)$. Since $\Phi$ is continuous and $\varphi(V)$ is dense in $\wht V$, we conclude that $\Phi$ is linear thanks to the following property.
\end{proof}


\begin{pp}\label{lb315}
Let $T:V\rightarrow W$ be a continuous map of normed vector spaces. Assume that $V_0$ is a dense linear subspace of $V$. Assume that $T|_{V_0}:V_0\rightarrow W$ is linear. Then $T$ is linear.
\end{pp}


\begin{proof}
This is same as the proof of the existence part of Thm. \ref{lb312}. Choose any $\alpha,\beta\in\Fbb$. Then the following continuous map
\begin{gather*}
(u,v)\in V\times V\mapsto T(\alpha u+\beta v)-(\alpha T(u)+\beta T(v))
\end{gather*}
is zero on the dense subset $V_0\times V_0$. So it is zero on $V\times V$.
\end{proof}


\begin{exe}
Let $V$ and $W$ be normed vector spaces over $\Fbb$, where $\Fbb=\Rbb$ (resp. $\Fbb=\Cbb$). Let $T:V\rightarrow W$ be a continuous map. Assume that $V_0$ is a dense $\Kbb$-linear subspace of $V$, where $\Kbb=\Qbb$ (resp. $\Kbb=\Qbb+\im\Qbb$). Assume that the restriction $T|_{V_0}:V_0\rightarrow W$ is $\Kbb$-linear. Prove that $T:V\rightarrow W$ is $\Fbb$-linear.
\end{exe}


\begin{rem}
So far in this course, we have proved a lot of results about functions whose codomains are normed vector spaces. Some results assume that these spaces are Banach (i.e. complete), some do not. Thanks to Thm. \ref{lb312}, we can assume that all these results are stated only for Banach spaces, and then check whether they also hold for normed vector spaces in general (which is not difficult). This will make us easier to remember theorems.

For example, suppose that we know that Thm. \ref{lb279} holds only for Banach spaces: Namely, suppose we know that for any Banach space $V$ and topological space $X$, if $(f_\alpha)$ is a net in $C(X,V)$ converging uniformly to $f:X\rightarrow V$, then $f$ is continuous. Then we know that this result also holds when $V$ is a normed vector space. To see this, consider the completion $V\subset\wht V$. Then $(f_\alpha)$ is a net of continuous functions $X\rightarrow\wht V$ converging uniformly to some $f:X\rightarrow \wht V$ (satisfying $f(X)\subset V$). Then $f:X\rightarrow\wht V$ is continuous. Hence $f:X\rightarrow V$ is continuous.

Consider Prop. \ref{lb288} as another example. It tells us that if $V$ is a Banach space and $(f_\alpha)$ is a net in $C(X,V)$ converging uniformly on a dense subset $E\subset X$, then $(f_\alpha)$ converges uniformly on $X$. Now assume that $V$ is only a normed vector space, and take completion $V\subset\wht V$. Then $(f_\alpha)$ is a net in $C(X,\wht V)$ converges uniformly on $E$ to a function $E\rightarrow V$. Thus, by Prop. \ref{lb288}, it converges uniformly to a function $f:X\rightarrow\wht V$. Although we know $f(E)\subset V$ by assumption, we do not know whether $f(X)\subset V$. So we cannot prove the normed vector space version of Prop. \ref{lb288}.  \hfill\qedsymbol 
\end{rem}



%% Proofread



\subsection{Bounded linear maps}

In this section, we consider normed vector spaces over a given field $\Fbb\in\{\Rbb,\Cbb\}$.

We have seen that uniform continuity is crucial to the construction of addition and scalar multiplication in Thm. \ref{lb312}. Indeed, uniform continuity is ubiquitous in the world of normed vector spaces:



\begin{pp}\label{lb313}
Let $T:V\rightarrow W$ be a linear map of normed vector spaces. Then the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $T$ is Lipschitz continuous.
\item $T$ is continuous.
\item $T$ is continuous at $0$.
\item $T$ is bounded on $\ovl B_V(0,1)=\{v\in V:\lVert v\lVert\leq 1\}$, namely, 
\begin{align}
\lVert T\lVert\xlongequal{\mathrm{def}} \sup_{v\in\ovl B_V(0,1)}\lVert Tv\lVert  \label{eq120}
\end{align}
is a finite number. We call $\lVert T\lVert$ the \textbf{operator norm} of $T$. \index{00@Operator norm}
\item There is $C\in\Rbb_{\geq0}$ such that
\begin{align}
\lVert Tv\lVert\leq C\lVert v\lVert\qquad (\forall v\in V)  \label{eq105}
\end{align}
\end{enumerate}
\end{pp}

\begin{proof}
Clearly (1)$\Rightarrow$(2) and (2)$\Rightarrow$(3). Suppose (3) is true. Note that $T0=0$. So there is $\delta>0$ such that $Tv\in B_W(0,1)$ for all $v\in B_V(0,\delta)$. Namely, for all $v\in V$ satisfying $\lVert v\lVert\leq \delta$ we have $\lVert Tv\lVert\leq 1$. Thus, if $\lVert v\lVert\leq 1$, then $\lVert \delta v\lVert\leq\delta$. So
\begin{align*}
\lVert Tv\lVert=\delta^{-1}\lVert T(\delta v)\lVert\leq \delta^{-1}
\end{align*}
This proves $\lVert T\lVert\leq\delta^{-1}$. So (4) is proved.

Assume (4). Let $C=\lVert T\lVert$. To prove \eqref{eq105}, it suffices to consider the case that $v\neq 0$. Then $\lVert v\lVert\neq 0$. Let $\lambda=\lVert v\lVert$. Then $\lambda^{-1}v\in \ovl B_V(0,1)$, and hence
\begin{align*}
\lVert Tv\lVert= \lambda\cdot \lVert T(\lambda^{-1} v)\lVert \lambda C=C\lVert v\lVert
\end{align*}
This proves (5) with $C=\lVert T\lVert$.

Finally, assume (5). Then for each $u,v\in V$ we have
\begin{align*}
\lVert T(u)-T(v)\lVert=\lVert T(u-v)\lVert\leq C\lVert u-v\lVert
\end{align*}
This proves (1).
\end{proof}


\begin{rem}
In the above proof, we have shown the useful fact that for any linear map of normed vector spaces $T:V\rightarrow W$, we have that
\begin{align}
\lVert Tv\lVert\leq \lVert T\lVert\cdot\lVert v\lVert\qquad (\forall v\in V) \label{eq118}
\end{align}
and that
\begin{align}
\lVert T\lVert<+\infty\qquad\Longrightarrow\qquad T\text{ has Lipschitz constant }\lVert T\lVert
\end{align}
\end{rem}



Due to Prop. \ref{lb313}-(4), we make the following definition:
\begin{df}
Let $V,W$ be normed vector spaces over $\Fbb$. We call $T:V\rightarrow W$ to be a \textbf{bounded linear map} \index{00@Bounded linear map} if $T$ is a continuous linear map. We write \index{LVW@$\fk L(V,W),\fk L(V)$}
\begin{align}
\fk L(V,W)=\{\text{bounded linear maps }V\rightarrow W\}\qquad \fk L(V)=\fk L(V,V)
\end{align}
\end{df}

Thus, the word ``bounded" means that the linear map $T$ is bounded on $\ovl B_V(0,1)$, but not that $T$ is bounded on $V$.

\begin{pp}\label{lb314}
$\fk L(V,W)$ is a linear subspace of $W^V$, and the operator norm $\lVert \cdot\lVert$ is a norm on $\fk L(V,W)$.
\end{pp}

\begin{proof}
Exercise.
\end{proof}


Since Lipschitz continuous functions are uniformly continuous, we have:
\begin{pp}
Let $V_0$ be a dense linear subspace of a normed vector space $V$. Let $W$ be a Banach space. Let $T_0:V_0\rightarrow W$ be a bounded linear map. Then there is a unique bounded linear map $T:V\rightarrow W$ such that $T|_{V_0}=T_0$.
\end{pp}

\begin{proof}
Uniqueness is clear from the density of $V_0$. By Prop. \ref{lb313}, $T_0$ is uniformly continuous. Therefore, by Cor. \ref{lb298}, $T_0$ can be extended to a continuous map $T:V\rightarrow W$, which is linear by Prop. \ref{lb315}.
\end{proof}











\subsection{Problems and supplementary material}

\begin{prob}
Give a direct proof of Thm. \ref{lb294} using open covers instead of using subsequences. Do not use Lebesgue numbers.
\end{prob}


The following Pb. \ref{lb296} gives another proof that sequentially compact metric spaces are compact. Therefore, do not use this fact in your solution of Pb. \ref{lb296}.

\begin{sprob}\label{lb296}
Let $X$ be a sequentially compact metric space. Let $\fk U\subset 2^X$ be an open cover of $X$.
\begin{enumerate}
\item Prove that $\fk U$ has a Lebesgue number. Namely, prove that there exists $\delta>0$ such that for every $x\in X$ there is $U\in\fk U$ satisfying $B_X(x,\delta)\subset U$.
\item In our proof that $X$ is separable (cf. Thm. \ref{lb252}), we showed that for every $\delta>0$ there exists a finite set $E\subset X$ such that $d(x,E)<\delta$ for all $x\in X$. Use this fact and Part 1 to prove that $\fk U$ has a finite subcover.
\end{enumerate}
\end{sprob}



\begin{prob}
Prove Prop. \ref{lb314}.
\end{prob}


\begin{df}
Two norms $\lVert\cdot\lVert_1$ and $\lVert\cdot\lVert_1$ on a vector space $\Vbb$ over $\Rbb$ or $\Cbb$ are called \textbf{equivalent} \index{00@Equivalent norms} if there exist $\alpha,\beta>0$ such that for all $v\in V$ we have
\begin{align*}
\lVert v\lVert_1\leq \alpha \lVert v\lVert_2\qquad \lVert v\lVert_2\leq \beta \lVert v\lVert_1
\end{align*}
Clearly, two equivalent norms induce equivalent metrics, and hence induce the same topology.
\end{df}


\begin{prob}
Let $\Fbb\in\{\Rbb,\Cbb\}$. Let $\lVert\cdot\lVert$ be the Euclidean norm on $\Fbb^n$. Let $\nu:\Fbb^n\rightarrow\Rbb_{\geq 0}$ be a norm on $\Fbb^n$.
\begin{enumerate}
\item Prove that there exists $\alpha>0$ such that $\nu(\mbf x)\leq \alpha\lVert\mbf x\lVert$ for all $\mbf x\in\Fbb^n$. In particular, show that $\nu$ is continuous (under the Euclidean topology).
\item Let $\beta=\inf\{\nu(\mbf x):\mbf x\in\Fbb^n,\lVert \mbf x\lVert=1\}$. Prove that $\beta>0$. Prove that $\lVert \mbf x\lVert\leq\beta^{-1}\cdot\nu(\mbf x)$ for all $\mbf x\in\Fbb^n$.
\end{enumerate}
\end{prob}


The above problem proves

\begin{thm}\label{lb363}
Let $\Fbb\in\{\Rbb,\Cbb\}$. Then any norm on $\Fbb^n$ is equivalent to the Euclidean norm. In particular, the operator norm on $\Fbb^{m\times n}$ (if we view an $m\times n$ matrix as an element of $\fk L(\Fbb^n,\Fbb^m)$) is equivalent to the Euclidean norm.
\end{thm}













\newpage







\section{Derivatives}

\subsection{Basic properties of derivatives}


Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Let $\Omega$ be a nonempty open subset of $\Cbb$. The following assumption will be often considered:
\begin{ass}\label{lb317}
$[a,b]$ satisfies $-\infty<a<b<+\infty$. 
\end{ass}



\begin{df}
Assume Asm. \ref{lb317}. Let $f:[a,b]\rightarrow V$  be a map with variable $t$, and let $p\in[a,b]$. The \textbf{derivative} \index{00@Derivative} of $f$ at $p$ is
\begin{align*}
f'(x)\equiv \frac{df}{dt}(p)\xlongequal{\mathrm{def}}\lim_{
\begin{subarray}{c}
t\in[a,b]\setminus\{x\}\\
t\rightarrow x
\end{subarray}
}
\frac{f(t)-f(x)}{t-x}
\end{align*}
provided that the RHS exists. In other words (cf. Def. \ref{lb197}),
\begin{itemize}
\item $f'(x)$ converges to $v\in V$ iff for every $\eps>0$ there exists $\delta>0$ such that for every $t\in[a,b]$ satisfying $0<|t-x|<\delta$ we have
\begin{align*}
\Big\lVert \frac{f(t)-f(x)}{t-x}-v  \Big\lVert<\eps
\end{align*}
\end{itemize}
To simplify subscripts, we write
\begin{align}\label{eq107}
f'(x)=\left\{
\begin{array}{ll}
\dps\lim_{t\rightarrow x}\frac{f(t)-f(x)}{t-x}&\text{ if }a<x<b\\[2ex]
\dps\lim_{t\rightarrow x^+}\frac{f(t)-f(x)}{t-x}&\text{ if }x=a\\[2ex]
\dps\lim_{t\rightarrow x^-}\frac{f(t)-f(x)}{t-x}&\text{ if }x=b
\end{array}
\right.
\end{align}
for the obvious reason. (Recall Def. \ref{lb200}.)

If $f'(x)$ exists for some $x$, we say that $f$ is \textbf{differentiable} \index{00@Differentiable} at $x$. If $f'(x)$ exists for  every $x\in [a,b]$, we say that $f$ is a \textbf{differentiable function} and view $f'$ as a function $[a,b]\rightarrow V$. 

Derivatives on intervals $[a,b),(a,b],(a,b)$ are understood in a similar way. \hfill\qedsymbol
\end{df}






\begin{rem}
Note that in the three cases of \eqref{eq107}, $f'(x)$ can also equals
\begin{align*}
\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}h\qquad \lim_{h\rightarrow 0^+}\frac{f(x+h)-f(x)}h\qquad \lim_{h\rightarrow 0^-}\frac{f(x+h)-f(x)}h
\end{align*}
\end{rem}



\begin{df}
Let $E$ be a subset of $\Rbb^n$. Let $f:E\rightarrow V$ be a function with variables $t_1,\dots,t_n$, and $\mbf x=(x_1,\dots,x_n)\in E$. Let $1\leq i\leq n$. Suppose that there are $a,b$ satisfying Asm. \ref{lb317} such that $(x_1,\dots,x_{i-1},t,x_{i+1},\dots,x_n)$ belongs to $E$ for all $t\in[a,b]$. The derivative of the function $t_i\mapsto f((x_1,\dots,x_{i-1},t_i,x_{i+1},\dots,x_n)$ at $t=x_i$ is denoted by
\begin{align*}
\frac{\partial f}{\partial t_i}(\mbf x)\equiv\partial_i f(\mbf x)
\end{align*} 
and called the \textbf{partial derivative} \index{00@Partial derivative} of $f$ at $\mbf x$ with respect to the variable $t_i$.
\end{df}



\begin{df}
If $V$ is over $\Cbb$, and if $f:\Omega\rightarrow V$ and $z\in\Omega$, we define the \textbf{derivative} of $f$ at $z$ to be
\begin{align*}
f'(z)=\lim_{
\begin{subarray}{c}
w\in\Omega\setminus\{z\}\\
w\rightarrow z
\end{subarray}
}\frac{f(w)-f(z)}{w-z}
\end{align*}
provided that the RHS exists, and simply write it as
\begin{align*}
\lim_{w\rightarrow z}\frac{f(w)-f(z)}{w-z}=\lim_{\zeta\rightarrow0}\frac{f(z+\zeta)-f(z)}\zeta
\end{align*}
\end{df}

\begin{cv}\label{lb332}
Unless otherwise stated, when talking about derivatives of a function defined on an interval $I$, we always assume that $I$ is inside $\Rbb$ and has at least two points.  When talking about derivatives of a function $f:\Omega\rightarrow V$, we always assume that $V$ is over $\Cbb$.
\end{cv}



\begin{df}
Given a function $f:E\rightarrow W$ where $E$ is an interval in $\Rbb$ with at least two points or $E=\Omega$, if $n\in\Nbb$ and $x\in E$, we define the \pmb{$n$}\textbf{-th derivative} $f^{(n)}(x)$ \index{fn@$f^{(n)}$}  inductively by $f^{(0)}=f$ and $f^{(n)}(x)=(f^{(n-1)})'(x)$ if $f^{(n-1)}$ exists on some neighborhood of $x$ with respect to $E$. $f'',f''',f'''',\dots$ mean $f^{(2)},f^{(3)},f^{(4)},\dots$.

The $n$-th partial derivative on the $i$-th variable is written as $\partial_i^nf$. \hfill\qedsymbol
\end{df}


It is desirable to use sequences or nets to study derivatives. For that purpose, the following lemma is useful:

\begin{lm}\label{lb322}
Let $E$ be  an interval in $\Rbb$ with at least two elements, or let $E=\Omega$. Let $z\in E$. Let $f:E\rightarrow V$. Let $v\in V$. 
The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item We have $f'(z)=v$.
\item For any sequence $(z_n)_{n\in\Zbb_+}$ in $E\setminus\{z\}$ converging to $z$, we have $\dps\lim_{n\rightarrow\infty} \frac{f(z_n)-f(z)}{z_n-z}=v$.
\item For any net $(z_\alpha)_{\alpha\in\mc I}$ in $E$ converging to $z$, we have $\dps\lim_{\alpha\in\mc I}\varphi(z_\alpha)=v$, where $\varphi:E\rightarrow V$ is defined by
\begin{align}\label{eq110}
\varphi(w)=\left\{
\begin{array}{ll}
\dps\frac{f(w)-f(z)}{w-z}&\text{ if }w\neq z\\[2ex]
v&\text{ if }w=z
\end{array}
\right.
\end{align}
\end{enumerate}
\end{lm}

\begin{proof}
The equivalence of (1)$\Leftrightarrow$(2) is due to Def. \ref{lb197}-(3m). Also, by Def. \ref{lb197}-(1), that $f'(z)=v$ is equivalent to that the function $\varphi$ in \eqref{eq110} is continuous at $z$. So it is equivalent (3) by Def. \ref{lb188}-(1).
\end{proof}






\begin{pp}\label{lb319}
If $f:[a,b]\rightarrow V$ or $f:\Omega\rightarrow V$ is differentiable at $x$, then $f$ is continuous at $x$.
\end{pp}


\begin{proof}
We consider the case $f:\Omega\rightarrow V$; the other case is similar. Choose any sequence $(z_n)$ in $\Omega\setminus\{z\}$ converging to $z$. By Lem. \ref{lb322}, we have $\lim_n \frac{f(z_n)-f(z)}{z_n-z}=v$. Since $\lim_n(z_n-z)=0$, by the continuity of scalar multiplication (Prop. \ref{lb82}),  we have
\begin{align*}
\lim_{n\rightarrow\infty}f(z_n)-f(z)=0\cdot v=0
\end{align*}
Thus, by Def. \ref{lb197}-(3m), we obtain $\lim_{w\neq z,w\rightarrow z}f(w)=f(z)$, which means by Def. \ref{lb197}-(1) that $f$ is continuous at $z$.
\end{proof}


\begin{pp}\label{lb320}
Suppose that $f,g$ are functions from $[a,b]$ (resp. $\Omega$) to $V$ such that $f'(x)$ and $g'(x)$ exist. Then $(f+g)'(x)$ exists, and
\begin{subequations}
\begin{align}
(f+g)'(x)=f'(x)+g'(x)
\end{align}
If $\lambda$ is a function from $[a,b]$ resp. $\Omega$ to $\Fbb$ such that $\lambda'(x)$ exists, then $(\lambda f)'(x)$ exists and satisfies the \textbf{Leibniz rule} \index{00@Leibniz rule}
\begin{align}
(\lambda f)'(x)=\lambda'(x)f(x)+\lambda(x)f'(x)  \label{eq111}
\end{align}
Assume moreover that $\lambda$ does not have value $0$. Then
\begin{align}
\Big(\frac 1\lambda\cdot f\Big)'(x)=\frac{-\lambda'(x)f(x)+\lambda(x)f'(x)}{\lambda(x)^2}
\end{align}
\end{subequations}
\end{pp}


\begin{proof}
The first formula is easy. To compute the second one, we choose any sequence $(x_n)$ in $[a,b]\setminus \{x\}$ or $\Omega\setminus\{x\}$ converging to $x$. Then
\begin{align*}
\frac{\lambda(x_n)f(x_n)-\lambda(x)f(x)}{x_n-x}=\frac{(\lambda(x_n)-\lambda(x))}{x_n-x}\cdot f(x_n)+\lambda(x)\frac{(f(x_n)-f(x))}{x_n-x}
\end{align*}
which, by Lem. \ref{lb322} and Prop. \ref{lb82} and the continuity of $f$ at $x$ (Prop. \ref{lb319}), converges to the RHS of \eqref{eq111}. This proves \eqref{eq111}, thanks to Lem. \ref{lb322}.

The third formula will follow from the second one if we can prove that $1/\lambda$ has derivative $-\frac{\lambda'(x)}{\lambda(x)^2}$ at $x$. This is not hard: Choose any sequence $x_n\rightarrow x$ but $x_n\neq x$. Then
\begin{align*}
\Big(\frac 1{\lambda(x_n)}-\frac 1{\lambda(x)}\Big)\Big/ (x_n-x)=-\frac{\lambda(x_n)-\lambda(x)}{x_n-x}\cdot \frac 1{\lambda(x_n)\lambda(x)}
\end{align*}
converges to $-\lambda'(x)\cdot \frac 1{\lambda(x)^2}$ as $n\rightarrow \infty$. Here, we have used the continuity of $\lambda$ at $x$ and Prop. \ref{lb82} again.
\end{proof}

\begin{eg}
The derivative of a constant function is $0$. Thus, by Leibniz rule, if $\lambda$ is a scalar, and if $f'(z)$ exists, then $(\lambda f)'(z)=\lambda\cdot f'(z)$.
\end{eg}

\begin{eg}\label{lb323}
The identity map $f:z\in\Cbb\mapsto z\in\Cbb$ has derivative $\lim_{w\rightarrow z}\frac{w-z}{w-z}=1$. Thus, by induction and Prop. \ref{lb320}, we have $(z^n)'=nz^{n-1}$ if $n\in\Zbb_+$. If $-n\in\Zbb_+$, then when $z\neq 0$ we have
\begin{align*}
(z^n)'=(1/z^{-1})'=-\frac{(z^{-n})'}{z^{-2n}}=-\frac{-nz^{-n-1}}{z^{-2n}}=nz^{n-1}
\end{align*}
We conclude that $(z^n)'=nz^{n-1}$ whenever $n\in\Nbb$, or whenever $n=-1,-2,\dots$ and $z\neq0$. The same conclusion holds for the real variable function $x^n$.
\end{eg}


\begin{eg}
Let $f:z\in\Cbb\mapsto\ovl z\in\Cbb$. We claim that for every $z\in\Cbb$, the limit
\begin{align}
f'(z)=\lim_{w\rightarrow z}\frac{\ovl w-\ovl z}{w-z}=\lim_{h\rightarrow 0}\ovl h/h
\end{align}
does not exist with the help of Rem. \ref{lb202}: Take $h_n=1/n$. Then $h_n\rightarrow 0$ and $\ovl {h_n}/h_n=1\rightarrow 1$ as $n\rightarrow\infty$. Take $h_n=\im/n$. Then $h_n\rightarrow 0$ and $\ovl {h_n}/h_n=-\im/\im=-1\rightarrow-1$ as $n\rightarrow\infty$. So $f'(z)$ does not exist.
\end{eg}



\begin{thm}[\textbf{Chain rule}] \index{00@Chain rule}\label{lb331}
Let $\Omega,\Gamma$ be nonempty open subsets of $\Cbb$. Assume that $f:\Omega\rightarrow\Gamma$ is differentiable at $z\in\Omega$, and that $g:\Gamma\rightarrow V$ is differentiable at $f(z)$. Then $g\circ f$ is differentiable at $z$, and
\begin{align}
(g\circ f)'(z)=g'(f(z))\cdot f'(z)\label{eq125}
\end{align}
The same conclusion holds if $\Omega$ is replaced by an interval in $\Rbb$, or if both $\Omega$ and $\Gamma$ are replaced by intervals in $\Rbb$.
\end{thm}

Recall Conv. \ref{lb332} for the assumption on the field $\Fbb$.


\begin{comment}
The most natural idea is to  compute the limit of
\begin{align*}
\frac{g\circ f(w)-g\circ f(z)}{w-z}=\frac{f(w)-f(z)}{w-z}\cdot\frac{g\circ f(w)-g\circ f(z)}{f(w)-f(z)}
\end{align*}
as $w\rightarrow z$ (understood as limits of nets as indicated in Rem. \ref{lb318}). However, the RHS of this expression does not make sense when $f(w)=f(z)$, and this bad situation may happen for many $w\in\Omega\setminus\{z\}$. We overcome this difficulty by changing the language of the proof slightly, without changing the key idea too much.
\end{comment}



\begin{proof}
Define a function $B:\Gamma\rightarrow\Cbb$ by
\begin{gather*}
A(\zeta)=\left\{
\begin{array}{ll}
\dps\frac{g(\zeta)-g\circ f(z)}{\zeta-f(z)}&\text{ if }\zeta\neq f(z)\\[2ex]
\dps g'(f(z))&\text{ if }\zeta=f(z)
\end{array}
\right.
\end{gather*}
Choose any sequence $(z_n)$ in $\Omega\setminus\{z\}$ converging to $z$. Then
\begin{align*}
\frac{g\circ f(z_n)-g\circ f(z)}{z_n-z}=A(f(z_n))\cdot\frac{f(z_n)-f(z)}{z_n-z}
\end{align*}
By Prop. \ref{lb319}, $f$ is continuous at $z$. So $\lim_n f(z_n)=f(z)$. Thus, by Lem. \ref{lb322}-(3), the above expression converges to $g'(f(z))f'(z)$ as $n\rightarrow\infty$.
\end{proof}



\begin{pp}\label{lb337}
Let $\Omega,\Gamma$ be nonempty open subsets of $\Cbb$. Let $f:\Omega\rightarrow\Gamma$ be a bijection. Let $z\in\Omega$. Suppose that $f'(z)$ exists and $f'(z)\neq 0$. Suppose also that $f^{-1}:\Gamma\rightarrow\Omega$ is continuous at $f(z)$. Then $f^{-1}$ is differentiable at $f(z)$, and
\begin{align}
(f^{-1})'(f(z))=\frac 1{f'(z)}
\end{align}
The same conclusion holds if $\Omega$ and $\Gamma$ are replaced by intervals of $\Rbb$.
\end{pp}


\begin{proof}
Choose any sequence $(w_n)$ in $\Gamma\setminus\{f(z)\}$ converging to $f(z)$. Then, as $n\rightarrow\infty$, we have $f^{-1}(w_n)\rightarrow f^{-1}(f(z))=z$ since $f^{-1}$ is continuous at $f(z)$, and hence
\begin{align}
\frac{f^{-1}(w_n)-f^{-1}(f(z))}{w_n-f(z)}=\frac{f^{-1}(w_n)-z}{f(f^{-1}(w_n))-f(z)}
\end{align}
converges to $1/f'(z)$ by Lem. \ref{lb322} and the continuity of the map $\zeta\in\Cbb^{\times}\mapsto 1/\zeta\in\Cbb$. This proves $(f^{-1})'(f(z))=1/f'(z)$, thanks to Lem. \ref{lb322}.
\end{proof}








\begin{comment}

The Leibniz rule \eqref{eq111} can be generalized to higher derivatives. 

\end{comment}


\subsection{Rolle's and Lagrange's mean value theorems (MVT)} \index{00@MVT=mean value theorem}

Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Assume $-\infty<a<b<+\infty$.

\subsubsection{MVT}



\begin{df}
Let $f:X\rightarrow \Rbb$ where $X$ is a topological space. We say that $f$ has a \textbf{local maximum} (resp. \textbf{local minimum}) \index{00@Local maximum/minimum} at $x\in X$, if there exists $U\in\Nbh_X(x)$ such that $f|_U$ attains its maximum (resp. minimum) at $x$. The word ``\textbf{local extremum}" \index{00@Local extremum} refers to either local maximum or local minimum.
\end{df}


You must know that derivatives can be used to find the monotonicity of real-valued real variable functions. Here is the precise statement:

\begin{pp}\label{lb324}
Assume that $f:[a,b]\rightarrow\Rbb$ is differentiable at $x\in[a,b]$, and $f'(x)>0$. Then there exists $\delta>0$ such that for any $y\in (x-\delta,x+\delta)\cap[a,b]$, we have
\begin{gather}
y>x\Rightarrow f(y)>f(x)\qquad y<x\Rightarrow f(y)<f(x)   \label{eq112}
\end{gather}
\end{pp}
We leave it to the readers the find the analogous statement for the case $f'(x)<0$.

\begin{proof}
Let $A=f'(x)>0$. Then there exists $\delta>0$ such that for all $y\in (x-\delta,x+\delta)\cap[a,b]$ not equal to $x$, we have $\dps\Big|\frac{f(y)-f(x)}{y-x}-A \Big|<\frac A2$, and hence $\dps\frac{f(y)-f(x)}{y-x}>\frac A2$. This proves \eqref{eq112}.
\end{proof}


\begin{co}\label{lb325}
Assume that $f:[a,b]\rightarrow\Rbb$ has a local extremum at $x\in (a,b)$. Assume that $f'(x)$ exists. Then $f'(x)=0$.
\end{co}
\begin{proof}
If $f'(x)$ is a non-zero number, then either $f'(x)>0$ or $(-f)'(x)>0$. In either case, Prop. \ref{lb324} indicates that $f$ cannot have a local extremum at $x$.
\end{proof}




From Prop. \ref{lb324}, it is clear that if $f'>0$ on $(a,b)$, then $f$ is strictly increasing. However, to prove that if $f'\geq0$ then $f$ is increasing, we need more preparation.


\begin{lm}[\textbf{Rolle's MVT}] \index{00@Rolle's MVT}
Suppose that $f\in C([a,b],\Rbb)$ is differentiable on $(a,b)$. Suppose moreover that $f(a)=f(b)$. Then there exists $x\in (a,b)$ such that $f'(x)=0$.
\end{lm}


\begin{proof}
If $f$ is constant than $f'=0$. Suppose that $f$ is not constant. Then there is $x\in(a,b)$ at which $f$ attains its maximum (if $f(t)>f(a)$ for some $t\in(a,b)$) or minimum (if $f(t)<f(b)$ for some $t\in(a,b)$). So $f'(x)=0$ by Cor. \ref{lb325}.
\end{proof}


\begin{eg}
Rolle's MVT does not hold for vector-valued functions. Especially, it does hot hold for functions to $\Cbb\simeq\Rbb^2$. Consider $f:[0,2\pi]\rightarrow\Cbb$ defined by $f(t)=e^{\im t}=\cos t+\im\sin t$. We assume that $\sin'=\cos$ and $\cos'=-\sin$ are proved. Then $f(0)=f(2\pi)=1$, whereas $f'(t)=-\sin t+\im\cos t$ is never zero.
\end{eg}





\begin{thm}[\textbf{Lagrange's MVT}] \index{00@Lagrange's MVT}\label{lb345}
Suppose that $f\in C([a,b],\Rbb)$ is differentiable on $(a,b)$. Then there is $x\in(a,b)$ such that
\begin{align}\label{eq113}
f'(x)=\frac{f(b)-f(a)}{b-a}
\end{align}
\end{thm}



\begin{proof}
The case $f(a)=f(b)$ is just Rolle's MVT. When $f(a)\neq f(b)$, we can ``shift the function $f$ vertically" so that its two end points have the same height. Technically, we consider $g(x)=f(x)-kx$ where $k=\frac{f(b)-f(a)}{b-a}$ is the slope of the interval from $(a,f(a))$ to $(b,f(b))$. Then $g(a)=g(b)$. By Rolle's MVT, there is $x\in(a,b)$ such that $g'(x)=0$, i.e. $f'(x)-k=0$.
\end{proof}


\subsubsection{Applications of MVT}








\begin{co}\label{lb330}
Assume that $f\in C([a,b],\Rbb)$ is differentiable on $(a,b)$. Then
\begin{align}
f'\geq0\text{ on }(a,b)\qquad\Longleftrightarrow\qquad f\text{ is increasing on }[a,b] \label{eq114}
\end{align}
Moreover, if $f'>0$ on $(a,b)$, then $f$ is strictly increasing on $[a,b]$.
\end{co}


\begin{proof}
If $f'(x)<0$ for some $x\in (a,b)$, then Prop. \ref{lb324} implies that $x$ has a neighborhood on which $f$ is not increasing. Conversely, suppose that $f'(x)\geq 0$ for all $x\in (a,b)$. Choose $x,y\in[a,b]$ satisfying $a\leq x<y\leq b$. Then by Lagrange's MVT, there is $z\in(x,y)$ such that $f(y)-f(x)=f'(z)(y-x)\geq0$. So $f$ is increasing on $[a,b]$. We have finished proving \eqref{eq114}.

Suppose that $f'>0$ on $(a,b)$. Then by Prop. \ref{lb324}, $f$ is strictly increasing on $(a,b)$. If $a<x<b$, then there is $\eps>0$ such that $f(a+1/n)\leq f(x)-\eps$ for sufficiently large $n$. Since $f$ is continuous at $a$, by taking $\lim_{n\rightarrow\infty}$ we get $f(a)\leq f(x)-\eps$ and hence $f(a)<f(x)$. A similar argument proves $f(b)>f(x)$. So $f$ is strictly increasing on $[a,b]$.
\end{proof}

\begin{co}
Suppose that $f\in C([a,b],\Rbb)$ is differentiable on $(a,b)$, and $f'(x)\neq 0$ for all $x\in(a,b)$. Then $f$ is strictly monotonic (i.e. strictly increasing or strictly decreasing). In particular, by Cor. \ref{lb330}, either $f'\geq0$ on $(a,b)$ or $f'\leq 0$ on $(a,b)$.
\end{co}

\begin{proof}
Rolle's MVT implies $f(b)-f(a)\neq 0$, and similarly, $f(y)-f(x)\neq 0$ whenever $a\leq x<y\leq b$. Therefore $f$ is injective. The strict monotonicity of $f$ follows from the following general fact:
\end{proof}

\begin{lm}\label{lb347}
Let $-\infty\leq a< b\leq +\infty$, and let $f:[a,b]\rightarrow\ovl\Rbb$ be a continuous injective function. Then $f$ is strictly monotonic.
\end{lm}


\begin{proof}
Choose a strictly increasing homeophism  $\varphi:[0,1]\rightarrow[a,b]$. It suffices to prove that $g=f\circ\varphi:[a,b]\rightarrow\ovl\Rbb$ is strictly monotonic. Assume for simplicity that $g(a)\leq g(b)$. Let us show that $g$ is increasing. Then the injectivity implies that $g$ is strictly increasing.

We claim that for every $x\in(0,1)$ we have $g(0)\leq g(x)\leq g(1)$. Suppose the claim is true. Then for every $0< x< y< 1$ we have $g(0)\leq g(y)\leq g(1)$. Applying the claim to the interval $[0,y]$ shows $g(0)\leq g(x)\leq g(y)$. So $f$ is increasing.

Let us prove the claim. Suppose the claim is not true. Then there is $x\in (0,1)$ such that either $g(0)\leq g(1)<g(x)$ or $g(x)< g(0)\leq g(1)$. In the first case, by intermediate value theorem (applied to $f|_{[0,x]}$), there is $p\in [0,x]$ such that $g(p)=g(1)$. So $g$ is not injective since $p<1$. This is impossible. Similarly, the second case is also impossible.
\end{proof}





Besides monotonicity, the uniqueness of antiderivatives is another classical application of MVT.

\begin{df}
An \textbf{antiderivative} \index{00@Antiderivative} of a function $f:E\rightarrow V$ is a differentiable function $g:E\rightarrow V$ satisfying $g'=f$ on $E$.
\end{df}

The famous fact that any two antiderivatives $g_1,g_2$ of $f:[a,b]\rightarrow V$ differ by a constant is immediate from the following fact (applied to $g_1-g_2$):

\begin{co}\label{lb326}
Suppose that $f:[a,b]\rightarrow V$ is differentiable on $(a,b)$ and satisfies $f'=0$ on $(a,b)$. Then $f$ is a constant function.
\end{co}

\begin{proof}[Proof for $V=\Fbb^N$]
Since $\Cbb^N\simeq\Rbb^{2N}$, it suffices to prove the case $V=\Rbb^N$. Since a sequence in $\Rbb^N$ converges to a vector iff each component of the sequence converges to the corresponding component of the vector, it suffices to prove the case $f:[a,b]\rightarrow\Rbb$.  

Choose any $x\in(a,b]$. By Lagrange's MVT, there exists $y\in(a,x)$ such that $\dps 0=f'(y)=\frac{f(x)-f(a)}{x-a}$. This proves $f(x)=f(x)$ for all $x\in(a,b]$.
\end{proof}


Now we discuss the general case. 

\begin{rem}
When $V$ is not necessarily finite-dimensional, the method of reducing Cor. \ref{lb326} to the case of scalar-valued functions is quite subtle: How should we understand the ``components" of an element $v$ in the Banach space $V$? In fact, in the general case, view
\begin{align*}
\bk{\varphi,v}\equiv\bk{v,\varphi}:=\varphi(v)
\end{align*}
as a component of $v$, where $\varphi$ is an element inside the \textbf{dual (Banach) space} \index{00@Dual (Banach) space} of $V$, defined by \index{V@$V^*$}
\begin{align}
V^*=\fk L(V,\Fbb)
\end{align}
An element $\varphi$ in $V^*$ is called a \textbf{bounded linear functional} on $V$. \index{00@Bounded linear functional} (In general, a \textbf{linear functional} \index{00@Linear functional} on a vectors space $W$ over a field $\Kbb$ is simply a linear map $W\rightarrow\Kbb$.)
\end{rem}


The vector space $V^*$, equipped with the operator norm, is indeed a Banach space. (This is why we call $V^*$ the dual \textit{Banach} space.) For the moment we do not need this fact. And we will discuss this topic in a later chapter.



\begin{rem}
In the future, we will prove that $V^*$ separates points of $V$. (Recall Def. \ref{lb327}.) By linearity, this is equivalent to saying that for any $v\in V$ we have
\begin{align}\label{eq115}
v=0\qquad\Longleftrightarrow\qquad\bk{\varphi,v}=0\text{ for all }\varphi\in V^*
\end{align}
In fact, we shall prove the famous \textbf{Hahn-Banach extension theorem}, which implies that if $v\in V$ then
\begin{align}
\lVert v\lVert =\sup_{\varphi\in \ovl B_{V^*}(0,1)} |\bk{\varphi,v}|  \label{eq116}
\end{align}
where $V^*$ is equipped with the operator norm. Clearly \eqref{eq116} implies \eqref{eq115}. By the way,  \eqref{eq116} can be stated in an  equivalent but fancy way, as shown in Rem. \ref{lb328}.
\end{rem}

From \eqref{eq118}, it is clear that for any $v\in V,\varphi\in V^*$ we have
\begin{align}
|\bk{\varphi,v}|\leq\lVert\varphi\lVert\cdot\lVert v\lVert \label{eq119}
\end{align}
Thus, the inequality ``$\geq$" trivially holds in \eqref{eq116}. The ``$\leq$" in \eqref{eq116} is nontrivial. 

\begin{rem}\label{lb328}
If we equip the \textbf{double dual space} \index{00@Double dual space $V^{**}$} $V^{**}=(V^*)^*$ \index{V@$V^{**}=(V^*)^*$} with the operator norm, then for each $\fk v\in V^{**}$, by \eqref{eq120} we have
\begin{align*}
\lVert\fk v\lVert=\sup_{\varphi\in \ovl B_{V^*}(0,1)}|\bk{\fk v,\varphi}|
\end{align*}
From this relation, it is clear that \eqref{eq116} holds if and only if the canonical linear map 
\begin{gather}
V\rightarrow V^{**}\qquad v\mapsto \bk{\cdot ,v}  \label{eq117}
\end{gather} 
(where $\bk{\cdot,v}$ sends each $\varphi\in V^*$ to $\bk{\varphi,v}=\varphi(v)$) is an isometry.
\end{rem}






\begin{proof}[\textbf{Proof of Cor. \ref{lb326} assuming Hahn-Banach}]
Let us prove that $f(x)=f(a)$ for any $x\in[a,b]$. Since $V^*$ separates points of $V$, it suffices to choose an arbitrary $\varphi\in V^*$ and prove that $\varphi\circ f(x)=\varphi\circ f(a)$. Indeed,  since $\varphi$ is continuous, for each sequence $(x_n)$ in $[a,b]\setminus\{x\}$ converging to $x$, in view of Lem. \ref{lb322} we have
\begin{align*}
\lim_{n\rightarrow\infty}\frac{\varphi\circ f(x_n)-\varphi\circ f(x)}{x_n-x}=\varphi\Big(\lim_{n\rightarrow\infty}\frac{f(x_n)-f(x)}{x_n-x}\Big)=\varphi(f'(x))=\varphi(0)=0
\end{align*}
if $a<x<b$. Thus $(\varphi\circ f)'=0$ on $(a,b)$. Therefore, by the finite-dimensional version of Cor. \ref{lb326}, we have that $\varphi\circ f$ is constant on $[a,b]$.
\end{proof}




Hahn-Banach theorem is extremely useful for reducing a problem about vector-valued functions to one about scalar-valued functions. In this course, we will also use Hahn-Banach to prove another fun fact: every Banach space over $\Fbb$ is isormorphic to a closed linear subspace of $C(X,\Fbb)$ where $X$ is a compact Hausdorff space. However, in the following section we would like to give an elementary proof of Cor. \ref{lb326} without using Hahn-Banach. 



\subsection{Finite-increment theorem}

Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Assume $-\infty<a<b<+\infty$.

Cor. \ref{lb326} follows immediately from the following theorem by taking $g=0$.


\begin{thm}[]\label{lb329}
Suppose that $f\in C([a,b],V)$ and $g\in C([a,b],\Rbb)$ are differentiable on $(a,b)$. Assume that for every $x\in(a,b)$ we have $\lVert f'(x)\lVert\leq g'(x)$. Then
\begin{align}
\lVert f(b)-f(a)\lVert\leq g(b)-g(a)  \label{eq124}
\end{align}
\end{thm}


\begin{proof}
By continuity, it suffices to prove $\lVert f(\beta)-f(\alpha)\lVert\leq g(\beta)-g(\alpha)$ for all $\alpha,\beta$ satisfying $a<\alpha<\beta<b$. Therefore, by replacing $[a,b]$ by $[\alpha,\beta]$, it suffices to assume that $f,g$ are differentiable on $[a,b]$.\\[-1ex]

Step 1. For each $\eps>0$, consider the condition on $x\in[a,b]$:
\begin{align}
\lVert f(x)-f(a)\lVert\leq g(x)-g(a)+\eps(x-a)\label{eq121}
\end{align}
The set
\begin{align*}
E_\eps=\big\{x\in[a,b]:x\text{ satisfies \eqref{eq121}}\big\}
\end{align*}
is nonempty because $a\in E_\eps$. One checks easily that $E_\eps$ is a closed subset of $[a,b]$: This is because, for example, $E_\eps$ is the inverse image of the closed subset $(-\infty,0]$ of $\Rbb$ under the map
\begin{gather}
[a,b]\rightarrow\Rbb\qquad x\mapsto \lVert f(x)-f(a)\lVert- g(x)+g(a)-\eps(x-a)  \label{eq122}
\end{gather}
We now fix $x=\sup E_\eps$. Then $x\in E_\eps$ because $E_\eps$ is closed. We shall prove that $x=b$. Then the fact that $b\in E_\eps$ for all $\eps>0$ proves \eqref{eq124}.\\[-1ex]


Step 2. Suppose that $x\neq b$. Then $a\leq x<b$. We shall prove that there exists $y\in(x,b)$ such that
\begin{align}
\lVert f(y)-f(x)\lVert\leq g(y)-g(x)+\eps(y-x) \label{eq123}
\end{align}
Add this inequality to \eqref{eq121} (which holds because $x\in E_\eps$). Then by triangle inequality, we obtain $y\in E_\eps$, contradicting $x=\sup E_\eps$.


Let us prove the existence of such $y$. For each $y\in (x,b)$, define $v(y)\in V,\lambda(y)\in\Rbb$ such that
\begin{gather*}
f(y)-f(x)=f'(x)(y-x)+v(y)(y-x)\\
g(y)-g(x)=g'(x)(y-x)+\lambda(y)(y-x)
\end{gather*}
The definition of $f'(x)$ and $g'(x)$ implies that $v(y)\rightarrow0$ and $\lambda(y)\rightarrow0$ as $y\rightarrow x$. Therefore, there exists $y\in(x,b)$ such that $\lVert v(u)\lVert<\eps/2$ and $|\lambda(y)|<\eps/2$. Thus
\begin{align*}
&\lVert f(y)-f(x)\lVert-(g(y)-g(x))\nonumber\\
\leq&(\lVert f'(x)\lVert-g'(x)) (y-x)+(\lVert v(y)\lVert-\lambda(y))\cdot (y-x)\nonumber\\
\leq&0\cdot(y-x)+\big(\frac\eps2+\frac\eps2\big)\cdot(y-x)=\eps(y-x)
\end{align*}
This proves \eqref{eq123}.
\end{proof}


\begin{rem}
If we apply Thm. \ref{lb329} to the special case that $f=0$, we see that if $g\in C([a,b],\Rbb)$ satisfies $g'\geq0$ on $(a,b)$, then $g$ is increasing. This gives another proof of \eqref{eq114} besides the one via Lagrange's MVT.
\end{rem}


An important special case of Thm. \ref{lb329} is:

\begin{co}[\textbf{Finite-increment theorem}] \index{00@Finite-increment theorem}\label{lb333}
Suppose that $f\in C([a,b],V)$ is differentiable on $(a,b)$, and that there exists $M\in\Rbb_{\geq0}$ such that $\lVert f'(x)\lVert\leq M$ for all $x\in(a,b)$. Then
\begin{align}
\lVert f(b)-f(a)\lVert\leq M|b-a|
\end{align}
\end{co}

\begin{proof}
Choose $g(x)=Mx$ in Thm. \ref{lb329}.
\end{proof}

It is fairly easy to prove finite-increment theorem for complex-variable functions.

\begin{df}
A subset $E$ of a real vector space is called \textbf{convex}, \index{00@Convex set} if for every $x,y\in E$, the interval \index{00@Interval $[x,y]$ in a real vector space}
\begin{align}
[x,y]=\{tx+(1-t)y:t\in[0,1]\}
\end{align}
is a subset of $E$.
\end{df}

When talking about convex subsets of $\Cbb$, we view $\Cbb$ as $\Rbb^2$. Then, for example, all open disks in $\Cbb$ are convex.

\begin{co}[\textbf{Finite-increment theorem}]\index{00@Finite-increment theorem}\label{lb335}
Assume $\Fbb=\Cbb$. Let $\Omega$ be a nonempty open convex subset of $\Cbb$. Let $f:\Omega\rightarrow V$ be differentiable on $\Omega$. Choose any $z_1,z_2\in\Omega$. Suppose that there exists $M\in\Rbb_{\geq0}$ such that $\lVert f'(z)\lVert\leq M$ for all $z$ in the interval $[z_1,z_2]$. Then 
\begin{align}
\lVert f(z_2)-f(z_1)\lVert\leq M|z_2-z_1|
\end{align}
\end{co}

\begin{proof}
Define $\gamma:[0,1]\rightarrow \Cbb$ by $\gamma(t)=(1-t)z_1+tz_2$. Then $\gamma([0,1])\subset\Omega$ because $\Omega$ is convex. By chain rule (Thm. \ref{lb331}), we have
\begin{align*}
(f\circ\gamma)'(t)=f'(\gamma(t))\cdot \gamma'(t)=(z_2-z_1)f'(\gamma(t))
\end{align*}
whose norm is bounded by $M|z_2-z_1|$. Applying Cor. \ref{lb333} to $f\circ\gamma$ finishes the proof.
\end{proof}





\begin{eg}
Let $X$ be an interval in $\Rbb$, or let $X=\Omega$ where $\Omega$ is convex. Let $\scr A$ be the set of differentiable functions $f:\Omega\rightarrow V$ satisfying $\lVert f'\lVert_{l^\infty}\leq M$. Then elements of $\scr A$ have Lipschitz constant $M$ by Finite-increment theorems. So $\scr A$ is equicontinuous.
\end{eg}







\subsection{Commutativity of derivatives and limits}

Fix a Banach space $V$ over $\Fbb\in\{\Rbb,\Cbb\}$. Let $\Omega$ be a nonempty open subset of $\Cbb$. Recall Conv. \ref{lb332}. Recall Def. \ref{lb334} for the meaning of locally uniform convergence.


\subsubsection{Main theorem}




\begin{thm}\label{lb336}
Let $X$ be $\Omega$ or an interval in $\Rbb$. Let $(f_\alpha)_{\alpha\in I}$ be a net of differentiable functions $X\rightarrow V$. Suppose that the following are true:
\begin{enumerate}[label=(\alph*)]
\item The net $(f_\alpha)_{\alpha\in I}$ converges pointwise to some $f:X\rightarrow V$.
\item The net $(f_\alpha')_{\alpha\in I}$ converges uniformly to some $g:X\rightarrow V$. 
\end{enumerate}
Then $f$ is differentiable on $X$, and $f'=g$.
\end{thm}


\begin{proof}
We prove the case $X=\Omega$. The other case is similar. Choose any $z\in \Omega$. By shrinking $\Omega$, we assume that $\Omega$ is an open disk centered at $z$. We know $\lim_{w\rightarrow z}\frac{f_\alpha(w)-f_\alpha(z)}{w-z}$ converges to $f_\alpha'(z)$ for each $\alpha$. Therefore, if we can show that $\frac{f_\alpha(w)-f_\alpha(z)}{w-z}$ converges uniformly (over all $w\in \Omega\setminus\{z\}$) under $\lim_\alpha$, then it must converge uniformly to $\frac{f(w)-f(z)}{w-z}$ since it converges pointwise to $\frac{f(w)-f(z)}{w-z}$ by (a). Then, by Thm. \ref{lb289}, we have
\begin{align*}
\lim_{w\rightarrow z}\lim_\alpha\frac{f_\alpha(w)-f_\alpha(z)}{w-z}=\lim_\alpha\lim_{w\rightarrow z}\frac{f_\alpha(w)-f_\alpha(z)}{w-z}=\lim_\alpha f_\alpha'(z)=g(z)
\end{align*}
finishing the proof.

To prove the uniform convergence, by the Cauchy condition on $V^{\Omega\setminus\{z\}}$ (equipped with a complete uniform convergence metric as in Exp. \ref{lb272}), it suffices to prove that
\begin{align}\label{eq126}
\sup_{w\in\Omega\setminus\{z\}}\Big\lVert \frac{f_\alpha(w)-f_\alpha(z)}{w-z}-\frac{f_\beta(w)-f_\beta(z)}{w-z}\Big\lVert
\end{align}
converges to $0$ under $\lim_{\alpha,\beta\in I}$. Applying Cor. \ref{lb335} to the function $f_\alpha-f_\beta$, we see
\begin{align}
&\lVert f_\alpha(w)-f_\beta(w)-f_\alpha(z)+f_\beta(z)\lVert\leq |w-z|\cdot\sup_{\zeta\in[z,w]} \lVert f_\alpha'(\zeta)-f_\beta'(\zeta)\lVert\nonumber\\
\leq&|w-z|\cdot \lVert f_\alpha'-f_\beta'\lVert_{l^\infty(\Omega,V)}\label{eq127}
\end{align}
Thus, we have 
\begin{align*}
\eqref{eq126}\leq \lVert f_\alpha'-f_\beta'\lVert_{l^\infty(\Omega,V)}
\end{align*}
where the RHS converges to $0$ under $\lim_{\alpha,\beta}$ due to (b). This finishes the proof.
\end{proof}


We didn't assume the uniform convergence of $(f_\alpha)$ in Thm. \ref{lb336} because it is often redundant:

\begin{lm}\label{lb343}
Assume that either $X$ is a bounded interval in $\Rbb$, or $X=\Omega$ where $\Omega$ is assumed to be bounded and convex. Then under the assumptions in Thm. \ref{lb336}, the net $(f_\alpha)_{\alpha\in I}$ converges uniformly to $f$. 
\end{lm}




\begin{proof}
We already know that $(f_\alpha)$ converges pointwise to $f$. Thus, motivated by the proof of Thm. \ref{lb336}, we fix $z\in X$, and prove  the Cauchy condition that
\begin{align*}
\lim_{\alpha,\beta\in I}\sup_{w\in\Omega}\lVert f_\alpha(w)-f_\beta(w)-f_\alpha(z)+f_\beta(z)\lVert=0
\end{align*}
Then $(f_\alpha-f_\alpha(z))$ converges uniformly, and hence $(f_\alpha)$ converges uniformly. Indeed, the Cauchy condition follows easily from \eqref{eq127}, in which $\sup_{w\in\Omega}|w-z|$ is a finite number because $X$ is bounded.
\end{proof}


Thus, whether or not $\Omega$ satisfies boundedness and convexity, the net $(f_\alpha)$ must converge locally uniformly to $f$. Knowing the locally uniform convergence is often enough for applications. And here is another proof of this fact:

\begin{proof}[\textbf{Another proof} that $(f_\alpha)$ converges locally uniformly to $f$]
We consider the case $X=\Omega$; the other case is similar. For each $z\in X$, choose a convex precompact $U\in\Nbh_X(x)$. (Namely, $\Cl_X(U)$ is compact.) By (b) of Thm. \ref{lb336}, there is $\mu\in I$ such that $\sup_{x\in\ovl U}\lVert f_\alpha'(x)-f_\mu'(x)\lVert\leq 1$ for all $\alpha\geq\mu$. Let $h_\alpha=f_\alpha-f_\mu$. By finite-increment Thm. \ref{lb335}, for each $x,y\in U$ we have
\begin{align*}
\lVert h_\alpha(x)-h_\alpha(y)\lVert\leq \lVert x-y\lVert
\end{align*}
i.e., $(h_\alpha|_U)_{\alpha\in I}$ has uniform Lipschitz constant $1$, and hence is an equicontinuous set of functions. Choose a closed ball $B$ centered at $z$ such that $B\subset U$. Then, since $(h_\alpha|_B)_{\alpha\in I}$ is equicontinuous and converges pointwise to $f-f_\mu$ (on $B$), by Cor. \ref{lb284}, $(f_\alpha-f_\mu)_{\alpha\in I}$ converges uniformly on $B$ to $f-f_\mu$. Thus $f_\alpha$ converges uniformly on $B$ to $f$.
\end{proof}



\subsubsection{An interpretation of Thm. \ref{lb336} in terms of Banach spaces}


We give a more concise way of understanding the two conditions in Thm. \ref{lb336}.


\begin{co}\label{lb344}
Let $X$ be $\Omega$ or an interval in $\Rbb$. Define \index{l1@$l^{1,\infty}$}
\begin{align*}
l^{1,\infty}(X,V)=\{f\in V^X: f\text{ is differentiable on }X\text{ and }\lVert f\lVert_{l^{1,\infty}}<+\infty\}
\end{align*}
Where $\lVert \cdot\lVert_{l^{1,\infty}}=\lVert\cdot\lVert_{1,\infty}$ is defined by 
\begin{align*}
\lVert f\lVert_{1,\infty}=\lVert f\lVert_{l^\infty}+\lVert f'\lVert_{l^\infty}=\sup_{x\in X}\lVert f(x)\lVert+\sup_{x\in X}\lVert f'(x)\lVert
\end{align*}
Then $l^{1,\infty}(X,V)$ is a Banach space with norm $\lVert\cdot\lVert_{1,\infty}.$
\end{co}


\begin{proof}
It is a routine check that $\lVert f\lVert_{D^1}$ defines a norm on $D^1(X,V)$. We now prove that $D^1(X,V)$ is complete. Let $(f_n)$ be a Cauchy sequence in $D^1(X,V)$. So $(f_n)$ and $(f_n')$ are Cauchy sequences in $l^\infty$, converging uniformly to $f,g\in V^X$ respectively. By Lem. \ref{lb336}, $f$ is differentiable, and $f'=g$. (In particular, $\lVert f'\lVert_\infty<+\infty$.) So $f_n'\rightrightarrows f'$. Thus
\begin{align*}
\lVert f_n-f\lVert_{1,\infty}=\lVert f_n-f\lVert_\infty+\lVert f_n'-f'\lVert_\infty\rightarrow 0
\end{align*} 
\end{proof}


\begin{rem}
Thm. \ref{lb336} and Cor. \ref{lb344} are almost equivalent. We have proved Cor. \ref{lb344} using Thm. \ref{lb336}. But we can also prove a slightly weaker version of Thm. \ref{lb336} using Cor. \ref{lb344} as follows: In the setting of Thm. \ref{lb336}, assume that each $f_\alpha:X\rightarrow V$ is differentiable, and that 
\begin{align}\label{eq128}
f_\alpha\rightrightarrows f\qquad f_\alpha'\rightrightarrows g
\end{align}
where $f,g\in l^\infty(X,V)$. Then  Cor. \ref{lb344} implies that $f$ is differentiable and $f'=g$. 
\end{rem}


\begin{proof}[$\star$ Proof]
By \eqref{eq128}, there is $\beta\in I$ such that $\lVert f_\alpha-f\lVert_\infty<+\infty$ and $\lVert f_\alpha'-g\lVert<+\infty$ for all $\alpha\geq\beta$. Thus, by replacing $I$ with $I_\beta$, we assume that $f_\alpha$ and $f_\alpha'$ are bounded on $X$. So $f_\alpha\in l^{1,\infty}(X,V)$. 

By \eqref{eq128}, both $(f_\alpha)$ and $(f'_\alpha)$ converge in $l^\infty(X,V)$. So they are Cauchy nets under the $l^\infty$-norm. So $(f_\alpha)$ is a Cauchy net in $l^{1,\infty}(X,V)$. By Cor. \ref{lb344}, $(f_\alpha)$ converges to $\wtd f\in l^{1,\infty}(X,V)$ under the $l^{1,\infty}$ norm. In particular, $f_\alpha\rightrightarrows \wtd f$ and $f_\alpha'\rightrightarrows \wtd f'$. Since, by assumption, we have $f_\alpha\rightrightarrows f$ and $f_\alpha'\rightrightarrows g$, we therefore get $f=\wtd f$ and $g=\wtd f'$.
\end{proof}



It is not surprising that Thm. \ref{lb336} can be rephrased in terms of the completeness of a normed vector space. After all, our proof of Thm. \ref{lb336} uses Cauchy nets in an essential way. 








\subsection{Derivatives of power series}


This section is a continuation of the previous one. We shall use Thm. \ref{lb336} to compute derivatives of power series.



\subsubsection{General result}


\begin{co}\label{lb338}
Assume that $V$ is over $\Cbb$. Let $f(z)=\sum_{n=0}^\infty v_nz^n$ be a power series in $V$. Assume that its radius of convergence is $R>0$. Then $f$ is differentiable on $B_\Cbb(0,R)$ and
\begin{align*}
f'(z)=\sum_{n=0}^\infty nv_nz^{n-1} 
\end{align*}
\end{co}

Note that since $\lim_n\sqrt[n]{n}=1$, we have
\begin{align}
\limsup_{n\rightarrow\infty}\sqrt[n]{\lVert v_n\lVert}=\limsup_{n\rightarrow\infty}\sqrt[n]{n\lVert v_n\lVert}
\end{align}
So $\sum v_nz^n$ and $\sum_{n=0}^\infty nv_nz^{n-1}$ have the same radius of convergence.

\begin{proof}[First proof]
For each $n\in\Nbb_+$, let $g_n=f_0+f_1+\cdots+f_n$. Then by Thm. \ref{lb112}, for each $0<r<R$, the sequences $(g_n)$ (resp. $(g_n')$) converges uniformly to $f$ (resp. converges uniformly to $h(z)=\sum_0^\infty nv_nz^{n-1}$) on $B_\Cbb(0,r)$. Therefore, by Thm. \ref{lb336}, we have $f'(z)=h(z)$ for all $z\in B_\Cbb(0,r)$, and hence all $z\in B_\Cbb(0,R)$ since $r$ is arbitrary.
\end{proof}

\begin{proof}[Second proof]
Choose any $0<r<R$, and let $X_r=B_\Cbb(0,r)$. Consider $\sum v_n z^n$ as a series in the Banach space $l^\infty(X_r,\Cbb)$ (cf. Cor. \ref{lb344}). The norm of each term is $\lVert v_n\lVert+(n+1)\lVert v_{n+1}\lVert$. So the radius of convergence is $R$. Thus  $\sum v_n z^n$ converges in $l^\infty(X_r,\Cbb)$ to some $h\in l^\infty(X_r,\Cbb)$. This means that, on $X_r$, $\sum v_nz^n$ converges uniformly to $h$ (and hence $f=h$ on $X_r$) and $\sum nv_nz^{n-1}$ converges uniformly  to $h'$. So $f'(z)=h'(z)=\sum nv_nz^{n-1}$ for all $z\in X_r$.
\end{proof}



\subsubsection{Examples}\label{lb353}

\begin{eg}
By Cor. \ref{lb338}, the function $\exp:\Cbb\rightarrow\Cbb$ is differentiable, and
\begin{align*}
\frac d{dz}e^z=e^z
\end{align*}
Thus, if $\gamma:[a,b]\rightarrow\Cbb$ is differentiable, then by chain rule, $\exp\circ\gamma$ is differentiable on $[a,b]$, and
\begin{align*}
\frac d{dt}e^{\gamma(t)}=e^{\gamma(t)}\cdot \frac d{dt}\gamma(t)
\end{align*}
For example:
\begin{align*}
(e^{\alpha t})'=\alpha e^{\alpha t}\qquad (e^{t^2+\im t})'=(2t+\im)e^{t^2+\im t}
\end{align*}
\end{eg}


\begin{eg}
Define functions $\sin:\Cbb\rightarrow\Cbb$ and $\cos:\Cbb\rightarrow\Cbb$ \index{sincos@$\sin,\cos$} by
\begin{gather*}
\cos z=\frac {e^{\im z}+e^{-\im z}}2\qquad \sin z=\frac{e^{\im z}-e^{-\im z}}{2\im}
\end{gather*}
It follows from $e^ze^w=e^{z+w}$ that
\begin{align}
\cos(z+w)=\cos z\cos w-\sin z\sin w\qquad \sin(z+w)=\sin z\cos w+\cos z\sin w  \label{eq138}
\end{align}
By chain rule, we have $(e^{\alpha z})'=\alpha e^{\alpha z}$. So
\begin{gather*}
\sin'z=\cos z\qquad \cos' z=-\sin z
\end{gather*}
\end{eg}


\begin{rem}
From $e^z=\sum_{n\in\Nbb}z^n/n!$, it is clear that the complex conjugate of $e^z$ is 
\begin{align*}
\ovl{e^z}=\ovl{\sum_{n=0}^\infty\frac {z^n}{n!}}=\sum_{n=0}^\infty\frac {\ovl z^n}{n!}=e^{\ovl z}
\end{align*}
Here, we have exchanged the order of conjugate and infinite sum, because the function $z\in\Cbb\mapsto \ovl z$ is continuous. Thus, if $t\in\Rbb$, then
\begin{align*}
\ovl{e^{\im t}}=e^{-\im t}\qquad |e^{\im t}|^2=\ovl{e^{\im t}} e^{\im t}=e^{-\im t}e^{\im t}=1
\end{align*}
It follows that
\begin{gather*}
\cos t=\Real(e^{\im t})\qquad\sin t=\Imag(e^{\im t})\\
(\cos t)^2+(\sin t)^2=(\Real(e^{\im t}))^2+(\Imag(e^{\im t}))^2=|e^{\im t}|^2=1
\end{gather*}
It also follows from $|e^{\im t}|=1$ that if $a,b\in\Rbb$ then, by $e^{a+b\im}=e^ae^{b\im}$, we have
\begin{align*}
e^{a+b\im}=a^a\qquad\in\Rbb_{>0}
\end{align*}
\end{rem}





\begin{eg}
By Prop. \ref{lb336}, the function $\log:\Rbb_{>0}\rightarrow\Rbb$ is differentiable, and
\begin{align*}
(\log x)'=\frac 1x
\end{align*}
\end{eg}

\begin{eg}
We have 
\begin{align*}
\lim_{t\rightarrow 0}\frac{\log(1+t)}t=(\log x)'|_{x=0}=1
\end{align*}
and hence $\lim_{x\rightarrow +\infty}x\log(1+1/x)=1$ by Def. \ref{lb197}-(3) (since $\lim_{x\rightarrow+\infty}1/x=0$ as a net limit). Taking exponential, and using the continuity of $\exp$ at $1$, we get 
\begin{align*}
\lim_{x\rightarrow+\infty}\Big(1+\frac 1x\Big)^x=e
\end{align*} 
\end{eg}




\begin{eg}
If $a>0$ and $z\in\Cbb$, recalling $a^z=e^{z\log a}$, we use chain rule find
\begin{align*}
\frac d{dz}a^z=a^z\log a
\end{align*}
Similarly, if $\alpha\in\Cbb$ and $x>0$, then the chain rule gives the derivative of $x^p$:
\begin{align*}
\frac d{dx}x^\alpha=\alpha\cdot x^{\alpha-1}
\end{align*}
\end{eg}



\begin{eg}
Compute $\dps\sum_{n=0}^\infty \frac {n^2}{3^n}$
\end{eg}


\begin{proof}
The series $\dps f(z)=\sum_{n=0}^\infty \frac{z^n}{3^n}$ has radius of convergence $3$. When $|z|<3$, it converges absolutely to $(1-\frac z3)^{-1}=3(3-z)^{-1}$. So, by Cor. \ref{lb338}, when $|z|<3$ we have
\begin{gather*}
    \sum_{n=0}^\infty \frac{nz^{n-1}}{3^n}=f'(z)=3(3-z)^{-2}\\
    \sum_{n=0}^\infty \frac{n(n-1)z^{n-2}}{3^n}=f''(z)=6(3-z)^{-3}
\end{gather*}
So the value of the original series equals
\begin{align*}
f''(1)+f'(1)=\frac 32
\end{align*}
\end{proof}


\subsubsection{A proof of (generalized) Leibniz rule}



We end this section by giving a fun proof of Leibniz rule for higher derivatives. For simplicity, we consider only scalar-valued functions.



\begin{pp}[\textbf{Leibniz rule}] \index{00@Leibniz rule}  \label{lb348}
Let $X$ be either a nonempty interval in $\Rbb$ (resp. a nonempty open subset of $\Cbb$). Let $f,g$ be functions from $X$ to $\Rbb$ (resp. to $\Cbb$). Let $z\in X$. Suppose that $f^{(n)}(z)$ and $g^{(n)}(z)$ exist. Then
\begin{align}
(fg)^{(n)}(z)=\sum_{j=0}^n {n\choose j}f^{(n-j)}(z)g^{(j)}(z)
\end{align}
\end{pp}




The above Leibniz rule is usually proved using the formula
\begin{align}
{n+1\choose j}={n\choose j-1}+{n\choose j}
\end{align}
where $n\in\Nbb$ and $1\leq j\leq n$. In the following, we give a proof without using this formula. We need the fact that the function
\begin{align}
\Cbb^n\rightarrow C(\Rbb,\Cbb)\qquad  (a_0,\dots,a_{n-1})\mapsto p(x)=\sum_{j=0}^{n-1}a_jx^j  \label{eq133}
\end{align}
is injective: this is because $j!\cdot a_j=f^{(j)}(0)$ by Exp. \ref{lb323}.



\begin{proof}[\textbf{Proof of Prop. \ref{lb348}}]
By induction on $n$ and by the classical Leibniz rule (Prop. \ref{lb320}), we have
\begin{align}
(fg)^{(n)}(z)=\sum_{j=0}^n C_{n,j}\cdot f^{(n-j)}(z)g^{(j)}(z) \label{eq132}
\end{align}
where each $C_{n,j}$ is an integer independent of $f$ and $g$. (In particular, $C_{n,j}$ is independent of whether the variables are real of complex.) Thus, to determine the value of $C_{n,j}$, we can use some special functions. 

We consider $f(x)=e^{s x}$ and $g(x)=e^{t x}$ where $s,t\in\Rbb$. Recall that we have proved that $(e^{\alpha t})'=\alpha e^{\alpha t}$ using chain rule and the derivative formula for exponentials. So \eqref{eq132} reads
\begin{align*}
(s+t)^n\cdot e^{(s+t)x}=\sum_{j=0}^n C_{n,j}\cdot s^{n-j}t^{j}\cdot e^{(s+t)x}
\end{align*}
Taking $x=0$ gives
\begin{align}
(s+t)^n=\sum_{j=0}^n C_{n,j}\cdot s^{n-j}t^j  \label{eq135}
\end{align}
Comparing this with the binomial formula \eqref{eq60}, and noticing the injectivity of  \eqref{eq133} (first applied to \eqref{eq135} for each fixed $t$, where \eqref{eq135} is viewed as a polynomial of $s$; then applied to each coefficient before $s^{n-j}$, which is a polynomial of $t$), we immediately get $C_{n,j}={n\choose j}$.
\end{proof}




\begin{comment}
\begin{rem}
By induction on $k$, one easily proves
\begin{align}
(z_1+\cdots+z_k)^n=\sum_{j_1+\cdots+j_k=n}{n\choose j_1,\dots,j_k}z_1^{j_1}\cdots z_k^{j_k}
\end{align}
where
\begin{align}
{n\choose j_1,\dots,j_k}=\frac{n!}{j_1!\cdots j_k!}
\end{align}
is the \textbf{multinomial coefficient}. Then, the same method as above proves
\begin{align}
(f_1\cdots f_k)^{(n)}=\sum_{j_1+\cdots+j_k=n}{n\choose j_1,\dots,j_k} f_1^{(j_1)}\cdots f_k^{(j_k)}
\end{align}
\end{rem}
\end{comment}

















\subsection{Problems and supplementary material}


Let $\Omega$ be nonempty open subset of $\Cbb$. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$. Assume that $\Fbb=\Cbb$ if we take derivatives with respect to complex variables.


\begin{df}\label{lb354}
Let $X$ be either $\Omega$ or an interval in $\Rbb$. Define \index{Cn@$C^n(X,V)$} \index{C@$C^\infty$}
\begin{gather*}
C^n(X,V)=\{f\in V^X:f,f',\dots,f^{(n)}\text{ exist and are continuous}\}\qquad (\text{if }n\in\Nbb)\\
C^\infty(X,V)=\bigcap_{n\in\Nbb}C^n(X,V)
\end{gather*}
If $X\subset\Rbb$, elements in $C^\infty(X,V)$ are called \textbf{smooth functions}. \index{00@Smooth functions}
\end{df}


\begin{prob}\label{lb355}
Let $X$ be either $\Omega$ or an interval in $\Rbb$. For each $n\in\Nbb$, define \index{ln@$l^{n,\infty}$}
\begin{align*}
l^{n,\infty}(X,V)=\{f\in V^X:f,f',\dots,f^{(n)}\text{ exist, and }\lVert f\lVert_{n,\infty}<+\infty\}
\end{align*}
where $\lVert f\lVert_{n,\infty}=\lVert f\lVert_{l^{n,\infty}}$ is defined by
\begin{align*}
\lVert f\lVert_{n,\infty}=\lVert f\lVert_\infty+\lVert f'\lVert_\infty+\cdots+\lVert f^{(n)}\lVert_\infty
\end{align*}
(In particular, we understand $l^{0,\infty}$ as $l^\infty$.) Clearly $\lVert\cdot\lVert_{n,\infty}$ is a norm. We have proved that $l^{n,\infty}(X,V)$ is complete when $n=0,1$. 
\begin{enumerate}
\item Prove by induction on $n$ that $l^{n,\infty}(X,V)$ is complete for every $n$.
\item Prove that for each $n\in\Nbb$, $C^n(X,V)\cap l^{n,\infty}(X,V)$ is a closed subset of $l^{n,\infty}(X,V)$ (and hence, is a Banach space by Prop. \ref{lb86}).  Prove that if $X$ is compact then $C^n(X,V)\subset l^{n,\infty}(X,V)$.
\end{enumerate}
\end{prob}







\begin{cv}
Unless otherwise stated, when $X$ is compact, we always choose $l^{n,\infty}$ to be the norm on $C^n(X,V)$.
\end{cv}








\newpage




\section{More on derivatives}

\subsection{Cauchy's MVT and L'H\^opital's rule}


The goal of this section is to prove L'H\^opital's rule. For that purpose, we first need to prove Cauchy's MVT.



\subsubsection{Main theorems}




\begin{thm}[\textbf{Cauchy's MVT}] \index{00@Cauchy's MVT}
Let $-\infty<a<b<+\infty$. Let $f,g\in C([a,b],\Rbb)$ be differentiable on $(a,b)$. Then there exists $x\in(a,b)$ such that
\begin{align*}
    f'(x) (g(b)-g(a))=g'(x)(f(b)-f(a))
\end{align*}
\end{thm}

In particular, if $g$ is injective, we can write the above formula as
\begin{align*}
\frac{f'(x)}{g'(x)}=\frac{f(b)-f(a)}{g(b)-g(a)}
\end{align*}


\begin{proof}
Cauchy's MVT specializes to Lagrange's MVT if we set $g(x)=x$. Moreover, in the proof of Lagrange's MVT (Thm. \ref{lb345}), we applied Rolle's MVT to the function $f(x)-kx$ where $k$ is the slope of a line segment. Motivated by this observation, we consider the function $\psi(x)=f(x)-kg(x)$. If one wants $\psi(a)=\psi(b)$, one then solves that $k=\frac{f(b)-f(a)}{g(b)-g(a)}$. But we would rather consider $(g(b)-g(a))\psi$ in order to avoid the issue that the denominator is possibly zero. So we set
\begin{align*}
h(x)=(g(b)-g(a))f(x)-(f(b)-f(a))g(x)
\end{align*}
Clearly $h(a)=h(b)$. By Rolle's MVT, there exists $x\in(a,b)$ such that
\begin{align*}
0=h'(x)=(g(b)-g(a))f'(x)-(f(b)-f(a))g'(x)
\end{align*}
\end{proof}








\begin{thm}[\textbf{L'H\^opital's rule}]  \index{00@L'H\^opital's rule}
Let $-\infty\leq a<b\leq+\infty$. Let $f,g\in C((a,b),\Rbb)$ be differentiable on $(a,b)$. Assume that $g'$ is nowhere zero on $(a,b)$. (So $g$ is strictly monotonic, cf. Lem. \ref{lb347}.) Assume 
\begin{align}
\lim_{x\rightarrow a}\frac{f'(x)}{g'(x)}=A\qquad \text{exists in }\ovl\Rbb    \label{eq129}
\end{align}
Assume that one of the following two cases are satisfied:
\begin{gather*}
\lim_{x\rightarrow a}f(x)=\lim_{x\rightarrow a}g(x)=0  \tag{Case $\frac 00$}\\
\lim_{x\rightarrow a}|g(x)|=+\infty \tag{Case $\frac *\infty$}
\end{gather*}
Then we have $\dps\lim_{x\rightarrow a}\frac{f(x)}{g(x)}=A$. The same conclusion holds if ``$x\rightarrow a$" is replaced by ``$x\rightarrow b$".
\end{thm}


\begin{rem}
Since $g$ is strictly monotonic, there is at most one $x\in(a,b)$ such that $g(x)=0$. So $\lim_{x\rightarrow a}f(x)/g(x)$ means the limit over $x\in (a,b)\setminus g^{-1}(0)$. Alternatively, one can assign an arbitrary value to $f(x)/g(x)$ when $g(x)=0$, and understand $\lim_{x\rightarrow a}$ as a limit over $x\in (a,b)$.
\end{rem}


\begin{eg}
Compute $\dps\lim_{x\rightarrow+\infty}\frac{x^n}{e^x}$
\end{eg}

\begin{proof}
By L'H\^opital's rule in the case $\frac *\infty$, we have
\begin{align*}
\lim_{x\rightarrow+\infty}\frac{x^n}{e^x}=\lim_{x\rightarrow+\infty}\frac{nx^{n-1}}{e^x}=\lim_{x\rightarrow+\infty}\frac{n(n-1)x^{n-2}}{e^x}=\cdots=\lim_{x\rightarrow+\infty}\frac{n!}{e^x}=0
\end{align*}
where the convergence of the limit is derived from right to left.
\end{proof}




\subsubsection{Proof of L'H\^opital's rule}\label{lb349}




We divide the proof of L'H\^opital's rule into several steps. Also, we only treat the case $x\rightarrow a$, since the other case is similar. In the following, $(a,b)$ means an interval, but not an element in the Cartesian product (which will be written as $a\times b$).


\begin{proof}[\textbf{Step 1}]
We let $\dps\frac{f(x)-f(y)}{g(x)-g(y)}$ take value $\dps\frac{f'(x)}{g'(x)}$ if $x=y$. In this step, we prove
\begin{align}\label{eq130}
\lim_{x\times y\rightarrow a\times a}~\frac{f(x)-f(y)}{g(x)-g(y)}=A
\end{align}
where $x\times y$ is defined on $(a,b)^2=(a,b)\times (a,b)$. In view of Def. \ref{lb197}-(3m), we pick any sequence $x_n\times y_n$ in $(a,b)^2$. By Cauchy's MVT, there is $\xi_n \in [x_n,y_n]$ (if $x_n\leq y_n$) or $\xi_n\in[y_n,x_n]$ (if $x_n\geq y_n$) such that
\begin{align*}
f'(\xi_n)(g(x_n)-g(y_n))=g'(\xi_n)(f(x_n)-f(y_n)).
\end{align*} 
So we have
\begin{align*}
    \frac{f'(\xi_n)}{g'(\xi_n)}=\frac{f(x_n)-f(y_n)}{g(x_n)-g(y_n)} 
\end{align*}
Since $\lim_n x_n=\lim_n y_n=a$, we clearly have $\lim_n \xi_n=a$.  Therefore, by \eqref{eq129} (this is the only place where we use \eqref{eq129}), we have
\begin{align*}
\lim_{n\rightarrow\infty} \frac{f(x_n)-f(y_n)}{g(x_n)-g(y_n)}=\lim_{n\rightarrow\infty}\frac{f'(\xi_n)}{g'(\xi_n)}=A
\end{align*}
This proves \eqref{eq130}.
\end{proof}





\begin{proof}[\textbf{Step 2}]
It follows from Def. \ref{lb197}-(3) that if $(x_n)$ and $(y_k)$ are sequences in $(a,b)$ converging to $a$, then
\begin{align}
\lim_{n,k\rightarrow\infty} \frac{f(x_n)-f(y_k)}{g(x_n)-g(y_k)}=A \label{eq131}
\end{align}  
In other words, we apply Def. \ref{lb197}-(3) to the net $(x_n\times y_k)_{n\times k\in\Zbb_+^2}$ which replaces the sequence $(x_n\times y_n)_{n\in\Zbb_+}$ in Step 1. We shall use \eqref{eq131} to prove the two cases of L'H\^opital's rule.


Let us prove the case $\frac 00$. This is the easier case. Choose any sequence $(x_n)$ in $(a,b)$ converging to $a$. We want to prove that $\lim_n f(x_n)/g(x_n)=A$. So we choose any sequence $(y_k)$ in $(a,b)$ converging to $a$, and we know that the double limit \eqref{eq131} exists. Moreover, if $n$ is fixed, then $\lim_k f(y_k)=\lim_k g(y_k)=0$ by assumption. Thus
\begin{align}
\lim_{k\rightarrow\infty}\frac{f(x_n)-f(y_k)}{g(x_n)-g(y_k)}=\frac{f(x_n)}{g(x_n)} \label{eq134}
\end{align}
Thus, by Thm. \ref{lb122}, when $n\rightarrow\infty$, the RHS of the above equation converges to \eqref{eq131}. This finishes the proof for the case $\frac 00$.
\end{proof}



\begin{proof}[\textbf{Step 3}]
Finally, we address the (more difficult) case $\frac *\infty$. Assume $\lim_{x\rightarrow a}|g(x)|=+\infty$. Again, we choose a sequence $(x_n)$ in $(a,b)$ converging to $a$. To prove $f(x_n)/g(x_n)\rightarrow A$, one may want to pick any sequence $(y_k)$ in $(a,b)$, and compute the limit on the LHS of \eqref{eq135}. Unfortunately, in this case, we do not know whether this limit converges or not: As one can compute, it is equal to $\lim_{k\rightarrow\infty}f(y_k)/g(y_k)$, whose convergence is part of the result we need to prove!

It is not hard to address this issue: Since $\ovl\Rbb$ is (sequentially) compact, by Thm. \ref{lb74}, it suffices to prove that any cluster point $B\in\ovl\Rbb$ of $(f(x_n)/g(x_n))_{n\in\Zbb_+}$ is equal to $A$. Thus, we let $(y_k)$ be any subsequence of $(x_n)$ such that $(f(y_k)/g(y_k))_{k\in\Zbb_+}$ converges to $B$. Let us prove $A=B$ using the same method as in Step 2. We compute that
\begin{align}
\frac{f(x_n)-f(y_k)}{g(x_n)-g(y_k)}=\frac{g(y_k)}{g(x_n)-g(y_k)}\cdot\frac{f(x_n)-f(y_k)}{g(y_k)}  \label{eq136}
\end{align}
Since $\lim_k |g(y_k)|=+\infty$, we have $\lim_{k\rightarrow\infty}C/g(y_k)\rightarrow 0$ for any $C\in\Rbb$ independent of $k$. Therefore, as $k\rightarrow\infty$, the first factor on the RHS of \eqref{eq136} converges to $-1$, and the second factor converges to $-B$. It follows that
\begin{align*}
\lim_{n\rightarrow\infty}\lim_{k\rightarrow\infty}\frac{f(x_n)-f(y_k)}{g(x_n)-g(y_k)}=\lim_{n\rightarrow\infty}B=B
\end{align*}
Therefore $A=B$ by Thm. \ref{lb122}.
\end{proof}




\begin{comment}

\begin{rem}
In this course, I have often encouraged the reader to give an elementary proof of a theorem that has been proved in fancy language or in a language different from the common one. However, I shall not make the same suggestion this time.  Many textbook proofs of case $\frac *\infty$ of L'H\^opital's rule use elementary techniques of estimation. However, these proofs seem difficult to digest because they (implicitly) involve approximations with respect to two variables, but they do not use the correct language to discuss such approximations: the language of double limits. Without the correct language, math will become a mess.
\end{rem}
\end{comment}


The proof of L`H\^opital's rule is now complete.


\begin{rem}
The above proof can be easily translated into a language without double limits. We consider the case of $\frac *\infty$ and assume for example that $-\infty<A<+\infty$ and $a\in\Rbb$, and sketch the proof as follows. 

By \eqref{eq129} and Cauchy' MVT, there is $\eps>0$ such that for all $a<x,y<a+\eps$ (where $x\neq y$) we have
\begin{align*}
A-\eps\leq\frac{f(x)-f(y)}{g(x)-g(y)}\leq A+\eps
\end{align*}
Choose any sequence $(x_n)$ in $(a,b)$ converging to $a$. Let $B$ be any cluster point of $(f(x_n)/g(x_n))_{n\in\Zbb_+}$ in $\ovl\Rbb$. We need to prove that $A=B$. Let $(y_k)_{k\in\Zbb_+}$ be any subsequence of $(x_n)$ such that $\lim_k f(y_k)/g(y_k)=B$. Then there is $N\in\Nbb$ such that for all $n\geq N,k\geq N$ satisfying $x_n\neq y_k$, we have
\begin{align*}
A-\eps\leq\frac{f(x_n)-f(y_k)}{g(x_n)-g(y_k)}\leq A+\eps
\end{align*}
For each $n\geq N$, apply $\lim_{k\rightarrow\infty}$ to the above inequality, and notice $\lim_k |g(y_k)|=+\infty$. Then we get $A-\eps\leq B\leq A+\eps$, finishing the proof. \hfill\qedsymbol
\end{rem}










\subsection{Trigonometric functions and $\pi$}



In this section, we prove that $\sin$, $\cos$, and $\pi$ satisfy the properties we learned in high schools. Some of them have already been proved in Subsec. \ref{lb353}. We leave the proof of the basic properties of the other trigonometric functions to the reader.

Let $x$ be a real variable. Recall that $\sin,\cos:\Rbb\rightarrow\Rbb$ are determined by the fact that $e^{\im x}=\cos x+\im\sin x$. In particular, that $|e^{\im x}|=1$ implies $(\cos x)^2+(\sin x)^2=1$. We have proved that
\begin{align*}
\sin'=\cos\qquad \cos'=-\sin
\end{align*}
Since $e^{\im\cdot 0}=1$, we have
\begin{align*}
(\sin x)'|_{x=0}=\cos 0=1\qquad (\cos x)'|_{x=0}=-\sin 0=0
\end{align*}
In particular, since $\sin'=\cos$ is strictly positive on a neighborhood of $0$, by Cor. \ref{lb330}, $\sin$ is strictly increasing on that neighborhood.




We shall define $\frac \pi 2$ to be the smallest positive zero $\cos$. However, we must first prove the existence of this number:


\begin{lm}
There exists $x\geq 0$ such that $\cos x=0$.
\end{lm}


\begin{proof}
Suppose this is not true. Then by $\cos 0=1$ and intermediate value theorem, we have $\cos x>0$ for all $x\geq 0$. In other words, $\sin'>0$ on $\Rbb_{\geq 0}$. Thus, by Cor. \ref{lb330}, $\sin$ is strictly increasing on $\Rbb_{\geq0}$. Therefore $A=\lim_{x\rightarrow+\infty}\sin x$ exists in $\ovl\Rbb$. Since $\sin 0=0$, we must have $A\in\ovl\Rbb_{>0}$. By L'H\^opital's rule in case $\frac *\infty$, we have
\begin{align*}
\lim_{x\rightarrow+\infty} \frac{\cos x}x=\lim_{x\rightarrow+\infty}-\sin x=-A<0
\end{align*}
contradicting the fact that $\cos x>0$ if $x\geq 0$.
\end{proof}



\begin{df}
We define the number $\pi$ to be \index{zz@$\pi$, the number pi}
\begin{align*}
\pi=2\cdot\inf\big(\Rbb_{\geq 0}\cap\cos^{-1}(0)\big)=2\cdot\inf\{x\in\Rbb_{\geq 0}:\cos x=0\}
\end{align*}
Note that $\big(\Rbb_{\geq 0}\cap\cos^{-1}(0)\big)$ is a closed subset of $\Rbb$. So its infinimum belongs to this set. Therefore, $\frac \pi 2$ is the smallest $x\geq0$ satisfying $\cos(x/2)=0$.
\end{df}




\begin{pp}
We have
\begin{align}\label{eq137}
\sin 0=0\qquad\sin\frac\pi 2=1\qquad\cos 0=1\qquad\cos\frac\pi2=0
\end{align}
On $(0,\pi/2)$, $\sin$ and $\cos$ are strictly positive. On $[0,\pi/2]$, $\sin$ is strictly increasing, and $\cos$ is strictly decreasing.
\end{pp}

\begin{proof}
All the formulas in \eqref{eq137}, except $\sin(\pi/2)=1$, has been proved. Since $(\sin\frac\pi 2)^2=1-(\cos\frac\pi2)^2$, we have $\sin\frac\pi2=\pm1$.

By the definition of $\pi$, we know that $\sin'=\cos$ is $>0$ on $[0,\frac\pi2)$. So $\sin$ is strictly increasing on $[0,\frac\pi2]$. Thus, $\sin0=0$ implies that $\sin>0$ on $(0,\frac\pi 2]$. In particular, $\sin\frac \pi2=1$. Since $\cos'=-\sin$ is $<0$ on $(0,\frac\pi2)$, $\cos$ is strictly decreasing on $[0,\frac\pi2]$.
\end{proof}


\begin{pp}
We have $\sin(-x)=-\sin x$ and $\cos(-x)=\cos x$.
\end{pp}


\begin{proof}
Immediate from $e^{-\im x}=1/e^{\im x}$ and the definitions of $\sin$ and $\cos$.
\end{proof}


From what we have proved, we know that the graph of $\sin$ and $\cos$ on $[-\frac\pi2,\frac\pi2]$ looks as follows.
\begin{align*}
\vcenter{\hbox{{
			\includegraphics[height=2.2cm]{fig2.png}}}}
\end{align*}

\begin{pp}
We have
\begin{gather*}
\sin x=\cos(x-\frac\pi2)=\cos(\frac\pi2-x)\qquad \cos(x)=\sin(\frac\pi 2-x)
\end{gather*}
\end{pp}

\begin{proof}
Immediate from \eqref{eq138} and \eqref{eq137}.
\end{proof}
Thus, the graph of $\sin$ is the rightward translation of that of $\cos$ by $\frac\pi 2$. Therefore, the graph of $\sin$ on $[-\frac\pi2,0]$ is translated to the graph of $\cos$ on $[-\pi,-\frac\pi2]$. That $\cos(x)=\cos(-x)$ gives us the graph of $\cos$ on $[\frac\pi2,\pi]$. Thus, we know the graph of $\cos$ on $[-\pi,\pi]$. 



\begin{thm}[\textbf{Euler's formula}] \index{00@Euler's formula}
We have $e^{\im\pi}=-1$, and hence $e^{2\im\pi}=e^{\im\pi}e^{\im\pi}=1$.
\end{thm}


\begin{proof}
We have $e^{\im\pi/2}=\cos(\frac\pi2)+\im\sin(\frac\pi2)=\im$. Hence $e^{\im\pi}=\im^2=-1$.
\end{proof}

\begin{pp}
We have $\sin x=\sin(x+2\pi)$ and $\cos x=\cos(x+2\pi)$
\end{pp}


\begin{proof}
Immediate from \eqref{eq138} and that $1=e^{2\im\pi}=\cos(2\pi)+\im\sin(2\pi)$.
\end{proof}




Thus, $\cos$ and $\sin$ have period $2\pi$. This completes the graphs of $\cos x$ and $\sin x=\cos(x-\frac\pi2)$ on $\Rbb$.



\subsection{Taylor's theorems}


Assume throughout this section that $-\infty<a<b<+\infty$ and $V$ is a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.


In this section, we generalize MVTs and finite-increment theorem to higher derivatives. These generalizations are all under the name ``Taylor theorem". Recall Def. \ref{lb354} and Pb. \ref{lb355} for the meaning of $l^{n,\infty},C^n,C^\infty$. We first discuss the generalization of finite-increment theorem, which can be applied to vector-valued functions.





 
\subsubsection{Taylor's theorems for vector-valued functions}



\begin{df}
Let $X$ be a normed vector space. Let $A\subset X$. Let $a\in\Cl_X(A)\setminus A$ (or more generally, assume $a\in\Cl_X(A\setminus \{a\})$). Let $f:A\rightarrow V$. Let $r\in\Rbb_{\geq0}$.
\begin{itemize}
\item We write $\dps f(x)=o(\lVert x-a\lVert^r)$ if $\dps\lim_{x\rightarrow a}\frac{f(x)}{\lVert x-a\lVert^r}=0$.
\item We write $\dps f(x)=O(\lVert x-a\lVert^r)$ if $\dps\limsup_{x\rightarrow a}\frac{\lVert f(x)\lVert}{\lVert x-a\lVert^r}<+\infty$ where $\limsup$ is the limit superior of a net (cf. Rem. \ref{lb269} and Pb. \ref{lb346}). In other words, there exists $U\in\Nbh_X(a)$ such that $\sup_{x\in A\cap U}\lVert f(x)\lVert/\lVert x-a\lVert^r<+\infty$.
\end{itemize}
When $r=0$, we simply write $o(1)$ and $O(1)$. The two symbols $o,O$ are called \textbf{Landau symbols}. \index{00@Landau symbols $o,O$}
\end{df}


In this section, we will frequently use the formula
\begin{align}
S_n(x)=\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k
\end{align}
whenever the RHS makes sense.


\begin{thm}[\textbf{Taylor's theorem, Peano form}]\index{00@Taylor's theorem, Peano form}\label{lb360}
Let $f:[a,b]\rightarrow V$ and $n\in\Zbb_+$. Assume that $f',f'',\dots,f^{(n)}$ exist at $a$ (resp. at $b$). Then for each $x\in (a,b)$ we have
\begin{subequations}
\begin{gather}
f(x)=\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k+o((x-a)^n)\label{eq142}\\
\text{resp.}\nonumber\\
f(x)=\sum_{k=0}^n\frac{f^{(k)}(b)}{k!}(x-b)^k+o((x-b)^n) \label{eq143}
\end{gather}
\end{subequations}
\end{thm}


Expressions of the form \eqref{eq142} are called \textbf{Taylor expansions} \index{00@Taylor expansion} of $f$ with \textbf{center} $a$.

\begin{rem}
In order for $f^{(n)}(a)$ to exist, it is assumed that $f^{(n-1)}$ exists on a neighborhood of $a$.
\end{rem}


\begin{proof}[Proof of Peano-form]
Since the two cases are similar, we only treat the case at $a$. Define \textbf{remainder} \index{00@Remainder of Taylor expansion} $g(x)=f(x)-S_n(x)$. Then
\begin{align}
g(a)=g'(a)=\cdots=g^{(n)}(a)=0  \label{eq140}
\end{align}
It suffices to prove Taylor's theorem for $g$, i.e.
\begin{align}
g(x)=o((x-a)^n)
\end{align}
We prove this by induction. The case $n=1$ is obvious from the definition of derivatives, which says
\begin{align}
\frac{g(x)-g(a)}{x-a}-g'(a)=o(1)
\end{align}
and hence $g(x)=(x-a)o(1)=o(x-a)$. 

Now assume $n\geq 2$. Assume that Peano form has been proved for case $n-1$. Applying this result to $g'$. We then get $g'(x)=o((x-a)^{n-1})$. Since $g''(a)$ exists,  $g'$ exists on $[a,c]$ where $a<c<b$. Therefore, by finite-increment Thm. \ref{lb333}, if $x\in(a,c)$, then
\begin{align*}
\lVert g(x)\lVert\leq (x-a)\cdot\sup_{a<t<x}\lVert g'(t)\lVert
\end{align*}
If we can prove $\sup_{a\leq t\leq x}\lVert g'(t)\lVert=o((x-a)^{n-1})$, then we immediately have $g(x)=o((x-a)^n)$. Thus, the proof of Peano form is finished by the next lemma.
\end{proof}

\begin{lm}
Assume that $f:[a,b]\rightarrow V$ satisfies $f(x)=o((x-a)^r)$. Define
\begin{align*}
\wtd f(x)=\sup_{a<t<x}\lVert f(t)\lVert
\end{align*}
Then $\wtd f(x)=o((x-a)^r)$.
\end{lm}


\begin{proof}
Choose any $\eps>0$. Since $f(x)=o((x-a)^r)$, we know that there is $c\in (a,b)$ such that for all $a<x<c$ we have $\lVert f(x)\lVert\leq \eps |x-a|^r$. Thus
\begin{align*}
|\wtd f(x)|\leq\sup_{a<t<x}\eps |t-a|^r =\eps|x-a|^r
\end{align*}
\end{proof}


Among all the versions of Taylor's theorem discussed in this section, the following one is most useful. Note that if we assume $f\in C^{(n+1)}([a,b],V)$ in Thm. \ref{lb360}, then Thm. \ref{lb359} immediately implies Thm. \ref{lb360}.  



\begin{thm}[\textbf{Higher order finite-increment theorem}]\index{00@Higher order finite-increment theorem}\label{lb359}
Let $n\in\Nbb$ and $f\in C^n([a,b],V)$. Assume that $f^{(n+1)}$ exists everywhere on $(a,b)$. Then, for every $x\in[a,b]$, we have
\begin{subequations}
\begin{gather}
\Big\lVert f(x)-\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k \Big\lVert \leq \frac{(x-a)^{n+1}}{(n+1)!}\cdot \sup_{a<t<x}\lVert f^{(n+1)}(t)\lVert  \label{eq139}\\
\Big\lVert f(x)-\sum_{k=0}^n\frac{f^{(k)}(b)}{k!}(x-b)^k \Big\lVert \leq \frac{(b-x)^{n+1}}{(n+1)!}\cdot \sup_{a<t<x}\lVert f^{(n+1)}(t)\lVert
\end{gather}
\end{subequations} 
\end{thm}





\begin{proof}
We only prove the first formula: applying the first formula to $\wtd f(x)=f(-x)$ implies the second one. We prove \eqref{eq139} by induction on $n$. Moreover, we shall prove \eqref{eq139} for the case $x=b$. The general case follows by restricting $f$ to $[a,x]$.  When $n=0$, \eqref{eq139} is the content of (classical) finite-increment Thm. \ref{lb333}. Assume case $n-1$ has been proved ($n\in\Nbb$). In case $n$, take $g(x)=f(x)-S_n(x)$. Then \eqref{eq140} is true. Let
\begin{align*}
M=\sup_{a<t<b}\lVert f^{(n+1)}(t)\lVert =\sup_{a<t<b}\lVert g^{(n+1)}(t)\lVert
\end{align*}
By case $n-1$, for each $a\leq x\leq b$ we have
\begin{align*}
\lVert g'(x)\lVert \leq\frac{(x-a)^n}{n!}\cdot M=h'(x)\qquad\text{where }~h(x)=\frac{M(x-a)^{n+1}}{(n+1)!} 
\end{align*}
Thus, by Thm. \ref{lb329}, we have
\begin{align*}
\lVert g(b)-g(a)\lVert\leq h(b)-h(a)=\frac{M(b-a)^{n+1}}{(n+1)!} 
\end{align*}
This proves \eqref{eq139} for $g$, and hence for $f$.
\end{proof}



\subsubsection{Taylor's theorem for real-valued functions}



\begin{thm}[\textbf{Taylor's theorem, Lagrange form}]\index{00@Taylor's theorem, Lagrange form}
Let $n\in\Nbb$ and $f\in C^n([a,b],\Rbb)$. Assume that $f^{(n+1)}$ exists everywhere on $(a,b)$. Then for every $x\in(a,b)$, there exist $\xi\in(a,x)$ and $\eta\in (x,b)$ such that
\begin{subequations}
\begin{gather}
f(x)=\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k+\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}\label{eq141}\\
f(x)=\sum_{k=0}^n\frac{f^{(k)}(b)}{k!}(x-b)^k+\frac{f^{(n+1)}(\eta)}{(n+1)!}(x-b)^{n+1}
\end{gather}
\end{subequations}
\end{thm}



\begin{proof}
We only prove \eqref{eq141}. Applying \eqref{eq141} to $\wtd f(x)=f(-x)$ (defined on $[-b,-a]$) implies the second formula. Again, it suffices to prove \eqref{eq141} for $g(x)=f(x)-S_n(x)$, which satisfies \eqref{eq140}. Thus, we want to find $xi\in(a,b)$ satisfying
\begin{align*}
g(x)=\frac{g^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}
\end{align*}
This can be proved by applying Cauchy's MVT repeatedly:
\begin{align*}
&\frac{g(x)}{(x-a)^{n+1}}\xlongequal{{\color{ForestGreen}\exists x_1\in (a,x)}}\frac{g'(x_1)}{(n+1)(x_1-a)^n}\xlongequal{{\color{ForestGreen}\exists x_2\in (a,x_1)}}\frac{g''(x_2)}{(n+1)n(x_2-a)^{n-1}}\\
=&\cdots \xlongequal{{\color{ForestGreen}\exists x_n\in (a,x_{n-1})}}\frac{g^{(n)}(x_n)}{(n+1)!(x_n-a)}\xlongequal{{\color{ForestGreen}\exists \xi\in (a,x_n)}}\frac{g^{(n+1)}(\xi)}{(n+1)!}
\end{align*}
\end{proof}




\subsection{Functions approximated by their Taylor series}


Taylor's theorems do not imply that a smooth function $f$ on an interval of $\Rbb$ can be approximated by its \textbf{Taylor series} \index{00@Taylor series} (with \textbf{center} $a$):
\begin{align}
\sum_{k=0}^\infty \frac{f^{(k)}(a)}{k!}(x-a)^k
\end{align}
The following is a typical example:

\begin{eg}\label{lb358}
Define $f:\Rbb\rightarrow\Rbb$ by
\begin{align*}
f(x)=\left\{
\begin{array}{ll}
\dps \exp\big(-\frac 1{x^2}\big)& (\text{if }x>0)\\[1ex]
0&(\text{if }x\leq 0)
\end{array}
\right.
\end{align*}
Then $f^{(n)}(0)=0$ for all $n\in\Nbb$. So the Taylor series of $f$ at $0$ is $0$, which cannot approximate $f(x)$ when $x>0$.
\end{eg}


On the contrary, if $f$ is defined on an open subset of $\Cbb$, and if $f'$ exists everywhere on its domain, then $f^{(n)}$ exists for all $n$, and $f$ can be approximated locally uniformly by its Taylor series. This is a deep result in complex analysis. In fact, a thorough understanding of power series is impossible without the help of complex analysis techniques. 

In the following, we will use Taylor's theorems (especially Thm. \ref{lb359}) to prove that some important real-valued functions can be approximated by their Taylor series. Actually, our proof can be simplified by complex analysis, since these examples are the restriction of some differentiable complex variable functions (aka \textbf{holomorphic functions}) to the real line. Since we haven't leaned complex analysis, we will stick to elementary methods. 


\begin{eg}
Consider the Taylor expansion of $\log:\Rbb_{>0}\rightarrow\Rbb$ at $x=1$. By induction on $n\in\Zbb_+$, one computes that
\begin{align*}
\log^{(n)}(x)=(-1)^{n-1}\frac{(n-1)!}{x^n}
\end{align*}
Therefore, its Taylor expansion in order $n$ is
\begin{align*}
\sum_{k=1}^n \frac{(-1)^{k-1}}{k} (x-1)^k+R_{n+1}(x)
\end{align*}
where $R_{n+1}(x)$ is the remainder. To show that $\log x$ is approximated uniformly (on certain domain) by its Taylor series, one need to show that $R_{n+1}$ converges uniformly to $0$ on that domain. 

We would like to prove that for every $0<r<1$ we have $R_{n+1}\rightrightarrows 0$ on $[1-r,1+r]$. This would imply that series on the RHS of the following formula (whose radius of convergence is $1$) converges uniformly to $f$ on $[1-r,1+r]$:
\begin{align}
\log(x)=\sum_{k=1}^n \frac{(-1)^{k-1}}{k} (x-1)^k
\end{align}
However, using the Taylor's theorems proved in the previous section, one can prove the uniform convergence only if $0<r\leq1/2$. The general case of $0<r<1$ should be proved by another method.   \hfill\qedsymbol
\end{eg}


\begin{proof}[\textbf{Proof for the case $0<r\leq \frac 12$}]
By Thm. \ref{lb359}, for all $x\in[1-r,1+r]$ we have
\begin{align*}
|R_{n+1}(x)|\leq \frac{|x-1|^{n+1}}{(n+1)!}\cdot\sup_{
\begin{subarray}{c}
1\leq t\leq x\\
\text{or }x\leq t\leq 1
\end{subarray}
}
\big|\log^{(n+1)}(t)\big|=\frac{|x-1|^{n+1}}{n+1}\cdot\sup_{
\begin{subarray}{c}
1\leq t\leq x\\
\text{or }x\leq t\leq 1
\end{subarray}
}\frac 1{t^{n+1}}
\end{align*}
where the $\sup$ is over $1\leq t\leq x$ or $x\leq t\leq 1$, depending on whether $1\leq x\leq 1+r$ or $1-r\leq x\leq 1$.

If $1\leq x\leq 1+r$, then $1\leq t\leq x$ implies $1/t^{n+1}\leq 1$. So
\begin{align*}
|R_{n+1}(x)|\leq\frac{(x-1)^{n+1}}{n+1}\leq\frac{r^{n+1}}{n+1} 
\end{align*}
where the RHS converges to $0$ as $n\rightarrow\infty$ whenever $0<r\leq 1$. 

If $1-r\leq x\leq 1$, then $x\leq t\leq 1$ implies $1/t^{n+1}\leq 1/x^{n+1}$. Thus
\begin{align*}
|R_{n+1}(x)|\leq\frac 1{n+1}\Big(\frac 1x-1 \Big)^{n+1}\leq \frac 1{n+1}\Big(\frac 1{1-r}-1 \Big)^{n+1}=\frac 1{n+1}\Big(\frac r{1-r}\Big)^{n+1}
\end{align*}
If $0<r\leq 1/2$, then $0<r/(1-r)\leq 1$, and hence the RHS above converges to $0$ as $n\rightarrow\infty$. This proves that $R_{n+1}\rightrightarrows0$ on $[1-r,1+r]$ when $0<r\leq1/2$.
\end{proof}



\begin{proof}[\textbf{Proof for the case $0<r<1$}]
Assume $0<r<1$. For the convenience of discussion, we prove instead that 
\begin{align}
\sum_{k=1}^\infty \frac{(-1)^{k-1}}{k} x^k  \label{eq144}
\end{align}
the Taylor series of $\log(1+x)$ at $0$, converges uniformly to $\log(1+x)$ on $[-r,r]$

The radius of convergence of the series \eqref{eq144} is $1$. Thus, by Thm. \ref{lb112}, \eqref{eq144} converges locally uniformly on $(-1,1)$ to some function $f:(-1,1)\rightarrow\Rbb$. By Cor. \ref{lb338}, $f$ is differentiable on $(-1,1)$, and the series
\begin{align}
\sum_{k=0}^\infty (-1)^kx^k   \label{eq145}
\end{align}
converges locally uniformly on $(-1,1)$ to $f'$. But we clearly have
\begin{align}
\eqref{eq145}=\frac{1}{1-(-x)}=\frac 1{1+x}
\end{align}
Thus $f'(x)=\frac d{dz}(\log(1+x))$. Now, \eqref{eq144} clearly implies that $f(0)=0=\log(1+0)$. Therefore, by Cor. \ref{lb326}, we obtain $f(x)=\log(1+x)$ on $(-1,1)$. In other words, \eqref{eq144} converges locally uniformly on $(-1,1)$ (and hence uniformly on $[-r,r]$) to $\log(1+x)$.
\end{proof}



\begin{eg}
Let $\alpha\in\Cbb$. Then
\begin{align}
\sum_{k=0}^\infty{\alpha\choose k}x^k   \label{eq146}
\end{align}
the Taylor series of $(1+x)^\alpha=\exp(\alpha\log(1+x))$ at $x=0$, converges locally uniformly on $(-1,1)$ to $(1+x)^\alpha$.
\end{eg}


\begin{proof}
Using Prop. \ref{lb109}, one easily checks that the radius of convergence of \eqref{eq146} is $1$. So Thm. \ref{lb112} implies that \eqref{eq146} converges locally on $(-1,1)$ to some $f:(-1,1)\rightarrow\Cbb$. Let
\begin{align*}
g(x)=(1+x)^\alpha
\end{align*}
Our goal is to prove $f=g$. 

It is clear that $(1+x)g'(x)=\alpha g(x)$. By Cor. \ref{lb338},  the series
\begin{align}
\sum_{k=0}^\infty k{\alpha\choose k}x^{k-1}=\sum_{k=0}^\infty (k+1){\alpha\choose k+1}x^k
\end{align}
converges locally uniformly on $(-1,1)$ to $f'$. Thus, by Cor. \ref{lb362}, we have
\begin{align*}
&(1+x)f'(x)=\sum_{k=0}^\infty \bigg( k{\alpha\choose k}+(k+1){\alpha\choose k+1}\bigg)x^k\\
=&\sum_{k=0}^\infty \alpha{\alpha\choose k}x^k=\alpha f(x)
\end{align*}
Therefore, $f$ satisfies the same differential equation as $g$, namely $(1+x)f'=\alpha f$. Moreover, we clearly have $f(0)=1=g(0)$. So we have $f=g$ on $(-1,1)$ if we apply the next lemma to $f-g$.
\end{proof}


\begin{lm}
Let $I$ be an interval of $\Rbb$ (with at least two points) such that $0\in I$. Let $\Fbb\in\{\Rbb,\Cbb\}$. Let $\varphi\in C(I,\Fbb^{n\times n})$. Assume that $f\in C^1(I,\Fbb^n)$ satisfies
\begin{align}
f'(x)=\varphi(x)\cdot f(x)\qquad f(0)=0
\end{align} 
Then for all $x\in I$ we have $f(x)=0$.
\end{lm}

\begin{proof}
We prove this lemma for the case that $I$ is an open interval. The proof for the other types of intervals is similar. Also, we equip $\Fbb^{n\times n}$ with the operator norm by viewing $n\times n$ matrices as elements of $\fk L(\Fbb^n)$. (Recall that the operator norm is equivalent to the Eucliden norm, see Thm. \ref{lb363}.) Let
\begin{align*}
\Omega=\{x\in I:f(x)=0\}=f^{-1}(0)
\end{align*}
which is a closed subset of $I$ (since the inverse image under a continuous map of a closed set is closed). Clearly $\Omega$ is nonempty since $0\in\Omega$. If we can prove that $\Omega$ is open, then we have $\Omega=I$ because $I$ is connected. This will finish the proof.

So let us prove that any $p\in\Omega$ is an interior point of $\Omega$ with respect to $I$. Choose $r>0$ such that $[p-r,p+r]\subset I$. Let
\begin{align*}
C=\sup_{-r\leq x\leq r}\lVert\varphi(x)\lVert
\end{align*}
which is a finite number by extreme value theorem. Let $\delta=\min\{\frac 1{2C},r\}$ which is $>0$. It suffices to prove that
\begin{align*}
M=\sup_{x\in[p-\delta,p+\delta]}\lVert f(x)\lVert
\end{align*}
is zero. (Note that $M<+\infty$ again by EVT.) Then we will have $[p-\delta,p+\delta]\subset\Omega$, finishing the proof. 

For each $x\in[p-\delta,p+\delta]$, by the inequality \eqref{eq118} for operator norm, we have
\begin{align*}
\lVert f'(x)\lVert=\lVert\varphi(x)f(x)\lVert\leq\lVert\varphi(x)\lVert\cdot \lVert f(x)\lVert\leq CM
\end{align*}
Thus, by finite-increment theorem, we have for all $x\in[p-\delta,p+\delta]$ that
\begin{align*}
\lVert f(x)\lVert=\lVert f(x)-f(p)\lVert\leq CM|x-p|\leq CM\cdot\frac{\delta}2\leq \frac M2
\end{align*}
Applying $\sup_{x\in[p-\delta,p+\delta]}$ to $\lVert f(x)\lVert$, we get $M\leq M/2$. So $M=0$.
\end{proof}



























\subsection{Problems and supplementary material}


Let $-\infty<a<b<+\infty$. Let $V$ be a Banach space over $\Fbb\in\{\Rbb,\Cbb\}$.


\begin{prob}
Let $f$ be the function in Exp. \ref{lb358}. Prove that $f^{(n)}(0)=0$ for all $n\in\Nbb$.
\end{prob}




\begin{prob}\label{lb361}
Let $n\in\Nbb$. Let $f:[a,b]\rightarrow V$ such that $f,f',\dots,f^{(n)}$ exist everywhere on $[a,b]$. Use the higher order finite-increment theorem to prove that
\begin{align*}
\Big\lVert f(x)-\sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k\Big\lVert\leq \frac{(x-a)^{n}}{n!}\cdot \sup_{a<t<x}\lVert f^{(n)}(t)-f^{(n)}(a)\lVert 
\end{align*}
\end{prob}



\begin{prob}
Use Thm. \ref{lb336} to prove the following theorem. (Pb. \ref{lb361} with $n=1$ might be helpful.)
\end{prob}



\begin{thm}
Let $I=[a,b]$ and $J=[c,d]$ be intervals in $\Rbb$ with at least two points. Let $f:I\times J\rightarrow V$ be a function such that $\partial_1f,\partial_2f,\partial_2\partial_1f$ exist on $I\times J$, and that $\partial_2\partial_1f$ is continuous. Then $\partial_1\partial_2f$ exists on $I\times J$ and equals $\partial_2\partial_1f$.
\end{thm}






\begin{prob}\label{lb357}
Let $n\in\Nbb$. Find the radius of convergence of
\begin{align}
f(z)=\sum_{k=n}^\infty {k\choose n}z^n
\end{align}
Find the explicit formula of $f(z)$.
\end{prob}



\begin{prob}
Prove the following higher order finite-increment theorem for complex variables. (For simplicity, it suffices prove the case that $z_0=0$.)
\end{prob}

\begin{thm}[\textbf{Higher order finite-increment theorem}]\index{00@Higher order finite-increment theorem}  \label{lb356}
Let $R>0$ and $z_0\in\Cbb$. Let $f:\Omega\rightarrow V$ where
\begin{align*}
\Omega=B_\Cbb(z_0,R)=\{z\in\Cbb:|z-z_0|<R\}
\end{align*}
Assume that $f',f'',\dots,f^{(n+1)}$ exist everywhere on $\Omega$. Then for every $z\in\Omega$ we have
\begin{align}
\Big\lVert f(z)-\sum_{k=0}^n\frac{f^{(k)}(z_0)}{k!}(z-z_0)^k \Big\lVert \leq\frac{|z-z_0|^{n+1}}{(n+1)!}\cdot \sup_{\xi\in[z_0,z]} \lVert f^{(n+1)}(\xi)\lVert
\end{align}
where $[z_0,z]=\{tz+(1-t)z_0:0\leq t\leq 1\}$.
\end{thm}



\begin{prob}
Let $V$ be a Banach space over $\Cbb$. Assume that the power series $f(z)=\sum_{k=0}^\infty v_kz^k$ (where $v_k\in V$) has radius of convergence $R>0$. Choose any $z_0\in B_\Cbb(0,R)$. Prove that there exists a neighborhood $\Omega\in\Nbh_\Cbb(z_0)$ contained inside $B_\Cbb(0,R)$, such that the Taylor series of $f$ at $z_0$ converges uniformly on $\Omega$ to $f$. Namely, prove that the series $g(z)$ converges uniformly to $f$ on $\Omega$ where
\begin{align}
g(z)=\sum_{k=0}^\infty\frac{f^{(k)}(z_0)}{k!}(z-z_0)^k
\end{align}
\end{prob}

\begin{proof}[Hint]
Use Thm. \ref{lb356}.  Pb.  \ref{lb357} might also be helpful.
\end{proof}






























































































\newpage

\printindex	






	\begin{thebibliography}{999999}
		\footnotesize	

\bibitem[Axl]{Axl}
Axler, S. (2015). Linear algebra done right. 3rd ed.

\bibitem[Fol]{Fol}
Folland, G. B. (1999). Real analysis: modern techniques and their applications (Vol. 40). John Wiley \& Sons.


\bibitem[Hau14]{Hau14}
Hausdorff, F. (1914) Grundz\"uge der Mengenlehre

\bibitem[Jah]{Jah}
Jahnke, H. N. (2003). A history of analysis (No. 24). American Mathematical Soc..


\bibitem[Kel]{Kel}
Kelley, J. L., General topology. 

%\bibitem[Kli]{Kli}
%Kline, M. (1990). Mathematical Thought from Ancient to Modern Times.

\bibitem[MS22]{MS22}
Moore, E. H., \& Smith, H. L. (1922). A general theory of limits. American journal of Mathematics, 44(2), 102-121.

\bibitem[Mun]{Mun}
Munkres, J. (2000). Topology. Second Edition.


\bibitem[RS]{RS}
Reed, M., \& Simon, B. (1972). Methods of Modern Mathematical Physics I: Functional analysis, Academic Press, New York.


\bibitem[Rud-P]{Rud-P}
Rudin, W. (1976). Principles of Mathematical Analysis. 3rd ed.

\bibitem[Rud-R]{Rud-R}
Rudin, W. (1987). Real and complex analysis. 3rd ed. 


\bibitem[Wil]{Wil}
Willard, S. (1970). General topology. 

		
\end{thebibliography}

\noindent {\small \sc Yau Mathematical Sciences Center, Tsinghua University, Beijing, China.}

\noindent {\textit{E-mail}}: binguimath@gmail.com\qquad bingui@tsinghua.edu.cn
\end{document}









